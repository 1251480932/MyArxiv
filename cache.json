{"2023-03-29T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.16894v1","updated":"2023-03-29T17:59:10Z","published":"2023-03-29T17:59:10Z","title":"ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with\n  GPT and Prototype Guidance","summary":"  Understanding 3D scenes from multi-view inputs has been proven to alleviate\nthe view discrepancy issue in 3D visual grounding. However, existing methods\nnormally neglect the view cues embedded in the text modality and fail to weigh\nthe relative importance of different views. In this paper, we propose\nViewRefer, a multi-view framework for 3D visual grounding exploring how to\ngrasp the view knowledge from both text and 3D modalities. For the text branch,\nViewRefer leverages the diverse linguistic knowledge of large-scale language\nmodels, e.g., GPT, to expand a single grounding text to multiple\ngeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer\nfusion module with inter-view attention is introduced to boost the interaction\nof objects across views. On top of that, we further present a set of learnable\nmulti-view prototypes, which memorize scene-agnostic knowledge for different\nviews, and enhance the framework from two perspectives: a view-guided attention\nmodule for more robust text features, and a view-guided scoring strategy during\nthe final prediction. With our designed paradigm, ViewRefer achieves superior\nperformance on three benchmarks and surpasses the second-best by +2.8%, +1.2%,\nand +0.73% on Sr3D, Nr3D, and ScanRefer. Code will be released at\nhttps://github.com/ZiyuGuo99/ViewRefer3D.\n","authors":["Ziyu Guo","Yiwen Tang","Renrui Zhang","Dong Wang","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2303.16894v1.pdf","comment":"Code will be released at https://github.com/ZiyuGuo99/ViewRefer3D"},{"id":"http://arxiv.org/abs/2303.16886v1","updated":"2023-03-29T17:55:50Z","published":"2023-03-29T17:55:50Z","title":"End-to-End $n$-ary Relation Extraction for Combination Drug Therapies","summary":"  Combination drug therapies are treatment regimens that involve two or more\ndrugs, administered more commonly for patients with cancer, HIV, malaria, or\ntuberculosis. Currently there are over 350K articles in PubMed that use the\n\"combination drug therapy\" MeSH heading with at least 10K articles published\nper year over the past two decades. Extracting combination therapies from\nscientific literature inherently constitutes an $n$-ary relation extraction\nproblem. Unlike in the general $n$-ary setting where $n$ is fixed (e.g.,\ndrug-gene-mutation relations where $n=3$), extracting combination therapies is\na special setting where $n \\geq 2$ is dynamic, depending on each instance.\nRecently, Tiktinsky et al. (NAACL 2022) introduced a first of its kind dataset,\nCombDrugExt, for extracting such therapies from literature. Here, we use a\nsequence-to-sequence style end-to-end extraction method to achieve an F1-Score\nof $66.7\\%$ on the CombDrugExt test set for positive (or effective)\ncombinations. This is an absolute $\\approx 5\\%$ F1-score improvement even over\nthe prior best relation classification score with spotted drug entities (hence,\nnot end-to-end). Thus our effort introduces a state-of-the-art first model for\nend-to-end extraction that is already superior to the best prior non end-to-end\nmodel for this task. Our model seamlessly extracts all drug entities and\nrelations in a single pass and is highly suitable for dynamic $n$-ary\nextraction scenarios.\n","authors":["Yuhang Jiang","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2303.16886v1.pdf","comment":"Accepted to appear in IEEE ICHI 2023. Code:\n  https://github.com/bionlproc/end-to-end-CombDrugExt"},{"id":"http://arxiv.org/abs/2303.16857v1","updated":"2023-03-29T17:07:26Z","published":"2023-03-29T17:07:26Z","title":"Did You Mean...? Confidence-based Trade-offs in Semantic Parsing","summary":"  We illustrate how a calibrated model can help balance common trade-offs in\ntask-oriented parsing. In a simulated annotator-in-the-loop experiment, we show\nthat well-calibrated confidence scores allow us to balance cost with annotator\nload, improving accuracy with a small number of interactions. We then examine\nhow confidence scores can help optimize the trade-off between usability and\nsafety. We show that confidence-based thresholding can substantially reduce the\nnumber of incorrect low-confidence programs executed; however, this comes at a\ncost to usability. We propose the DidYouMean system which better balances\nusability and safety.\n","authors":["Elias Stengel-Eskin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2303.16857v1.pdf","comment":"9 pages. arXiv admin note: substantial text overlap with\n  arXiv:2211.07443"},{"id":"http://arxiv.org/abs/2303.16854v1","updated":"2023-03-29T17:03:21Z","published":"2023-03-29T17:03:21Z","title":"AnnoLLM: Making Large Language Models to Be Better Crowdsourced\n  Annotators","summary":"  Many natural language processing (NLP) tasks rely on labeled data to train\nmachine learning models to achieve high performance. However, data annotation\ncan be a time-consuming and expensive process, especially when the task\ninvolves a large amount of data or requires specialized domains. Recently,\nGPT-3.5 series models have demonstrated remarkable few-shot and zero-shot\nability across various NLP tasks. In this paper, we first claim that large\nlanguage models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced\nannotator by providing them with sufficient guidance and demonstrated examples.\nTo make LLMs to be better annotators, we propose a two-step approach,\n'explain-then-annotate'. To be more precise, we begin by creating prompts for\nevery demonstrated example, which we subsequently utilize to prompt a LLM to\nprovide an explanation for why the specific ground truth answer/label was\nchosen for that particular example. Following this, we construct the few-shot\nchain-of-thought prompt with the self-generated explanation and employ it to\nannotate the unlabeled data. We conduct experiments on three tasks, including\nuser input and keyword relevance assessment, BoolQ and WiC. The annotation\nresults from GPT-3.5 surpasses those from crowdsourced annotation for user\ninput and keyword relevance assessment. Additionally, for the other two tasks,\nGPT-3.5 achieves results that are comparable to those obtained through\ncrowdsourced annotation.\n","authors":["Xingwei He","Zhenghao Lin","Yeyun Gong","A-Long Jin","Hang Zhang","Chen Lin","Jian Jiao","Siu Ming Yiu","Nan Duan","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.16854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04089v2","updated":"2023-03-29T16:52:08Z","published":"2022-12-08T05:50:53Z","title":"Editing Models with Task Arithmetic","summary":"  Changing how pre-trained models behave -- e.g., improving their performance\non a downstream task or mitigating biases learned during pre-training -- is a\ncommon practice when developing machine learning systems. In this work, we\npropose a new paradigm for steering the behavior of neural networks, centered\naround \\textit{task vectors}. A task vector specifies a direction in the weight\nspace of a pre-trained model, such that movement in that direction improves\nperformance on the task. We build task vectors by subtracting the weights of a\npre-trained model from the weights of the same model after fine-tuning on a\ntask. We show that these task vectors can be modified and combined together\nthrough arithmetic operations such as negation and addition, and the behavior\nof the resulting model is steered accordingly. Negating a task vector decreases\nperformance on the target task, with little change in model behavior on control\ntasks. Moreover, adding task vectors together can improve performance on\nmultiple tasks at once. Finally, when tasks are linked by an analogy\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\nthree of the tasks can improve performance on the fourth, even when no data\nfrom the fourth task is used for training. Overall, our experiments with\nseveral models, modalities and tasks show that task arithmetic is a simple,\nefficient and effective way of editing models.\n","authors":["Gabriel Ilharco","Marco Tulio Ribeiro","Mitchell Wortsman","Suchin Gururangan","Ludwig Schmidt","Hannaneh Hajishirzi","Ali Farhadi"],"pdf_url":"https://arxiv.org/pdf/2212.04089v2.pdf","comment":"In Proceedings of the 11th International Conference on Learning\n  Representations (ICLR 2023)"},{"id":"http://arxiv.org/abs/2303.16839v1","updated":"2023-03-29T16:42:30Z","published":"2023-03-29T16:42:30Z","title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks","summary":"  The development of language models have moved from encoder-decoder to\ndecoder-only designs. In addition, the common knowledge has it that the two\nmost popular multimodal tasks, the generative and contrastive tasks, tend to\nconflict with one another, are hard to accommodate in one architecture, and\nfurther need complex adaptations for downstream tasks. We propose a novel\nparadigm of training with a decoder-only model for multimodal tasks, which is\nsurprisingly effective in jointly learning of these disparate vision-language\ntasks. This is done with a simple model, called MaMMUT. It consists of a single\nvision encoder and a text decoder, and is able to accommodate contrastive and\ngenerative learning by a novel two-pass approach on the text decoder. We\ndemonstrate that joint training of these diverse-objective tasks is simple,\neffective, and maximizes the weight-sharing of the model. Furthermore, the same\narchitecture enables straightforward extensions to open-vocabulary object\ndetection and video-language tasks. The model tackles a diverse range of tasks,\nwhile being modest in capacity. Our model achieves the SOTA on image-text and\ntext-image retrieval, video question answering and open-vocabulary detection\ntasks, outperforming much larger and more extensively trained foundational\nmodels. It shows competitive results on VQA and Video Captioning, especially\nconsidering its size. Ablations confirm the flexibility and advantages of our\napproach.\n","authors":["Weicheng Kuo","AJ Piergiovanni","Dahun Kim","Xiyang Luo","Ben Caine","Wei Li","Abhijit Ogale","Luowei Zhou","Andrew Dai","Zhifeng Chen","Claire Cui","Anelia Angelova"],"pdf_url":"https://arxiv.org/pdf/2303.16839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16835v1","updated":"2023-03-29T16:28:43Z","published":"2023-03-29T16:28:43Z","title":"Zero-shot Entailment of Leaderboards for Empirical AI Research","summary":"  We present a large-scale empirical investigation of the zero-shot learning\nphenomena in a specific recognizing textual entailment (RTE) task category,\ni.e. the automated mining of leaderboards for Empirical AI Research. The prior\nreported state-of-the-art models for leaderboards extraction formulated as an\nRTE task, in a non-zero-shot setting, are promising with above 90% reported\nperformances. However, a central research question remains unexamined: did the\nmodels actually learn entailment? Thus, for the experiments in this paper, two\nprior reported state-of-the-art models are tested out-of-the-box for their\nability to generalize or their capacity for entailment, given leaderboard\nlabels that were unseen during training. We hypothesize that if the models\nlearned entailment, their zero-shot performances can be expected to be\nmoderately high as well--perhaps, concretely, better than chance. As a result\nof this work, a zero-shot labeled dataset is created via distant labeling\nformulating the leaderboard extraction RTE task.\n","authors":["Salomon Kabongo","Jennifer D'Souza","Sören Auer"],"pdf_url":"https://arxiv.org/pdf/2303.16835v1.pdf","comment":"5 pages, 1 figure. Accepted for publication at JCDL 2023 - Late\n  Breaking Results and Datasets track\n  (https://2023.jcdl.org/calls/papers/#paper_types), official citation\n  forthcoming"},{"id":"http://arxiv.org/abs/2211.07443v4","updated":"2023-03-29T15:59:32Z","published":"2022-11-14T15:17:55Z","title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing","summary":"  Sequence generation models are increasingly being used to translate language\ninto executable programs, i.e. to perform executable semantic parsing. The fact\nthat semantic parsing aims to execute actions in the real world motivates\ndeveloping safe systems, which in turn makes measuring calibration -- a central\ncomponent to safety -- particularly important. We investigate the calibration\nof common generation models across four popular semantic parsing datasets,\nfinding that it varies across models and datasets. We then analyze factors\nassociated with calibration error and release new confidence-based challenge\nsplits of two parsing datasets. To facilitate the inclusion of calibration in\nsemantic parsing evaluations, we release a library for computing calibration\nmetrics.\n","authors":["Elias Stengel-Eskin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2211.07443v4.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2212.10537v2","updated":"2023-03-29T15:34:23Z","published":"2022-12-20T18:46:28Z","title":"Does CLIP Bind Concepts? Probing Compositionality in Large Image Models","summary":"  Large-scale neural network models combining text and images have made\nincredible progress in recent years. However, it remains an open question to\nwhat extent such models encode compositional representations of the concepts\nover which they operate, such as correctly identifying ''red cube'' by\nreasoning over the constituents ''red'' and ''cube''. In this work, we focus on\nthe ability of a large pretrained vision and language model (CLIP) to encode\ncompositional concepts and to bind variables in a structure-sensitive way\n(e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). In\norder to inspect the performance of CLIP, we compare several architectures from\nresearch on compositional distributional semantics models (CDSMs), a line of\nresearch that attempts to implement traditional compositional linguistic\nstructures within embedding spaces. We find that CLIP can compose concepts in a\nsingle-object setting, but in situations where concept binding is needed,\nperformance drops dramatically. At the same time, CDSMs also perform poorly,\nwith best performance at chance level.\n","authors":["Martha Lewis","Nihal V. Nayak","Peilin Yu","Qinan Yu","Jack Merullo","Stephen H. Bach","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2212.10537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16742v1","updated":"2023-03-29T14:49:29Z","published":"2023-03-29T14:49:29Z","title":"Evaluating NLG systems: A brief introduction","summary":"  This year the International Conference on Natural Language Generation (INLG)\nwill feature an award for the paper with the best evaluation. The purpose of\nthis award is to provide an incentive for NLG researchers to pay more attention\nto the way they assess the output of their systems. This essay provides a short\nintroduction to evaluation in NLG, explaining key terms and distinctions.\n","authors":["Emiel van Miltenburg"],"pdf_url":"https://arxiv.org/pdf/2303.16742v1.pdf","comment":"To be published on the INLG2023 conference website"},{"id":"http://arxiv.org/abs/2303.16726v1","updated":"2023-03-29T14:25:30Z","published":"2023-03-29T14:25:30Z","title":"Text revision in Scientific Writing Assistance: An Overview","summary":"  Writing a scientific article is a challenging task as it is a highly codified\ngenre. Good writing skills are essential to properly convey ideas and results\nof research work. Since the majority of scientific articles are currently\nwritten in English, this exercise is all the more difficult for non-native\nEnglish speakers as they additionally have to face language issues. This\narticle aims to provide an overview of text revision in writing assistance in\nthe scientific domain.\n  We will examine the specificities of scientific writing, including the format\nand conventions commonly used in research articles.\n  Additionally, this overview will explore the various types of writing\nassistance tools available for text revision. Despite the evolution of the\ntechnology behind these tools through the years, from rule-based approaches to\ndeep neural-based ones, challenges still exist (tools' accessibility, limited\nconsideration of the context, inexplicit use of discursive information, etc.)\n","authors":["Léane Jourdan","Florian Boudin","Richard Dufour","Nicolas Hernandez"],"pdf_url":"https://arxiv.org/pdf/2303.16726v1.pdf","comment":"Published at 13th International Workshop on Bibliometric-enhanced\n  Information Retrieval 12 pages"},{"id":"http://arxiv.org/abs/2302.12095v4","updated":"2023-03-29T14:21:51Z","published":"2023-02-22T11:01:20Z","title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution\n  Perspective","summary":"  ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance to\nunexpected inputs, is still unclear to the public. Robustness is of particular\nconcern in responsible AI, especially for safety-critical applications. In this\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\nseveral popular foundation models as baselines. Results show that ChatGPT shows\nconsistent advantages on most adversarial and OOD classification and\ntranslation tasks. However, the absolute performance is far from perfection,\nwhich suggests that adversarial and OOD robustness remains a significant threat\nto foundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.\n","authors":["Jindong Wang","Xixu Hu","Wenxin Hou","Hao Chen","Runkai Zheng","Yidong Wang","Linyi Yang","Haojun Huang","Wei Ye","Xiubo Geng","Binxin Jiao","Yue Zhang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2302.12095v4.pdf","comment":"Technical report; code is at:\n  https://github.com/microsoft/robustlearn"},{"id":"http://arxiv.org/abs/2303.16694v1","updated":"2023-03-29T13:46:07Z","published":"2023-03-29T13:46:07Z","title":"Using Semantic Similarity and Text Embedding to Measure the Social Media\n  Echo of Strategic Communications","summary":"  Online discourse covers a wide range of topics and many actors tailor their\ncontent to impact online discussions through carefully crafted messages and\ntargeted campaigns. Yet the scale and diversity of online media content make it\ndifficult to evaluate the impact of a particular message. In this paper, we\npresent a new technique that leverages semantic similarity to quantify the\nchange in the discussion after a particular message has been published. We use\na set of press releases from environmental organisations and tweets from the\nclimate change debate to show that our novel approach reveals a heavy-tailed\ndistribution of response in online discourse to strategic communications.\n","authors":["Tristan J. B. Cann","Ben Dennes","Travis Coan","Saffron O'Neill","Hywel T. P. Williams"],"pdf_url":"https://arxiv.org/pdf/2303.16694v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.13351v3","updated":"2023-03-29T13:37:52Z","published":"2023-03-23T15:29:21Z","title":"DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly\n  Knowledge Graph","summary":"  In this work we create a question answering dataset over the DBLP scholarly\nknowledge graph (KG). DBLP is an on-line reference for bibliographic\ninformation on major computer science publications that indexes over 4.4\nmillion publications published by more than 2.2 million authors. Our dataset\nconsists of 10,000 question answer pairs with the corresponding SPARQL queries\nwhich can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD\nis the largest scholarly question answering dataset.\n","authors":["Debayan Banerjee","Sushil Awale","Ricardo Usbeck","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2303.13351v3.pdf","comment":"12 pages ceur-ws 1 column accepted at International Bibliometric\n  Information Retrieval Workshp @ ECIR 2023"},{"id":"http://arxiv.org/abs/2210.17406v7","updated":"2023-03-29T13:29:51Z","published":"2022-10-31T15:43:57Z","title":"Emergent Linguistic Structures in Neural Networks are Fragile","summary":"  Large Language Models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structure. In this paper, focusing on the\nability of language models to represent syntax, we propose a framework to\nassess the consistency and robustness of linguistic representations. To this\nend, we introduce measures of robustness of neural network models that leverage\nrecent advances in extracting linguistic constructs from LLMs via probing\ntasks, i.e., simple tasks used to extract meaningful information about a single\nfacet of a language model, such as syntax reconstruction and root\nidentification. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures by analysing their\nperformance and robustness with respect to syntax-preserving perturbations. We\nprovide evidence that context-free representation (e.g., GloVe) are in some\ncases competitive with context-dependent representations from modern LLMs\n(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our key\nobservation is that emergent syntactic representations in neural networks are\nbrittle. We make the code, trained models and logs available to the community\nas a contribution to the debate about the capabilities of LLMs.\n","authors":["Emanuele La Malfa","Matthew Wicker","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2210.17406v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.15469v2","updated":"2023-03-29T13:29:35Z","published":"2022-09-30T13:50:25Z","title":"Zero-Shot Retrieval with Search Agents and Hybrid Environments","summary":"  Learning to search is the task of building artificial agents that learn to\nautonomously use a search box to find information. So far, it has been shown\nthat current language models can learn symbolic query reformulation policies,\nin combination with traditional term-based retrieval, but fall short of\noutperforming neural retrievers. We extend the previous learning to search\nsetup to a hybrid environment, which accepts discrete query refinement\noperations, after a first-pass retrieval step via a dual encoder. Experiments\non the BEIR task show that search agents, trained via behavioral cloning,\noutperform the underlying search system based on a combined dual encoder\nretriever and cross encoder reranker. Furthermore, we find that simple\nheuristic Hybrid Retrieval Environments (HRE) can improve baseline performance\nby several nDCG points. The search agent based on HRE (HARE) matches\nstate-of-the-art performance, balanced in both zero-shot and in-domain\nevaluations, via interpretable actions, and at twice the speed.\n","authors":["Michelle Chen Huebscher","Christian Buck","Massimiliano Ciaramita","Sascha Rothe"],"pdf_url":"https://arxiv.org/pdf/2209.15469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16657v1","updated":"2023-03-29T13:05:17Z","published":"2023-03-29T13:05:17Z","title":"Summarizing Indian Languages using Multilingual Transformers based\n  Models","summary":"  With the advent of multilingual models like mBART, mT5, IndicBART etc.,\nsummarization in low resource Indian languages is getting a lot of attention\nnow a days. But still the number of datasets is low in number. In this work, we\n(Team HakunaMatata) study how these multilingual models perform on the datasets\nwhich have Indian languages as source and target text while performing\nsummarization. We experimented with IndicBART and mT5 models to perform the\nexperiments and report the ROUGE-1, ROUGE-2, ROUGE-3 and ROUGE-4 scores as a\nperformance metric.\n","authors":["Dhaval Taunk","Vasudeva Varma"],"pdf_url":"https://arxiv.org/pdf/2303.16657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16634v1","updated":"2023-03-29T12:46:54Z","published":"2023-03-29T12:46:54Z","title":"GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment","summary":"  The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent GPTEval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that GPTEval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts.\n","authors":["Yang Liu","Dan Iter","Yichong Xu","Shuohang Wang","Ruochen Xu","Chenguang Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.16634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16621v1","updated":"2023-03-29T12:22:17Z","published":"2023-03-29T12:22:17Z","title":"AraSpot: Arabic Spoken Command Spotting","summary":"  Spoken keyword spotting (KWS) is the task of identifying a keyword in an\naudio stream and is widely used in smart devices at the edge in order to\nactivate voice assistants and perform hands-free tasks. The task is daunting as\nthere is a need, on the one hand, to achieve high accuracy while at the same\ntime ensuring that such systems continue to run efficiently on low power and\npossibly limited computational capabilities devices. This work presents AraSpot\nfor Arabic keyword spotting trained on 40 Arabic keywords, using different\nonline data augmentation, and introducing ConformerGRU model architecture.\nFinally, we further improve the performance of the model by training a\ntext-to-speech model for synthetic data generation. AraSpot achieved a\nState-of-the-Art SOTA 99.59% result outperforming previous approaches.\n","authors":["Mahmoud Salhab","Haidar Harmanani"],"pdf_url":"https://arxiv.org/pdf/2303.16621v1.pdf","comment":"A preprint"},{"id":"http://arxiv.org/abs/2303.16618v1","updated":"2023-03-29T12:19:23Z","published":"2023-03-29T12:19:23Z","title":"Personalised Language Modelling of Screen Characters Using Rich Metadata\n  Annotations","summary":"  Personalisation of language models for dialogue sensitises them to better\ncapture the speaking patterns of people of specific characteristics, and/or in\nspecific environments. However, rich character annotations are difficult to\ncome by and to successfully leverage. In this work, we release and describe a\nnovel set of manual annotations for 863 speakers from the popular Cornell Movie\nDialog Corpus, including features like characteristic quotes and character\ndescriptions, and a set of six automatically extracted metadata for over 95% of\nthe featured films. We perform extensive experiments on two corpora and show\nthat such annotations can be effectively used to personalise language models,\nreducing perplexity by up to 8.5%. Our method can be applied even zero-shot for\nspeakers for whom no prior training data is available, by relying on\ncombinations of characters' demographic characteristics. Since collecting such\nmetadata is costly, we also contribute a cost-benefit analysis to highlight\nwhich annotations were most cost-effective relative to the reduction in\nperplexity.\n","authors":["Sebastian Vincent","Rowanne Sumner","Alice Dowek","Charlotte Blundell","Emily Preston","Chris Bayliss","Chris Oakley","Carolina Scarton"],"pdf_url":"https://arxiv.org/pdf/2303.16618v1.pdf","comment":"9 pages; 4 figures; 6 tables. Preprint"},{"id":"http://arxiv.org/abs/2212.09561v2","updated":"2023-03-29T11:52:10Z","published":"2022-12-19T15:51:52Z","title":"Large Language Models are reasoners with Self-Verification","summary":"  When a large language model (LLM) performs complex reasoning by chain of\nthought (CoT), it can be highly sensitive to individual mistakes. We have had\nto train verifiers to address this issue. As we all know, after human inferring\na conclusion, they often check it by re-verifying it, which can avoid some\nmistakes. We propose a new method called self-verification that uses the\nconclusion of the CoT as a condition to build a new sample and asks the LLM to\nre-predict the original conditions which be masked. We calculate an explainable\nverification score based on the accuracy. This method can improve the accuracy\nof multiple arithmetics and logical reasoning datasets when using few-shot\nlearning. we have demonstrated that LLMs can conduct explainable\nself-verification of their own conclusions and achieve competitive reasoning\nperformance. Extensive experimentals have demonstrated that our method can help\nmultiple large language models with self-verification can avoid interference\nfrom incorrect CoT. Code is available at\n\\url{https://github.com/WENGSYX/Self-Verification}\n","authors":["Yixuan Weng","Minjun Zhu","Fei Xia","Bin Li","Shizhu He","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2212.09561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.06847v2","updated":"2023-03-29T11:35:52Z","published":"2021-10-13T16:35:22Z","title":"Ousiometrics and Telegnomics: The essence of meaning conforms to a\n  two-dimensional powerful-weak and dangerous-safe framework with diverse\n  corpora presenting a safety bias","summary":"  We define `ousiometrics' to be the study of essential meaning in whatever\ncontext that meaningful signals are communicated, and `telegnomics' as the\nstudy of remotely sensed knowledge. From work emerging through the middle of\nthe 20th century, the essence of meaning has become generally accepted as being\nwell captured by the three orthogonal dimensions of evaluation, potency, and\nactivation (EPA). By re-examining first types and then tokens for the English\nlanguage, and through the use of automatically annotated histograms --\n`ousiograms' -- we find here that: 1. The essence of meaning conveyed by words\nis instead best described by a compass-like power-danger (PD) framework, and 2.\nAnalysis of a disparate collection of large-scale English language corpora --\nliterature, news, Wikipedia, talk radio, and social media -- shows that natural\nlanguage exhibits a systematic bias toward safe, low danger words -- a\nreinterpretation of the Pollyanna principle's positivity bias for written\nexpression. To help justify our choice of dimension names and to help address\nthe problems with representing observed ousiometric dimensions by bipolar\nadjective pairs, we introduce and explore `synousionyms' and `antousionyms' --\nousiometric counterparts of synonyms and antonyms. We further show that the PD\nframework revises the circumplex model of affect as a more general model of\nstate of mind. Finally, we use our findings to construct and test a prototype\n`ousiometer', a telegnomic instrument that measures ousiometric time series for\ntemporal corpora. We contend that our power-danger ousiometric framework\nprovides a complement for entropy-based measurements, and may be of value for\nthe study of a wide variety of communication across biological and artificial\nlife.\n","authors":["P. S. Dodds","T. Alshaabi","M. I. Fudolig","J. W. Zimmerman","J. Lovato","S. Beaulieu","J. R. Minot","M. V. Arnold","A. J. Reagan","C. M. Danforth"],"pdf_url":"https://arxiv.org/pdf/2110.06847v2.pdf","comment":"40 pages (34 page main manuscript, 6 page appendix), 15 figures (9\n  main, 6 appendix), 4 tables"},{"id":"http://arxiv.org/abs/2303.09859v2","updated":"2023-03-29T09:00:21Z","published":"2023-03-17T09:53:33Z","title":"Trained on 100 million words and still in shape: BERT meets British\n  National Corpus","summary":"  While modern masked language models (LMs) are trained on ever larger corpora,\nwe here explore the effects of down-scaling training to a modestly-sized but\nrepresentative, well-balanced, and publicly available English text source --\nthe British National Corpus. We show that pre-training on this carefully\ncurated corpus can reach better performance than the original BERT model. We\nargue that this type of corpora has great potential as a language modeling\nbenchmark. To showcase this potential, we present fair, reproducible and\ndata-efficient comparative studies of LMs, in which we evaluate several\ntraining objectives and model architectures and replicate previous empirical\nresults in a systematic way. We propose an optimized LM architecture called\nLTG-BERT.\n","authors":["David Samuel","Andrey Kutuzov","Lilja Øvrelid","Erik Velldal"],"pdf_url":"https://arxiv.org/pdf/2303.09859v2.pdf","comment":"Accepted to EACL 2023"},{"id":"http://arxiv.org/abs/2303.16537v1","updated":"2023-03-29T08:59:44Z","published":"2023-03-29T08:59:44Z","title":"LMExplainer: a Knowledge-Enhanced Explainer for Language Models","summary":"  Large language models (LMs) such as GPT-4 are very powerful and can process\ndifferent kinds of natural language processing (NLP) tasks. However, it can be\ndifficult to interpret the results due to the multi-layer nonlinear model\nstructure and millions of parameters. Lack of understanding of how the model\nworks can make the model unreliable and dangerous for everyday users in\nreal-world scenarios. Most recent works exploit the weights of attention to\nprovide explanations for model predictions. However, pure attention-based\nexplanation is unable to support the growing complexity of the models, and\ncannot reason about their decision-making processes. Thus, we propose\nLMExplainer, a knowledge-enhanced interpretation module for language models\nthat can provide human-understandable explanations. We use a knowledge graph\n(KG) and a graph attention neural network to extract the key decision signals\nof the LM. We further explore whether interpretation can also help AI\nunderstand the task better. Our experimental results show that LMExplainer\noutperforms existing LM+KG methods on CommonsenseQA and OpenBookQA. We also\ncompare the explanation results with generated explanation methods and\nhuman-annotated results. The comparison shows our method can provide more\ncomprehensive and clearer explanations. LMExplainer demonstrates the potential\nto enhance model performance and furnish explanations for the reasoning\nprocesses of models in natural language.\n","authors":["Zichen Chen","Ambuj K Singh","Misha Sra"],"pdf_url":"https://arxiv.org/pdf/2303.16537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04231v2","updated":"2023-03-29T08:48:35Z","published":"2022-12-08T12:28:23Z","title":"Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level\n  Natural Language Explanations","summary":"  Natural language explanations promise to offer intuitively understandable\nexplanations of a neural network's decision process in complex vision-language\ntasks, as pursued in recent VL-NLE models. While current models offer\nimpressive performance on task accuracy and explanation plausibility, they\nsuffer from a range of issues: Some models feature a modular design where the\nexplanation generation module is poorly integrated with a separate module for\ntask-answer prediction, employ backbone models trained on limited sets of\ntasks, or incorporate ad hoc solutions to increase performance on single\ndatasets. We propose to evade these limitations by applying recent advances in\nlarge-scale multi-task pretraining of generative Transformer models to the\nproblem of VL-NLE tasks. Our approach outperforms recent models by a large\nmargin, with human annotators preferring the generated explanations over the\nground truth in two out of three evaluated datasets. As a novel challenge in\nVL-NLE research, we propose the problem of multi-task VL-NLE and show that\njointly training on multiple tasks can increase the explanation quality. We\ndiscuss the ethical implications of high-quality NLE generation and other\nissues in recent VL-NLE research.\n","authors":["Björn Plüster","Jakob Ambsdorf","Lukas Braach","Jae Hee Lee","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2212.04231v2.pdf","comment":"Minor changes"},{"id":"http://arxiv.org/abs/2303.16528v1","updated":"2023-03-29T08:34:01Z","published":"2023-03-29T08:34:01Z","title":"Building a Knowledge Graph of Distributed Ledger Technologies","summary":"  Distributed ledger systems have become more prominent and successful in\nrecent years, with a focus on blockchains and cryptocurrency. This has led to\nvarious misunderstandings about both the technology itself and its\ncapabilities, as in many cases blockchain and cryptocurrency is used\nsynonymously and other applications are often overlooked. Therefore, as a\nwhole, the view of distributed ledger technology beyond blockchains and\ncryptocurrencies is very limited. Existing vocabularies and ontologies often\nfocus on single aspects of the technology, or in some cases even just on one\nproduct. This potentially leads to other types of distributed ledgers and their\npossible use cases being neglected. In this paper, we present a knowledge graph\nand an ontology for distributed ledger technologies, which includes security\nconsiderations to model aspects such as threats and vulnerabilities,\napplication domains, as well as relevant standards and regulations. Such a\nknowledge graph improves the overall understanding of distributed ledgers,\nreveals their strengths, and supports the work of security personnel, i.e.\nanalysts and system architects. We discuss potential uses and follow semantic\nweb best practices to evaluate and publish the ontology and knowledge graph.\n","authors":["Lukas König","Sebastian Neumaier"],"pdf_url":"https://arxiv.org/pdf/2303.16528v1.pdf","comment":"URI: https://w3id.org/DLTOntology"},{"id":"http://arxiv.org/abs/2303.16166v2","updated":"2023-03-29T07:49:54Z","published":"2023-03-28T17:28:52Z","title":"Reproducibility is Nothing without Correctness: The Importance of\n  Testing Code in NLP","summary":"  Despite its pivotal role in research experiments, code correctness is often\npresumed only on the basis of the perceived quality of the results. This comes\nwith the risk of erroneous outcomes and potentially misleading findings. To\naddress this issue, we posit that the current focus on result reproducibility\nshould go hand in hand with the emphasis on coding best practices. We bolster\nour call to the NLP community by presenting a case study, in which we identify\n(and correct) three bugs in widely used open-source implementations of the\nstate-of-the-art Conformer architecture. Through comparative experiments on\nautomatic speech recognition and translation in various language settings, we\ndemonstrate that the existence of bugs does not prevent the achievement of good\nand reproducible results and can lead to incorrect conclusions that potentially\nmisguide future research. In response to this, this study is a call to action\ntoward the adoption of coding best practices aimed at fostering correctness and\nimproving the quality of the developed software.\n","authors":["Sara Papi","Marco Gaido","Andrea Pilzer","Matteo Negri"],"pdf_url":"https://arxiv.org/pdf/2303.16166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01117v4","updated":"2023-03-29T06:50:57Z","published":"2022-12-02T12:04:48Z","title":"Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning","summary":"  The spread of rumors along with breaking events seriously hinders the truth\nin the era of social media. Previous studies reveal that due to the lack of\nannotated resources, rumors presented in minority languages are hard to be\ndetected. Furthermore, the unforeseen breaking events not involved in\nyesterday's news exacerbate the scarcity of data resources. In this work, we\npropose a novel zero-shot framework based on prompt learning to detect rumors\nfalling in different domains or presented in different languages. More\nspecifically, we firstly represent rumor circulated on social media as diverse\npropagation threads, then design a hierarchical prompt encoding mechanism to\nlearn language-agnostic contextual representations for both prompts and rumor\ndata. To further enhance domain adaptation, we model the domain-invariant\nstructural features from the propagation threads, to incorporate structural\nposition representations of influential community response. In addition, a new\nvirtual response augmentation method is used to improve model training.\nExtensive experiments conducted on three real-world datasets demonstrate that\nour proposed model achieves much better performance than state-of-the-art\nmethods and exhibits a superior capacity for detecting rumors at early stages.\n","authors":["Hongzhan Lin","Pengyao Yi","Jing Ma","Haiyun Jiang","Ziyang Luo","Shuming Shi","Ruifang Liu"],"pdf_url":"https://arxiv.org/pdf/2212.01117v4.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.15430v2","updated":"2023-03-29T04:49:46Z","published":"2023-03-27T17:54:32Z","title":"TextMI: Textualize Multimodal Information for Integrating Non-verbal\n  Cues in Pre-trained Language Models","summary":"  Pre-trained large language models have recently achieved ground-breaking\nperformance in a wide variety of language understanding tasks. However, the\nsame model can not be applied to multimodal behavior understanding tasks (e.g.,\nvideo sentiment/humor detection) unless non-verbal features (e.g., acoustic and\nvisual) can be integrated with language. Jointly modeling multiple modalities\nsignificantly increases the model complexity, and makes the training process\ndata-hungry. While an enormous amount of text data is available via the web,\ncollecting large-scale multimodal behavioral video datasets is extremely\nexpensive, both in terms of time and money. In this paper, we investigate\nwhether large language models alone can successfully incorporate non-verbal\ninformation when they are presented in textual form. We present a way to\nconvert the acoustic and visual information into corresponding textual\ndescriptions and concatenate them with the spoken text. We feed this augmented\ninput to a pre-trained BERT model and fine-tune it on three downstream\nmultimodal tasks: sentiment, humor, and sarcasm detection. Our approach,\nTextMI, significantly reduces model complexity, adds interpretability to the\nmodel's decision, and can be applied for a diverse set of tasks while achieving\nsuperior (multimodal sarcasm detection) or near SOTA (multimodal sentiment\nanalysis and multimodal humor detection) performance. We propose TextMI as a\ngeneral, competitive baseline for multimodal behavioral analysis tasks,\nparticularly in a low-resource setting.\n","authors":["Md Kamrul Hasan","Md Saiful Islam","Sangwu Lee","Wasifur Rahman","Iftekhar Naim","Mohammed Ibrahim Khan","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2303.15430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16445v1","updated":"2023-03-29T04:00:53Z","published":"2023-03-29T04:00:53Z","title":"Larger Probes Tell a Different Story: Extending Psycholinguistic\n  Datasets Via In-Context Learning","summary":"  Language model probing is often used to test specific capabilities of these\nmodels. However, conclusions from such studies may be limited when the probing\nbenchmarks are small and lack statistical power. In this work, we introduce\nnew, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)\ninspired by psycholinguistic studies. We dramatically extend existing NEG-136\nand ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44\nsentence pairs to 750 each. We also create another version of extended negation\ndataset (NEG-1500-SIMP-TEMP), created using template-based generation. It\nconsists of 770 sentence pairs. We evaluate 22 models on the extended datasets,\nseeing model performance dip 20-57% compared to the original smaller\nbenchmarks. We observe high levels of negation sensitivity in models like BERT\nand ALBERT demonstrating that previous findings might have been skewed due to\nsmaller test sets. Finally, we observe that while GPT3 has generated all the\nexamples in ROLE-1500 is only able to solve 24.6% of them during probing.\n","authors":["Namrata Shivagunde","Vladislav Lialin","Anna Rumshisky"],"pdf_url":"https://arxiv.org/pdf/2303.16445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16434v1","updated":"2023-03-29T03:30:38Z","published":"2023-03-29T03:30:38Z","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with\n  Millions of APIs","summary":"  Artificial Intelligence (AI) has made incredible progress recently. On the\none hand, advanced foundation models like ChatGPT can offer powerful\nconversation, in-context learning and code generation abilities on a broad\nrange of open-domain tasks. They can also generate high-level solution outlines\nfor domain-specific tasks based on the common sense knowledge they have\nacquired. However, they still face difficulties with some specialized tasks\nbecause they lack enough domain-specific data during pre-training or they often\nhave errors in their neural network computations on those tasks that need\naccurate executions. On the other hand, there are also many existing models and\nsystems (symbolic-based or neural-based) that can do some domain-specific tasks\nvery well. However, due to the different implementation or working mechanisms,\nthey are not easily accessible or compatible with foundation models. Therefore,\nthere is a clear and pressing need for a mechanism that can leverage foundation\nmodels to propose task solution outlines and then automatically match some of\nthe sub-tasks in the outlines to the off-the-shelf models and systems with\nspecial functionalities to complete them. Inspired by this, we introduce\nTaskMatrix.AI as a new AI ecosystem that connects foundation models with\nmillions of APIs for task completion. Unlike most previous work that aimed to\nimprove a single AI model, TaskMatrix.AI focuses more on using existing\nfoundation models (as a brain-like central system) and APIs of other AI models\nand systems (as sub-task solvers) to achieve diversified tasks in both digital\nand physical domains. As a position paper, we will present our vision of how to\nbuild such an ecosystem, explain each key component, and use study cases to\nillustrate both the feasibility of this vision and the main challenges we need\nto address next.\n","authors":["Yaobo Liang","Chenfei Wu","Ting Song","Wenshan Wu","Yan Xia","Yu Liu","Yang Ou","Shuai Lu","Lei Ji","Shaoguang Mao","Yun Wang","Linjun Shou","Ming Gong","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2303.16434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09038v3","updated":"2023-03-29T03:22:52Z","published":"2023-03-16T02:21:39Z","title":"Translating Radiology Reports into Plain Language using ChatGPT and\n  GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential","summary":"  The large language model called ChatGPT has drawn extensively attention\nbecause of its human-like expression and reasoning abilities. In this study, we\ninvestigate the feasibility of using ChatGPT in experiments on using ChatGPT to\ntranslate radiology reports into plain language for patients and healthcare\nproviders so that they are educated for improved healthcare. Radiology reports\nfrom 62 low-dose chest CT lung cancer screening scans and 76 brain MRI\nmetastases screening scans were collected in the first half of February for\nthis study. According to the evaluation by radiologists, ChatGPT can\nsuccessfully translate radiology reports into plain language with an average\nscore of 4.27 in the five-point system with 0.08 places of information missing\nand 0.07 places of misinformation. In terms of the suggestions provided by\nChatGPT, they are general relevant such as keeping following-up with doctors\nand closely monitoring any symptoms, and for about 37% of 138 cases in total\nChatGPT offers specific suggestions based on findings in the report. ChatGPT\nalso presents some randomness in its responses with occasionally\nover-simplified or neglected information, which can be mitigated using a more\ndetailed prompt. Furthermore, ChatGPT results are compared with a newly\nreleased large model GPT-4, showing that GPT-4 can significantly improve the\nquality of translated reports. Our results show that it is feasible to utilize\nlarge language models in clinical education, and further efforts are needed to\naddress limitations and maximize their potential.\n","authors":["Qing Lyu","Josh Tan","Michael E. Zapadka","Janardhana Ponnatapura","Chuang Niu","Kyle J. Myers","Ge Wang","Christopher T. Whitlow"],"pdf_url":"https://arxiv.org/pdf/2303.09038v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16421v1","updated":"2023-03-29T03:05:43Z","published":"2023-03-29T03:05:43Z","title":"ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of\n  Commonsense Problem in Large Language Models","summary":"  Large language models (LLMs) such as ChatGPT and GPT-4 have made significant\nprogress in NLP. However, their ability to memorize, represent, and leverage\ncommonsense knowledge has been a well-known pain point for LLMs. It remains\nunclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are\nGPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying\ncommonsense knowledge for answering a specific question? (4) Can GPTs\neffectively leverage commonsense for answering questions? To evaluate the above\ncommonsense problems, we conduct a series of experiments to evaluate ChatGPT's\ncommonsense abilities, and the experimental results show that: (1) GPTs can\nachieve good QA accuracy in commonsense tasks, while they still struggle with\ncertain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately\ngenerate most of the commonsense knowledge using knowledge prompts. (3) Despite\nits knowledge, ChatGPT is an inexperienced commonsense problem solver, which\ncannot precisely identify the needed commonsense knowledge for answering a\nspecific question, i.e., ChatGPT does not precisely know what commonsense\nknowledge is required to answer a question. The above findings raise the need\nto investigate better mechanisms for utilizing commonsense knowledge in LLMs,\nsuch as instruction following, better commonsense guidance, etc.\n","authors":["Ning Bian","Xianpei Han","Le Sun","Hongyu Lin","Yaojie Lu","Ben He"],"pdf_url":"https://arxiv.org/pdf/2303.16421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16416v1","updated":"2023-03-29T02:46:18Z","published":"2023-03-29T02:46:18Z","title":"Zero-shot Clinical Entity Recognition using ChatGPT","summary":"  In this study, we investigated the potential of ChatGPT, a large language\nmodel developed by OpenAI, for the clinical named entity recognition task\ndefined in the 2010 i2b2 challenge, in a zero-shot setting with two different\nprompt strategies. We compared its performance with GPT-3 in a similar\nzero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of\nsynthetic clinical notes from MTSamples. Our findings revealed that ChatGPT\noutperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250)\nand 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover,\nprompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores\nof 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's\nperformance was still lower than that of the supervised BioClinicalBERT model\n(i.e., relaxed-matching F1 scores of 0.628 vs. 0.870), our study demonstrates\nthe great potential of ChatGPT for clinical NER tasks in a zero-shot setting,\nwhich is much more appealing as it does not require any annotation.\n","authors":["Yan Hu","Iqra Ameer","Xu Zuo","Xueqing Peng","Yujia Zhou","Zehan Li","Yiming Li","Jianfu Li","Xiaoqian Jiang","Hua Xu"],"pdf_url":"https://arxiv.org/pdf/2303.16416v1.pdf","comment":"7 pages, 5 tables, 1 figure"},{"id":"http://arxiv.org/abs/2303.16406v1","updated":"2023-03-29T02:33:54Z","published":"2023-03-29T02:33:54Z","title":"Hierarchical Video-Moment Retrieval and Step-Captioning","summary":"  There is growing interest in searching for information from large video\ncorpora. Prior works have studied relevant tasks, such as text-based video\nretrieval, moment retrieval, video summarization, and video captioning in\nisolation, without an end-to-end setup that can jointly search from video\ncorpora and generate summaries. Such an end-to-end setup would allow for many\ninteresting applications, e.g., a text-based search that finds a relevant video\nfrom a video corpus, extracts the most relevant moment from that video, and\nsegments the moment into important steps with captions. To address this, we\npresent the HiREST (HIerarchical REtrieval and STep-captioning) dataset and\npropose a new benchmark that covers hierarchical information retrieval and\nvisual/textual stepwise summarization from an instructional video corpus.\nHiREST consists of 3.4K text-video pairs from an instructional video dataset,\nwhere 1.1K videos have annotations of moment spans relevant to text query and\nbreakdown of each moment into key instruction steps with caption and timestamps\n(totaling 8.6K step captions). Our hierarchical benchmark consists of video\nretrieval, moment retrieval, and two novel moment segmentation and step\ncaptioning tasks. In moment segmentation, models break down a video moment into\ninstruction steps and identify start-end boundaries. In step captioning, models\ngenerate a textual summary for each step. We also present starting point\ntask-specific and end-to-end joint baseline models for our new benchmark. While\nthe baseline models show some promising results, there still exists large room\nfor future improvement by the community. Project website:\nhttps://hirest-cvpr2023.github.io\n","authors":["Abhay Zala","Jaemin Cho","Satwik Kottur","Xilun Chen","Barlas Oğuz","Yasher Mehdad","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2303.16406v1.pdf","comment":"CVPR 2023 (15 pages; the first two authors contributed equally;\n  Project website: https://hirest-cvpr2023.github.io)"},{"id":"http://arxiv.org/abs/2302.07267v4","updated":"2023-03-29T23:30:35Z","published":"2023-02-13T17:57:50Z","title":"\"Correct answers\" from the psychology of artificial intelligence","summary":"  We re-replicate 14 psychology studies from the Many Labs 2 replication\nproject (Klein et al., 2018) with OpenAI's text-davinci-003 model, colloquially\nknown as GPT3.5. Among the eight studies we could analyse, our GPT sample\nreplicated 37.5% of the original results and 37.5% of the Many Labs 2 results.\nWe could not analyse the remaining six studies, due to an unexpected phenomenon\nwe call the \"correct answer\" effect. Different runs of GPT3.5 answered nuanced\nquestions probing political orientation, economic preference, judgement, and\nmoral philosophy with zero or near-zero variation in responses: with the\nsupposedly \"correct answer.\" Most but not all of these \"correct answers\" were\nrobust to changing the order of answer choices. One exception occurred in the\nMoral Foundations Theory survey (Graham et al., 2009), for which GPT3.5 almost\nalways identified as a conservative in the original condition (N=1,030, 99.6%)\nand as a liberal in the reverse-order condition (N=1,030, 99.3%). GPT3.5's\nresponses to subsequent questions revealed post-hoc rationalisation; there was\na relative bias in the direction of its previously reported political\norientation. But both self-reported GPT conservatives and self-reported GPT\nliberals revealed right-leaning Moral Foundations, although the right-leaning\nbias of self-reported GPT liberals was weaker. We hypothesise that this pattern\nwas learned from a conservative bias in the model's largely Internet-based\ntraining data. Since AI models of the future may be trained on much of the same\nInternet data as GPT3.5, our results raise concerns that a hypothetical AI-led\nfuture may be subject to a diminished diversity of thought.\n","authors":["Peter S. Park","Philipp Schoenegger","Chongyang Zhu"],"pdf_url":"https://arxiv.org/pdf/2302.07267v4.pdf","comment":"62 pages (38-page main text, 24-page SI); 11 visualizations (four\n  tables and two figures in the main text, five figures in the SI); added\n  corrections regarding the previously erroneous survey for Study 4's\n  replication of Graham et al. (2009); preregistered OSF database is available\n  at https://osf.io/dzp8t/"},{"id":"http://arxiv.org/abs/2303.17006v1","updated":"2023-03-29T20:21:45Z","published":"2023-03-29T20:21:45Z","title":"How do decoding algorithms distribute information in dialogue responses?","summary":"  Humans tend to follow the Uniform Information Density (UID) principle by\ndistributing information evenly in utterances. We study if decoding algorithms\nimplicitly follow this UID principle, and under what conditions adherence to\nUID might be desirable for dialogue generation. We generate responses using\ndifferent decoding algorithms with GPT-2 on the Persona-Chat dataset and\ncollect human judgments on their quality using Amazon Mechanical Turk. We find\nthat (i) surprisingly, model-generated responses follow the UID principle to a\ngreater extent than human responses, and (ii) decoding algorithms that promote\nUID do not generate higher-quality responses. Instead, when we control for\nsurprisal, non-uniformity of information density correlates with the quality of\nresponses with very low/high surprisal. Our findings indicate that encouraging\nnon-uniform responses is a potential solution to the ``likelihood trap''\nproblem (quality degradation in very high-likelihood text). Our dataset\ncontaining multiple candidate responses per dialog history along with\nhuman-annotated quality ratings is available at\nhttps://huggingface.co/datasets/saranya132/dialog_uid_gpt2.\n","authors":["Saranya Venkatraman","He He","David Reitter"],"pdf_url":"https://arxiv.org/pdf/2303.17006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17003v1","updated":"2023-03-29T20:10:13Z","published":"2023-03-29T20:10:13Z","title":"Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission\n  Exams","summary":"  The present study aims to explore the capabilities of Language Models (LMs)\nin tackling high-stakes multiple-choice tests, represented here by the Exame\nNacional do Ensino M\\'edio (ENEM), a multidisciplinary entrance examination\nwidely adopted by Brazilian universities. This exam poses challenging tasks for\nLMs, since its questions may span into multiple fields of knowledge, requiring\nunderstanding of information from diverse domains. For instance, a question may\nrequire comprehension of both statistics and biology to be solved. This work\nanalyzed responses generated by GPT-3.5 and GPT-4 models for questions\npresented in the 2009-2017 exams, as well as for questions of the 2022 exam,\nwhich were made public after the training of the models was completed.\nFurthermore, different prompt strategies were tested, including the use of\nChain-of-Thought (CoT) prompts to generate explanations for answers. On the\n2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy\nof 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on\nexperiments are available at https://github.com/piresramon/gpt-4-enem.\n","authors":["Desnes Nunes","Ricardo Primi","Ramon Pires","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2303.17003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16992v1","updated":"2023-03-29T19:43:26Z","published":"2023-03-29T19:43:26Z","title":"ContraSim -- A Similarity Measure Based on Contrastive Learning","summary":"  Recent work has compared neural network representations via similarity-based\nanalyses, shedding light on how different aspects (architecture, training data,\netc.) affect models' internal representations. The quality of a similarity\nmeasure is typically evaluated by its success in assigning a high score to\nrepresentations that are expected to be matched. However, existing similarity\nmeasures perform mediocrely on standard benchmarks. In this work, we develop a\nnew similarity measure, dubbed ContraSim, based on contrastive learning. In\ncontrast to common closed-form similarity measures, ContraSim learns a\nparameterized measure by using both similar and dissimilar examples. We perform\nan extensive experimental evaluation of our method, with both language and\nvision models, on the standard layer prediction benchmark and two new\nbenchmarks that we introduce: the multilingual benchmark and the image-caption\nbenchmark. In all cases, ContraSim achieves much higher accuracy than previous\nsimilarity measures, even when presented with challenging examples, and reveals\nnew insights not captured by previous measures.\n","authors":["Adir Rahamim","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2303.16992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16985v1","updated":"2023-03-29T19:25:43Z","published":"2023-03-29T19:25:43Z","title":"Adapting to the Low-Resource Double-Bind: Investigating Low-Compute\n  Methods on Low-Resource African Languages","summary":"  Many natural language processing (NLP) tasks make use of massively\npre-trained language models, which are computationally expensive. However,\naccess to high computational resources added to the issue of data scarcity of\nAfrican languages constitutes a real barrier to research experiments on these\nlanguages. In this work, we explore the applicability of low-compute approaches\nsuch as language adapters in the context of this low-resource double-bind. We\nintend to answer the following question: do language adapters allow those who\nare doubly bound by data and compute to practically build useful models?\nThrough fine-tuning experiments on African languages, we evaluate their\neffectiveness as cost-effective approaches to low-resource African NLP. Using\nsolely free compute resources, our results show that language adapters achieve\ncomparable performances to massive pre-trained language models which are heavy\non computational resources. This opens the door to further experimentation and\nexploration on full-extent of language adapters capacities.\n","authors":["Colin Leong","Herumb Shandilya","Bonaventure F. P. Dossou","Atnafu Lambebo Tonja","Joel Mathew","Abdul-Hakeem Omotayo","Oreen Yousuf","Zainab Akinjobi","Chris Chinenye Emezue","Shamsudeen Muhammad","Steven Kolawole","Younwoo Choi","Tosin Adewumi"],"pdf_url":"https://arxiv.org/pdf/2303.16985v1.pdf","comment":"Accepted to AfricaNLP workshop at ICLR2023"},{"id":"http://arxiv.org/abs/2303.16974v1","updated":"2023-03-29T19:16:19Z","published":"2023-03-29T19:16:19Z","title":"BEVERS: A General, Simple, and Performant Framework for Automatic Fact\n  Verification","summary":"  Automatic fact verification has become an increasingly popular topic in\nrecent years and among datasets the Fact Extraction and VERification (FEVER)\ndataset is one of the most popular. In this work we present BEVERS, a tuned\nbaseline system for the FEVER dataset. Our pipeline uses standard approaches\nfor document retrieval, sentence selection, and final claim classification,\nhowever, we spend considerable effort ensuring optimal performance for each\ncomponent. The results are that BEVERS achieves the highest FEVER score and\nlabel accuracy among all systems, published or unpublished. We also apply this\npipeline to another fact verification dataset, Scifact, and achieve the highest\nlabel accuracy among all systems on that dataset as well. We also make our full\ncode available.\n","authors":["Mitchell DeHaven","Stephen Scott"],"pdf_url":"https://arxiv.org/pdf/2303.16974v1.pdf","comment":"Accepted to the Sixth FEVER Workshop at EACL 2023"},{"id":"http://arxiv.org/abs/2209.15236v3","updated":"2023-03-29T18:37:22Z","published":"2022-09-30T05:02:42Z","title":"Language-Family Adapters for Low-Resource Multilingual Neural Machine\n  Translation","summary":"  Large multilingual models trained with self-supervision achieve\nstate-of-the-art results in a wide range of natural language processing tasks.\nSelf-supervised pretrained models are often fine-tuned on parallel data from\none or multiple language pairs for machine translation. Multilingual\nfine-tuning improves performance on low-resource languages but requires\nmodifying the entire model and can be prohibitively expensive. Training a new\nadapter on each language pair or training a single adapter on all language\npairs without updating the pretrained model has been proposed as a\nparameter-efficient alternative. However, the former does not permit any\nsharing between languages, while the latter shares parameters for all languages\nand is susceptible to negative interference. In this paper, we propose training\nlanguage-family adapters on top of mBART-50 to facilitate cross-lingual\ntransfer. Our approach outperforms related baselines, yielding higher\ntranslation scores on average when translating from English to 17 different\nlow-resource languages. We also show that language-family adapters provide an\neffective method to translate to languages unseen during pretraining.\n","authors":["Alexandra Chronopoulou","Dario Stojanovski","Alexander Fraser"],"pdf_url":"https://arxiv.org/pdf/2209.15236v3.pdf","comment":"LoResMT (@EACL 2023) camera-ready version"},{"id":"http://arxiv.org/abs/2205.13115v2","updated":"2023-03-29T18:26:34Z","published":"2022-05-26T02:46:09Z","title":"Fine-grained Image Captioning with CLIP Reward","summary":"  Modern image captioning models are usually trained with text similarity\nobjectives. However, since reference captions in public datasets often describe\nthe most salient common objects, models trained with text similarity objectives\ntend to ignore specific and detailed aspects of an image that distinguish it\nfrom others. Toward more descriptive and distinctive caption generation, we\npropose using CLIP, a multimodal encoder trained on huge image-text pairs from\nweb, to calculate multimodal similarity and use it as a reward function. We\nalso propose a simple finetuning strategy of the CLIP text encoder to improve\ngrammar that does not require extra text annotation. This completely eliminates\nthe need for reference captions during the reward computation. To\ncomprehensively evaluate descriptive captions, we introduce FineCapEval, a new\ndataset for caption evaluation with fine-grained criteria: overall, background,\nobject, relations. In our experiments on text-to-image retrieval and\nFineCapEval, the proposed CLIP-guided model generates more distinctive captions\nthan the CIDEr-optimized model. We also show that our unsupervised grammar\nfinetuning of the CLIP text encoder alleviates the degeneration problem of the\nnaive CLIP reward. Lastly, we show human analysis where the annotators strongly\nprefer the CLIP reward to the CIDEr and MLE objectives according to various\ncriteria. Code and Data: https://github.com/j-min/CLIP-Caption-Reward\n","authors":["Jaemin Cho","Seunghyun Yoon","Ajinkya Kale","Franck Dernoncourt","Trung Bui","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2205.13115v2.pdf","comment":"NAACL Findings 2022"},{"id":"http://arxiv.org/abs/2303.17579v1","updated":"2023-03-29T04:00:47Z","published":"2023-03-29T04:00:47Z","title":"Multimodal Image-Text Matching Improves Retrieval-based Chest X-Ray\n  Report Generation","summary":"  Automated generation of clinically accurate radiology reports can improve\npatient care. Previous report generation methods that rely on image captioning\nmodels often generate incoherent and incorrect text due to their lack of\nrelevant domain knowledge, while retrieval-based attempts frequently retrieve\nreports that are irrelevant to the input image. In this work, we propose\nContrastive X-Ray REport Match (X-REM), a novel retrieval-based radiology\nreport generation module that uses an image-text matching score to measure the\nsimilarity of a chest X-ray image and radiology report for report retrieval. We\nobserve that computing the image-text matching score with a language-image\nmodel can effectively capture the fine-grained interaction between image and\ntext that is often lost when using cosine similarity. X-REM outperforms\nmultiple prior radiology report generation modules in terms of both natural\nlanguage and clinical metrics. Human evaluation of the generated reports\nsuggests that X-REM increased the number of zero-error reports and decreased\nthe average error severity compared to the baseline retrieval approach. Our\ncode is available at: https://github.com/rajpurkarlab/X-REM\n","authors":["Jaehwan Jeong","Katherine Tian","Andrew Li","Sina Hartung","Fardad Behzadi","Juan Calle","David Osayande","Michael Pohlen","Subathra Adithan","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2303.17579v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.16899v1","updated":"2023-03-29T17:59:58Z","published":"2023-03-29T17:59:58Z","title":"AutoAD: Movie Description in Context","summary":"  The objective of this paper is an automatic Audio Description (AD) model that\ningests movies and outputs AD in text form. Generating high-quality movie AD is\nchallenging due to the dependency of the descriptions on context, and the\nlimited amount of training data available. In this work, we leverage the power\nof pretrained foundation models, such as GPT and CLIP, and only train a mapping\nnetwork that bridges the two models for visually-conditioned text generation.\nIn order to obtain high-quality AD, we make the following four contributions:\n(i) we incorporate context from the movie clip, AD from previous clips, as well\nas the subtitles; (ii) we address the lack of training data by pretraining on\nlarge-scale datasets, where visual or contextual information is unavailable,\ne.g. text-only AD without movies or visual captioning datasets without context;\n(iii) we improve on the currently available AD datasets, by removing label\nnoise in the MAD dataset, and adding character naming information; and (iv) we\nobtain strong results on the movie AD task compared with previous methods.\n","authors":["Tengda Han","Max Bain","Arsha Nagrani","Gül Varol","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2303.16899v1.pdf","comment":"CVPR2023 Highlight. Project page:\n  https://www.robots.ox.ac.uk/~vgg/research/autoad/"},{"id":"http://arxiv.org/abs/2303.16900v1","updated":"2023-03-29T17:59:58Z","published":"2023-03-29T17:59:58Z","title":"InceptionNeXt: When Inception Meets ConvNeXt","summary":"  Inspired by the long-range modeling ability of ViTs, large-kernel\nconvolutions are widely studied and adopted recently to enlarge the receptive\nfield and improve model performance, like the remarkable work ConvNeXt which\nemploys 7x7 depthwise convolution. Although such depthwise operator only\nconsumes a few FLOPs, it largely harms the model efficiency on powerful\ncomputing devices due to the high memory access costs. For example, ConvNeXt-T\nhas similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained\non A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt\ncan improve speed, it results in significant performance degradation. It is\nstill unclear how to speed up large-kernel-based CNN models while preserving\ntheir performance. To tackle this issue, inspired by Inceptions, we propose to\ndecompose large-kernel depthwise convolution into four parallel branches along\nchannel dimension, i.e. small square kernel, two orthogonal band kernels, and\nan identity mapping. With this new Inception depthwise convolution, we build a\nseries of networks, namely IncepitonNeXt, which not only enjoy high throughputs\nbut also maintain competitive performance. For instance, InceptionNeXt-T\nachieves 1.6x higher training throughputs than ConvNeX-T, as well as attains\n0.2% top-1 accuracy improvement on ImageNet-1K. We anticipate InceptionNeXt can\nserve as an economical baseline for future architecture design to reduce carbon\nfootprint. Code is available at https://github.com/sail-sg/inceptionnext.\n","authors":["Weihao Yu","Pan Zhou","Shuicheng Yan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.16900v1.pdf","comment":"Code: https://github.com/sail-sg/inceptionnext"},{"id":"http://arxiv.org/abs/2303.16897v1","updated":"2023-03-29T17:59:53Z","published":"2023-03-29T17:59:53Z","title":"Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos","summary":"  Modeling sounds emitted from physical object interactions is critical for\nimmersive perceptual experiences in real and virtual worlds. Traditional\nmethods of impact sound synthesis use physics simulation to obtain a set of\nphysics parameters that could represent and synthesize the sound. However, they\nrequire fine details of both the object geometries and impact locations, which\nare rarely available in the real world and can not be applied to synthesize\nimpact sounds from common videos. On the other hand, existing video-driven deep\nlearning-based approaches could only capture the weak correspondence between\nvisual content and impact sounds since they lack of physics knowledge. In this\nwork, we propose a physics-driven diffusion model that can synthesize\nhigh-fidelity impact sound for a silent video clip. In addition to the video\ncontent, we propose to use additional physics priors to guide the impact sound\nsynthesis procedure. The physics priors include both physics parameters that\nare directly estimated from noisy real-world impact sound examples without\nsophisticated setup and learned residual parameters that interpret the sound\nenvironment via neural networks. We further implement a novel diffusion model\nwith specific training and inference strategies to combine physics priors and\nvisual information for impact sound synthesis. Experimental results show that\nour model outperforms several existing systems in generating realistic impact\nsounds. More importantly, the physics-based representations are fully\ninterpretable and transparent, thus enabling us to perform sound editing\nflexibly.\n","authors":["Kun Su","Kaizhi Qian","Eli Shlizerman","Antonio Torralba","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2303.16897v1.pdf","comment":"CVPR 2023. Project page:\n  https://sukun1045.github.io/video-physics-sound-diffusion/"},{"id":"http://arxiv.org/abs/2303.16894v1","updated":"2023-03-29T17:59:10Z","published":"2023-03-29T17:59:10Z","title":"ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with\n  GPT and Prototype Guidance","summary":"  Understanding 3D scenes from multi-view inputs has been proven to alleviate\nthe view discrepancy issue in 3D visual grounding. However, existing methods\nnormally neglect the view cues embedded in the text modality and fail to weigh\nthe relative importance of different views. In this paper, we propose\nViewRefer, a multi-view framework for 3D visual grounding exploring how to\ngrasp the view knowledge from both text and 3D modalities. For the text branch,\nViewRefer leverages the diverse linguistic knowledge of large-scale language\nmodels, e.g., GPT, to expand a single grounding text to multiple\ngeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer\nfusion module with inter-view attention is introduced to boost the interaction\nof objects across views. On top of that, we further present a set of learnable\nmulti-view prototypes, which memorize scene-agnostic knowledge for different\nviews, and enhance the framework from two perspectives: a view-guided attention\nmodule for more robust text features, and a view-guided scoring strategy during\nthe final prediction. With our designed paradigm, ViewRefer achieves superior\nperformance on three benchmarks and surpasses the second-best by +2.8%, +1.2%,\nand +0.73% on Sr3D, Nr3D, and ScanRefer. Code will be released at\nhttps://github.com/ZiyuGuo99/ViewRefer3D.\n","authors":["Ziyu Guo","Yiwen Tang","Renrui Zhang","Dong Wang","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2303.16894v1.pdf","comment":"Code will be released at https://github.com/ZiyuGuo99/ViewRefer3D"},{"id":"http://arxiv.org/abs/2303.16892v1","updated":"2023-03-29T17:58:40Z","published":"2023-03-29T17:58:40Z","title":"Multi-scale Hierarchical Vision Transformer with Cascaded Attention\n  Decoding for Medical Image Segmentation","summary":"  Transformers have shown great success in medical image segmentation. However,\ntransformers may exhibit a limited generalization ability due to the underlying\nsingle-scale self-attention (SA) mechanism. In this paper, we address this\nissue by introducing a Multi-scale hiERarchical vIsion Transformer (MERIT)\nbackbone network, which improves the generalizability of the model by computing\nSA at multiple scales. We also incorporate an attention-based decoder, namely\nCascaded Attention Decoding (CASCADE), for further refinement of multi-stage\nfeatures generated by MERIT. Finally, we introduce an effective multi-stage\nfeature mixing loss aggregation (MUTATION) method for better model training via\nimplicit ensembling. Our experiments on two widely used medical image\nsegmentation benchmarks (i.e., Synapse Multi-organ, ACDC) demonstrate the\nsuperior performance of MERIT over state-of-the-art methods. Our MERIT\narchitecture and MUTATION loss aggregation can be used with downstream medical\nimage and semantic segmentation tasks.\n","authors":["Md Mostafijur Rahman","Radu Marculescu"],"pdf_url":"https://arxiv.org/pdf/2303.16892v1.pdf","comment":"19 pages, 4 figures, MIDL 2023"},{"id":"http://arxiv.org/abs/2303.16891v1","updated":"2023-03-29T17:58:39Z","published":"2023-03-29T17:58:39Z","title":"Mask-free OVIS: Open-Vocabulary Instance Segmentation without Manual\n  Mask Annotations","summary":"  Existing instance segmentation models learn task-specific information using\nmanual mask annotations from base (training) categories. These mask annotations\nrequire tremendous human effort, limiting the scalability to annotate novel\n(new) categories. To alleviate this problem, Open-Vocabulary (OV) methods\nleverage large-scale image-caption pairs and vision-language models to learn\nnovel categories. In summary, an OV method learns task-specific information\nusing strong supervision from base annotations and novel category information\nusing weak supervision from image-captions pairs. This difference between\nstrong and weak supervision leads to overfitting on base categories, resulting\nin poor generalization towards novel categories. In this work, we overcome this\nissue by learning both base and novel categories from pseudo-mask annotations\ngenerated by the vision-language model in a weakly supervised manner using our\nproposed Mask-free OVIS pipeline. Our method automatically generates\npseudo-mask annotations by leveraging the localization ability of a pre-trained\nvision-language model for objects present in image-caption pairs. The generated\npseudo-mask annotations are then used to supervise an instance segmentation\nmodel, freeing the entire pipeline from any labour-expensive instance-level\nannotations and overfitting. Our extensive experiments show that our method\ntrained with just pseudo-masks significantly improves the mAP scores on the\nMS-COCO dataset and OpenImages dataset compared to the recent state-of-the-art\nmethods trained with manual masks. Codes and models are provided in\nhttps://vibashan.github.io/ovis-web/.\n","authors":["Vibashan VS","Ning Yu","Chen Xing","Can Qin","Mingfei Gao","Juan Carlos Niebles","Vishal M. Patel","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.16891v1.pdf","comment":"Accepted to CVPR 2023. Project site:\n  https://vibashan.github.io/ovis-web/"},{"id":"http://arxiv.org/abs/2303.16890v1","updated":"2023-03-29T17:58:33Z","published":"2023-03-29T17:58:33Z","title":"DPF: Learning Dense Prediction Fields with Weak Supervision","summary":"  Nowadays, many visual scene understanding problems are addressed by dense\nprediction networks. But pixel-wise dense annotations are very expensive (e.g.,\nfor scene parsing) or impossible (e.g., for intrinsic image decomposition),\nmotivating us to leverage cheap point-level weak supervision. However, existing\npointly-supervised methods still use the same architecture designed for full\nsupervision. In stark contrast to them, we propose a new paradigm that makes\npredictions for point coordinate queries, as inspired by the recent success of\nimplicit representations, like distance or radiance fields. As such, the method\nis named as dense prediction fields (DPFs). DPFs generate expressive\nintermediate features for continuous sub-pixel locations, thus allowing outputs\nof an arbitrary resolution. DPFs are naturally compatible with point-level\nsupervision. We showcase the effectiveness of DPFs using two substantially\ndifferent tasks: high-level semantic parsing and low-level intrinsic image\ndecomposition. In these two cases, supervision comes in the form of\nsingle-point semantic category and two-point relative reflectance,\nrespectively. As benchmarked by three large-scale public datasets\nPASCALContext, ADE20K and IIW, DPFs set new state-of-the-art performance on all\nof them with significant margins.\n  Code can be accessed at https://github.com/cxx226/DPF.\n","authors":["Xiaoxue Chen","Yuhang Zheng","Yupeng Zheng","Qiang Zhou","Hao Zhao","Guyue Zhou","Ya-Qin Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.16890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16203v2","updated":"2023-03-29T17:58:24Z","published":"2023-03-28T17:59:56Z","title":"Your Diffusion Model is Secretly a Zero-Shot Classifier","summary":"  The recent wave of large-scale text-to-image diffusion models has\ndramatically increased our text-based image generation abilities. These models\ncan generate realistic images for a staggering variety of prompts and exhibit\nimpressive compositional generalization abilities. Almost all use cases thus\nfar have solely focused on sampling; however, diffusion models can also provide\nconditional density estimates, which are useful for tasks beyond image\ngeneration. In this paper, we show that the density estimates from large-scale\ntext-to-image diffusion models like Stable Diffusion can be leveraged to\nperform zero-shot classification without any additional training. Our\ngenerative approach to classification, which we call Diffusion Classifier,\nattains strong results on a variety of benchmarks and outperforms alternative\nmethods of extracting knowledge from diffusion models. Although a gap remains\nbetween generative and discriminative approaches on zero-shot recognition\ntasks, we find that our diffusion-based approach has stronger multimodal\nrelational reasoning abilities than competing discriminative approaches.\nFinally, we use Diffusion Classifier to extract standard classifiers from\nclass-conditional diffusion models trained on ImageNet. Even though these\nmodels are trained with weak augmentations and no regularization, they approach\nthe performance of SOTA discriminative classifiers. Overall, our results are a\nstep toward using generative over discriminative models for downstream tasks.\nResults and visualizations at https://diffusion-classifier.github.io/\n","authors":["Alexander C. Li","Mihir Prabhudesai","Shivam Duggal","Ellis Brown","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.16203v2.pdf","comment":"Website at https://diffusion-classifier.github.io/"},{"id":"http://arxiv.org/abs/2211.14293v2","updated":"2023-03-29T17:57:09Z","published":"2022-11-25T18:50:04Z","title":"RbA: Segmenting Unknown Regions Rejected by All","summary":"  Standard semantic segmentation models owe their success to curated datasets\nwith a fixed set of semantic categories, without contemplating the possibility\nof identifying unknown objects from novel categories. Existing methods in\noutlier detection suffer from a lack of smoothness and objectness in their\npredictions, due to limitations of the per-pixel classification paradigm.\nFurthermore, additional training for detecting outliers harms the performance\nof known classes. In this paper, we explore another paradigm with region-level\nclassification to better segment unknown objects. We show that the object\nqueries in mask classification tend to behave like one \\vs all classifiers.\nBased on this finding, we propose a novel outlier scoring function called RbA\nby defining the event of being an outlier as being rejected by all known\nclasses. Our extensive experiments show that mask classification improves the\nperformance of the existing outlier detection methods, and the best results are\nachieved with the proposed RbA. We also propose an objective to optimize RbA\nusing minimal outlier supervision. Further fine-tuning with outliers improves\nthe unknown performance, and unlike previous methods, it does not degrade the\ninlier performance.\n","authors":["Nazir Nayal","Mısra Yavuz","João F. Henriques","Fatma Güney"],"pdf_url":"https://arxiv.org/pdf/2211.14293v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16887v1","updated":"2023-03-29T17:56:36Z","published":"2023-03-29T17:56:36Z","title":"Towards Understanding the Effect of Pretraining Label Granularity","summary":"  In this paper, we study how pretraining label granularity affects the\ngeneralization of deep neural networks in image classification tasks. We focus\non the \"fine-to-coarse\" transfer learning setting where the pretraining label\nis more fine-grained than that of the target problem. We experiment with this\nmethod using the label hierarchy of iNaturalist 2021, and observe a 8.76%\nrelative improvement of the error rate over the baseline. We find the following\nconditions are key for the improvement: 1) the pretraining dataset has a strong\nand meaningful label hierarchy, 2) its label function strongly aligns with that\nof the target task, and most importantly, 3) an appropriate level of\npretraining label granularity is chosen. The importance of pretraining label\ngranularity is further corroborated by our transfer learning experiments on\nImageNet. Most notably, we show that pretraining at the leaf labels of\nImageNet21k produces better transfer results on ImageNet1k than pretraining at\nother coarser granularity levels, which supports the common practice.\nTheoretically, through an analysis on a two-layer convolutional ReLU network,\nwe prove that: 1) models trained on coarse-grained labels only respond strongly\nto the common or \"easy-to-learn\" features; 2) with the dataset satisfying the\nright conditions, fine-grained pretraining encourages the model to also learn\nrarer or \"harder-to-learn\" features well, thus improving the model's\ngeneralization.\n","authors":["Guan Zhe Hong","Yin Cui","Ariel Fuxman","Stanley H. Chan","Enming Luo"],"pdf_url":"https://arxiv.org/pdf/2303.16887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16884v1","updated":"2023-03-29T17:53:20Z","published":"2023-03-29T17:53:20Z","title":"Instant Neural Radiance Fields Stylization","summary":"  We present Instant Neural Radiance Fields Stylization, a novel approach for\nmulti-view image stylization for the 3D scene. Our approach models a neural\nradiance field based on neural graphics primitives, which use a hash\ntable-based position encoder for position embedding. We split the position\nencoder into two parts, the content and style sub-branches, and train the\nnetwork for normal novel view image synthesis with the content and style\ntargets. In the inference stage, we execute AdaIN to the output features of the\nposition encoder, with content and style voxel grid features as reference. With\nthe adjusted features, the stylization of novel view images could be obtained.\nOur method extends the style target from style images to image sets of scenes\nand does not require additional network training for stylization. Given a set\nof images of 3D scenes and a style target(a style image or another set of 3D\nscenes), our method can generate stylized novel views with a consistent\nappearance at various view angles in less than 10 minutes on modern GPU\nhardware. Extensive experimental results demonstrate the validity and\nsuperiority of our method.\n","authors":["Shaoxu Li","Ye Pan"],"pdf_url":"https://arxiv.org/pdf/2303.16884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16878v1","updated":"2023-03-29T17:35:23Z","published":"2023-03-29T17:35:23Z","title":"Photometric LiDAR and RGB-D Bundle Adjustment","summary":"  The joint optimization of the sensor trajectory and 3D map is a crucial\ncharacteristic of Simultaneous Localization and Mapping (SLAM) systems. To\nachieve this, the gold standard is Bundle Adjustment (BA). Modern 3D LiDARs now\nretain higher resolutions that enable the creation of point cloud images\nresembling those taken by conventional cameras. Nevertheless, the typical\neffective global refinement techniques employed for RGB-D sensors are not\nwidely applied to LiDARs. This paper presents a novel BA photometric strategy\nthat accounts for both RGB-D and LiDAR in the same way. Our work can be used on\ntop of any SLAM/GNSS estimate to improve and refine the initial trajectory. We\nconducted different experiments using these two depth sensors on public\nbenchmarks. Our results show that our system performs on par or better compared\nto other state-of-the-art ad-hoc SLAM/BA strategies, free from data association\nand without making assumptions about the environment. In addition, we present\nthe benefit of jointly using RGB-D and LiDAR within our unified method. We\nfinally release an open-source CUDA/C++ implementation.\n","authors":["Luca Di Giammarino","Emanuele Giacomini","Leonardo Brizi","Omar Salem","Giorgio Grisetti"],"pdf_url":"https://arxiv.org/pdf/2303.16878v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.16874v1","updated":"2023-03-29T17:30:53Z","published":"2023-03-29T17:30:53Z","title":"CheckerPose: Progressive Dense Keypoint Localization for Object Pose\n  Estimation with Graph Neural Network","summary":"  Estimating the 6-DoF pose of a rigid object from a single RGB image is a\ncrucial yet challenging task. Recent studies have shown the great potential of\ndense correspondence-based solutions, yet improvements are still needed to\nreach practical deployment. In this paper, we propose a novel pose estimation\nalgorithm named CheckerPose, which improves on three main aspects. Firstly,\nCheckerPose densely samples 3D keypoints from the surface of the 3D object and\nfinds their 2D correspondences progressively in the 2D image. Compared to\nprevious solutions that conduct dense sampling in the image space, our strategy\nenables the correspondence searching in a 2D grid (i.e., pixel coordinate).\nSecondly, for our 3D-to-2D correspondence, we design a compact binary code\nrepresentation for 2D image locations. This representation not only allows for\nprogressive correspondence refinement but also converts the correspondence\nregression to a more efficient classification problem. Thirdly, we adopt a\ngraph neural network to explicitly model the interactions among the sampled 3D\nkeypoints, further boosting the reliability and accuracy of the\ncorrespondences. Together, these novel components make our CheckerPose a strong\npose estimation algorithm. When evaluated on the popular Linemod, Linemod-O,\nand YCB-V object pose estimation benchmarks, CheckerPose clearly boosts the\naccuracy of correspondence-based methods and achieves state-of-the-art\nperformances.\n","authors":["Ruyi Lian","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2303.16874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1903.01192v3","updated":"2023-03-29T17:30:25Z","published":"2019-03-04T11:56:53Z","title":"STEFANN: Scene Text Editor using Font Adaptive Neural Network","summary":"  Textual information in a captured scene plays an important role in scene\ninterpretation and decision making. Though there exist methods that can\nsuccessfully detect and interpret complex text regions present in a scene, to\nthe best of our knowledge, there is no significant prior work that aims to\nmodify the textual information in an image. The ability to edit text directly\non images has several advantages including error correction, text restoration\nand image reusability. In this paper, we propose a method to modify text in an\nimage at character-level. We approach the problem in two stages. At first, the\nunobserved character (target) is generated from an observed character (source)\nbeing modified. We propose two different neural network architectures - (a)\nFANnet to achieve structural consistency with source font and (b) Colornet to\npreserve source color. Next, we replace the source character with the generated\ncharacter maintaining both geometric and visual consistency with neighboring\ncharacters. Our method works as a unified platform for modifying text in\nimages. We present the effectiveness of our method on COCO-Text and ICDAR\ndatasets both qualitatively and quantitatively.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/1903.01192v3.pdf","comment":"Accepted in The IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2020"},{"id":"http://arxiv.org/abs/2303.16867v1","updated":"2023-03-29T17:24:21Z","published":"2023-03-29T17:24:21Z","title":"A Video-based End-to-end Pipeline for Non-nutritive Sucking Action\n  Recognition and Segmentation in Young Infants","summary":"  We present an end-to-end computer vision pipeline to detect non-nutritive\nsucking (NNS) -- an infant sucking pattern with no nutrition delivered -- as a\npotential biomarker for developmental delays, using off-the-shelf baby monitor\nvideo footage. One barrier to clinical (or algorithmic) assessment of NNS stems\nfrom its sparsity, requiring experts to wade through hours of footage to find\nminutes of relevant activity. Our NNS activity segmentation algorithm solves\nthis problem by identifying periods of NNS with high certainty -- up to 94.0\\%\naverage precision and 84.9\\% average recall across 30 heterogeneous 60 s clips,\ndrawn from our manually annotated NNS clinical in-crib dataset of 183 hours of\novernight baby monitor footage from 19 infants. Our method is based on an\nunderlying NNS action recognition algorithm, which uses spatiotemporal deep\nlearning networks and infant-specific pose estimation, achieving 94.9\\%\naccuracy in binary classification of 960 2.5 s balanced NNS vs. non-NNS clips.\nTested on our second, independent, and public NNS in-the-wild dataset, NNS\nrecognition classification reaches 92.3\\% accuracy, and NNS segmentation\nachieves 90.8\\% precision and 84.2\\% recall.\n","authors":["Shaotong Zhu","Michael Wan","Elaheh Hatamimajoumerd","Kashish Jain","Samuel Zlota","Cholpady Vikram Kamath","Cassandra B. Rowan","Emma C. Grace","Matthew S. Goodwin","Marie J. Hayes","Rebecca A. Schwartz-Mette","Emily Zimmerman","Sarah Ostadabbas"],"pdf_url":"https://arxiv.org/pdf/2303.16867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16866v1","updated":"2023-03-29T17:24:12Z","published":"2023-03-29T17:24:12Z","title":"ALUM: Adversarial Data Uncertainty Modeling from Latent Model\n  Uncertainty Compensation","summary":"  It is critical that the models pay attention not only to accuracy but also to\nthe certainty of prediction. Uncertain predictions of deep models caused by\nnoisy data raise significant concerns in trustworthy AI areas. To explore and\nhandle uncertainty due to intrinsic data noise, we propose a novel method\ncalled ALUM to simultaneously handle the model uncertainty and data uncertainty\nin a unified scheme. Rather than solely modeling data uncertainty in the\nultimate layer of a deep model based on randomly selected training data, we\npropose to explore mined adversarial triplets to facilitate data uncertainty\nmodeling and non-parametric uncertainty estimations to compensate for the\ninsufficiently trained latent model layers. Thus, the critical data uncertainty\nand model uncertainty caused by noisy data can be readily quantified for\nimproving model robustness. Our proposed ALUM is model-agnostic which can be\neasily implemented into any existing deep model with little extra computation\noverhead. Extensive experiments on various noisy learning tasks validate the\nsuperior robustness and generalization ability of our method. The code is\nreleased at https://github.com/wwzjer/ALUM.\n","authors":["Wei Wei","Jiahuan Zhou","Hongze Li","Ying Wu"],"pdf_url":"https://arxiv.org/pdf/2303.16866v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.16861v1","updated":"2023-03-29T17:18:58Z","published":"2023-03-29T17:18:58Z","title":"Beyond Empirical Risk Minimization: Local Structure Preserving\n  Regularization for Improving Adversarial Robustness","summary":"  It is broadly known that deep neural networks are susceptible to being fooled\nby adversarial examples with perturbations imperceptible by humans. Various\ndefenses have been proposed to improve adversarial robustness, among which\nadversarial training methods are most effective. However, most of these methods\ntreat the training samples independently and demand a tremendous amount of\nsamples to train a robust network, while ignoring the latent structural\ninformation among these samples. In this work, we propose a novel Local\nStructure Preserving (LSP) regularization, which aims to preserve the local\nstructure of the input space in the learned embedding space. In this manner,\nthe attacking effect of adversarial samples lying in the vicinity of clean\nsamples can be alleviated. We show strong empirical evidence that with or\nwithout adversarial training, our method consistently improves the performance\nof adversarial robustness on several image classification datasets compared to\nthe baselines and some state-of-the-art approaches, thus providing promising\ndirection for future research.\n","authors":["Wei Wei","Jiahuan Zhou","Ying Wu"],"pdf_url":"https://arxiv.org/pdf/2303.16861v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.16856v1","updated":"2023-03-29T17:06:08Z","published":"2023-03-29T17:06:08Z","title":"Robust Dancer: Long-term 3D Dance Synthesis Using Unpaired Data","summary":"  How to automatically synthesize natural-looking dance movements based on a\npiece of music is an incrementally popular yet challenging task. Most existing\ndata-driven approaches require hard-to-get paired training data and fail to\ngenerate long sequences of motion due to error accumulation of autoregressive\nstructure. We present a novel 3D dance synthesis system that only needs\nunpaired data for training and could generate realistic long-term motions at\nthe same time. For the unpaired data training, we explore the disentanglement\nof beat and style, and propose a Transformer-based model free of reliance upon\npaired data. For the synthesis of long-term motions, we devise a new\nlong-history attention strategy. It first queries the long-history embedding\nthrough an attention computation and then explicitly fuses this embedding into\nthe generation pipeline via multimodal adaptation gate (MAG). Objective and\nsubjective evaluations show that our results are comparable to strong baseline\nmethods, despite not requiring paired training data, and are robust when\ninferring long-term music. To our best knowledge, we are the first to achieve\nunpaired data training - an ability that enables to alleviate data limitations\neffectively. Our code is released on https://github.com/BFeng14/RobustDancer\n","authors":["Bin Feng","Tenglong Ao","Zequn Liu","Wei Ju","Libin Liu","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.16856v1.pdf","comment":"Preliminary video demo: https://youtu.be/gJbxG9QlcUU"},{"id":"http://arxiv.org/abs/2212.04089v2","updated":"2023-03-29T16:52:08Z","published":"2022-12-08T05:50:53Z","title":"Editing Models with Task Arithmetic","summary":"  Changing how pre-trained models behave -- e.g., improving their performance\non a downstream task or mitigating biases learned during pre-training -- is a\ncommon practice when developing machine learning systems. In this work, we\npropose a new paradigm for steering the behavior of neural networks, centered\naround \\textit{task vectors}. A task vector specifies a direction in the weight\nspace of a pre-trained model, such that movement in that direction improves\nperformance on the task. We build task vectors by subtracting the weights of a\npre-trained model from the weights of the same model after fine-tuning on a\ntask. We show that these task vectors can be modified and combined together\nthrough arithmetic operations such as negation and addition, and the behavior\nof the resulting model is steered accordingly. Negating a task vector decreases\nperformance on the target task, with little change in model behavior on control\ntasks. Moreover, adding task vectors together can improve performance on\nmultiple tasks at once. Finally, when tasks are linked by an analogy\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\nthree of the tasks can improve performance on the fourth, even when no data\nfrom the fourth task is used for training. Overall, our experiments with\nseveral models, modalities and tasks show that task arithmetic is a simple,\nefficient and effective way of editing models.\n","authors":["Gabriel Ilharco","Marco Tulio Ribeiro","Mitchell Wortsman","Suchin Gururangan","Ludwig Schmidt","Hannaneh Hajishirzi","Ali Farhadi"],"pdf_url":"https://arxiv.org/pdf/2212.04089v2.pdf","comment":"In Proceedings of the 11th International Conference on Learning\n  Representations (ICLR 2023)"},{"id":"http://arxiv.org/abs/2303.15214v2","updated":"2023-03-29T16:51:15Z","published":"2023-03-27T13:55:07Z","title":"Generalizable Denoising of Microscopy Images using Generative\n  Adversarial Networks and Contrastive Learning","summary":"  Microscopy images often suffer from high levels of noise, which can hinder\nfurther analysis and interpretation. Content-aware image restoration (CARE)\nmethods have been proposed to address this issue, but they often require large\namounts of training data and suffer from over-fitting. To overcome these\nchallenges, we propose a novel framework for few-shot microscopy image\ndenoising. Our approach combines a generative adversarial network (GAN) trained\nvia contrastive learning (CL) with two structure preserving loss terms\n(Structural Similarity Index and Total Variation loss) to further improve the\nquality of the denoised images using little data. We demonstrate the\neffectiveness of our method on three well-known microscopy imaging datasets,\nand show that we can drastically reduce the amount of training data while\nretaining the quality of the denoising, thus alleviating the burden of\nacquiring paired data and enabling few-shot learning. The proposed framework\ncan be easily extended to other image restoration tasks and has the potential\nto significantly advance the field of microscopy image analysis.\n","authors":["Felix Fuentes-Hurtado","Jean-Baptiste Sibarita","Virgile Viasnoff"],"pdf_url":"https://arxiv.org/pdf/2303.15214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1807.10108v5","updated":"2023-03-29T16:48:45Z","published":"2018-07-26T13:20:57Z","title":"Effects of Degradations on Deep Neural Network Architectures","summary":"  Deep convolutional neural networks (CNN) have massively influenced recent\nadvances in large-scale image classification. More recently, a dynamic routing\nalgorithm with capsules (groups of neurons) has shown state-of-the-art\nrecognition performance. However, the behavior of such networks in the presence\nof a degrading signal (noise) is mostly unexplored. An analytical study on\ndifferent network architectures toward noise robustness is essential for\nselecting the appropriate model in a specific application scenario. This paper\npresents an extensive performance analysis of six deep architectures for image\nclassification on six most common image degradation models. In this study, we\nhave compared VGG-16, VGG-19, ResNet-50, Inception-v3, MobileNet and CapsuleNet\narchitectures on Gaussian white, Gaussian color, salt-and-pepper, Gaussian\nblur, motion blur and JPEG compression noise models.\n","authors":["Prasun Roy","Subhankar Ghosh","Saumik Bhattacharya","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/1807.10108v5.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2303.16839v1","updated":"2023-03-29T16:42:30Z","published":"2023-03-29T16:42:30Z","title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks","summary":"  The development of language models have moved from encoder-decoder to\ndecoder-only designs. In addition, the common knowledge has it that the two\nmost popular multimodal tasks, the generative and contrastive tasks, tend to\nconflict with one another, are hard to accommodate in one architecture, and\nfurther need complex adaptations for downstream tasks. We propose a novel\nparadigm of training with a decoder-only model for multimodal tasks, which is\nsurprisingly effective in jointly learning of these disparate vision-language\ntasks. This is done with a simple model, called MaMMUT. It consists of a single\nvision encoder and a text decoder, and is able to accommodate contrastive and\ngenerative learning by a novel two-pass approach on the text decoder. We\ndemonstrate that joint training of these diverse-objective tasks is simple,\neffective, and maximizes the weight-sharing of the model. Furthermore, the same\narchitecture enables straightforward extensions to open-vocabulary object\ndetection and video-language tasks. The model tackles a diverse range of tasks,\nwhile being modest in capacity. Our model achieves the SOTA on image-text and\ntext-image retrieval, video question answering and open-vocabulary detection\ntasks, outperforming much larger and more extensively trained foundational\nmodels. It shows competitive results on VQA and Video Captioning, especially\nconsidering its size. Ablations confirm the flexibility and advantages of our\napproach.\n","authors":["Weicheng Kuo","AJ Piergiovanni","Dahun Kim","Xiyang Luo","Ben Caine","Wei Li","Abhijit Ogale","Luowei Zhou","Andrew Dai","Zhifeng Chen","Claire Cui","Anelia Angelova"],"pdf_url":"https://arxiv.org/pdf/2303.16839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16833v1","updated":"2023-03-29T16:28:11Z","published":"2023-03-29T16:28:11Z","title":"Multi-View Keypoints for Reliable 6D Object Pose Estimation","summary":"  6D Object pose estimation is a fundamental component in robotics enabling\nefficient interaction with the environment. It is particularly challenging in\nbin-picking applications, where many objects are low-feature and reflective,\nand self-occlusion between objects of the same type is common. We propose a\nnovel multi-view approach leveraging known camera transformations from an\neye-in-hand setup to combine heatmap and keypoint estimates into a probability\ndensity map over 3D space. The result is a robust approach that is scalable in\nthe number of views. It relies on a confidence score composed of keypoint\nprobabilities and point-cloud alignment error, which allows reliable rejection\nof false positives. We demonstrate an average pose estimation error of\napproximately 0.5mm and 2 degrees across a variety of difficult low-feature and\nreflective objects in the ROBI dataset, while also surpassing the state-of-art\ncorrect detection rate, measured using the 10% object diameter threshold on ADD\nerror.\n","authors":["Alan Li","Angela P. Schoellig"],"pdf_url":"https://arxiv.org/pdf/2303.16833v1.pdf","comment":"To be published in ICRA 2023 conference proceedings"},{"id":"http://arxiv.org/abs/2302.11510v3","updated":"2023-03-29T16:25:43Z","published":"2023-02-22T17:27:03Z","title":"Selective experience replay compression using coresets for lifelong deep\n  reinforcement learning in medical imaging","summary":"  Selective experience replay is a popular strategy for integrating lifelong\nlearning with deep reinforcement learning. Selective experience replay aims to\nrecount selected experiences from previous tasks to avoid catastrophic\nforgetting. Furthermore, selective experience replay based techniques are model\nagnostic and allow experiences to be shared across different models. However,\nstoring experiences from all previous tasks make lifelong learning using\nselective experience replay computationally very expensive and impractical as\nthe number of tasks increase. To that end, we propose a reward\ndistribution-preserving coreset compression technique for compressing\nexperience replay buffers stored for selective experience replay.\n  We evaluated the coreset compression technique on the brain tumor\nsegmentation (BRATS) dataset for the task of ventricle localization and on the\nwhole-body MRI for localization of left knee cap, left kidney, right\ntrochanter, left lung, and spleen. The coreset lifelong learning models trained\non a sequence of 10 different brain MR imaging environments demonstrated\nexcellent performance localizing the ventricle with a mean pixel error distance\nof 12.93 for the compression ratio of 10x. In comparison, the conventional\nlifelong learning model localized the ventricle with a mean pixel distance of\n10.87. Similarly, the coreset lifelong learning models trained on whole-body\nMRI demonstrated no significant difference (p=0.28) between the 10x compressed\ncoreset lifelong learning models and conventional lifelong learning models for\nall the landmarks. The mean pixel distance for the 10x compressed models across\nall the landmarks was 25.30, compared to 19.24 for the conventional lifelong\nlearning models. Our results demonstrate that the potential of the\ncoreset-based ERB compression method for compressing experiences without a\nsignificant drop in performance.\n","authors":["Guangyao Zheng","Samson Zhou","Vishwa S. Parekh","Michael A. Jacobs","Vladimir Braverman"],"pdf_url":"https://arxiv.org/pdf/2302.11510v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05489v3","updated":"2023-03-29T16:13:22Z","published":"2023-01-13T11:27:26Z","title":"A Residual Diffusion Model for High Perceptual Quality Codec\n  Augmentation","summary":"  Diffusion probabilistic models have recently achieved remarkable success in\ngenerating high quality image and video data. In this work, we build on this\nclass of generative models and introduce a method for lossy compression of high\nresolution images. The resulting codec, which we call DIffuson-based Residual\nAugmentation Codec (DIRAC), is the first neural codec to allow smooth traversal\nof the rate-distortion-perception tradeoff at test time, while obtaining\ncompetitive performance with GAN-based methods in perceptual quality.\nFurthermore, while sampling from diffusion probabilistic models is notoriously\nexpensive, we show that in the compression setting the number of steps can be\ndrastically reduced.\n","authors":["Noor Fathima Ghouse","Jens Petersen","Auke Wiggers","Tianlin Xu","Guillaume Sautière"],"pdf_url":"https://arxiv.org/pdf/2301.05489v3.pdf","comment":"v1: 26 pages, 13 figures v2: corrected typo in first author name in\n  arxiv metadata v3: major paper update to add base codecs and lpips loss"},{"id":"http://arxiv.org/abs/2303.16818v1","updated":"2023-03-29T16:08:59Z","published":"2023-03-29T16:08:59Z","title":"BEVSimDet: Simulated Multi-modal Distillation in Bird's-Eye View for\n  Multi-view 3D Object Detection","summary":"  Multi-view camera-based 3D object detection has gained popularity due to its\nlow cost. But accurately inferring 3D geometry solely from camera data remains\nchallenging, which impacts model performance. One promising approach to address\nthis issue is to distill precise 3D geometry knowledge from LiDAR data.\nHowever, transferring knowledge between different sensor modalities is hindered\nby the significant modality gap. In this paper, we approach this challenge from\nthe perspective of both architecture design and knowledge distillation and\npresent a new simulated multi-modal 3D object detection method named BEVSimDet.\nWe first introduce a novel framework that includes a LiDAR and camera\nfusion-based teacher and a simulated multi-modal student, where the student\nsimulates multi-modal features with image-only input. To facilitate effective\ndistillation, we propose a simulated multi-modal distillation scheme that\nsupports intra-modal, cross-modal, and multi-modal distillation simultaneously.\nBy combining them together, BEVSimDet can learn better feature representations\nfor 3D object detection while enjoying cost-effective camera-only deployment.\nExperimental results on the challenging nuScenes benchmark demonstrate the\neffectiveness and superiority of BEVSimDet over recent representative methods.\nThe source code will be released.\n","authors":["Haimei Zhao","Qiming Zhang","Shanshan Zhao","Jing Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.16818v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2303.16817v1","updated":"2023-03-29T16:07:06Z","published":"2023-03-29T16:07:06Z","title":"Adaptive Superpixel for Active Learning in Semantic Segmentation","summary":"  Learning semantic segmentation requires pixel-wise annotations, which can be\ntime-consuming and expensive. To reduce the annotation cost, we propose a\nsuperpixel-based active learning (AL) framework, which collects a dominant\nlabel per superpixel instead. To be specific, it consists of adaptive\nsuperpixel and sieving mechanisms, fully dedicated to AL. At each round of AL,\nwe adaptively merge neighboring pixels of similar learned features into\nsuperpixels. We then query a selected subset of these superpixels using an\nacquisition function assuming no uniform superpixel size. This approach is more\nefficient than existing methods, which rely only on innate features such as RGB\ncolor and assume uniform superpixel sizes. Obtaining a dominant label per\nsuperpixel drastically reduces annotators' burden as it requires fewer clicks.\nHowever, it inevitably introduces noisy annotations due to mismatches between\nsuperpixel and ground truth segmentation. To address this issue, we further\ndevise a sieving mechanism that identifies and excludes potentially noisy\nannotations from learning. Our experiments on both Cityscapes and PASCAL VOC\ndatasets demonstrate the efficacy of adaptive superpixel and sieving\nmechanisms.\n","authors":["Hoyoung Kim","Minhyeon Oh","Sehyun Hwang","Suha Kwak","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2303.16817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07983v4","updated":"2023-03-29T15:55:03Z","published":"2022-10-14T17:27:56Z","title":"Improving Transfer Learning with a Dual Image and Video Transformer for\n  Multi-label Movie Trailer Genre Classification","summary":"  In this paper, we study the transferability of ImageNet spatial and Kinetics\nspatio-temporal representations to multi-label Movie Trailer Genre\nClassification (MTGC). In particular, we present an extensive evaluation of the\ntransferability of ConvNet and Transformer models pretrained on ImageNet and\nKinetics to Trailers12k, a new manually-curated movie trailer dataset composed\nof 12,000 videos labeled with 10 different genres and associated metadata. We\nanalyze different aspects that can influence transferability, such as frame\nrate, input video extension, and spatio-temporal modeling. In order to reduce\nthe spatio-temporal structure gap between ImageNet/Kinetics and Trailers12k, we\npropose Dual Image and Video Transformer Architecture (DIViTA), which performs\nshot detection so as to segment the trailer into highly correlated clips,\nproviding a more cohesive input for pretrained backbones and improving\ntransferability (a 1.83% increase for ImageNet and 3.75% for Kinetics). Our\nresults demonstrate that representations learned on either ImageNet or Kinetics\nare comparatively transferable to Trailers12k. Moreover, both datasets provide\ncomplementary information that can be combined to improve classification\nperformance (a 2.91% gain compared to the top single pretraining).\nInterestingly, using lightweight ConvNets as pretrained backbones resulted in\nonly a 3.46% drop in classification performance compared with the top\nTransformer while requiring only 11.82% of its parameters and 0.81% of its\nFLOPS.\n","authors":["Ricardo Montalvo-Lezama","Berenice Montalvo-Lezama","Gibran Fuentes-Pineda"],"pdf_url":"https://arxiv.org/pdf/2210.07983v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16117v3","updated":"2023-03-29T15:35:55Z","published":"2022-10-28T13:25:59Z","title":"Improving the Transferability of Adversarial Attacks on Face Recognition\n  with Beneficial Perturbation Feature Augmentation","summary":"  Face recognition (FR) models can be easily fooled by adversarial examples,\nwhich are crafted by adding imperceptible perturbations on benign face images.\nThe existence of adversarial face examples poses a great threat to the security\nof society. In order to build a more sustainable digital nation, in this paper,\nwe improve the transferability of adversarial face examples to expose more\nblind spots of existing FR models. Though generating hard samples has shown its\neffectiveness in improving the generalization of models in training tasks, the\neffectiveness of utilizing this idea to improve the transferability of\nadversarial face examples remains unexplored. To this end, based on the\nproperty of hard samples and the symmetry between training tasks and\nadversarial attack tasks, we propose the concept of hard models, which have\nsimilar effects as hard samples for adversarial attack tasks. Utilizing the\nconcept of hard models, we propose a novel attack method called Beneficial\nPerturbation Feature Augmentation Attack (BPFA), which reduces the overfitting\nof adversarial examples to surrogate FR models by constantly generating new\nhard models to craft the adversarial examples. Specifically, in the\nbackpropagation, BPFA records the gradients on pre-selected feature maps and\nuses the gradient on the input image to craft the adversarial example. In the\nnext forward propagation, BPFA leverages the recorded gradients to add\nbeneficial perturbations on their corresponding feature maps to increase the\nloss. Extensive experiments demonstrate that BPFA can significantly boost the\ntransferability of adversarial attacks on FR.\n","authors":["Fengfan Zhou","Hefei Ling","Yuxuan Shi","Jiazhong Chen","Zongyi Li","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2210.16117v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2212.10537v2","updated":"2023-03-29T15:34:23Z","published":"2022-12-20T18:46:28Z","title":"Does CLIP Bind Concepts? Probing Compositionality in Large Image Models","summary":"  Large-scale neural network models combining text and images have made\nincredible progress in recent years. However, it remains an open question to\nwhat extent such models encode compositional representations of the concepts\nover which they operate, such as correctly identifying ''red cube'' by\nreasoning over the constituents ''red'' and ''cube''. In this work, we focus on\nthe ability of a large pretrained vision and language model (CLIP) to encode\ncompositional concepts and to bind variables in a structure-sensitive way\n(e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). In\norder to inspect the performance of CLIP, we compare several architectures from\nresearch on compositional distributional semantics models (CDSMs), a line of\nresearch that attempts to implement traditional compositional linguistic\nstructures within embedding spaces. We find that CLIP can compose concepts in a\nsingle-object setting, but in situations where concept binding is needed,\nperformance drops dramatically. At the same time, CDSMs also perform poorly,\nwith best performance at chance level.\n","authors":["Martha Lewis","Nihal V. Nayak","Peilin Yu","Qinan Yu","Jack Merullo","Stephen H. Bach","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2212.10537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16783v1","updated":"2023-03-29T15:19:01Z","published":"2023-03-29T15:19:01Z","title":"Exploring Asymmetric Tunable Blind-Spots for Self-supervised Denoising\n  in Real-World Scenarios","summary":"  Self-supervised denoising has attracted widespread attention due to its\nability to train without clean images. However, noise in real-world scenarios\nis often spatially correlated, which causes many self-supervised algorithms\nbased on the pixel-wise independent noise assumption to perform poorly on\nreal-world images. Recently, asymmetric pixel-shuffle downsampling (AP) has\nbeen proposed to disrupt the spatial correlation of noise. However,\ndownsampling introduces aliasing effects, and the post-processing to eliminate\nthese effects can destroy the spatial structure and high-frequency details of\nthe image, in addition to being time-consuming. In this paper, we\nsystematically analyze downsampling-based methods and propose an Asymmetric\nTunable Blind-Spot Network (AT-BSN) to address these issues. We design a\nblind-spot network with a freely tunable blind-spot size, using a large\nblind-spot during training to suppress local spatially correlated noise while\nminimizing damage to the global structure, and a small blind-spot during\ninference to minimize information loss. Moreover, we propose blind-spot\nself-ensemble and distillation of non-blind-spot network to further improve\nperformance and reduce computational complexity. Experimental results\ndemonstrate that our method achieves state-of-the-art results while\ncomprehensively outperforming other self-supervised methods in terms of image\ntexture maintaining, parameter count, computation cost, and inference time.\n","authors":["Shiyan Chen","Jiyuan Zhang","Zhaofei Yu","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2303.16783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13654v2","updated":"2023-03-29T15:12:24Z","published":"2023-03-23T20:22:01Z","title":"NEWTON: Neural View-Centric Mapping for On-the-Fly Large-Scale SLAM","summary":"  Neural field-based 3D representations have recently been adopted in many\nareas including SLAM systems. Current neural SLAM or online mapping systems\nlead to impressive results in the presence of simple captures, but they rely on\na world-centric map representation as only a single neural field model is used.\nTo define such a world-centric representation, accurate and static prior\ninformation about the scene, such as its boundaries and initial camera poses,\nare required. However, in real-time and on-the-fly scene capture applications,\nthis prior knowledge cannot be assumed as fixed or static, since it dynamically\nchanges and it is subject to significant updates based on run-time\nobservations. Particularly in the context of large-scale mapping, significant\ncamera pose drift is inevitable, necessitating the correction via loop closure.\nTo overcome this limitation, we propose NEWTON, a view-centric mapping method\nthat dynamically constructs neural fields based on run-time observation. In\ncontrast to prior works, our method enables camera pose updates using loop\nclosures and scene boundary updates by representing the scene with multiple\nneural fields, where each is defined in a local coordinate system of a selected\nkeyframe. The experimental results demonstrate the superior performance of our\nmethod over existing world-centric neural field-based SLAM systems, in\nparticular for large-scale scenes subject to camera pose updates.\n","authors":["Hidenobu Matsuki","Keisuke Tateno","Michael Niemeyer","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2303.13654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16769v1","updated":"2023-03-29T15:00:02Z","published":"2023-03-29T15:00:02Z","title":"Sketch-an-Anchor: Sub-epoch Fast Model Adaptation for Zero-shot\n  Sketch-based Image Retrieval","summary":"  Sketch-an-Anchor is a novel method to train state-of-the-art Zero-shot\nSketch-based Image Retrieval (ZSSBIR) models in under an epoch. Most studies\nbreak down the problem of ZSSBIR into two parts: domain alignment between\nimages and sketches, inherited from SBIR, and generalization to unseen data,\ninherent to the zero-shot protocol. We argue one of these problems can be\nconsiderably simplified and re-frame the ZSSBIR problem around the\nalready-stellar yet underexplored Zero-shot Image-based Retrieval performance\nof off-the-shelf models. Our fast-converging model keeps the single-domain\nperformance while learning to extract similar representations from sketches. To\nthis end we introduce our Semantic Anchors -- guiding embeddings learned from\nword-based semantic spaces and features from off-the-shelf models -- and\ncombine them with our novel Anchored Contrastive Loss. Empirical evidence shows\nwe can achieve state-of-the-art performance on all benchmark datasets while\ntraining for 100x less iterations than other methods.\n","authors":["Leo Sampaio Ferraz Ribeiro","Moacir Antonelli Ponti"],"pdf_url":"https://arxiv.org/pdf/2303.16769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16765v1","updated":"2023-03-29T14:57:54Z","published":"2023-03-29T14:57:54Z","title":"MDP: A Generalized Framework for Text-Guided Image Editing by\n  Manipulating the Diffusion Path","summary":"  Image generation using diffusion can be controlled in multiple ways. In this\npaper, we systematically analyze the equations of modern generative diffusion\nnetworks to propose a framework, called MDP, that explains the design space of\nsuitable manipulations. We identify 5 different manipulations, including\nintermediate latent, conditional embedding, cross attention maps, guidance, and\npredicted noise. We analyze the corresponding parameters of these manipulations\nand the manipulation schedule. We show that some previous editing methods fit\nnicely into our framework. Particularly, we identified one specific\nconfiguration as a new type of control by manipulating the predicted noise,\nwhich can perform higher-quality edits than previous work for a variety of\nlocal and global edits.\n","authors":["Qian Wang","Biao Zhang","Michael Birsak","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2303.16765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07804v2","updated":"2023-03-29T14:44:10Z","published":"2022-11-14T23:50:52Z","title":"Diffusion Models for Medical Image Analysis: A Comprehensive Survey","summary":"  Denoising diffusion models, a class of generative models, have garnered\nimmense interest lately in various deep-learning problems. A diffusion\nprobabilistic model defines a forward diffusion stage where the input data is\ngradually perturbed over several steps by adding Gaussian noise and then learns\nto reverse the diffusion process to retrieve the desired noise-free data from\nnoisy data samples. Diffusion models are widely appreciated for their strong\nmode coverage and quality of the generated samples despite their known\ncomputational burdens. Capitalizing on the advances in computer vision, the\nfield of medical imaging has also observed a growing interest in diffusion\nmodels. To help the researcher navigate this profusion, this survey intends to\nprovide a comprehensive overview of diffusion models in the discipline of\nmedical image analysis. Specifically, we introduce the solid theoretical\nfoundation and fundamental concepts behind diffusion models and the three\ngeneric diffusion modelling frameworks: diffusion probabilistic models,\nnoise-conditioned score networks, and stochastic differential equations. Then,\nwe provide a systematic taxonomy of diffusion models in the medical domain and\npropose a multi-perspective categorization based on their application, imaging\nmodality, organ of interest, and algorithms. To this end, we cover extensive\napplications of diffusion models in the medical domain. Furthermore, we\nemphasize the practical use case of some selected approaches, and then we\ndiscuss the limitations of the diffusion models in the medical domain and\npropose several directions to fulfill the demands of this field. Finally, we\ngather the overviewed studies with their available open-source implementations\nat\nhttps://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging.\n","authors":["Amirhossein Kazerouni","Ehsan Khodapanah Aghdam","Moein Heidari","Reza Azad","Mohsen Fayyaz","Ilker Hacihaliloglu","Dorit Merhof"],"pdf_url":"https://arxiv.org/pdf/2211.07804v2.pdf","comment":"Second revision: including more papers and further discussions"},{"id":"http://arxiv.org/abs/2303.16739v1","updated":"2023-03-29T14:42:30Z","published":"2023-03-29T14:42:30Z","title":"Active Implicit Object Reconstruction using Uncertainty-guided\n  Next-Best-View Optimziation","summary":"  Actively planning sensor views during object reconstruction is essential to\nautonomous mobile robots. This task is usually performed by evaluating\ninformation gain from an explicit uncertainty map. Existing algorithms compare\noptions among a set of preset candidate views and select the next-best-view\nfrom them. In contrast to these, we take the emerging implicit representation\nas the object model and seamlessly combine it with the active reconstruction\ntask. To fully integrate observation information into the model, we propose a\nsupervision method specifically for object-level reconstruction that considers\nboth valid and free space. Additionally, to directly evaluate view information\nfrom the implicit object model, we introduce a sample-based uncertainty\nevaluation method. It samples points on rays directly from the object model and\nuses variations of implicit function inferences as the uncertainty metrics,\nwith no need for voxel traversal or an additional information map. Leveraging\nthe differentiability of our metrics, it is possible to optimize the\nnext-best-view by maximizing the uncertainty continuously. This does away with\nthe traditionally-used candidate views setting, which may provide sub-optimal\nresults. Experiments in simulations and real-world scenes show that our method\neffectively improves the reconstruction accuracy and the view-planning\nefficiency of active reconstruction tasks. The proposed system is going to open\nsource at https://github.com/HITSZ-NRSL/ActiveImplicitRecon.git.\n","authors":["Dongyu Yan","Jianheng Liu","Fengyu Quan","Haoyao Chen","Mengmeng Fu"],"pdf_url":"https://arxiv.org/pdf/2303.16739v1.pdf","comment":"8 pages, 10 figures, Submitted to IEEE Robotics and Automation\n  Letters (RA-L)"},{"id":"http://arxiv.org/abs/2303.16730v1","updated":"2023-03-29T14:34:54Z","published":"2023-03-29T14:34:54Z","title":"TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation","summary":"  Test-time adaptation methods have been gaining attention recently as a\npractical solution for addressing source-to-target domain gaps by gradually\nupdating the model without requiring labels on the target data. In this paper,\nwe propose a method of test-time adaptation for category-level object pose\nestimation called TTA-COPE. We design a pose ensemble approach with a\nself-training loss using pose-aware confidence. Unlike previous unsupervised\ndomain adaptation methods for category-level object pose estimation, our\napproach processes the test data in a sequential, online manner, and it does\nnot require access to the source domain at runtime. Extensive experimental\nresults demonstrate that the proposed pose ensemble and the self-training loss\nimprove category-level object pose performance during test time under both\nsemi-supervised and unsupervised settings. Project page:\nhttps://taeyeop.com/ttacope\n","authors":["Taeyeop Lee","Jonathan Tremblay","Valts Blukis","Bowen Wen","Byeong-Uk Lee","Inkyu Shin","Stan Birchfield","In So Kweon","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2303.16730v1.pdf","comment":"Accepted to CVPR 2023, Project page: https://taeyeop.com/ttacope"},{"id":"http://arxiv.org/abs/2303.16727v1","updated":"2023-03-29T14:28:41Z","published":"2023-03-29T14:28:41Z","title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking","summary":"  Scale is the primary factor for building a powerful foundation model that\ncould well generalize to a variety of downstream tasks. However, it is still\nchallenging to train video foundation models with billions of parameters. This\npaper shows that video masked autoencoder (VideoMAE) is a scalable and general\nself-supervised pre-trainer for building video foundation models. We scale the\nVideoMAE in both model and data with a core design. Specifically, we present a\ndual masking strategy for efficient pre-training, with an encoder operating on\na subset of video tokens and a decoder processing another subset of video\ntokens. Although VideoMAE is very efficient due to high masking ratio in\nencoder, masking decoder can still further reduce the overall computational\ncost. This enables the efficient pre-training of billion-level models in video.\nWe also use a progressive training paradigm that involves an initial\npre-training on a diverse multi-sourced unlabeled dataset, followed by a\npost-pre-training on a mixed labeled dataset. Finally, we successfully train a\nvideo ViT model with a billion parameters, which achieves a new\nstate-of-the-art performance on the datasets of Kinetics (90.0% on K400 and\n89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). In\naddition, we extensively verify the pre-trained video ViT models on a variety\nof downstream tasks, demonstrating its effectiveness as a general video\nrepresentation learner.\n","authors":["Limin Wang","Bingkun Huang","Zhiyu Zhao","Zhan Tong","Yinan He","Yi Wang","Yali Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.16727v1.pdf","comment":"CVPR 2023 camera-ready version"},{"id":"http://arxiv.org/abs/2303.16710v1","updated":"2023-03-29T14:04:59Z","published":"2023-03-29T14:04:59Z","title":"An intelligent modular real-time vision-based system for environment\n  perception","summary":"  A significant portion of driving hazards is caused by human error and\ndisregard for local driving regulations; Consequently, an intelligent\nassistance system can be beneficial. This paper proposes a novel vision-based\nmodular package to ensure drivers' safety by perceiving the environment. Each\nmodule is designed based on accuracy and inference time to deliver real-time\nperformance. As a result, the proposed system can be implemented on a wide\nrange of vehicles with minimum hardware requirements. Our modular package\ncomprises four main sections: lane detection, object detection, segmentation,\nand monocular depth estimation. Each section is accompanied by novel techniques\nto improve the accuracy of others along with the entire system. Furthermore, a\nGUI is developed to display perceived information to the driver. In addition to\nusing public datasets, like BDD100K, we have also collected and annotated a\nlocal dataset that we utilize to fine-tune and evaluate our system. We show\nthat the accuracy of our system is above 80% in all the sections. Our code and\ndata are available at\nhttps://github.com/Pandas-Team/Autonomous-Vehicle-Environment-Perception\n","authors":["Amirhossein Kazerouni","Amirhossein Heydarian","Milad Soltany","Aida Mohammadshahi","Abbas Omidi","Saeed Ebadollahi"],"pdf_url":"https://arxiv.org/pdf/2303.16710v1.pdf","comment":"Accepted in NeurIPS 2022 Workshop on Machine Learning for Autonomous\n  Driving"},{"id":"http://arxiv.org/abs/2212.07834v2","updated":"2023-03-29T14:03:20Z","published":"2022-12-15T13:43:11Z","title":"Unsupervised Object Localization: Observing the Background to Discover\n  Objects","summary":"  Recent advances in self-supervised visual representation learning have paved\nthe way for unsupervised methods tackling tasks such as object discovery and\ninstance segmentation. However, discovering objects in an image with no\nsupervision is a very hard task; what are the desired objects, when to separate\nthem into parts, how many are there, and of what classes? The answers to these\nquestions depend on the tasks and datasets of evaluation. In this work, we take\na different approach and propose to look for the background instead. This way,\nthe salient objects emerge as a by-product without any strong assumption on\nwhat an object should be. We propose FOUND, a simple model made of a single\n$conv1\\times1$ initialized with coarse background masks extracted from\nself-supervised patch-based representations. After fast training and refining\nthese seed masks, the model reaches state-of-the-art results on unsupervised\nsaliency detection and object discovery benchmarks. Moreover, we show that our\napproach yields good results in the unsupervised semantic segmentation\nretrieval task. The code to reproduce our results is available at\nhttps://github.com/valeoai/FOUND.\n","authors":["Oriane Siméoni","Chloé Sekkat","Gilles Puy","Antonin Vobecky","Éloi Zablocki","Patrick Pérez"],"pdf_url":"https://arxiv.org/pdf/2212.07834v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2006.13726v4","updated":"2023-03-29T13:57:28Z","published":"2020-06-24T13:41:37Z","title":"Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial\n  Robustness","summary":"  Evaluating the robustness of a defense model is a challenging task in\nadversarial robustness research. Obfuscated gradients have previously been\nfound to exist in many defense methods and cause a false signal of robustness.\nIn this paper, we identify a more subtle situation called Imbalanced Gradients\nthat can also cause overestimated adversarial robustness. The phenomenon of\nimbalanced gradients occurs when the gradient of one term of the margin loss\ndominates and pushes the attack towards to a suboptimal direction. To exploit\nimbalanced gradients, we formulate a Margin Decomposition (MD) attack that\ndecomposes a margin loss into individual terms and then explores the\nattackability of these terms separately via a two-stage process. We also\npropose a multi-targeted and ensemble version of our MD attack. By\ninvestigating 24 defense models proposed since 2018, we find that 11 models are\nsusceptible to a certain degree of imbalanced gradients and our MD attack can\ndecrease their robustness evaluated by the best standalone baseline attack by\nmore than 1%. We also provide an in-depth investigation on the likely causes of\nimbalanced gradients and effective countermeasures. Our code is available at\nhttps://github.com/HanxunH/MDAttack.\n","authors":["Xingjun Ma","Linxi Jiang","Hanxun Huang","Zejia Weng","James Bailey","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2006.13726v4.pdf","comment":"To appear in Machine Learning"},{"id":"http://arxiv.org/abs/2303.16697v1","updated":"2023-03-29T13:50:01Z","published":"2023-03-29T13:50:01Z","title":"Latent Feature Relation Consistency for Adversarial Robustness","summary":"  Deep neural networks have been applied in many computer vision tasks and\nachieved state-of-the-art performance. However, misclassification will occur\nwhen DNN predicts adversarial examples which add human-imperceptible\nadversarial noise to natural examples. This limits the application of DNN in\nsecurity-critical fields. To alleviate this problem, we first conducted an\nempirical analysis of the latent features of both adversarial and natural\nexamples and found the similarity matrix of natural examples is more compact\nthan those of adversarial examples. Motivated by this observation, we propose\n\\textbf{L}atent \\textbf{F}eature \\textbf{R}elation \\textbf{C}onsistency\n(\\textbf{LFRC}), which constrains the relation of adversarial examples in\nlatent space to be consistent with the natural examples. Importantly, our LFRC\nis orthogonal to the previous method and can be easily combined with them to\nachieve further improvement. To demonstrate the effectiveness of LFRC, we\nconduct extensive experiments using different neural networks on benchmark\ndatasets. For instance, LFRC can bring 0.78\\% further improvement compared to\nAT, and 1.09\\% improvement compared to TRADES, against AutoAttack on CIFAR10.\nCode is available at https://github.com/liuxingbin/LFRC.\n","authors":["Xingbin Liu","Huafeng Kuang","Hong Liu","Xianming Lin","Yongjian Wu","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2303.16697v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2303.16191v2","updated":"2023-03-29T13:44:14Z","published":"2023-03-28T17:54:56Z","title":"Hard Nominal Example-aware Template Mutual Matching for Industrial\n  Anomaly Detection","summary":"  Anomaly detectors are widely used in industrial production to detect and\nlocalize unknown defects in query images. These detectors are trained on\nnominal images and have shown success in distinguishing anomalies from most\nnormal samples. However, hard-nominal examples are scattered and far apart from\nmost normalities, they are often mistaken for anomalies by existing anomaly\ndetectors. To address this problem, we propose a simple yet efficient method:\n\\textbf{H}ard Nominal \\textbf{E}xample-aware \\textbf{T}emplate \\textbf{M}utual\n\\textbf{M}atching (HETMM). Specifically, \\textit{HETMM} aims to construct a\nrobust prototype-based decision boundary, which can precisely distinguish\nbetween hard-nominal examples and anomalies, yielding fewer false-positive and\nmissed-detection rates. Moreover, \\textit{HETMM} mutually explores the\nanomalies in two directions between queries and the template set, and thus it\nis capable to capture the logical anomalies. This is a significant advantage\nover most anomaly detectors that frequently fail to detect logical anomalies.\nAdditionally, to meet the speed-accuracy demands, we further propose\n\\textbf{P}ixel-level \\textbf{T}emplate \\textbf{S}election (PTS) to streamline\nthe original template set. \\textit{PTS} selects cluster centres and\nhard-nominal examples to form a tiny set, maintaining the original decision\nboundaries. Comprehensive experiments on five real-world datasets demonstrate\nthat our methods yield outperformance than existing advances under the\nreal-time inference speed. Furthermore, \\textit{HETMM} can be hot-updated by\ninserting novel samples, which may promptly address some incremental learning\nissues.\n","authors":["Zixuan Chen","Xiaohua Xie","Lingxiao Yang","jianhuang Lai"],"pdf_url":"https://arxiv.org/pdf/2303.16191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09311v2","updated":"2023-03-29T13:31:37Z","published":"2023-02-18T12:01:23Z","title":"Temporal Interpolation Is All You Need for Dynamic Neural Radiance\n  Fields","summary":"  Temporal interpolation often plays a crucial role to learn meaningful\nrepresentations in dynamic scenes. In this paper, we propose a novel method to\ntrain spatiotemporal neural radiance fields of dynamic scenes based on temporal\ninterpolation of feature vectors. Two feature interpolation methods are\nsuggested depending on underlying representations, neural networks or grids. In\nthe neural representation, we extract features from space-time inputs via\nmultiple neural network modules and interpolate them based on time frames. The\nproposed multi-level feature interpolation network effectively captures\nfeatures of both short-term and long-term time ranges. In the grid\nrepresentation, space-time features are learned via four-dimensional hash\ngrids, which remarkably reduces training time. The grid representation shows\nmore than 100 times faster training speed than the previous neural-net-based\nmethods while maintaining the rendering quality. Concatenating static and\ndynamic features and adding a simple smoothness term further improve the\nperformance of our proposed models. Despite the simplicity of the model\narchitectures, our method achieved state-of-the-art performance both in\nrendering quality for the neural representation and in training speed for the\ngrid representation.\n","authors":["Sungheon Park","Minjung Son","Seokhwan Jang","Young Chun Ahn","Ji-Yeon Kim","Nahyup Kang"],"pdf_url":"https://arxiv.org/pdf/2302.09311v2.pdf","comment":"CVPR 2023. Project page:\n  https://sungheonpark.github.io/tempinterpnerf"},{"id":"http://arxiv.org/abs/2303.16666v1","updated":"2023-03-29T13:18:33Z","published":"2023-03-29T13:18:33Z","title":"SC-VAE: Sparse Coding-based Variational Autoencoder","summary":"  Learning rich data representations from unlabeled data is a key challenge\ntowards applying deep learning algorithms in downstream supervised tasks.\nSeveral variants of variational autoencoders have been proposed to learn\ncompact data representaitons by encoding high-dimensional data in a lower\ndimensional space. Two main classes of VAEs methods may be distinguished\ndepending on the characteristics of the meta-priors that are enforced in the\nrepresentation learning step. The first class of methods derives a continuous\nencoding by assuming a static prior distribution in the latent space. The\nsecond class of methods learns instead a discrete latent representation using\nvector quantization (VQ) along with a codebook. However, both classes of\nmethods suffer from certain challenges, which may lead to suboptimal image\nreconstruction results. The first class of methods suffers from posterior\ncollapse, whereas the second class of methods suffers from codebook collapse.\nTo address these challenges, we introduce a new VAE variant, termed SC-VAE\n(sparse coding-based VAE), which integrates sparse coding within variational\nautoencoder framework. Instead of learning a continuous or discrete latent\nrepresentation, the proposed method learns a sparse data representation that\nconsists of a linear combination of a small number of learned atoms. The sparse\ncoding problem is solved using a learnable version of the iterative shrinkage\nthresholding algorithm (ISTA). Experiments on two image datasets demonstrate\nthat our model can achieve improved image reconstruction results compared to\nstate-of-the-art methods. Moreover, the use of learned sparse code vectors\nallows us to perform downstream task like coarse image segmentation through\nclustering image patches.\n","authors":["Pan Xiao","Peijie Qiu","Aristeidis Sotiras"],"pdf_url":"https://arxiv.org/pdf/2303.16666v1.pdf","comment":"15 pages, 11 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2303.14152v2","updated":"2023-03-29T13:13:35Z","published":"2023-03-24T17:03:40Z","title":"Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken\n  Objects and Their Complete Counterparts","summary":"  Automated shape repair approaches currently lack access to datasets that\ndescribe real-world damaged geometry. We present Fantastic Breaks (and Where to\nFind Them:\nhttps://terascale-all-sensing-research-studio.github.io/FantasticBreaks), a\ndataset containing scanned, waterproofed, and cleaned 3D meshes for 150 broken\nobjects, paired and geometrically aligned with complete counterparts. Fantastic\nBreaks contains class and material labels, proxy repair parts that join to\nbroken meshes to generate complete meshes, and manually annotated fracture\nboundaries. Through a detailed analysis of fracture geometry, we reveal\ndifferences between Fantastic Breaks and synthetic fracture datasets generated\nusing geometric and physics-based methods. We show experimental shape repair\nevaluation with Fantastic Breaks using multiple learning-based approaches\npre-trained with synthetic datasets and re-trained with subset of Fantastic\nBreaks.\n","authors":["Nikolas Lamb","Cameron Palmer","Benjamin Molloy","Sean Banerjee","Natasha Kholgade Banerjee"],"pdf_url":"https://arxiv.org/pdf/2303.14152v2.pdf","comment":"To be published at CVPR 2023"},{"id":"http://arxiv.org/abs/2209.11047v3","updated":"2023-03-29T12:59:42Z","published":"2022-09-22T14:43:52Z","title":"MIDMs: Matching Interleaved Diffusion Models for Exemplar-based Image\n  Translation","summary":"  We present a novel method for exemplar-based image translation, called\nmatching interleaved diffusion models (MIDMs). Most existing methods for this\ntask were formulated as GAN-based matching-then-generation framework. However,\nin this framework, matching errors induced by the difficulty of semantic\nmatching across cross-domain, e.g., sketch and photo, can be easily propagated\nto the generation step, which in turn leads to degenerated results. Motivated\nby the recent success of diffusion models overcoming the shortcomings of GANs,\nwe incorporate the diffusion models to overcome these limitations.\nSpecifically, we formulate a diffusion-based matching-and-generation framework\nthat interleaves cross-domain matching and diffusion steps in the latent space\nby iteratively feeding the intermediate warp into the noising process and\ndenoising it to generate a translated image. In addition, to improve the\nreliability of the diffusion process, we design a confidence-aware process\nusing cycle-consistency to consider only confident regions during translation.\nExperimental results show that our MIDMs generate more plausible images than\nstate-of-the-art methods.\n","authors":["Junyoung Seo","Gyuseong Lee","Seokju Cho","Jiyoung Lee","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2209.11047v3.pdf","comment":"Project page https://ku-cvlab.github.io/MIDMs/"},{"id":"http://arxiv.org/abs/2303.16646v1","updated":"2023-03-29T12:57:27Z","published":"2023-03-29T12:57:27Z","title":"Structured Epipolar Matcher for Local Feature Matching","summary":"  Local feature matching is challenging due to the textureless and repetitive\npattern. Existing methods foucs on using appearance features and global\ninteraction and matching, while the importance of geometry prior in local\nfeature matching has not been fully exploited. Different from these methods, in\nthis paper, we delve into the importance of geometry prior and propose\nStructured Epipolar Matcher (SEM) for local feature matching, which can\nleverage the geometric information in a iterative matching way. The proposed\nmodel enjoys several merits. First, our proposed Structured Feature Extractor\ncan model the relative positional relationship between pixels and\nhigh-confidence anchor points. Second, our proposed Epipolar Attention and\nMatching can filter out irrelevant areas by utilizing the epipolar constraint.\nExtensive experimental results on five standard benchmarks demonstrate the\nsuperior performance of our SEM compared to state-of-the-art methods.\n","authors":["Jiahao Chang","Jiahuan Yu"],"pdf_url":"https://arxiv.org/pdf/2303.16646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16637v1","updated":"2023-03-29T12:52:27Z","published":"2023-03-29T12:52:27Z","title":"MuRAL: Multi-Scale Region-based Active Learning for Object Detection","summary":"  Obtaining large-scale labeled object detection dataset can be costly and\ntime-consuming, as it involves annotating images with bounding boxes and class\nlabels. Thus, some specialized active learning methods have been proposed to\nreduce the cost by selecting either coarse-grained samples or fine-grained\ninstances from unlabeled data for labeling. However, the former approaches\nsuffer from redundant labeling, while the latter methods generally lead to\ntraining instability and sampling bias. To address these challenges, we propose\na novel approach called Multi-scale Region-based Active Learning (MuRAL) for\nobject detection. MuRAL identifies informative regions of various scales to\nreduce annotation costs for well-learned objects and improve training\nperformance. The informative region score is designed to consider both the\npredicted confidence of instances and the distribution of each object category,\nenabling our method to focus more on difficult-to-detect classes. Moreover,\nMuRAL employs a scale-aware selection strategy that ensures diverse regions are\nselected from different scales for labeling and downstream finetuning, which\nenhances training stability. Our proposed method surpasses all existing\ncoarse-grained and fine-grained baselines on Cityscapes and MS COCO datasets,\nand demonstrates significant improvement in difficult category performance.\n","authors":["Yi-Syuan Liou","Tsung-Han Wu","Jia-Fong Yeh","Wen-Chin Chen","Winston H. Hsu"],"pdf_url":"https://arxiv.org/pdf/2303.16637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08863v2","updated":"2023-03-29T12:42:48Z","published":"2023-03-15T18:32:52Z","title":"Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield\n  Images with Class Labels","summary":"  Image-to-image reconstruction problems with free or inexpensive metadata in\nthe form of class labels appear often in biological and medical image domains.\nExisting text-guided or style-transfer image-to-image approaches do not\ntranslate to datasets where additional information is provided as discrete\nclasses. We introduce and implement a model which combines image-to-image and\nclass-guided denoising diffusion probabilistic models. We train our model on a\nreal-world dataset of microscopy images used for drug discovery, with and\nwithout incorporating metadata labels. By exploring the properties of\nimage-to-image diffusion with relevant labels, we show that class-guided\nimage-to-image diffusion can improve the meaningful content of the\nreconstructed images and outperform the unguided model in useful downstream\ntasks.\n","authors":["Jan Oscar Cross-Zamirski","Praveen Anand","Guy Williams","Elizabeth Mouchet","Yinhai Wang","Carola-Bibiane Schönlieb"],"pdf_url":"https://arxiv.org/pdf/2303.08863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16628v1","updated":"2023-03-29T12:33:55Z","published":"2023-03-29T12:33:55Z","title":"DORT: Modeling Dynamic Objects in Recurrent for Multi-Camera 3D Object\n  Detection and Tracking","summary":"  Recent multi-camera 3D object detectors usually leverage temporal information\nto construct multi-view stereo that alleviates the ill-posed depth estimation.\nHowever, they typically assume all the objects are static and directly\naggregate features across frames. This work begins with a theoretical and\nempirical analysis to reveal that ignoring the motion of moving objects can\nresult in serious localization bias. Therefore, we propose to model Dynamic\nObjects in RecurrenT (DORT) to tackle this problem. In contrast to previous\nglobal Bird-Eye-View (BEV) methods, DORT extracts object-wise local volumes for\nmotion estimation that also alleviates the heavy computational burden. By\niteratively refining the estimated object motion and location, the preceding\nfeatures can be precisely aggregated to the current frame to mitigate the\naforementioned adverse effects. The simple framework has two significant\nappealing properties. It is flexible and practical that can be plugged into\nmost camera-based 3D object detectors. As there are predictions of object\nmotion in the loop, it can easily track objects across frames according to\ntheir nearest center distances. Without bells and whistles, DORT outperforms\nall the previous methods on the nuScenes detection and tracking benchmarks with\n62.5\\% NDS and 57.6\\% AMOTA, respectively. The source code will be released.\n","authors":["Qing Lian","Tai Wang","Dahua Lin","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2303.16628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14961v2","updated":"2023-03-29T12:31:06Z","published":"2023-03-27T07:52:58Z","title":"Diffusion Denoised Smoothing for Certified and Adversarial Robust\n  Out-Of-Distribution Detection","summary":"  As the use of machine learning continues to expand, the importance of\nensuring its safety cannot be overstated. A key concern in this regard is the\nability to identify whether a given sample is from the training distribution,\nor is an \"Out-Of-Distribution\" (OOD) sample. In addition, adversaries can\nmanipulate OOD samples in ways that lead a classifier to make a confident\nprediction. In this study, we present a novel approach for certifying the\nrobustness of OOD detection within a $\\ell_2$-norm around the input, regardless\nof network architecture and without the need for specific components or\nadditional training. Further, we improve current techniques for detecting\nadversarial attacks on OOD samples, while providing high levels of certified\nand adversarial robustness on in-distribution samples. The average of all OOD\ndetection metrics on CIFAR10/100 shows an increase of $\\sim 13 \\% / 5\\%$\nrelative to previous approaches.\n","authors":["Nicola Franco","Daniel Korth","Jeanette Miriam Lorenz","Karsten Roscher","Stephan Guennemann"],"pdf_url":"https://arxiv.org/pdf/2303.14961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07634v2","updated":"2023-03-29T12:28:03Z","published":"2023-03-14T05:29:34Z","title":"I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via\n  Raytracing in Neural SDFs","summary":"  In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene\nreconstruction and editing using differentiable Monte Carlo raytracing on\nneural signed distance fields (SDFs). Our holistic neural SDF-based framework\njointly recovers the underlying shapes, incident radiance and materials from\nmulti-view images. We introduce a novel bubble loss for fine-grained small\nobjects and error-guided adaptive sampling scheme to largely improve the\nreconstruction quality on large-scale indoor scenes. Further, we propose to\ndecompose the neural radiance field into spatially-varying material of the\nscene as a neural field through surface-based, differentiable Monte Carlo\nraytracing and emitter semantic segmentations, which enables physically based\nand photorealistic scene relighting and editing applications. Through a number\nof qualitative and quantitative experiments, we demonstrate the superior\nquality of our method on indoor scene reconstruction, novel view synthesis, and\nscene editing compared to state-of-the-art baselines.\n","authors":["Jingsen Zhu","Yuchi Huo","Qi Ye","Fujun Luan","Jifan Li","Dianbing Xi","Lisha Wang","Rui Tang","Wei Hua","Hujun Bao","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2303.07634v2.pdf","comment":"Accepted by CVPR 2023, project page:\n  https://jingsenzhu.github.io/i2-sdf"},{"id":"http://arxiv.org/abs/2303.16624v1","updated":"2023-03-29T12:28:01Z","published":"2023-03-29T12:28:01Z","title":"Adaptive Spot-Guided Transformer for Consistent Local Feature Matching","summary":"  Local feature matching aims at finding correspondences between a pair of\nimages. Although current detector-free methods leverage Transformer\narchitecture to obtain an impressive performance, few works consider\nmaintaining local consistency. Meanwhile, most methods struggle with large\nscale variations. To deal with the above issues, we propose Adaptive\nSpot-Guided Transformer (ASTR) for local feature matching, which jointly models\nthe local consistency and scale variations in a unified coarse-to-fine\narchitecture. The proposed ASTR enjoys several merits. First, we design a\nspot-guided aggregation module to avoid interfering with irrelevant areas\nduring feature aggregation. Second, we design an adaptive scaling module to\nadjust the size of grids according to the calculated depth information at fine\nstage. Extensive experimental results on five standard benchmarks demonstrate\nthat our ASTR performs favorably against state-of-the-art methods. Our code\nwill be released on https://astr2023.github.io.\n","authors":["Jiahuan Yu","Jiahao Chang","Jianfeng He","Tianzhu Zhang","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2303.16624v1.pdf","comment":"Accepted to CVPR 2023. Project page: https://astr2023.github.io/"},{"id":"http://arxiv.org/abs/2201.12577v3","updated":"2023-03-29T12:14:21Z","published":"2022-01-29T12:40:19Z","title":"Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving\n  Neural Networks (Inference)","summary":"  In this work, we present a novel matrix-encoding method that is particularly\nconvenient for neural networks to make predictions in a privacy-preserving\nmanner using homomorphic encryption. Based on this encoding method, we\nimplement a convolutional neural network for handwritten image classification\nover encryption. For two matrices $A$ and $B$ to perform homomorphic\nmultiplication, the main idea behind it, in a simple version, is to encrypt\nmatrix $A$ and the transpose of matrix $B$ into two ciphertexts respectively.\nWith additional operations, the homomorphic matrix multiplication can be\ncalculated over encrypted matrices efficiently. For the convolution operation,\nwe in advance span each convolution kernel to a matrix space of the same size\nas the input image so as to generate several ciphertexts, each of which is\nlater used together with the ciphertext encrypting input images for calculating\nsome of the final convolution results. We accumulate all these intermediate\nresults and thus complete the convolution operation.\n  In a public cloud with 40 vCPUs, our convolutional neural network\nimplementation on the MNIST testing dataset takes $\\sim$ 287 seconds to compute\nten likelihoods of 32 encrypted images of size $28 \\times 28$ simultaneously.\nThe data owner only needs to upload one ciphertext ($\\sim 19.8$ MB) encrypting\nthese 32 images to the public cloud.\n","authors":["John Chiang"],"pdf_url":"https://arxiv.org/pdf/2201.12577v3.pdf","comment":"The encoding method we proposed in this work, $\\texttt{Volley\n  Revolver}$, is particularly tailored for privacy-preserving neural networks.\n  There is a good chance that it can be used to assist the private neural\n  networks training, in which case for the backpropagation algorithm of the\n  fully-connected layer the first matrix $A$ is revolved while the second\n  matrix $B$ is settled to be still"},{"id":"http://arxiv.org/abs/2303.16617v1","updated":"2023-03-29T12:05:19Z","published":"2023-03-29T12:05:19Z","title":"NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field\n  Indirect Illumination","summary":"  Inverse rendering methods aim to estimate geometry, materials and\nillumination from multi-view RGB images. In order to achieve better\ndecomposition, recent approaches attempt to model indirect illuminations\nreflected from different materials via Spherical Gaussians (SG), which,\nhowever, tends to blur the high-frequency reflection details. In this paper, we\npropose an end-to-end inverse rendering pipeline that decomposes materials and\nillumination from multi-view images, while considering near-field indirect\nillumination. In a nutshell, we introduce the Monte Carlo sampling based path\ntracing and cache the indirect illumination as neural radiance, enabling a\nphysics-faithful and easy-to-optimize inverse rendering method. To enhance\nefficiency and practicality, we leverage SG to represent the smooth environment\nilluminations and apply importance sampling techniques. To supervise indirect\nilluminations from unobserved directions, we develop a novel radiance\nconsistency constraint between implicit neural radiance and path tracing\nresults of unobserved rays along with the joint optimization of materials and\nilluminations, thus significantly improving the decomposition performance.\nExtensive experiments demonstrate that our method outperforms the\nstate-of-the-art on multiple synthetic and real datasets, especially in terms\nof inter-reflection decomposition.\n","authors":["Haoqian Wu","Zhipeng Hu","Lincheng Li","Yongqiang Zhang","Changjie Fan","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2303.16617v1.pdf","comment":"Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.16616v1","updated":"2023-03-29T12:02:18Z","published":"2023-03-29T12:02:18Z","title":"Nearest Neighbor Based Out-of-Distribution Detection in Remote Sensing\n  Scene Classification","summary":"  Deep learning models for image classification are typically trained under the\n\"closed-world\" assumption with a predefined set of image classes. However, when\nthe models are deployed they may be faced with input images not belonging to\nthe classes encountered during training. This type of scenario is common in\nremote sensing image classification where images come from different geographic\nareas, sensors, and imaging conditions. In this paper we deal with the problem\nof detecting remote sensing images coming from a different distribution\ncompared to the training data - out of distribution images. We propose a\nbenchmark for out of distribution detection in remote sensing scene\nclassification and evaluate detectors based on maximum softmax probability and\nnearest neighbors. The experimental results show convincing advantages of the\nmethod based on nearest neighbors.\n","authors":["Dajana Dimitrić","Mitar Simić","Vladimir Risojević"],"pdf_url":"https://arxiv.org/pdf/2303.16616v1.pdf","comment":"2023 22nd International Symposium INFOTEH-JAHORINA"},{"id":"http://arxiv.org/abs/2303.16611v1","updated":"2023-03-29T11:50:21Z","published":"2023-03-29T11:50:21Z","title":"4D Facial Expression Diffusion Model","summary":"  Facial expression generation is one of the most challenging and long-sought\naspects of character animation, with many interesting applications. The\nchallenging task, traditionally having relied heavily on digital craftspersons,\nremains yet to be explored. In this paper, we introduce a generative framework\nfor generating 3D facial expression sequences (i.e. 4D faces) that can be\nconditioned on different inputs to animate an arbitrary 3D face mesh. It is\ncomposed of two tasks: (1) Learning the generative model that is trained over a\nset of 3D landmark sequences, and (2) Generating 3D mesh sequences of an input\nfacial mesh driven by the generated landmark sequences. The generative model is\nbased on a Denoising Diffusion Probabilistic Model (DDPM), which has achieved\nremarkable success in generative tasks of other domains. While it can be\ntrained unconditionally, its reverse process can still be conditioned by\nvarious condition signals. This allows us to efficiently develop several\ndownstream tasks involving various conditional generation, by using expression\nlabels, text, partial sequences, or simply a facial geometry. To obtain the\nfull mesh deformation, we then develop a landmark-guided encoder-decoder to\napply the geometrical deformation embedded in landmarks on a given facial mesh.\nExperiments show that our model has learned to generate realistic, quality\nexpressions solely from the dataset of relatively small size, improving over\nthe state-of-the-art methods. Videos and qualitative comparisons with other\nmethods can be found at https://github.com/ZOUKaifeng/4DFM. Code and models\nwill be made available upon acceptance.\n","authors":["Kaifeng Zou","Sylvain Faisan","Boyang Yu","Sébastien Valette","Hyewon Seo"],"pdf_url":"https://arxiv.org/pdf/2303.16611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16609v1","updated":"2023-03-29T11:48:46Z","published":"2023-03-29T11:48:46Z","title":"Modified watershed approach for segmentation of complex optical\n  coherence tomographic images","summary":"  Watershed segmentation method has been used in various applications. But many\na times, due to its over-segmentation attributes, it underperforms in several\ntasks where noise is a dominant source. In this study, Optical Coherence\nTomography images have been acquired, and segmentation has been performed to\nanalyse the different regions of fluid filled sacs in a lemon. A modified\nwatershed algorithm has been proposed which gives promising results for\nsegmentation of internal lemon structures.\n","authors":["Maryam Viqar","Violeta Madjarova","Elena Stoykova"],"pdf_url":"https://arxiv.org/pdf/2303.16609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15975v2","updated":"2023-03-29T11:46:22Z","published":"2023-03-28T13:47:16Z","title":"Large-scale Pre-trained Models are Surprisingly Strong in Incremental\n  Novel Class Discovery","summary":"  Discovering novel concepts from unlabelled data and in a continuous manner is\nan important desideratum of lifelong learners. In the literature such problems\nhave been partially addressed under very restricted settings, where either\naccess to labelled data is provided for discovering novel concepts (e.g., NCD)\nor learning occurs for a limited number of incremental steps (e.g.,\nclass-iNCD). In this work we challenge the status quo and propose a more\nchallenging and practical learning paradigm called MSc-iNCD, where learning\noccurs continuously and unsupervisedly, while exploiting the rich priors from\nlarge-scale pre-trained models. To this end, we propose simple baselines that\nare not only resilient under longer learning scenarios, but are surprisingly\nstrong when compared with sophisticated state-of-the-art methods. We conduct\nextensive empirical evaluation on a multitude of benchmarks and show the\neffectiveness of our proposed baselines, which significantly raises the bar.\n","authors":["Mingxuan Liu","Subhankar Roy","Zhun Zhong","Nicu Sebe","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2303.15975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16604v1","updated":"2023-03-29T11:37:41Z","published":"2023-03-29T11:37:41Z","title":"Bi-directional Training for Composed Image Retrieval via Text Prompt\n  Learning","summary":"  Composed image retrieval searches for a target image based on a multi-modal\nuser query comprised of a reference image and modification text describing the\ndesired changes. Existing approaches to solving this challenging task learn a\nmapping from the (reference image, modification text)-pair to an image\nembedding that is then matched against a large image corpus. One area that has\nnot yet been explored is the reverse direction, which asks the question, what\nreference image when modified as describe by the text would produce the given\ntarget image? In this work we propose a bi-directional training scheme that\nleverages such reversed queries and can be applied to existing composed image\nretrieval architectures. To encode the bi-directional query we prepend a\nlearnable token to the modification text that designates the direction of the\nquery and then finetune the parameters of the text embedding module. We make no\nother changes to the network architecture. Experiments on two standard datasets\nshow that our novel approach achieves improved performance over a baseline\nBLIP-based model that itself already achieves state-of-the-art performance.\n","authors":["Zheyuan Liu","Weixuan Sun","Yicong Hong","Damien Teney","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.16604v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.16580v1","updated":"2023-03-29T10:29:25Z","published":"2023-03-29T10:29:25Z","title":"Generalized Relation Modeling for Transformer Tracking","summary":"  Compared with previous two-stream trackers, the recent one-stream tracking\npipeline, which allows earlier interaction between the template and search\nregion, has achieved a remarkable performance gain. However, existing\none-stream trackers always let the template interact with all parts inside the\nsearch region throughout all the encoder layers. This could potentially lead to\ntarget-background confusion when the extracted feature representations are not\nsufficiently discriminative. To alleviate this issue, we propose a generalized\nrelation modeling method based on adaptive token division. The proposed method\nis a generalized formulation of attention-based relation modeling for\nTransformer tracking, which inherits the merits of both previous two-stream and\none-stream pipelines whilst enabling more flexible relation modeling by\nselecting appropriate search tokens to interact with template tokens. An\nattention masking strategy and the Gumbel-Softmax technique are introduced to\nfacilitate the parallel computation and end-to-end learning of the token\ndivision module. Extensive experiments show that our method is superior to the\ntwo-stream and one-stream pipelines and achieves state-of-the-art performance\non six challenging benchmarks with a real-time running speed.\n","authors":["Shenyuan Gao","Chunluan Zhou","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.16580v1.pdf","comment":"Accepted by CVPR 2023. Code and models are publicly available at\n  https://github.com/Little-Podi/GRM"},{"id":"http://arxiv.org/abs/2302.12248v2","updated":"2023-03-29T10:23:40Z","published":"2023-02-23T18:59:05Z","title":"Learning Visual Representations via Language-Guided Sampling","summary":"  Although an object may appear in numerous contexts, we often describe it in a\nlimited number of ways. Language allows us to abstract away visual variation to\nrepresent and communicate concepts. Building on this intuition, we propose an\nalternative approach to visual representation learning: using language\nsimilarity to sample semantically similar image pairs for contrastive learning.\nOur approach diverges from image-based contrastive learning by sampling view\npairs using language similarity instead of hand-crafted augmentations or\nlearned clusters. Our approach also differs from image-text contrastive\nlearning by relying on pre-trained language models to guide the learning rather\nthan directly minimizing a cross-modal loss. Through a series of experiments,\nwe show that language-guided learning yields better features than image-based\nand image-text representation learning approaches.\n","authors":["Mohamed El Banani","Karan Desai","Justin Johnson"],"pdf_url":"https://arxiv.org/pdf/2302.12248v2.pdf","comment":"Accepted to CVPR 2023. v2 is camera-ready version with additional\n  ImageNet evaluations. Project page: https://github.com/mbanani/lgssl"},{"id":"http://arxiv.org/abs/2303.16576v1","updated":"2023-03-29T10:19:26Z","published":"2023-03-29T10:19:26Z","title":"WordStylist: Styled Verbatim Handwritten Text Generation with Latent\n  Diffusion Models","summary":"  Text-to-Image synthesis is the task of generating an image according to a\nspecific text description. Generative Adversarial Networks have been considered\nthe standard method for image synthesis virtually since their introduction;\ntoday, Denoising Diffusion Probabilistic Models are recently setting a new\nbaseline, with remarkable results in Text-to-Image synthesis, among other\nfields. Aside its usefulness per se, it can also be particularly relevant as a\ntool for data augmentation to aid training models for other document image\nprocessing tasks. In this work, we present a latent diffusion-based method for\nstyled text-to-text-content-image generation on word-level. Our proposed method\nmanages to generate realistic word image samples from different writer styles,\nby using class index styles and text content prompts without the need of\nadversarial training, writer recognition, or text recognition. We gauge system\nperformance with Frechet Inception Distance, writer recognition accuracy, and\nwriter retrieval. We show that the proposed model produces samples that are\naesthetically pleasing, help boosting text recognition performance, and gets\nsimilar writer retrieval score as real data.\n","authors":["Konstantina Nikolaidou","George Retsinas","Vincent Christlein","Mathias Seuret","Giorgos Sfikas","Elisa Barney Smith","Hamam Mokayed","Marcus Liwicki"],"pdf_url":"https://arxiv.org/pdf/2303.16576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16574v1","updated":"2023-03-29T10:16:55Z","published":"2023-03-29T10:16:55Z","title":"FEND: A Future Enhanced Distribution-Aware Contrastive Learning\n  Framework for Long-tail Trajectory Prediction","summary":"  Predicting the future trajectories of the traffic agents is a gordian\ntechnique in autonomous driving. However, trajectory prediction suffers from\ndata imbalance in the prevalent datasets, and the tailed data is often more\ncomplicated and safety-critical. In this paper, we focus on dealing with the\nlong-tail phenomenon in trajectory prediction. Previous methods dealing with\nlong-tail data did not take into account the variety of motion patterns in the\ntailed data. In this paper, we put forward a future enhanced contrastive\nlearning framework to recognize tail trajectory patterns and form a feature\nspace with separate pattern clusters. Furthermore, a distribution aware hyper\npredictor is brought up to better utilize the shaped feature space. Our method\nis a model-agnostic framework and can be plugged into many well-known\nbaselines. Experimental results show that our framework outperforms the\nstate-of-the-art long-tail prediction method on tailed samples by 9.5% on ADE\nand 8.5% on FDE, while maintaining or slightly improving the averaged\nperformance. Our method also surpasses many long-tail techniques on trajectory\nprediction task.\n","authors":["Yuning Wang","Pu Zhang","Lei Bai","Jianru Xue"],"pdf_url":"https://arxiv.org/pdf/2303.16574v1.pdf","comment":"Accepted for publication at the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition 2023 (CVPR 2023)"},{"id":"http://arxiv.org/abs/2303.16570v1","updated":"2023-03-29T10:08:29Z","published":"2023-03-29T10:08:29Z","title":"Point2Vec for Self-Supervised Representation Learning on Point Clouds","summary":"  Recently, the self-supervised learning framework data2vec has shown inspiring\nperformance for various modalities using a masked student-teacher approach.\nHowever, it remains open whether such a framework generalizes to the unique\nchallenges of 3D point clouds. To answer this question, we extend data2vec to\nthe point cloud domain and report encouraging results on several downstream\ntasks. In an in-depth analysis, we discover that the leakage of positional\ninformation reveals the overall object shape to the student even under heavy\nmasking and thus hampers data2vec to learn strong representations for point\nclouds. We address this 3D-specific shortcoming by proposing point2vec, which\nunleashes the full potential of data2vec-like pre-training on point clouds. Our\nexperiments show that point2vec outperforms other self-supervised methods on\nshape classification and few-shot learning on ModelNet40 and ScanObjectNN,\nwhile achieving competitive results on part segmentation on ShapeNetParts.\nThese results suggest that the learned representations are strong and\ntransferable, highlighting point2vec as a promising direction for\nself-supervised learning of point cloud representations.\n","authors":["Karim Abou Zeid","Jonas Schult","Alexander Hermans","Bastian Leibe"],"pdf_url":"https://arxiv.org/pdf/2303.16570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04428v2","updated":"2023-03-29T10:05:04Z","published":"2022-10-10T04:19:53Z","title":"A Simple Baseline that Questions the Use of Pretrained-Models in\n  Continual Learning","summary":"  With the success of pretraining techniques in representation learning, a\nnumber of continual learning methods based on pretrained models have been\nproposed. Some of these methods design continual learning mechanisms on the\npre-trained representations and only allow minimum updates or even no updates\nof the backbone models during the training of continual learning. In this\npaper, we question whether the complexity of these models is needed to achieve\ngood performance by comparing them to a simple baseline that we designed. We\nargue that the pretrained feature extractor itself can be strong enough to\nachieve a competitive or even better continual learning performance on\nSplit-CIFAR100 and CoRe 50 benchmarks. To validate this, we conduct a very\nsimple baseline that 1) use the frozen pretrained model to extract image\nfeatures for every class encountered during the continual learning stage and\ncompute their corresponding mean features on training data, and 2) predict the\nclass of the input based on the nearest neighbor distance between test samples\nand mean features of the classes; i.e., Nearest Mean Classifier (NMC). This\nbaseline is single-headed, exemplar-free, and can be task-free (by updating the\nmeans continually). This baseline achieved 88.53% on 10-Split-CIFAR-100,\nsurpassing most state-of-the-art continual learning methods that are all\ninitialized using the same pretrained transformer model. We hope our baseline\nmay encourage future progress in designing learning systems that can\ncontinually add quality to the learning representations even if they started\nfrom some pretrained weights.\n","authors":["Paul Janson","Wenxuan Zhang","Rahaf Aljundi","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2210.04428v2.pdf","comment":"6 pages, Workshop on Distribution Shifts 2022 , Code available at\n  https://github.com/Pauljanson002/pretrained-cl.git"},{"id":"http://arxiv.org/abs/2303.16565v1","updated":"2023-03-29T09:47:48Z","published":"2023-03-29T09:47:48Z","title":"PMAA: A Progressive Multi-scale Attention Autoencoder Model for\n  High-Performance Cloud Removal from Multi-temporal Satellite Imagery","summary":"  Satellite imagery analysis plays a vital role in remote sensing, but the\ninformation loss caused by cloud cover seriously hinders its application. This\nstudy presents a high-performance cloud removal architecture called Progressive\nMulti-scale Attention Autoencoder (PMAA), which simultaneously leverages global\nand local information. It mainly consists of a cloud detection backbone and a\ncloud removal module. The cloud detection backbone uses cloud masks to\nreinforce cloudy areas to prompt the cloud removal module. The cloud removal\nmodule mainly comprises a novel Multi-scale Attention Module (MAM) and a Local\nInteraction Module (LIM). PMAA establishes the long-range dependency of\nmulti-scale features using MAM and modulates the reconstruction of the\nfine-grained details using LIM, allowing for the simultaneous representation of\nfine- and coarse-grained features at the same level. With the help of diverse\nand multi-scale feature representation, PMAA outperforms the previous\nstate-of-the-art model CTGAN consistently on the Sen2_MTC_Old and Sen2_MTC_New\ndatasets. Furthermore, PMAA has a considerable efficiency advantage, with only\n0.5% and 14.6% of the parameters and computational complexity of CTGAN,\nrespectively. These extensive results highlight the potential of PMAA as a\nlightweight cloud removal network suitable for deployment on edge devices. We\nwill release the code and trained models to facilitate the study in this\ndirection.\n","authors":["Xuechao Zou","Kai Li","Junliang Xing","Pin Tao","Yachao Cui"],"pdf_url":"https://arxiv.org/pdf/2303.16565v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.16564v1","updated":"2023-03-29T09:47:35Z","published":"2023-03-29T09:47:35Z","title":"Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a\n  Bayesian Neural Network","summary":"  The fairness of a deep neural network is strongly affected by dataset bias\nand spurious correlations, both of which are usually present in modern\nfeature-rich and complex visual datasets. Due to the difficulty and variability\nof the task, no single de-biasing method has been universally successful. In\nparticular, implicit methods not requiring explicit knowledge of bias variables\nare especially relevant for real-world applications. We propose a novel\nimplicit mitigation method using a Bayesian neural network, allowing us to\nleverage the relationship between epistemic uncertainties and the presence of\nbias or spurious correlations in a sample. Our proposed posterior estimate\nsharpening procedure encourages the network to focus on core features that do\nnot contribute to high uncertainties. Experimental results on three benchmark\ndatasets demonstrate that Bayesian networks with sharpened posterior estimates\nperform comparably to prior existing methods and show potential worthy of\nfurther exploration.\n","authors":["Rebecca S Stone","Nishant Ravikumar","Andrew J Bulpitt","David C Hogg"],"pdf_url":"https://arxiv.org/pdf/2303.16564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16557v1","updated":"2023-03-29T09:26:54Z","published":"2023-03-29T09:26:54Z","title":"Self-accumulative Vision Transformer for Bone Age Assessment Using the\n  Sauvegrain Method","summary":"  This study presents a novel approach to bone age assessment (BAA) using a\nmulti-view, multi-task classification model based on the Sauvegrain method. A\nstraightforward solution to automating the Sauvegrain method, which assesses a\nmaturity score for each landmark in the elbow and predicts the bone age, is to\ntrain classifiers independently to score each region of interest (RoI), but\nthis approach limits the accessible information to local morphologies and\nincreases computational costs. As a result, this work proposes a\nself-accumulative vision transformer (SAT) that mitigates anisotropic behavior,\nwhich usually occurs in multi-view, multi-task problems and limits the\neffectiveness of a vision transformer, by applying token replay and regional\nattention bias. A number of experiments show that SAT successfully exploits the\nrelationships between landmarks and learns global morphological features,\nresulting in a mean absolute error of BAA that is 0.11 lower than that of the\nprevious work. Additionally, the proposed SAT has four times reduced parameters\nthan an ensemble of individual classifiers of the previous work. Lastly, this\nwork also provides informative implications for clinical practice, improving\nthe accuracy and efficiency of BAA in diagnosing abnormal growth in\nadolescents.\n","authors":["Hong-Jun Choi","Dongbin Na","Kyungjin Cho","Byunguk Bae","Seo Taek Kong","Hyunjoon Ah"],"pdf_url":"https://arxiv.org/pdf/2303.16557v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2210.02992v4","updated":"2023-03-29T09:18:05Z","published":"2022-10-06T15:34:29Z","title":"COVID-19 Detection Using Segmentation, Region Extraction and\n  Classification Pipeline","summary":"  The main purpose of this study is to develop a pipeline for COVID-19\ndetection from a big and challenging database of Computed Tomography (CT)\nimages. The proposed pipeline includes a segmentation part, a lung extraction\npart, and a classifier part. Optional slice removal techniques after UNet-based\nsegmentation of slices were also tried. The methodologies tried in the\nsegmentation part are traditional segmentation methods as well as UNet-based\nmethods. In the classification part, a Convolutional Neural Network (CNN) was\nused to take the final diagnosis decisions. In terms of the results: in the\nsegmentation part, the proposed segmentation methods show high dice scores on a\npublicly available dataset. In the classification part, the results were\ncompared at slice-level and at patient-level as well. At slice-level, methods\nwere compared and showed high validation accuracy indicating efficiency in\npredicting 2D slices. At patient level, the proposed methods were also compared\nin terms of validation accuracy and macro F1 score on the validation set. The\ndataset used for classification is COV-19CT Database. The method proposed here\nshowed improvement from our precious results on the same dataset. In\nConclusion, the improved work in this paper has potential clinical usages for\nCOVID-19 detection and diagnosis via CT images. The code is on github at\nhttps://github.com/IDU-CVLab/COV19D_3rd\n","authors":["Kenan Morani"],"pdf_url":"https://arxiv.org/pdf/2210.02992v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15932v2","updated":"2023-03-29T09:11:51Z","published":"2023-03-28T12:42:12Z","title":"Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology\n  Report Generation","summary":"  Automatic radiology report generation has attracted enormous research\ninterest due to its practical value in reducing the workload of radiologists.\nHowever, simultaneously establishing global correspondences between the image\n(e.g., Chest X-ray) and its related report and local alignments between image\npatches and keywords remains challenging. To this end, we propose an Unify,\nAlign and then Refine (UAR) approach to learn multi-level cross-modal\nalignments and introduce three novel modules: Latent Space Unifier (LSU),\nCross-modal Representation Aligner (CRA) and Text-to-Image Refiner (TIR).\nSpecifically, LSU unifies multimodal data into discrete tokens, making it\nflexible to learn common knowledge among modalities with a shared network. The\nmodality-agnostic CRA learns discriminative features via a set of orthonormal\nbasis and a dual-gate mechanism first and then globally aligns visual and\ntextual representations under a triplet contrastive loss. TIR boosts\ntoken-level local alignment via calibrating text-to-image attention with a\nlearnable mask. Additionally, we design a two-stage training procedure to make\nUAR gradually grasp cross-modal alignments at different levels, which imitates\nradiologists' workflow: writing sentence by sentence first and then checking\nword by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXR\nbenchmark datasets demonstrate the superiority of our UAR against varied\nstate-of-the-art methods.\n","authors":["Yaowei Li","Bang Yang","Xuxin Cheng","Zhihong Zhu","Hongxiang Li","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2303.15932v2.pdf","comment":"8 pages,6 figures,4 tables"},{"id":"http://arxiv.org/abs/2303.16541v1","updated":"2023-03-29T09:07:31Z","published":"2023-03-29T09:07:31Z","title":"Sounding Video Generator: A Unified Framework for Text-guided Sounding\n  Video Generation","summary":"  As a combination of visual and audio signals, video is inherently\nmulti-modal. However, existing video generation methods are primarily intended\nfor the synthesis of visual frames, whereas audio signals in realistic videos\nare disregarded. In this work, we concentrate on a rarely investigated problem\nof text guided sounding video generation and propose the Sounding Video\nGenerator (SVG), a unified framework for generating realistic videos along with\naudio signals. Specifically, we present the SVG-VQGAN to transform visual\nframes and audio melspectrograms into discrete tokens. SVG-VQGAN applies a\nnovel hybrid contrastive learning method to model inter-modal and intra-modal\nconsistency and improve the quantized representations. A cross-modal attention\nmodule is employed to extract associated features of visual frames and audio\nsignals for contrastive learning. Then, a Transformer-based decoder is used to\nmodel associations between texts, visual frames, and audio signals at token\nlevel for auto-regressive sounding video generation. AudioSetCap, a human\nannotated text-video-audio paired dataset, is produced for training SVG.\nExperimental results demonstrate the superiority of our method when compared\nwith existing textto-video generation methods as well as audio generation\nmethods on Kinetics and VAS datasets.\n","authors":["Jiawei Liu","Weining Wang","Sihan Chen","Xinxin Zhu","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2303.16541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04231v2","updated":"2023-03-29T08:48:35Z","published":"2022-12-08T12:28:23Z","title":"Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level\n  Natural Language Explanations","summary":"  Natural language explanations promise to offer intuitively understandable\nexplanations of a neural network's decision process in complex vision-language\ntasks, as pursued in recent VL-NLE models. While current models offer\nimpressive performance on task accuracy and explanation plausibility, they\nsuffer from a range of issues: Some models feature a modular design where the\nexplanation generation module is poorly integrated with a separate module for\ntask-answer prediction, employ backbone models trained on limited sets of\ntasks, or incorporate ad hoc solutions to increase performance on single\ndatasets. We propose to evade these limitations by applying recent advances in\nlarge-scale multi-task pretraining of generative Transformer models to the\nproblem of VL-NLE tasks. Our approach outperforms recent models by a large\nmargin, with human annotators preferring the generated explanations over the\nground truth in two out of three evaluated datasets. As a novel challenge in\nVL-NLE research, we propose the problem of multi-task VL-NLE and show that\njointly training on multiple tasks can increase the explanation quality. We\ndiscuss the ethical implications of high-quality NLE generation and other\nissues in recent VL-NLE research.\n","authors":["Björn Plüster","Jakob Ambsdorf","Lukas Braach","Jae Hee Lee","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2212.04231v2.pdf","comment":"Minor changes"},{"id":"http://arxiv.org/abs/2303.16533v1","updated":"2023-03-29T08:41:22Z","published":"2023-03-29T08:41:22Z","title":"Robust Tumor Detection from Coarse Annotations via Multi-Magnification\n  Ensembles","summary":"  Cancer detection and classification from gigapixel whole slide images of\nstained tissue specimens has recently experienced enormous progress in\ncomputational histopathology. The limitation of available pixel-wise annotated\nscans shifted the focus from tumor localization to global slide-level\nclassification on the basis of (weakly-supervised) multiple-instance learning\ndespite the clinical importance of local cancer detection. However, the worse\nperformance of these techniques in comparison to fully supervised methods has\nlimited their usage until now for diagnostic interventions in domains of\nlife-threatening diseases such as cancer. In this work, we put the focus back\non tumor localization in form of a patch-level classification task and take up\nthe setting of so-called coarse annotations, which provide greater training\nsupervision while remaining feasible from a clinical standpoint. To this end,\nwe present a novel ensemble method that not only significantly improves the\ndetection accuracy of metastasis on the open CAMELYON16 data set of sentinel\nlymph nodes of breast cancer patients, but also considerably increases its\nrobustness against noise while training on coarse annotations. Our experiments\nshow that better results can be achieved with our technique making it\nclinically feasible to use for cancer diagnosis and opening a new avenue for\ntranslational and clinical research.\n","authors":["Mehdi Naouar","Gabriel Kalweit","Ignacio Mastroleo","Philipp Poxleitner","Marc Metzger","Joschka Boedecker","Maria Kalweit"],"pdf_url":"https://arxiv.org/pdf/2303.16533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16531v1","updated":"2023-03-29T08:38:55Z","published":"2023-03-29T08:38:55Z","title":"RusTitW: Russian Language Text Dataset for Visual Text in-the-Wild\n  Recognition","summary":"  Information surrounds people in modern life. Text is a very efficient type of\ninformation that people use for communication for centuries. However, automated\ntext-in-the-wild recognition remains a challenging problem. The major\nlimitation for a DL system is the lack of training data. For the competitive\nperformance, training set must contain many samples that replicate the\nreal-world cases. While there are many high-quality datasets for English text\nrecognition; there are no available datasets for Russian language. In this\npaper, we present a large-scale human-labeled dataset for Russian text\nrecognition in-the-wild. We also publish a synthetic dataset and code to\nreproduce the generation process\n","authors":["Igor Markov","Sergey Nesteruk","Andrey Kuznetsov","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2303.16531v1.pdf","comment":"5 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.16527v1","updated":"2023-03-29T08:32:16Z","published":"2023-03-29T08:32:16Z","title":"Understanding and Improving Features Learned in Deep Functional Maps","summary":"  Deep functional maps have recently emerged as a successful paradigm for\nnon-rigid 3D shape correspondence tasks. An essential step in this pipeline\nconsists in learning feature functions that are used as constraints to solve\nfor a functional map inside the network. However, the precise nature of the\ninformation learned and stored in these functions is not yet well understood.\nSpecifically, a major question is whether these features can be used for any\nother objective, apart from their purely algebraic role in solving for\nfunctional map matrices. In this paper, we show that under some mild\nconditions, the features learned within deep functional map approaches can be\nused as point-wise descriptors and thus are directly comparable across\ndifferent shapes, even without the necessity of solving for a functional map at\ntest time. Furthermore, informed by our analysis, we propose effective\nmodifications to the standard deep functional map pipeline, which promote\nstructural properties of learned features, significantly improving the matching\nresults. Finally, we demonstrate that previously unsuccessful attempts at using\nextrinsic architectures for deep functional map feature extraction can be\nremedied via simple architectural changes, which encourage the theoretical\nproperties suggested by our analysis. We thus bridge the gap between intrinsic\nand extrinsic surface-based learning, suggesting the necessary and sufficient\nconditions for successful shape matching. Our code is available at\nhttps://github.com/pvnieo/clover.\n","authors":["Souhaib Attaiki","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2303.16527v1.pdf","comment":"16 pages, 8 figures, 8 tables, to be published in 2023 The IEEE\n  Conference on Computer Vision and Pattern Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2303.16526v1","updated":"2023-03-29T08:28:45Z","published":"2023-03-29T08:28:45Z","title":"HybridPoint: Point Cloud Registration Based on Hybrid Point Sampling and\n  Matching","summary":"  Patch-to-point matching has become a robust way of point cloud registration.\nHowever, previous patch-matching methods employ superpoints with poor\nlocalization precision as nodes, which may lead to ambiguous patch partitions.\nIn this paper, we propose a HybridPoint-based network to find more robust and\naccurate correspondences. Firstly, we propose to use salient points with\nprominent local features as nodes to increase patch repeatability, and\nintroduce some uniformly distributed points to complete the point cloud, thus\nconstituting hybrid points. Hybrid points not only have better localization\nprecision but also give a complete picture of the whole point cloud.\nFurthermore, based on the characteristic of hybrid points, we propose a\ndual-classes patch matching module, which leverages the matching results of\nsalient points and filters the matching noise of non-salient points.\nExperiments show that our model achieves state-of-the-art performance on\n3DMatch, 3DLoMatch, and KITTI odometry, especially with 93.0% Registration\nRecall on the 3DMatch dataset. Our code and models are available at\nhttps://github.com/liyih/HybridPoint.\n","authors":["Yiheng Li","Canhui Tang","Runzhao Yao","Aixue Ye","Feng Wen","Shaoyi Du"],"pdf_url":"https://arxiv.org/pdf/2303.16526v1.pdf","comment":"Accepted by IEEE International Conference on Multimedia and Expo\n  (ICME), 2023"},{"id":"http://arxiv.org/abs/2303.16522v1","updated":"2023-03-29T08:24:27Z","published":"2023-03-29T08:24:27Z","title":"Development of a deep learning-based tool to assist wound classification","summary":"  This paper presents a deep learning-based wound classification tool that can\nassist medical personnel in non-wound care specialization to classify five key\nwound conditions, namely deep wound, infected wound, arterial wound, venous\nwound, and pressure wound, given color images captured using readily available\ncameras. The accuracy of the classification is vital for appropriate wound\nmanagement. The proposed wound classification method adopts a multi-task deep\nlearning framework that leverages the relationships among the five key wound\nconditions for a unified wound classification architecture. With differences in\nCohen's kappa coefficients as the metrics to compare our proposed model with\nhumans, the performance of our model was better or non-inferior to those of all\nhuman medical personnel. Our convolutional neural network-based model is the\nfirst to classify five tasks of deep, infected, arterial, venous, and pressure\nwounds simultaneously with good accuracy. The proposed model is compact and\nmatches or exceeds the performance of human doctors and nurses. Medical\npersonnel who do not specialize in wound care can potentially benefit from an\napp equipped with the proposed deep learning model.\n","authors":["Po-Hsuan Huang","Yi-Hsiang Pan","Ying-Sheng Luo","Yi-Fan Chen","Yu-Cheng Lo","Trista Pei-Chun Chen","Cherng-Kang Perng"],"pdf_url":"https://arxiv.org/pdf/2303.16522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16520v1","updated":"2023-03-29T08:21:54Z","published":"2023-03-29T08:21:54Z","title":"Fair Federated Medical Image Segmentation via Client Contribution\n  Estimation","summary":"  How to ensure fairness is an important topic in federated learning (FL).\nRecent studies have investigated how to reward clients based on their\ncontribution (collaboration fairness), and how to achieve uniformity of\nperformance across clients (performance fairness). Despite achieving progress\non either one, we argue that it is critical to consider them together, in order\nto engage and motivate more diverse clients joining FL to derive a high-quality\nglobal model. In this work, we propose a novel method to optimize both types of\nfairness simultaneously. Specifically, we propose to estimate client\ncontribution in gradient and data space. In gradient space, we monitor the\ngradient direction differences of each client with respect to others. And in\ndata space, we measure the prediction error on client data using an auxiliary\nmodel. Based on this contribution estimation, we propose a FL method, federated\ntraining via contribution estimation (FedCE), i.e., using estimation as global\nmodel aggregation weights. We have theoretically analyzed our method and\nempirically evaluated it on two real-world medical datasets. The effectiveness\nof our approach has been validated with significant performance improvements,\nbetter collaboration fairness, better performance fairness, and comprehensive\nanalytical studies.\n","authors":["Meirui Jiang","Holger R Roth","Wenqi Li","Dong Yang","Can Zhao","Vishwesh Nath","Daguang Xu","Qi Dou","Ziyue Xu"],"pdf_url":"https://arxiv.org/pdf/2303.16520v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2212.06512v2","updated":"2023-03-29T08:03:18Z","published":"2022-12-13T11:52:33Z","title":"DifFace: Blind Face Restoration with Diffused Error Contraction","summary":"  While deep learning-based methods for blind face restoration have achieved\nunprecedented success, they still suffer from two major limitations. First,\nmost of them deteriorate when facing complex degradations out of their training\ndata. Second, these methods require multiple constraints, e.g., fidelity,\nperceptual, and adversarial losses, which require laborious hyper-parameter\ntuning to stabilize and balance their influences. In this work, we propose a\nnovel method named DifFace that is capable of coping with unseen and complex\ndegradations more gracefully without complicated loss designs. The key of our\nmethod is to establish a posterior distribution from the observed low-quality\n(LQ) image to its high-quality (HQ) counterpart. In particular, we design a\ntransition distribution from the LQ image to the intermediate state of a\npre-trained diffusion model and then gradually transmit from this intermediate\nstate to the HQ target by recursively applying a pre-trained diffusion model.\nThe transition distribution only relies on a restoration backbone that is\ntrained with $L_2$ loss on some synthetic data, which favorably avoids the\ncumbersome training process in existing methods. Moreover, the transition\ndistribution can contract the error of the restoration backbone and thus makes\nour method more robust to unknown degradations. Comprehensive experiments show\nthat DifFace is superior to current state-of-the-art methods, especially in\ncases with severe degradations. Code and model are available at\nhttps://github.com/zsyOAOA/DifFace.\n","authors":["Zongsheng Yue","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2212.06512v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2303.08648v2","updated":"2023-03-29T07:51:05Z","published":"2023-03-15T14:24:01Z","title":"An End-to-End Multi-Task Learning Model for Image-based Table\n  Recognition","summary":"  Image-based table recognition is a challenging task due to the diversity of\ntable styles and the complexity of table structures. Most of the previous\nmethods focus on a non-end-to-end approach which divides the problem into two\nseparate sub-problems: table structure recognition; and cell-content\nrecognition and then attempts to solve each sub-problem independently using two\nseparate systems. In this paper, we propose an end-to-end multi-task learning\nmodel for image-based table recognition. The proposed model consists of one\nshared encoder, one shared decoder, and three separate decoders which are used\nfor learning three sub-tasks of table recognition: table structure recognition,\ncell detection, and cell-content recognition. The whole system can be easily\ntrained and inferred in an end-to-end approach. In the experiments, we evaluate\nthe performance of the proposed model on two large-scale datasets: FinTabNet\nand PubTabNet. The experiment results show that the proposed model outperforms\nthe state-of-the-art methods in all benchmark datasets.\n","authors":["Nam Tuan Ly","Atsuhiro Takasu"],"pdf_url":"https://arxiv.org/pdf/2303.08648v2.pdf","comment":"10 pages, VISAPP2023. arXiv admin note: substantial text overlap with\n  arXiv:2303.07641"},{"id":"http://arxiv.org/abs/2212.10878v3","updated":"2023-03-29T07:45:01Z","published":"2022-12-21T09:41:25Z","title":"Automatic Network Adaptation for Ultra-Low Uniform-Precision\n  Quantization","summary":"  Uniform-precision neural network quantization has gained popularity since it\nsimplifies densely packed arithmetic unit for high computing capability.\nHowever, it ignores heterogeneous sensitivity to the impact of quantization\nerrors across the layers, resulting in sub-optimal inference accuracy. This\nwork proposes a novel neural architecture search called neural channel\nexpansion that adjusts the network structure to alleviate accuracy degradation\nfrom ultra-low uniform-precision quantization. The proposed method selectively\nexpands channels for the quantization sensitive layers while satisfying\nhardware constraints (e.g., FLOPs, PARAMs). Based on in-depth analysis and\nexperiments, we demonstrate that the proposed method can adapt several popular\nnetworks channels to achieve superior 2-bit quantization accuracy on CIFAR10\nand ImageNet. In particular, we achieve the best-to-date Top-1/Top-5 accuracy\nfor 2-bit ResNet50 with smaller FLOPs and the parameter size.\n","authors":["Seongmin Park","Beomseok Kwon","Jieun Lim","Kyuyoung Sim","Tae-Ho Kim","Jungwook Choi"],"pdf_url":"https://arxiv.org/pdf/2212.10878v3.pdf","comment":"Accepted as a full paper by the TinyML Research Symposium 2023"},{"id":"http://arxiv.org/abs/2303.16513v1","updated":"2023-03-29T07:41:56Z","published":"2023-03-29T07:41:56Z","title":"Cascaded Local Implicit Transformer for Arbitrary-Scale Super-Resolution","summary":"  Implicit neural representation has recently shown a promising ability in\nrepresenting images with arbitrary resolutions. In this paper, we present a\nLocal Implicit Transformer (LIT), which integrates the attention mechanism and\nfrequency encoding technique into a local implicit image function. We design a\ncross-scale local attention block to effectively aggregate local features. To\nfurther improve representative power, we propose a Cascaded LIT (CLIT) that\nexploits multi-scale features, along with a cumulative training strategy that\ngradually increases the upsampling scales during training. We have conducted\nextensive experiments to validate the effectiveness of these components and\nanalyze various training strategies. The qualitative and quantitative results\ndemonstrate that LIT and CLIT achieve favorable results and outperform the\nprior works in arbitrary super-resolution tasks.\n","authors":["Hao-Wei Chen","Yu-Syuan Xu","Min-Fong Hong","Yi-Min Tsai","Hsien-Kai Kuo","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2303.16513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16509v1","updated":"2023-03-29T07:35:56Z","published":"2023-03-29T07:35:56Z","title":"HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images","summary":"  Diffusion models have emerged as the best approach for generative modeling of\n2D images. Part of their success is due to the possibility of training them on\nmillions if not billions of images with a stable learning objective. However,\nextending these models to 3D remains difficult for two reasons. First, finding\na large quantity of 3D training data is much more complex than for 2D images.\nSecond, while it is conceptually trivial to extend the models to operate on 3D\nrather than 2D grids, the associated cubic growth in memory and compute\ncomplexity makes this infeasible. We address the first challenge by introducing\na new diffusion setup that can be trained, end-to-end, with only posed 2D\nimages for supervision; and the second challenge by proposing an image\nformation model that decouples model memory from spatial memory. We evaluate\nour method on real-world data, using the CO3D dataset which has not been used\nto train 3D generative models before. We show that our diffusion models are\nscalable, train robustly, and are competitive in terms of sample quality and\nfidelity to existing approaches for 3D generative modeling.\n","authors":["Animesh Karnewar","Andrea Vedaldi","David Novotny","Niloy Mitra"],"pdf_url":"https://arxiv.org/pdf/2303.16509v1.pdf","comment":"CVPR 2023 conference; project page at:\n  https://holodiffusion.github.io/"},{"id":"http://arxiv.org/abs/2203.12613v3","updated":"2023-03-29T07:34:40Z","published":"2022-03-23T17:58:56Z","title":"Hybrid Mesh-neural Representation for 3D Transparent Object\n  Reconstruction","summary":"  We propose a novel method to reconstruct the 3D shapes of transparent objects\nusing hand-held captured images under natural light conditions. It combines the\nadvantage of explicit mesh and multi-layer perceptron (MLP) network, a hybrid\nrepresentation, to simplify the capture setting used in recent contributions.\nAfter obtaining an initial shape through the multi-view silhouettes, we\nintroduce surface-based local MLPs to encode the vertex displacement field\n(VDF) for the reconstruction of surface details. The design of local MLPs\nallows to represent the VDF in a piece-wise manner using two layer MLP\nnetworks, which is beneficial to the optimization algorithm. Defining local\nMLPs on the surface instead of the volume also reduces the searching space.\nSuch a hybrid representation enables us to relax the ray-pixel correspondences\nthat represent the light path constraint to our designed ray-cell\ncorrespondences, which significantly simplifies the implementation of\nsingle-image based environment matting algorithm. We evaluate our\nrepresentation and reconstruction algorithm on several transparent objects with\nground truth models. Our experiments show that our method can produce\nhigh-quality reconstruction results superior to state-of-the-art methods using\na simplified data acquisition setup.\n","authors":["Jiamin Xu","Zihan Zhu","Hujun Bao","Weiwei Xu"],"pdf_url":"https://arxiv.org/pdf/2203.12613v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16507v1","updated":"2023-03-29T07:34:20Z","published":"2023-03-29T07:34:20Z","title":"Improving Object Detection in Medical Image Analysis through Multiple\n  Expert Annotators: An Empirical Investigation","summary":"  The work discusses the use of machine learning algorithms for anomaly\ndetection in medical image analysis and how the performance of these algorithms\ndepends on the number of annotators and the quality of labels. To address the\nissue of subjectivity in labeling with a single annotator, we introduce a\nsimple and effective approach that aggregates annotations from multiple\nannotators with varying levels of expertise. We then aim to improve the\nefficiency of predictive models in abnormal detection tasks by estimating\nhidden labels from multiple annotations and using a re-weighted loss function\nto improve detection performance. Our method is evaluated on a real-world\nmedical imaging dataset and outperforms relevant baselines that do not consider\ndisagreements among annotators.\n","authors":["Hieu H. Pham","Khiem H. Le","Tuan V. Tran","Ha Q. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.16507v1.pdf","comment":"This is a short version submitted to the Midwest Machine Learning\n  Symposium (MMLS 2023), Chicago, IL, USA"},{"id":"http://arxiv.org/abs/2204.08467v2","updated":"2023-03-29T07:25:54Z","published":"2022-04-16T08:26:19Z","title":"IOP-FL: Inside-Outside Personalization for Federated Medical Image\n  Segmentation","summary":"  Federated learning (FL) allows multiple medical institutions to\ncollaboratively learn a global model without centralizing client data. It is\ndifficult, if possible at all, for such a global model to commonly achieve\noptimal performance for each individual client, due to the heterogeneity of\nmedical images from various scanners and patient demographics. This problem\nbecomes even more significant when deploying the global model to unseen clients\noutside the FL with unseen distributions not presented during federated\ntraining. To optimize the prediction accuracy of each individual client for\nmedical imaging tasks, we propose a novel unified framework for both\n\\textit{Inside and Outside model Personalization in FL} (IOP-FL). Our inside\npersonalization uses a lightweight gradient-based approach that exploits the\nlocal adapted model for each client, by accumulating both the global gradients\nfor common knowledge and the local gradients for client-specific optimization.\nMoreover, and importantly, the obtained local personalized models and the\nglobal model can form a diverse and informative routing space to personalize an\nadapted model for outside FL clients. Hence, we design a new test-time routing\nscheme using the consistency loss with a shape constraint to dynamically\nincorporate the models, given the distribution information conveyed by the test\ndata. Our extensive experimental results on two medical image segmentation\ntasks present significant improvements over SOTA methods on both inside and\noutside personalization, demonstrating the potential of our IOP-FL scheme for\nclinical practice.\n","authors":["Meirui Jiang","Hongzheng Yang","Chen Cheng","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2204.08467v2.pdf","comment":"Accepted by IEEE TMI special issue on federated learning for medical\n  imaging"},{"id":"http://arxiv.org/abs/2303.16501v1","updated":"2023-03-29T07:24:28Z","published":"2023-03-29T07:24:28Z","title":"AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot\n  AV-ASR","summary":"  Audiovisual automatic speech recognition (AV-ASR) aims to improve the\nrobustness of a speech recognition system by incorporating visual information.\nTraining fully supervised multimodal models for this task from scratch, however\nis limited by the need for large labelled audiovisual datasets (in each\ndownstream domain of interest). We present AVFormer, a simple method for\naugmenting audio-only models with visual information, at the same time\nperforming lightweight domain adaptation. We do this by (i) injecting visual\nembeddings into a frozen ASR model using lightweight trainable adaptors. We\nshow that these can be trained on a small amount of weakly labelled video data\nwith minimum additional training time and parameters. (ii) We also introduce a\nsimple curriculum scheme during training which we show is crucial to enable the\nmodel to jointly process audio and visual information effectively; and finally\n(iii) we show that our model achieves state of the art zero-shot results on\nthree different AV-ASR benchmarks (How2, VisSpeech and Ego4D), while also\ncrucially preserving decent performance on traditional audio-only speech\nrecognition benchmarks (LibriSpeech). Qualitative results show that our model\neffectively leverages visual information for robust speech recognition.\n","authors":["Paul Hongsuck Seo","Arsha Nagrani","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2303.16501v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.16493v1","updated":"2023-03-29T07:03:51Z","published":"2023-03-29T07:03:51Z","title":"AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural\n  Representation","summary":"  To apply optical flow in practice, it is often necessary to resize the input\nto smaller dimensions in order to reduce computational costs. However,\ndownsizing inputs makes the estimation more challenging because objects and\nmotion ranges become smaller. Even though recent approaches have demonstrated\nhigh-quality flow estimation, they tend to fail to accurately model small\nobjects and precise boundaries when the input resolution is lowered,\nrestricting their applicability to high-resolution inputs. In this paper, we\nintroduce AnyFlow, a robust network that estimates accurate flow from images of\nvarious resolutions. By representing optical flow as a continuous\ncoordinate-based representation, AnyFlow generates outputs at arbitrary scales\nfrom low-resolution inputs, demonstrating superior performance over prior works\nin capturing tiny objects with detail preservation on a wide range of scenes.\nWe establish a new state-of-the-art performance of cross-dataset generalization\non the KITTI dataset, while achieving comparable accuracy on the online\nbenchmarks to other SOTA methods.\n","authors":["Hyunyoung Jung","Zhuo Hui","Lei Luo","Haitao Yang","Feng Liu","Sungjoo Yoo","Rakesh Ranjan","Denis Demandolx"],"pdf_url":"https://arxiv.org/pdf/2303.16493v1.pdf","comment":"CVPR 2023 (Highlight)"},{"id":"http://arxiv.org/abs/2303.16491v1","updated":"2023-03-29T07:02:20Z","published":"2023-03-29T07:02:20Z","title":"Implicit Diffusion Models for Continuous Super-Resolution","summary":"  Image super-resolution (SR) has attracted increasing attention due to its\nwide applications. However, current SR methods generally suffer from\nover-smoothing and artifacts, and most work only with fixed magnifications.\nThis paper introduces an Implicit Diffusion Model (IDM) for high-fidelity\ncontinuous image super-resolution. IDM integrates an implicit neural\nrepresentation and a denoising diffusion model in a unified end-to-end\nframework, where the implicit neural representation is adopted in the decoding\nprocess to learn continuous-resolution representation. Furthermore, we design a\nscale-controllable conditioning mechanism that consists of a low-resolution\n(LR) conditioning network and a scaling factor. The scaling factor regulates\nthe resolution and accordingly modulates the proportion of the LR information\nand generated features in the final output, which enables the model to\naccommodate the continuous-resolution requirement. Extensive experiments\nvalidate the effectiveness of our IDM and demonstrate its superior performance\nover prior arts.\n","authors":["Sicheng Gao","Xuhui Liu","Bohan Zeng","Sheng Xu","Yanjing Li","Xiaoyan Luo","Jianzhuang Liu","Xiantong Zhen","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.16491v1.pdf","comment":"8 pages, 9 figures, published to CVPR2023"},{"id":"http://arxiv.org/abs/2212.10556v2","updated":"2023-03-29T06:49:51Z","published":"2022-12-20T18:57:06Z","title":"Unleashing the Power of Visual Prompting At the Pixel Level","summary":"  This paper presents a simple and effective visual prompting method for\nadapting pre-trained models to downstream recognition tasks. Our method\nincludes two key designs. First, rather than directly adding together the\nprompt and the image, we treat the prompt as an extra and independent learnable\ncomponent. We show that the strategy of reconciling the prompt and the image\nmatters, and find that warping the prompt around a properly shrinked image\nempirically works the best. Second, we re-introduce two \"old tricks\" commonly\nused in building transferable adversarial examples, i.e., input diversity and\ngradient normalization, into visual prompting. These techniques improve\noptimization and enable the prompt to generalize better. We provide extensive\nexperimental results to demonstrate the effectiveness of our method. Using a\nCLIP model, our prompting method sets a new record of 82.8% average accuracy\nacross 12 popular classification datasets, substantially surpassing the prior\nart by +5.6%. It is worth noting that this prompting performance already\noutperforms linear probing by +2.1% and can even match fully fine-tuning in\ncertain datasets. In addition, our prompting method shows competitive\nperformance across different data scales and against distribution shifts. The\ncode is publicly available at https://github.com/UCSC-VLAA/EVP.\n","authors":["Junyang Wu","Xianhang Li","Chen Wei","Huiyu Wang","Alan Yuille","Yuyin Zhou","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2212.10556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.10960v2","updated":"2023-03-29T06:39:50Z","published":"2022-10-20T02:07:23Z","title":"Diffusion Models already have a Semantic Latent Space","summary":"  Diffusion models achieve outstanding generative performance in various\ndomains. Despite their great success, they lack semantic latent space which is\nessential for controlling the generative process. To address the problem, we\npropose asymmetric reverse process (Asyrp) which discovers the semantic latent\nspace in frozen pretrained diffusion models. Our semantic latent space, named\nh-space, has nice properties for accommodating semantic image manipulation:\nhomogeneity, linearity, robustness, and consistency across timesteps. In\naddition, we introduce a principled design of the generative process for\nversatile editing and quality boost ing by quantifiable measures: editing\nstrength of an interval and quality deficiency at a timestep. Our method is\napplicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets\n(CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page:\nhttps://kwonminki.github.io/Asyrp/\n","authors":["Mingi Kwon","Jaeseok Jeong","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2210.10960v2.pdf","comment":"ICLR2023 (Notable - Top 25%)"},{"id":"http://arxiv.org/abs/2210.12350v2","updated":"2023-03-29T06:35:48Z","published":"2022-10-22T04:38:00Z","title":"Instance-Aware Image Completion","summary":"  Image completion is a task that aims to fill in the missing region of a\nmasked image with plausible contents. However, existing image completion\nmethods tend to fill in the missing region with the surrounding texture instead\nof hallucinating a visual instance that is suitable in accordance with the\ncontext of the scene. In this work, we propose a novel image completion model,\ndubbed ImComplete, that hallucinates the missing instance that harmonizes well\nwith - and thus preserves - the original context. ImComplete first adopts a\ntransformer architecture that considers the visible instances and the location\nof the missing region. Then, ImComplete completes the semantic segmentation\nmasks within the missing region, providing pixel-level semantic and structural\nguidance. Finally, the image synthesis blocks generate photo-realistic content.\nWe perform a comprehensive evaluation of the results in terms of visual quality\n(LPIPS and FID) and contextual preservation scores (CLIPscore and object\ndetection accuracy) with COCO-panoptic and Visual Genome datasets. Experimental\nresults show the superiority of ImComplete on various natural images.\n","authors":["Jinoh Cho","Minguk Kang","Vibhav Vineet","Jaesik Park"],"pdf_url":"https://arxiv.org/pdf/2210.12350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16485v1","updated":"2023-03-29T06:34:12Z","published":"2023-03-29T06:34:12Z","title":"TriVol: Point Cloud Rendering via Triple Volumes","summary":"  Existing learning-based methods for point cloud rendering adopt various 3D\nrepresentations and feature querying mechanisms to alleviate the sparsity\nproblem of point clouds. However, artifacts still appear in rendered images,\ndue to the challenges in extracting continuous and discriminative 3D features\nfrom point clouds. In this paper, we present a dense while lightweight 3D\nrepresentation, named TriVol, that can be combined with NeRF to render\nphoto-realistic images from point clouds. Our TriVol consists of triple slim\nvolumes, each of which is encoded from the point cloud. TriVol has two\nadvantages. First, it fuses respective fields at different scales and thus\nextracts local and non-local features for discriminative representation.\nSecond, since the volume size is greatly reduced, our 3D decoder can be\nefficiently inferred, allowing us to increase the resolution of the 3D space to\nrender more point details. Extensive experiments on different benchmarks with\nvarying kinds of scenes/objects demonstrate our framework's effectiveness\ncompared with current approaches. Moreover, our framework has excellent\ngeneralization ability to render a category of scenes/objects without\nfine-tuning.\n","authors":["Tao Hu","Xiaogang Xu","Ruihang Chu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2303.16485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16482v1","updated":"2023-03-29T06:26:55Z","published":"2023-03-29T06:26:55Z","title":"Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance\n  Fields","summary":"  Synthesizing photo-realistic images from a point cloud is challenging because\nof the sparsity of point cloud representation. Recent Neural Radiance Fields\nand extensions are proposed to synthesize realistic images from 2D input. In\nthis paper, we present Point2Pix as a novel point renderer to link the 3D\nsparse point clouds with 2D dense image pixels. Taking advantage of the point\ncloud 3D prior and NeRF rendering pipeline, our method can synthesize\nhigh-quality images from colored point clouds, generally for novel indoor\nscenes. To improve the efficiency of ray sampling, we propose point-guided\nsampling, which focuses on valid samples. Also, we present Point Encoding to\nbuild Multi-scale Radiance Fields that provide discriminative 3D point\nfeatures. Finally, we propose Fusion Encoding to efficiently synthesize\nhigh-quality images. Extensive experiments on the ScanNet and ArkitScenes\ndatasets demonstrate the effectiveness and generalization.\n","authors":["Tao Hu","Xiaogang Xu","Shu Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2303.16482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16479v1","updated":"2023-03-29T06:23:44Z","published":"2023-03-29T06:23:44Z","title":"Visibility Aware Human-Object Interaction Tracking from Single RGB\n  Camera","summary":"  Capturing the interactions between humans and their environment in 3D is\nimportant for many applications in robotics, graphics, and vision. Recent works\nto reconstruct the 3D human and object from a single RGB image do not have\nconsistent relative translation across frames because they assume a fixed\ndepth. Moreover, their performance drops significantly when the object is\noccluded. In this work, we propose a novel method to track the 3D human,\nobject, contacts between them, and their relative translation across frames\nfrom a single RGB camera, while being robust to heavy occlusions. Our method is\nbuilt on two key insights. First, we condition our neural field reconstructions\nfor human and object on per-frame SMPL model estimates obtained by pre-fitting\nSMPL to a video sequence. This improves neural reconstruction accuracy and\nproduces coherent relative translation across frames. Second, human and object\nmotion from visible frames provides valuable information to infer the occluded\nobject. We propose a novel transformer-based neural network that explicitly\nuses object visibility and human motion to leverage neighbouring frames to make\npredictions for the occluded frames. Building on these insights, our method is\nable to track both human and object robustly even under occlusions. Experiments\non two datasets show that our method significantly improves over the\nstate-of-the-art methods. Our code and pretrained models are available at:\nhttps://virtualhumans.mpi-inf.mpg.de/VisTracker\n","authors":["Xianghui Xie","Bharat Lal Bhatnagar","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2303.16479v1.pdf","comment":"accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.09941v3","updated":"2023-03-29T06:14:47Z","published":"2023-03-17T12:55:22Z","title":"Leaping Into Memories: Space-Time Deep Feature Synthesis","summary":"  The success of deep learning models has led to their adaptation and adoption\nby prominent video understanding methods. The majority of these approaches\nencode features in a joint space-time modality for which the inner workings and\nlearned representations are difficult to visually interpret. We propose LEArned\nPreconscious Synthesis (LEAPS), an architecture-agnostic method for\nsynthesizing videos from the internal spatiotemporal representations of models.\nUsing a stimulus video and a target class, we prime a fixed space-time model\nand iteratively optimize a video initialized with random noise. We incorporate\nadditional regularizers to improve the feature diversity of the synthesized\nvideos as well as the cross-frame temporal coherence of motions. We\nquantitatively and qualitatively evaluate the applicability of LEAPS by\ninverting a range of spatiotemporal convolutional and attention-based\narchitectures trained on Kinetics-400, which to the best of our knowledge has\nnot been previously accomplished.\n","authors":["Alexandros Stergiou","Nikos Deligiannis"],"pdf_url":"https://arxiv.org/pdf/2303.09941v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16465v1","updated":"2023-03-29T05:34:54Z","published":"2023-03-29T05:34:54Z","title":"NerVE: Neural Volumetric Edges for Parametric Curve Extraction from\n  Point Cloud","summary":"  Extracting parametric edge curves from point clouds is a fundamental problem\nin 3D vision and geometry processing. Existing approaches mainly rely on\nkeypoint detection, a challenging procedure that tends to generate noisy\noutput, making the subsequent edge extraction error-prone. To address this\nissue, we propose to directly detect structured edges to circumvent the\nlimitations of the previous point-wise methods. We achieve this goal by\npresenting NerVE, a novel neural volumetric edge representation that can be\neasily learned through a volumetric learning framework. NerVE can be seamlessly\nconverted to a versatile piece-wise linear (PWL) curve representation, enabling\na unified strategy for learning all types of free-form curves. Furthermore, as\nNerVE encodes rich structural information, we show that edge extraction based\non NerVE can be reduced to a simple graph search problem. After converting\nNerVE to the PWL representation, parametric curves can be obtained via\noff-the-shelf spline fitting algorithms. We evaluate our method on the\nchallenging ABC dataset. We show that a simple network based on NerVE can\nalready outperform the previous state-of-the-art methods by a great margin.\nProject page: https://dongdu3.github.io/projects/2023/NerVE/.\n","authors":["Xiangyu Zhu","Dong Du","Weikai Chen","Zhiyou Zhao","Yinyu Nie","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2303.16465v1.pdf","comment":"Accepted by CVPR2023. Project page:\n  https://dongdu3.github.io/projects/2023/NerVE/"},{"id":"http://arxiv.org/abs/2303.16456v1","updated":"2023-03-29T04:54:42Z","published":"2023-03-29T04:54:42Z","title":"Global Adaptation meets Local Generalization: Unsupervised Domain\n  Adaptation for 3D Human Pose Estimation","summary":"  When applying a pre-trained 2D-to-3D human pose lifting model to a target\nunseen dataset, large performance degradation is commonly encountered due to\ndomain shift issues. We observe that the degradation is caused by two factors:\n1) the large distribution gap over global positions of poses between the source\nand target datasets due to variant camera parameters and settings, and 2) the\ndeficient diversity of local structures of poses in training. To this end, we\ncombine \\textbf{global adaptation} and \\textbf{local generalization} in\n\\textit{PoseDA}, a simple yet effective framework of unsupervised domain\nadaptation for 3D human pose estimation. Specifically, global adaptation aims\nto align global positions of poses from the source domain to the target domain\nwith a proposed global position alignment (GPA) module. And local\ngeneralization is designed to enhance the diversity of 2D-3D pose mapping with\na local pose augmentation (LPA) module. These modules bring significant\nperformance improvement without introducing additional learnable parameters. In\naddition, we propose local pose augmentation (LPA) to enhance the diversity of\n3D poses following an adversarial training scheme consisting of 1) a\naugmentation generator that generates the parameters of pre-defined pose\ntransformations and 2) an anchor discriminator to ensure the reality and\nquality of the augmented data. Our approach can be applicable to almost all\n2D-3D lifting models. \\textit{PoseDA} achieves 61.3 mm of MPJPE on MPI-INF-3DHP\nunder a cross-dataset evaluation setup, improving upon the previous\nstate-of-the-art method by 10.2\\%.\n","authors":["Wenhao Chai","Zhongyu Jiang","Jenq-Neng Hwang","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.16456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15698v2","updated":"2023-03-29T04:37:04Z","published":"2023-03-28T03:00:28Z","title":"TFS-ViT: Token-Level Feature Stylization for Domain Generalization","summary":"  Standard deep learning models such as convolutional neural networks (CNNs)\nlack the ability of generalizing to domains which have not been seen during\ntraining. This problem is mainly due to the common but often wrong assumption\nof such models that the source and target data come from the same i.i.d.\ndistribution. Recently, Vision Transformers (ViTs) have shown outstanding\nperformance for a broad range of computer vision tasks. However, very few\nstudies have investigated their ability to generalize to new domains. This\npaper presents a first Token-level Feature Stylization (TFS-ViT) approach for\ndomain generalization, which improves the performance of ViTs to unseen data by\nsynthesizing new domains. Our approach transforms token features by mixing the\nnormalization statistics of images from different domains. We further improve\nthis approach with a novel strategy for attention-aware stylization, which uses\nthe attention maps of class (CLS) tokens to compute and mix normalization\nstatistics of tokens corresponding to different image regions. The proposed\nmethod is flexible to the choice of backbone model and can be easily applied to\nany ViT-based architecture with a negligible increase in computational\ncomplexity. Comprehensive experiments show that our approach is able to achieve\nstate-of-the-art performance on five challenging benchmarks for domain\ngeneralization, and demonstrate its ability to deal with different types of\ndomain shifts. The implementation is available at:\nhttps://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.\n","authors":["Mehrdad Noori","Milad Cheraghalikhani","Ali Bahri","Gustavo A. Vargas Hakim","David Osowiechi","Ismail Ben Ayed","Christian Desrosiers"],"pdf_url":"https://arxiv.org/pdf/2303.15698v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16450v1","updated":"2023-03-29T04:27:11Z","published":"2023-03-29T04:27:11Z","title":"Self-positioning Point-based Transformer for Point Cloud Understanding","summary":"  Transformers have shown superior performance on various computer vision tasks\nwith their capabilities to capture long-range dependencies. Despite the\nsuccess, it is challenging to directly apply Transformers on point clouds due\nto their quadratic cost in the number of points. In this paper, we present a\nSelf-Positioning point-based Transformer (SPoTr), which is designed to capture\nboth local and global shape contexts with reduced complexity. Specifically,\nthis architecture consists of local self-attention and self-positioning\npoint-based global cross-attention. The self-positioning points, adaptively\nlocated based on the input shape, consider both spatial and semantic\ninformation with disentangled attention to improve expressive power. With the\nself-positioning points, we propose a novel global cross-attention mechanism\nfor point clouds, which improves the scalability of global self-attention by\nallowing the attention module to compute attention weights with only a small\nset of self-positioning points. Experiments show the effectiveness of SPoTr on\nthree point cloud tasks such as shape classification, part segmentation, and\nscene segmentation. In particular, our proposed model achieves an accuracy gain\nof 2.6% over the previous best models on shape classification with\nScanObjectNN. We also provide qualitative analyses to demonstrate the\ninterpretability of self-positioning points. The code of SPoTr is available at\nhttps://github.com/mlvlab/SPoTr.\n","authors":["Jinyoung Park","Sanghyeok Lee","Sihyeon Kim","Yunyang Xiong","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2303.16450v1.pdf","comment":"Accepted paper at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.16447v1","updated":"2023-03-29T04:10:14Z","published":"2023-03-29T04:10:14Z","title":"Multi-View Azimuth Stereo via Tangent Space Consistency","summary":"  We present a method for 3D reconstruction only using calibrated multi-view\nsurface azimuth maps. Our method, multi-view azimuth stereo, is effective for\ntextureless or specular surfaces, which are difficult for conventional\nmulti-view stereo methods. We introduce the concept of tangent space\nconsistency: Multi-view azimuth observations of a surface point should be\nlifted to the same tangent space. Leveraging this consistency, we recover the\nshape by optimizing a neural implicit surface representation. Our method\nharnesses the robust azimuth estimation capabilities of photometric stereo\nmethods or polarization imaging while bypassing potentially complex zenith\nangle estimation. Experiments using azimuth maps from various sources validate\nthe accurate shape recovery with our method, even without zenith angles.\n","authors":["Xu Cao","Hiroaki Santo","Fumio Okura","Yasuyuki Matsushita"],"pdf_url":"https://arxiv.org/pdf/2303.16447v1.pdf","comment":"CVPR 2023 camera-ready. Appendices after references. 16 pages, 20\n  figures. Project page: https://xucao-42.github.io/mvas_homepage/"},{"id":"http://arxiv.org/abs/2303.13095v2","updated":"2023-03-29T03:49:20Z","published":"2023-03-23T08:21:16Z","title":"Modeling Entities as Semantic Points for Visual Information Extraction\n  in the Wild","summary":"  Recently, Visual Information Extraction (VIE) has been becoming increasingly\nimportant in both the academia and industry, due to the wide range of\nreal-world applications. Previously, numerous works have been proposed to\ntackle this problem. However, the benchmarks used to assess these methods are\nrelatively plain, i.e., scenarios with real-world complexity are not fully\nrepresented in these benchmarks. As the first contribution of this work, we\ncurate and release a new dataset for VIE, in which the document images are much\nmore challenging in that they are taken from real applications, and\ndifficulties such as blur, partial occlusion, and printing shift are quite\ncommon. All these factors may lead to failures in information extraction.\nTherefore, as the second contribution, we explore an alternative approach to\nprecisely and robustly extract key information from document images under such\ntough conditions. Specifically, in contrast to previous methods, which usually\neither incorporate visual information into a multi-modal architecture or train\ntext spotting and information extraction in an end-to-end fashion, we\nexplicitly model entities as semantic points, i.e., center points of entities\nare enriched with semantic information describing the attributes and\nrelationships of different entities, which could largely benefit entity\nlabeling and linking. Extensive experiments on standard benchmarks in this\nfield as well as the proposed dataset demonstrate that the proposed method can\nachieve significantly enhanced performance on entity labeling and linking,\ncompared with previous state-of-the-art models. Dataset is available at\nhttps://www.modelscope.cn/datasets/damo/SIBR/summary.\n","authors":["Zhibo Yang","Rujiao Long","Pengfei Wang","Sibo Song","Humen Zhong","Wenqing Cheng","Xiang Bai","Cong Yao"],"pdf_url":"https://arxiv.org/pdf/2303.13095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16438v1","updated":"2023-03-29T03:43:51Z","published":"2023-03-29T03:43:51Z","title":"Random Weights Networks Work as Loss Prior Constraint for Image\n  Restoration","summary":"  In this paper, orthogonal to the existing data and model studies, we instead\nresort our efforts to investigate the potential of loss function in a new\nperspective and present our belief ``Random Weights Networks can Be Acted as\nLoss Prior Constraint for Image Restoration''. Inspired by Functional theory,\nwe provide several alternative solutions to implement our belief in the strict\nmathematical manifolds including Taylor's Unfolding Network, Invertible Neural\nNetwork, Central Difference Convolution and Zero-order Filtering as ``random\nweights network prototype'' with respect of the following four levels: 1) the\ndifferent random weights strategies; 2) the different network architectures,\n\\emph{eg,} pure convolution layer or transformer; 3) the different network\narchitecture depths; 4) the different numbers of random weights network\ncombination. Furthermore, to enlarge the capability of the randomly initialized\nmanifolds, we devise the manner of random weights in the following two\nvariants: 1) the weights are randomly initialized only once during the whole\ntraining procedure; 2) the weights are randomly initialized at each training\niteration epoch. Our propose belief can be directly inserted into existing\nnetworks without any training and testing computational cost. Extensive\nexperiments across multiple image restoration tasks, including image\nde-noising, low-light image enhancement, guided image super-resolution\ndemonstrate the consistent performance gains obtained by introducing our\nbelief. To emphasize, our main focus is to spark the realms of loss function\nand save their current neglected status. Code will be publicly available.\n","authors":["Man Zhou","Naishan Zheng","Jie Huang","Xiangyu Rui","Chunle Guo","Deyu Meng","Chongyi Li","Jinwei Gu"],"pdf_url":"https://arxiv.org/pdf/2303.16438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16435v1","updated":"2023-03-29T03:33:54Z","published":"2023-03-29T03:33:54Z","title":"Domain Adaptive Semantic Segmentation by Optimal Transport","summary":"  Scene segmentation is widely used in the field of autonomous driving for\nenvironment perception, and semantic scene segmentation (3S) has received a\ngreat deal of attention due to the richness of the semantic information it\ncontains. It aims to assign labels to pixels in an image, thus enabling\nautomatic image labeling. Current approaches are mainly based on convolutional\nneural networks (CNN), but they rely on a large number of labels. Therefore,\nhow to use a small size of labeled data to achieve semantic segmentation\nbecomes more and more important. In this paper, we propose a domain adaptation\n(DA) framework based on optimal transport (OT) and attention mechanism to\naddress this issue. Concretely, first we generate the output space via CNN due\nto its superiority of feature representation. Second, we utilize OT to achieve\na more robust alignment of source and target domains in output space, where the\nOT plan defines a well attention mechanism to improve the adaptation of the\nmodel. In particular, with OT, the number of network parameters has been\nreduced and the network has been better interpretable. Third, to better\ndescribe the multi-scale property of features, we construct a multi-scale\nsegmentation network to perform domain adaptation. Finally, in order to verify\nthe performance of our proposed method, we conduct experimental comparison with\nthree benchmark and four SOTA methods on three scene datasets, and the mean\nintersection-over-union (mIOU) has been significant improved, and visualization\nresults under multiple domain adaptation scenarios also show that our proposed\nmethod has better performance than compared semantic segmentation methods.\n","authors":["Yaqian Guo","Xin Wang","Ce Li","Shihui Ying"],"pdf_url":"https://arxiv.org/pdf/2303.16435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1705.01450v4","updated":"2023-03-29T03:27:18Z","published":"2017-05-03T14:37:55Z","title":"Gabor Convolutional Networks","summary":"  Steerable properties dominate the design of traditional filters, e.g., Gabor\nfilters, and endow features the capability of dealing with spatial\ntransformations. However, such excellent properties have not been well explored\nin the popular deep convolutional neural networks (DCNNs). In this paper, we\npropose a new deep model, termed Gabor Convolutional Networks (GCNs or Gabor\nCNNs), which incorporates Gabor filters into DCNNs to enhance the resistance of\ndeep learned features to the orientation and scale changes. By only\nmanipulating the basic element of DCNNs based on Gabor filters, i.e., the\nconvolution operator, GCNs can be easily implemented and are compatible with\nany popular deep learning architecture. Experimental results demonstrate the\nsuper capability of our algorithm in recognizing objects, where the scale and\nrotation changes occur frequently. The proposed GCNs have much fewer learnable\nnetwork parameters, and thus is easier to train with an end-to-end pipeline.\n","authors":["Shangzhen Luan","Baochang Zhang","Chen Chen","Xianbin Cao","Jungong Han","Jianzhuang Liu"],"pdf_url":"https://arxiv.org/pdf/1705.01450v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.06956v3","updated":"2023-03-29T03:25:08Z","published":"2022-11-13T17:04:05Z","title":"Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked\n  Modeling for Vision Decoding","summary":"  Decoding visual stimuli from brain recordings aims to deepen our\nunderstanding of the human visual system and build a solid foundation for\nbridging human and computer vision through the Brain-Computer Interface.\nHowever, reconstructing high-quality images with correct semantics from brain\nrecordings is a challenging problem due to the complex underlying\nrepresentations of brain signals and the scarcity of data annotations. In this\nwork, we present MinD-Vis: Sparse Masked Brain Modeling with Double-Conditioned\nLatent Diffusion Model for Human Vision Decoding. Firstly, we learn an\neffective self-supervised representation of fMRI data using mask modeling in a\nlarge latent space inspired by the sparse coding of information in the primary\nvisual cortex. Then by augmenting a latent diffusion model with\ndouble-conditioning, we show that MinD-Vis can reconstruct highly plausible\nimages with semantically matching details from brain recordings using very few\npaired annotations. We benchmarked our model qualitatively and quantitatively;\nthe experimental results indicate that our method outperformed state-of-the-art\nin both semantic mapping (100-way semantic classification) and generation\nquality (FID) by 66% and 41% respectively. An exhaustive ablation study was\nalso conducted to analyze our framework.\n","authors":["Zijiao Chen","Jiaxin Qing","Tiange Xiang","Wan Lin Yue","Juan Helen Zhou"],"pdf_url":"https://arxiv.org/pdf/2211.06956v3.pdf","comment":"8 pages, 9 figures, 2 tables, accepted by CVPR2023, see\n  https://mind-vis.github.io/ for more information"},{"id":"http://arxiv.org/abs/2303.16425v1","updated":"2023-03-29T03:10:28Z","published":"2023-03-29T03:10:28Z","title":"Real-time Controllable Denoising for Image and Video","summary":"  Controllable image denoising aims to generate clean samples with human\nperceptual priors and balance sharpness and smoothness. In traditional\nfilter-based denoising methods, this can be easily achieved by adjusting the\nfiltering strength. However, for NN (Neural Network)-based models, adjusting\nthe final denoising strength requires performing network inference each time,\nmaking it almost impossible for real-time user interaction. In this paper, we\nintroduce Real-time Controllable Denoising (RCD), the first deep image and\nvideo denoising pipeline that provides a fully controllable user interface to\nedit arbitrary denoising levels in real-time with only one-time network\ninference. Unlike existing controllable denoising methods that require multiple\ndenoisers and training stages, RCD replaces the last output layer (which\nusually outputs a single noise map) of an existing CNN-based model with a\nlightweight module that outputs multiple noise maps. We propose a novel Noise\nDecorrelation process to enforce the orthogonality of the noise feature maps,\nallowing arbitrary noise level control through noise map interpolation. This\nprocess is network-free and does not require network inference. Our experiments\nshow that RCD can enable real-time editable image and video denoising for\nvarious existing heavy-weight models without sacrificing their original\nperformance.\n","authors":["Zhaoyang Zhang","Yitong Jiang","Wenqi Shao","Xiaogang Wang","Ping Luo","Kaimo Lin","Jinwei Gu"],"pdf_url":"https://arxiv.org/pdf/2303.16425v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2209.05299v4","updated":"2023-03-29T02:53:29Z","published":"2022-09-12T15:05:41Z","title":"Deep Convolutional Pooling Transformer for Deepfake Detection","summary":"  Recently, Deepfake has drawn considerable public attention due to security\nand privacy concerns in social media digital forensics. As the wildly spreading\nDeepfake videos on the Internet become more realistic, traditional detection\ntechniques have failed in distinguishing between real and fake. Most existing\ndeep learning methods mainly focus on local features and relations within the\nface image using convolutional neural networks as a backbone. However, local\nfeatures and relations are insufficient for model training to learn enough\ngeneral information for Deepfake detection. Therefore, the existing Deepfake\ndetection methods have reached a bottleneck to further improve the detection\nperformance. To address this issue, we propose a deep convolutional Transformer\nto incorporate the decisive image features both locally and globally.\nSpecifically, we apply convolutional pooling and re-attention to enrich the\nextracted features and enhance efficacy. Moreover, we employ the barely\ndiscussed image keyframes in model training for performance improvement and\nvisualize the feature quantity gap between the key and normal image frames\ncaused by video compression. We finally illustrate the transferability with\nextensive experiments on several Deepfake benchmark datasets. The proposed\nsolution consistently outperforms several state-of-the-art baselines on both\nwithin- and cross-dataset experiments.\n","authors":["Tianyi Wang","Harry Cheng","Kam Pui Chow","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2209.05299v4.pdf","comment":"Accepted to be published in ACM TOMM"},{"id":"http://arxiv.org/abs/2303.16417v1","updated":"2023-03-29T02:50:59Z","published":"2023-03-29T02:50:59Z","title":"Problems and shortcuts in deep learning for screening mammography","summary":"  This work reveals undiscovered challenges in the performance and\ngeneralizability of deep learning models. We (1) identify spurious shortcuts\nand evaluation issues that can inflate performance and (2) propose training and\nanalysis methods to address them.\n  We trained an AI model to classify cancer on a retrospective dataset of\n120,112 US exams (3,467 cancers) acquired from 2008 to 2017 and 16,693 UK exams\n(5,655 cancers) acquired from 2011 to 2015.\n  We evaluated on a screening mammography test set of 11,593 US exams (102\ncancers; 7,594 women; age 57.1 \\pm 11.0) and 1,880 UK exams (590 cancers; 1,745\nwomen; age 63.3 \\pm 7.2). A model trained on images of only view markers (no\nbreast) achieved a 0.691 AUC. The original model trained on both datasets\nachieved a 0.945 AUC on the combined US+UK dataset but paradoxically only 0.838\nand 0.892 on the US and UK datasets, respectively. Sampling cancers equally\nfrom both datasets during training mitigated this shortcut. A similar AUC\nparadox (0.903) occurred when evaluating diagnostic exams vs screening exams\n(0.862 vs 0.861, respectively). Removing diagnostic exams during training\nalleviated this bias. Finally, the model did not exhibit the AUC paradox over\nscanner models but still exhibited a bias toward Selenia Dimension (SD) over\nHologic Selenia (HS) exams. Analysis showed that this AUC paradox occurred when\na dataset attribute had values with a higher cancer prevalence (dataset bias)\nand the model consequently assigned a higher probability to these attribute\nvalues (model bias). Stratification and balancing cancer prevalence can\nmitigate shortcuts during evaluation.\n  Dataset and model bias can introduce shortcuts and the AUC paradox,\npotentially pervasive issues within the healthcare AI space. Our methods can\nverify and mitigate shortcuts while providing a clear understanding of\nperformance.\n","authors":["Trevor Tsue","Brent Mombourquette","Ahmed Taha","Thomas Paul Matthews","Yen Nhi Truong Vu","Jason Su"],"pdf_url":"https://arxiv.org/pdf/2303.16417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08427v3","updated":"2023-03-29T02:47:13Z","published":"2022-07-18T08:22:18Z","title":"Adaptive Assignment for Geometry Aware Local Feature Matching","summary":"  The detector-free feature matching approaches are currently attracting great\nattention thanks to their excellent performance. However, these methods still\nstruggle at large-scale and viewpoint variations, due to the geometric\ninconsistency resulting from the application of the mutual nearest neighbour\ncriterion (\\ie, one-to-one assignment) in patch-level matching.Accordingly, we\nintroduce AdaMatcher, which first accomplishes the feature correlation and\nco-visible area estimation through an elaborate feature interaction module,\nthen performs adaptive assignment on patch-level matching while estimating the\nscales between images, and finally refines the co-visible matches through scale\nalignment and sub-pixel regression module.Extensive experiments show that\nAdaMatcher outperforms solid baselines and achieves state-of-the-art results on\nmany downstream tasks. Additionally, the adaptive assignment and sub-pixel\nrefinement module can be used as a refinement network for other matching\nmethods, such as SuperGlue, to boost their performance further. The code will\nbe publicly available at https://github.com/AbyssGaze/AdaMatcher.\n","authors":["Dihe Huang","Ying Chen","Shang Xu","Yong Liu","Wenlong Wu","Yikang Ding","Chengjie Wang","Fan Tang"],"pdf_url":"https://arxiv.org/pdf/2207.08427v3.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.16411v1","updated":"2023-03-29T02:41:08Z","published":"2023-03-29T02:41:08Z","title":"Unlocking Masked Autoencoders as Loss Function for Image and Video\n  Restoration","summary":"  Image and video restoration has achieved a remarkable leap with the advent of\ndeep learning. The success of deep learning paradigm lies in three key\ncomponents: data, model, and loss. Currently, many efforts have been devoted to\nthe first two while seldom study focuses on loss function. With the question\n``are the de facto optimization functions e.g., $L_1$, $L_2$, and perceptual\nlosses optimal?'', we explore the potential of loss and raise our belief\n``learned loss function empowers the learning capability of neural networks for\nimage and video restoration''.\n  Concretely, we stand on the shoulders of the masked Autoencoders (MAE) and\nformulate it as a `learned loss function', owing to the fact the pre-trained\nMAE innately inherits the prior of image reasoning. We investigate the efficacy\nof our belief from three perspectives: 1) from task-customized MAE to native\nMAE, 2) from image task to video task, and 3) from transformer structure to\nconvolution neural network structure. Extensive experiments across multiple\nimage and video tasks, including image denoising, image super-resolution, image\nenhancement, guided image super-resolution, video denoising, and video\nenhancement, demonstrate the consistent performance improvements introduced by\nthe learned loss function. Besides, the learned loss function is preferable as\nit can be directly plugged into existing networks during training without\ninvolving computations in the inference stage. Code will be publicly available.\n","authors":["Man Zhou","Naishan Zheng","Jie Huang","Chunle Guo","Chongyi Li"],"pdf_url":"https://arxiv.org/pdf/2303.16411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16408v1","updated":"2023-03-29T02:36:32Z","published":"2023-03-29T02:36:32Z","title":"The Need for Inherently Privacy-Preserving Vision in Trustworthy\n  Autonomous Systems","summary":"  Vision is a popular and effective sensor for robotics from which we can\nderive rich information about the environment: the geometry and semantics of\nthe scene, as well as the age, gender, identity, activity and even emotional\nstate of humans within that scene. This raises important questions about the\nreach, lifespan, and potential misuse of this information. This paper is a call\nto action to consider privacy in the context of robotic vision. We propose a\nspecific form privacy preservation in which no images are captured or could be\nreconstructed by an attacker even with full remote access. We present a set of\nprinciples by which such systems can be designed, and through a case study in\nlocalisation demonstrate in simulation a specific implementation that delivers\nan important robotic capability in an inherently privacy-preserving manner.\nThis is a first step, and we hope to inspire future works that expand the range\nof applications open to sighted robotic systems.\n","authors":["Adam K. Taras","Niko Suenderhauf","Peter Corke","Donald G. Dansereau"],"pdf_url":"https://arxiv.org/pdf/2303.16408v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.16406v1","updated":"2023-03-29T02:33:54Z","published":"2023-03-29T02:33:54Z","title":"Hierarchical Video-Moment Retrieval and Step-Captioning","summary":"  There is growing interest in searching for information from large video\ncorpora. Prior works have studied relevant tasks, such as text-based video\nretrieval, moment retrieval, video summarization, and video captioning in\nisolation, without an end-to-end setup that can jointly search from video\ncorpora and generate summaries. Such an end-to-end setup would allow for many\ninteresting applications, e.g., a text-based search that finds a relevant video\nfrom a video corpus, extracts the most relevant moment from that video, and\nsegments the moment into important steps with captions. To address this, we\npresent the HiREST (HIerarchical REtrieval and STep-captioning) dataset and\npropose a new benchmark that covers hierarchical information retrieval and\nvisual/textual stepwise summarization from an instructional video corpus.\nHiREST consists of 3.4K text-video pairs from an instructional video dataset,\nwhere 1.1K videos have annotations of moment spans relevant to text query and\nbreakdown of each moment into key instruction steps with caption and timestamps\n(totaling 8.6K step captions). Our hierarchical benchmark consists of video\nretrieval, moment retrieval, and two novel moment segmentation and step\ncaptioning tasks. In moment segmentation, models break down a video moment into\ninstruction steps and identify start-end boundaries. In step captioning, models\ngenerate a textual summary for each step. We also present starting point\ntask-specific and end-to-end joint baseline models for our new benchmark. While\nthe baseline models show some promising results, there still exists large room\nfor future improvement by the community. Project website:\nhttps://hirest-cvpr2023.github.io\n","authors":["Abhay Zala","Jaemin Cho","Satwik Kottur","Xilun Chen","Barlas Oğuz","Yasher Mehdad","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2303.16406v1.pdf","comment":"CVPR 2023 (15 pages; the first two authors contributed equally;\n  Project website: https://hirest-cvpr2023.github.io)"},{"id":"http://arxiv.org/abs/2303.13397v2","updated":"2023-03-29T02:33:18Z","published":"2023-03-23T16:15:18Z","title":"DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh\n  Recovery from a Video","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications such as gaming, human-computer interaction, and virtual\nreality. Compared to single image-based methods, video-based methods can\nutilize temporal information to further improve performance by incorporating\nhuman body motion priors. However, many-to-many approaches such as VIBE suffer\nfrom motion smoothness and temporal inconsistency. While many-to-one approaches\nsuch as TCMR and MPS-Net rely on the future frames, which is non-causal and\ntime inefficient during inference. To address these challenges, a novel\nDiffusion-Driven Transformer-based framework (DDT) for video-based HMR is\npresented. DDT is designed to decode specific motion patterns from the input\nsequence, enhancing motion smoothness and temporal consistency. As a\nmany-to-many approach, the decoder of our DDT outputs the human mesh of all the\nframes, making DDT more viable for real-world applications where time\nefficiency is crucial and a causal model is desired. Extensive experiments are\nconducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),\nwhich demonstrated the effectiveness and efficiency of our DDT.\n","authors":["Ce Zheng","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.09484v4","updated":"2023-03-29T02:22:29Z","published":"2022-09-20T05:52:54Z","title":"Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action\n  Recognition from Egocentric RGB Videos","summary":"  Understanding dynamic hand motions and actions from egocentric RGB videos is\na fundamental yet challenging task due to self-occlusion and ambiguity. To\naddress occlusion and ambiguity, we develop a transformer-based framework to\nexploit temporal information for robust estimation. Noticing the different\ntemporal granularity of and the semantic correlation between hand pose\nestimation and action recognition, we build a network hierarchy with two\ncascaded transformer encoders, where the first one exploits the short-term\ntemporal cue for hand pose estimation, and the latter aggregates per-frame pose\nand object information over a longer time span to recognize the action. Our\napproach achieves competitive results on two first-person hand action\nbenchmarks, namely FPHA and H2O. Extensive ablation studies verify our design\nchoices.\n","authors":["Yilin Wen","Hao Pan","Lei Yang","Jia Pan","Taku Komura","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2209.09484v4.pdf","comment":"Accepted by CVPR 2023; Project page:\n  https://fylwen.github.io/htt.html"},{"id":"http://arxiv.org/abs/2203.02194v5","updated":"2023-03-29T02:13:23Z","published":"2022-03-04T09:04:55Z","title":"Rethinking Reconstruction Autoencoder-Based Out-of-Distribution\n  Detection","summary":"  In some scenarios, classifier requires detecting out-of-distribution samples\nfar from its training data. With desirable characteristics, reconstruction\nautoencoder-based methods deal with this problem by using input reconstruction\nerror as a metric of novelty vs. normality. We formulate the essence of such\napproach as a quadruplet domain translation with an intrinsic bias to only\nquery for a proxy of conditional data uncertainty. Accordingly, an improvement\ndirection is formalized as maximumly compressing the autoencoder's latent space\nwhile ensuring its reconstructive power for acting as a described domain\ntranslator. From it, strategies are introduced including semantic\nreconstruction, data certainty decomposition and normalized L2 distance to\nsubstantially improve original methods, which together establish\nstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of\nCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method\nworks without any additional data, hard-to-implement structure, time-consuming\npipeline, and even harming the classification accuracy of known classes.\n","authors":["Yibo Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.02194v5.pdf","comment":"Accepted('Poster' presentation) as main conference paper of CVPR2022"},{"id":"http://arxiv.org/abs/2303.13767v2","updated":"2023-03-29T01:59:37Z","published":"2023-03-24T02:42:16Z","title":"Learning Spatial-Temporal Implicit Neural Representations for\n  Event-Guided Video Super-Resolution","summary":"  Event cameras sense the intensity changes asynchronously and produce event\nstreams with high dynamic range and low latency. This has inspired research\nendeavors utilizing events to guide the challenging video superresolution (VSR)\ntask. In this paper, we make the first attempt to address a novel problem of\nachieving VSR at random scales by taking advantages of the high temporal\nresolution property of events. This is hampered by the difficulties of\nrepresenting the spatial-temporal information of events when guiding VSR. To\nthis end, we propose a novel framework that incorporates the spatial-temporal\ninterpolation of events to VSR in a unified framework. Our key idea is to learn\nimplicit neural representations from queried spatial-temporal coordinates and\nfeatures from both RGB frames and events. Our method contains three parts.\nSpecifically, the Spatial-Temporal Fusion (STF) module first learns the 3D\nfeatures from events and RGB frames. Then, the Temporal Filter (TF) module\nunlocks more explicit motion information from the events near the queried\ntimestamp and generates the 2D features. Lastly, the SpatialTemporal Implicit\nRepresentation (STIR) module recovers the SR frame in arbitrary resolutions\nfrom the outputs of these two modules. In addition, we collect a real-world\ndataset with spatially aligned events and RGB frames. Extensive experiments\nshow that our method significantly surpasses the prior-arts and achieves VSR\nwith random scales, e.g., 6.5. Code and dataset are available at https:\n//vlis2022.github.io/cvpr23/egvsr.\n","authors":["Yunfan Lu","Zipeng Wang","Minjie Liu","Hongjian Wang","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13767v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.15840v2","updated":"2023-03-29T01:56:40Z","published":"2023-03-28T09:23:19Z","title":"Enhancing Depth Completion with Multi-View Monitored Distillation","summary":"  This paper presents a novel method for depth completion, which leverages\nmulti-view improved monitored distillation to generate more precise depth maps.\nOur approach builds upon the state-of-the-art ensemble distillation method, in\nwhich we introduce a stereo-based model as a teacher model to improve the\naccuracy of the student model for depth completion. By minimizing the\nreconstruction error for a given image during ensemble distillation, we can\navoid learning inherent error modes of completion-based teachers. To provide\nself-supervised information, we also employ multi-view depth consistency and\nmulti-scale minimum reprojection. These techniques utilize existing structural\nconstraints to yield supervised signals for student model training, without\nrequiring costly ground truth depth information. Our extensive experimental\nevaluation demonstrates that our proposed method significantly improves the\naccuracy of the baseline monitored distillation method.\n","authors":["Jia-Wei Guo","Cong Li","Sen-Hua Zhu","Chang-Zheng Zhang","Ming Ouyang","Ning Ding","Hung-Chyun Chou"],"pdf_url":"https://arxiv.org/pdf/2303.15840v2.pdf","comment":"6 pages, 5 figures, references added"},{"id":"http://arxiv.org/abs/2303.14157v2","updated":"2023-03-29T01:54:49Z","published":"2023-03-24T17:12:38Z","title":"Efficient Scale-Invariant Generator with Column-Row Entangled Pixel\n  Synthesis","summary":"  Any-scale image synthesis offers an efficient and scalable solution to\nsynthesize photo-realistic images at any scale, even going beyond 2K\nresolution. However, existing GAN-based solutions depend excessively on\nconvolutions and a hierarchical architecture, which introduce inconsistency and\nthe $``$texture sticking$\"$ issue when scaling the output resolution. From\nanother perspective, INR-based generators are scale-equivariant by design, but\ntheir huge memory footprint and slow inference hinder these networks from being\nadopted in large-scale or real-time systems. In this work, we propose\n$\\textbf{C}$olumn-$\\textbf{R}$ow $\\textbf{E}$ntangled $\\textbf{P}$ixel\n$\\textbf{S}$ynthesis ($\\textbf{CREPS}$), a new generative model that is both\nefficient and scale-equivariant without using any spatial convolutions or\ncoarse-to-fine design. To save memory footprint and make the system scalable,\nwe employ a novel bi-line representation that decomposes layer-wise feature\nmaps into separate $``$thick$\"$ column and row encodings. Experiments on\nvarious datasets, including FFHQ, LSUN-Church, MetFaces, and Flickr-Scenery,\nconfirm CREPS' ability to synthesize scale-consistent and alias-free images at\nany arbitrary resolution with proper training and inference speed. Code is\navailable at https://github.com/VinAIResearch/CREPS.\n","authors":["Thuan Hoang Nguyen","Thanh Van Le","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2303.14157v2.pdf","comment":"Accepted to CVPR 2023; Project Page:\n  https://thuanz123.github.io/creps/"},{"id":"http://arxiv.org/abs/2303.15786v2","updated":"2023-03-29T01:53:04Z","published":"2023-03-28T07:54:54Z","title":"HOICLIP: Efficient Knowledge Transfer for HOI Detection with\n  Vision-Language Models","summary":"  Human-Object Interaction (HOI) detection aims to localize human-object pairs\nand recognize their interactions. Recently, Contrastive Language-Image\nPre-training (CLIP) has shown great potential in providing interaction prior\nfor HOI detectors via knowledge distillation. However, such approaches often\nrely on large-scale training data and suffer from inferior performance under\nfew/zero-shot scenarios. In this paper, we propose a novel HOI detection\nframework that efficiently extracts prior knowledge from CLIP and achieves\nbetter generalization. In detail, we first introduce a novel interaction\ndecoder to extract informative regions in the visual feature map of CLIP via a\ncross-attention mechanism, which is then fused with the detection backbone by a\nknowledge integration block for more accurate human-object pair detection. In\naddition, prior knowledge in CLIP text encoder is leveraged to generate a\nclassifier by embedding HOI descriptions. To distinguish fine-grained\ninteractions, we build a verb classifier from training data via visual semantic\narithmetic and a lightweight verb representation adapter. Furthermore, we\npropose a training-free enhancement to exploit global HOI predictions from\nCLIP. Extensive experiments demonstrate that our method outperforms the state\nof the art by a large margin on various settings, e.g. +4.04 mAP on HICO-Det.\nThe source code is available in https://github.com/Artanic30/HOICLIP.\n","authors":["Shan Ning","Longtian Qiu","Yongfei Liu","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2303.15786v2.pdf","comment":"CVPR 2023.Open sourced, Code and Model Available"},{"id":"http://arxiv.org/abs/2303.16382v1","updated":"2023-03-29T01:42:54Z","published":"2023-03-29T01:42:54Z","title":"ARMBench: An Object-centric Benchmark Dataset for Robotic Manipulation","summary":"  This paper introduces Amazon Robotic Manipulation Benchmark (ARMBench), a\nlarge-scale, object-centric benchmark dataset for robotic manipulation in the\ncontext of a warehouse. Automation of operations in modern warehouses requires\na robotic manipulator to deal with a wide variety of objects, unstructured\nstorage, and dynamically changing inventory. Such settings pose challenges in\nperceiving the identity, physical characteristics, and state of objects during\nmanipulation. Existing datasets for robotic manipulation consider a limited set\nof objects or utilize 3D models to generate synthetic scenes with limitation in\ncapturing the variety of object properties, clutter, and interactions. We\npresent a large-scale dataset collected in an Amazon warehouse using a robotic\nmanipulator performing object singulation from containers with heterogeneous\ncontents. ARMBench contains images, videos, and metadata that corresponds to\n235K+ pick-and-place activities on 190K+ unique objects. The data is captured\nat different stages of manipulation, i.e., pre-pick, during transfer, and after\nplacement. Benchmark tasks are proposed by virtue of high-quality annotations\nand baseline performance evaluation are presented on three visual perception\nchallenges, namely 1) object segmentation in clutter, 2) object identification,\nand 3) defect detection. ARMBench can be accessed at http://armbench.com\n","authors":["Chaitanya Mitash","Fan Wang","Shiyang Lu","Vikedo Terhuja","Tyler Garaas","Felipe Polido","Manikantan Nambi"],"pdf_url":"https://arxiv.org/pdf/2303.16382v1.pdf","comment":"To appear at the IEEE Conference on Robotics and Automation (ICRA),\n  2023"},{"id":"http://arxiv.org/abs/2303.16378v1","updated":"2023-03-29T01:24:25Z","published":"2023-03-29T01:24:25Z","title":"A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion","summary":"  Despite the record-breaking performance in Text-to-Image (T2I) generation by\nStable Diffusion, less research attention is paid to its adversarial\nrobustness. In this work, we study the problem of adversarial attack generation\nfor Stable Diffusion and ask if an adversarial text prompt can be obtained even\nin the absence of end-to-end model queries. We call the resulting problem\n'query-free attack generation'. To resolve this problem, we show that the\nvulnerability of T2I models is rooted in the lack of robustness of text\nencoders, e.g., the CLIP text encoder used for attacking Stable Diffusion.\nBased on such insight, we propose both untargeted and targeted query-free\nattacks, where the former is built on the most influential dimensions in the\ntext embedding space, which we call steerable key dimensions. By leveraging the\nproposed attacks, we empirically show that only a five-character perturbation\nto the text prompt is able to cause the significant content shift of\nsynthesized images using Stable Diffusion. Moreover, we show that the proposed\ntarget attack can precisely steer the diffusion model to scrub the targeted\nimage content without causing much change in untargeted image content.\n","authors":["Haomin Zhuang","Yihua Zhang","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2303.16378v1.pdf","comment":"The 3rd Workshop of Adversarial Machine Learning on Computer Vision:\n  Art of Robustness"},{"id":"http://arxiv.org/abs/2303.05768v2","updated":"2023-03-29T01:13:00Z","published":"2023-03-10T08:09:40Z","title":"Learning Global-Local Correspondence with Semantic Bottleneck for\n  Logical Anomaly Detection","summary":"  This paper presents a novel framework, named Global-Local Correspondence\nFramework (GLCF), for visual anomaly detection with logical constraints. Visual\nanomaly detection has become an active research area in various real-world\napplications, such as industrial anomaly detection and medical disease\ndiagnosis. However, most existing methods focus on identifying local structural\ndegeneration anomalies and often fail to detect high-level functional anomalies\nthat involve logical constraints. To address this issue, we propose a\ntwo-branch approach that consists of a local branch for detecting structural\nanomalies and a global branch for detecting logical anomalies. To facilitate\nlocal-global feature correspondence, we introduce a novel semantic bottleneck\nenabled by the visual Transformer. Moreover, we develop feature estimation\nnetworks for each branch separately to detect anomalies. Our proposed framework\nis validated using various benchmarks, including industrial datasets, Mvtec AD,\nMvtec Loco AD, and the Retinal-OCT medical dataset. Experimental results show\nthat our method outperforms existing methods, particularly in detecting logical\nanomalies.\n","authors":["Haiming Yao","Wenyong Yu","Wei Luo","Zhenfeng Qiang","Donghao Luo","Xiaotian Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.05768v2.pdf","comment":"Submission to IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO\n  TECHNOLOGY"},{"id":"http://arxiv.org/abs/2212.12130v4","updated":"2023-03-29T01:05:39Z","published":"2022-12-23T03:54:59Z","title":"Learning to Detect and Segment for Open Vocabulary Object Detection","summary":"  Open vocabulary object detection has been greatly advanced by the recent\ndevelopment of vision-language pretrained model, which helps recognize novel\nobjects with only semantic categories. The prior works mainly focus on\nknowledge transferring to the object proposal classification and employ\nclass-agnostic box and mask prediction. In this work, we propose CondHead, a\nprincipled dynamic network design to better generalize the box regression and\nmask segmentation for open vocabulary setting. The core idea is to\nconditionally parameterize the network heads on semantic embedding and thus the\nmodel is guided with class-specific knowledge to better detect novel\ncategories. Specifically, CondHead is composed of two streams of network heads,\nthe dynamically aggregated head and the dynamically generated head. The former\nis instantiated with a set of static heads that are conditionally aggregated,\nthese heads are optimized as experts and are expected to learn sophisticated\nprediction. The latter is instantiated with dynamically generated parameters\nand encodes general class-specific information. With such a conditional design,\nthe detection model is bridged by the semantic embedding to offer strongly\ngeneralizable class-wise box and mask prediction. Our method brings significant\nimprovement to the state-of-the-art open vocabulary object detection methods\nwith very minor overhead, e.g., it surpasses a RegionClip model by 3.0\ndetection AP on novel categories, with only 1.1% more computation.\n","authors":["Tao Wang","Nan Li"],"pdf_url":"https://arxiv.org/pdf/2212.12130v4.pdf","comment":"Accepted to CVPR2023, code will be available later"},{"id":"http://arxiv.org/abs/2203.05714v2","updated":"2023-03-29T23:32:40Z","published":"2022-03-11T01:53:30Z","title":"Computational Image-based Stroke Assessment for Evaluation of\n  Cerebroprotectants with Longitudinal and Multi-site Preclinical MRI","summary":"  While ischemic stroke is a leading cause of death worldwide, there has been\nlittle success translating putative cerebroprotectants from rodent preclinical\ntrials to human patients. We investigated computational image-based assessment\ntools for practical improvement of the quality, scalability, and outlook for\nlarge scale preclinical screening for potential therapeutic interventions in\nrodent models. We developed, evaluated, and deployed a pipeline for image-based\nstroke outcome quantification for the Stroke Preclinical Assessment Network\n(SPAN), a multi-site, multi-arm, multi-stage study evaluating a suite of\ncerebroprotectant interventions. Our fully automated pipeline combines\nstate-of-the-art algorithmic and data analytic approaches to assess stroke\noutcomes from multi-parameter MRI data collected longitudinally from a rodent\nmodel of middle cerebral artery occlusion (MCAO), including measures of infarct\nvolume, brain atrophy, midline shift, and data quality. We applied our approach\nto 1,368 scans and report population level results of lesion extent and\nlongitudinal changes from injury. We validated our system by comparison with\nboth manual annotations of coronal MRI slices and tissue sections from the same\nbrain, using crowdsourcing from blinded stroke experts from the network. Our\nresults demonstrate the efficacy and robustness of our image-based stroke\nassessments. The pipeline may provide a promising resource for ongoing rodent\npreclinical studies conducted by SPAN and other networks in the future.\n","authors":["Ryan P. Cabeen","Joseph Mandeville","Fahmeed Hyder","Basavaraju G. Sanganahalli","Daniel R. Thedens","Ali Arbab","Shuning Huang","Adnan Bibic","Erendiz Tarakci","Jelena Mihailovic","Andreia Morais","Jessica Lamb","Karisma Nagarkatti","Arthur W. Toga","Patrick Lyden","Cenk Ayata"],"pdf_url":"https://arxiv.org/pdf/2203.05714v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17061v1","updated":"2023-03-29T23:23:01Z","published":"2023-03-29T23:23:01Z","title":"A Tensor-based Convolutional Neural Network for Small Dataset\n  Classification","summary":"  Inspired by the ConvNets with structured hidden representations, we propose a\nTensor-based Neural Network, TCNN. Different from ConvNets, TCNNs are composed\nof structured neurons rather than scalar neurons, and the basic operation is\nneuron tensor transformation. Unlike other structured ConvNets, where the\npart-whole relationships are modeled explicitly, the relationships are learned\nimplicitly in TCNNs. Also, the structured neurons in TCNNs are high-rank\ntensors rather than vectors or matrices. We compare TCNNs with current popular\nConvNets, including ResNets, MobileNets, EfficientNets, RegNets, etc., on\nCIFAR10, CIFAR100, and Tiny ImageNet. The experiment shows that TCNNs have\nhigher efficiency in terms of parameters. TCNNs also show higher robustness\nagainst white-box adversarial attacks on MNIST compared to ConvNets.\n","authors":["Zhenhua Chen","David Crandall"],"pdf_url":"https://arxiv.org/pdf/2303.17061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17056v1","updated":"2023-03-29T22:58:55Z","published":"2023-03-29T22:58:55Z","title":"Audio-Visual Grouping Network for Sound Localization from Mixtures","summary":"  Sound source localization is a typical and challenging task that predicts the\nlocation of sound sources in a video. Previous single-source methods mainly\nused the audio-visual association as clues to localize sounding objects in each\nimage. Due to the mixed property of multiple sound sources in the original\nspace, there exist rare multi-source approaches to localizing multiple sources\nsimultaneously, except for one recent work using a contrastive random walk in\nthe graph with images and separated sound as nodes. Despite their promising\nperformance, they can only handle a fixed number of sources, and they cannot\nlearn compact class-aware representations for individual sources. To alleviate\nthis shortcoming, in this paper, we propose a novel audio-visual grouping\nnetwork, namely AVGN, that can directly learn category-wise semantic features\nfor each source from the input audio mixture and image to localize multiple\nsources simultaneously. Specifically, our AVGN leverages learnable audio-visual\nclass tokens to aggregate class-aware source features. Then, the aggregated\nsemantic features for each source can be used as guidance to localize the\ncorresponding visual regions. Compared to existing multi-source methods, our\nnew framework can localize a flexible number of sources and disentangle\ncategory-aware audio-visual representations for individual sound sources. We\nconduct extensive experiments on MUSIC, VGGSound-Instruments, and VGG-Sound\nSources benchmarks. The results demonstrate that the proposed AVGN can achieve\nstate-of-the-art sounding object localization performance on both single-source\nand multi-source scenarios. Code is available at\n\\url{https://github.com/stoneMo/AVGN}.\n","authors":["Shentong Mo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2303.17056v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17051v1","updated":"2023-03-29T22:50:05Z","published":"2023-03-29T22:50:05Z","title":"Transductive few-shot adapters for medical image segmentation","summary":"  With the recent raise of foundation models in computer vision and NLP, the\npretrain-and-adapt strategy, where a large-scale model is fine-tuned on\ndownstream tasks, is gaining popularity. However, traditional fine-tuning\napproaches may still require significant resources and yield sub-optimal\nresults when the labeled data of the target task is scarce. This is especially\nthe case in clinical settings. To address this challenge, we formalize few-shot\nefficient fine-tuning (FSEFT), a novel and realistic setting for medical image\nsegmentation. Furthermore, we introduce a novel parameter-efficient fine-tuning\nstrategy tailored to medical image segmentation, with (a) spatial adapter\nmodules that are more appropriate for dense prediction tasks; and (b) a\nconstrained transductive inference, which leverages task-specific prior\nknowledge. Our comprehensive experiments on a collection of public CT datasets\nfor organ segmentation reveal the limitations of standard fine-tuning methods\nin few-shot scenarios, point to the potential of vision adapters and\ntransductive inference, and confirm the suitability of foundation models.\n","authors":["Julio Silva-Rodríguez","Jose Dolz","Ismail Ben Ayed"],"pdf_url":"https://arxiv.org/pdf/2303.17051v1.pdf","comment":"The project code is available in\n  https://github.com/jusiro/fewshot-finetuning"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.16604v1","updated":"2023-03-29T11:37:41Z","published":"2023-03-29T11:37:41Z","title":"Bi-directional Training for Composed Image Retrieval via Text Prompt\n  Learning","summary":"  Composed image retrieval searches for a target image based on a multi-modal\nuser query comprised of a reference image and modification text describing the\ndesired changes. Existing approaches to solving this challenging task learn a\nmapping from the (reference image, modification text)-pair to an image\nembedding that is then matched against a large image corpus. One area that has\nnot yet been explored is the reverse direction, which asks the question, what\nreference image when modified as describe by the text would produce the given\ntarget image? In this work we propose a bi-directional training scheme that\nleverages such reversed queries and can be applied to existing composed image\nretrieval architectures. To encode the bi-directional query we prepend a\nlearnable token to the modification text that designates the direction of the\nquery and then finetune the parameters of the text embedding module. We make no\nother changes to the network architecture. Experiments on two standard datasets\nshow that our novel approach achieves improved performance over a baseline\nBLIP-based model that itself already achieves state-of-the-art performance.\n","authors":["Zheyuan Liu","Weixuan Sun","Yicong Hong","Damien Teney","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.16604v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2206.14649v2","updated":"2023-03-29T10:01:09Z","published":"2022-06-28T03:41:50Z","title":"Cooperative Retriever and Ranker in Deep Recommenders","summary":"  Deep recommender systems (DRS) are intensively applied in modern web\nservices. To deal with the massive web contents, DRS employs a two-stage\nworkflow: retrieval and ranking, to generate its recommendation results. The\nretriever aims to select a small set of relevant candidates from the entire\nitems with high efficiency; while the ranker, usually more precise but\ntime-consuming, is supposed to further refine the best items from the retrieved\ncandidates. Traditionally, the two components are trained either independently\nor within a simple cascading pipeline, which is prone to poor collaboration\neffect. Though some latest works suggested to train retriever and ranker\njointly, there still exist many severe limitations: item distribution shift\nbetween training and inference, false negative, and misalignment of ranking\norder. As such, it remains to explore effective collaborations between\nretriever and ranker.\n","authors":["Xu Huang","Defu Lian","Jin Chen","Zheng Liu","Xing Xie","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2206.14649v2.pdf","comment":"12pages, 4 figures, WWW'23"},{"id":"http://arxiv.org/abs/2209.10267v3","updated":"2023-03-29T08:45:38Z","published":"2022-09-21T11:17:10Z","title":"Clustering Without Knowing How To: Application and Evaluation","summary":"  Crowdsourcing allows running simple human intelligence tasks on a large crowd\nof workers, enabling solving problems for which it is difficult to formulate an\nalgorithm or train a machine learning model in reasonable time. One of such\nproblems is data clustering by an under-specified criterion that is simple for\nhumans, but difficult for machines. In this demonstration paper, we build a\ncrowdsourced system for image clustering and release its code under a free\nlicense at https://github.com/Toloka/crowdclustering. Our experiments on two\ndifferent image datasets, dresses from Zalando's FEIDEGGER and shoes from the\nToloka Shoes Dataset, confirm that one can yield meaningful clusters with no\nmachine learning algorithms purely with crowdsourcing.\n","authors":["Daniil Likhobaba","Daniil Fedulov","Dmitry Ustalov"],"pdf_url":"https://arxiv.org/pdf/2209.10267v3.pdf","comment":"accepted at ECIR 2023 Demonstration Track"},{"id":"http://arxiv.org/abs/2303.15851v2","updated":"2023-03-29T00:42:05Z","published":"2023-03-28T09:43:39Z","title":"Genetic Analysis of Prostate Cancer with Computer Science Methods","summary":"  Metastatic prostate cancer is one of the most common cancers in men. In the\nadvanced stages of prostate cancer, tumours can metastasise to other tissues in\nthe body, which is fatal. In this thesis, we performed a genetic analysis of\nprostate cancer tumours at different metastatic sites using data science,\nmachine learning and topological network analysis methods. We presented a\ngeneral procedure for pre-processing gene expression datasets and pre-filtering\nsignificant genes by analytical methods. We then used machine learning models\nfor further key gene filtering and secondary site tumour classification.\nFinally, we performed gene co-expression network analysis and community\ndetection on samples from different prostate cancer secondary site types. In\nthis work, 13 of the 14,379 genes were selected as the most metastatic prostate\ncancer related genes, achieving approximately 92% accuracy under\ncross-validation. In addition, we provide preliminary insights into the\nco-expression patterns of genes in gene co-expression networks. Project code is\navailable at https://github.com/zcablii/Master_cancer_project.\n","authors":["Yuxuan Li","Shi Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.15851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02757v2","updated":"2023-03-29T21:54:18Z","published":"2022-02-06T11:05:47Z","title":"A Review of Modern Fashion Recommender Systems","summary":"  The textile and apparel industries have grown tremendously over the last few\nyears. Customers no longer have to visit many stores, stand in long queues, or\ntry on garments in dressing rooms as millions of products are now available in\nonline catalogs. However, given the plethora of options available, an effective\nrecommendation system is necessary to properly sort, order, and communicate\nrelevant product material or information to users. Effective fashion RS can\nhave a noticeable impact on billions of customers' shopping experiences and\nincrease sales and revenues on the provider side.\n  The goal of this survey is to provide a review of recommender systems that\noperate in the specific vertical domain of garment and fashion products. We\nhave identified the most pressing challenges in fashion RS research and created\na taxonomy that categorizes the literature according to the objective they are\ntrying to accomplish (e.g., item or outfit recommendation, size recommendation,\nexplainability, among others) and type of side-information (users, items,\ncontext). We have also identified the most important evaluation goals and\nperspectives (outfit generation, outfit recommendation, pairing recommendation,\nand fill-in-the-blank outfit compatibility prediction) and the most commonly\nused datasets and evaluation metrics.\n","authors":["Yashar Deldjoo","Fatemeh Nazary","Arnau Ramisa","Julian Mcauley","Giovanni Pellegrini","Alejandro Bellogin","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2202.02757v2.pdf","comment":"37 pages, 2 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.16900v1","updated":"2023-03-29T17:59:58Z","published":"2023-03-29T17:59:58Z","title":"InceptionNeXt: When Inception Meets ConvNeXt","summary":"  Inspired by the long-range modeling ability of ViTs, large-kernel\nconvolutions are widely studied and adopted recently to enlarge the receptive\nfield and improve model performance, like the remarkable work ConvNeXt which\nemploys 7x7 depthwise convolution. Although such depthwise operator only\nconsumes a few FLOPs, it largely harms the model efficiency on powerful\ncomputing devices due to the high memory access costs. For example, ConvNeXt-T\nhas similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained\non A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt\ncan improve speed, it results in significant performance degradation. It is\nstill unclear how to speed up large-kernel-based CNN models while preserving\ntheir performance. To tackle this issue, inspired by Inceptions, we propose to\ndecompose large-kernel depthwise convolution into four parallel branches along\nchannel dimension, i.e. small square kernel, two orthogonal band kernels, and\nan identity mapping. With this new Inception depthwise convolution, we build a\nseries of networks, namely IncepitonNeXt, which not only enjoy high throughputs\nbut also maintain competitive performance. For instance, InceptionNeXt-T\nachieves 1.6x higher training throughputs than ConvNeX-T, as well as attains\n0.2% top-1 accuracy improvement on ImageNet-1K. We anticipate InceptionNeXt can\nserve as an economical baseline for future architecture design to reduce carbon\nfootprint. Code is available at https://github.com/sail-sg/inceptionnext.\n","authors":["Weihao Yu","Pan Zhou","Shuicheng Yan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2303.16900v1.pdf","comment":"Code: https://github.com/sail-sg/inceptionnext"},{"id":"http://arxiv.org/abs/2303.16897v1","updated":"2023-03-29T17:59:53Z","published":"2023-03-29T17:59:53Z","title":"Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos","summary":"  Modeling sounds emitted from physical object interactions is critical for\nimmersive perceptual experiences in real and virtual worlds. Traditional\nmethods of impact sound synthesis use physics simulation to obtain a set of\nphysics parameters that could represent and synthesize the sound. However, they\nrequire fine details of both the object geometries and impact locations, which\nare rarely available in the real world and can not be applied to synthesize\nimpact sounds from common videos. On the other hand, existing video-driven deep\nlearning-based approaches could only capture the weak correspondence between\nvisual content and impact sounds since they lack of physics knowledge. In this\nwork, we propose a physics-driven diffusion model that can synthesize\nhigh-fidelity impact sound for a silent video clip. In addition to the video\ncontent, we propose to use additional physics priors to guide the impact sound\nsynthesis procedure. The physics priors include both physics parameters that\nare directly estimated from noisy real-world impact sound examples without\nsophisticated setup and learned residual parameters that interpret the sound\nenvironment via neural networks. We further implement a novel diffusion model\nwith specific training and inference strategies to combine physics priors and\nvisual information for impact sound synthesis. Experimental results show that\nour model outperforms several existing systems in generating realistic impact\nsounds. More importantly, the physics-based representations are fully\ninterpretable and transparent, thus enabling us to perform sound editing\nflexibly.\n","authors":["Kun Su","Kaizhi Qian","Eli Shlizerman","Antonio Torralba","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2303.16897v1.pdf","comment":"CVPR 2023. Project page:\n  https://sukun1045.github.io/video-physics-sound-diffusion/"},{"id":"http://arxiv.org/abs/2303.16203v2","updated":"2023-03-29T17:58:24Z","published":"2023-03-28T17:59:56Z","title":"Your Diffusion Model is Secretly a Zero-Shot Classifier","summary":"  The recent wave of large-scale text-to-image diffusion models has\ndramatically increased our text-based image generation abilities. These models\ncan generate realistic images for a staggering variety of prompts and exhibit\nimpressive compositional generalization abilities. Almost all use cases thus\nfar have solely focused on sampling; however, diffusion models can also provide\nconditional density estimates, which are useful for tasks beyond image\ngeneration. In this paper, we show that the density estimates from large-scale\ntext-to-image diffusion models like Stable Diffusion can be leveraged to\nperform zero-shot classification without any additional training. Our\ngenerative approach to classification, which we call Diffusion Classifier,\nattains strong results on a variety of benchmarks and outperforms alternative\nmethods of extracting knowledge from diffusion models. Although a gap remains\nbetween generative and discriminative approaches on zero-shot recognition\ntasks, we find that our diffusion-based approach has stronger multimodal\nrelational reasoning abilities than competing discriminative approaches.\nFinally, we use Diffusion Classifier to extract standard classifiers from\nclass-conditional diffusion models trained on ImageNet. Even though these\nmodels are trained with weak augmentations and no regularization, they approach\nthe performance of SOTA discriminative classifiers. Overall, our results are a\nstep toward using generative over discriminative models for downstream tasks.\nResults and visualizations at https://diffusion-classifier.github.io/\n","authors":["Alexander C. Li","Mihir Prabhudesai","Shivam Duggal","Ellis Brown","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2303.16203v2.pdf","comment":"Website at https://diffusion-classifier.github.io/"},{"id":"http://arxiv.org/abs/2303.16887v1","updated":"2023-03-29T17:56:36Z","published":"2023-03-29T17:56:36Z","title":"Towards Understanding the Effect of Pretraining Label Granularity","summary":"  In this paper, we study how pretraining label granularity affects the\ngeneralization of deep neural networks in image classification tasks. We focus\non the \"fine-to-coarse\" transfer learning setting where the pretraining label\nis more fine-grained than that of the target problem. We experiment with this\nmethod using the label hierarchy of iNaturalist 2021, and observe a 8.76%\nrelative improvement of the error rate over the baseline. We find the following\nconditions are key for the improvement: 1) the pretraining dataset has a strong\nand meaningful label hierarchy, 2) its label function strongly aligns with that\nof the target task, and most importantly, 3) an appropriate level of\npretraining label granularity is chosen. The importance of pretraining label\ngranularity is further corroborated by our transfer learning experiments on\nImageNet. Most notably, we show that pretraining at the leaf labels of\nImageNet21k produces better transfer results on ImageNet1k than pretraining at\nother coarser granularity levels, which supports the common practice.\nTheoretically, through an analysis on a two-layer convolutional ReLU network,\nwe prove that: 1) models trained on coarse-grained labels only respond strongly\nto the common or \"easy-to-learn\" features; 2) with the dataset satisfying the\nright conditions, fine-grained pretraining encourages the model to also learn\nrarer or \"harder-to-learn\" features well, thus improving the model's\ngeneralization.\n","authors":["Guan Zhe Hong","Yin Cui","Ariel Fuxman","Stanley H. Chan","Enming Luo"],"pdf_url":"https://arxiv.org/pdf/2303.16887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16880v1","updated":"2023-03-29T17:39:21Z","published":"2023-03-29T17:39:21Z","title":"The Hidden-Manifold Hopfield Model and a learning phase transition","summary":"  The Hopfield model has a long-standing tradition in statistical physics,\nbeing one of the few neural networks for which a theory is available. Extending\nthe theory of Hopfield models for correlated data could help understand the\nsuccess of deep neural networks, for instance describing how they extract\nfeatures from data. Motivated by this, we propose and investigate a generalized\nHopfield model that we name Hidden-Manifold Hopfield Model: we generate the\ncouplings from $P=\\alpha N$ examples with the Hebb rule using a non-linear\ntransformation of $D=\\alpha_D N$ random vectors that we call factors, with $N$\nthe number of neurons. Using the replica method, we obtain a phase diagram for\nthe model that shows a phase transition where the factors hidden in the\nexamples become attractors of the dynamics; this phase exists above a critical\nvalue of $\\alpha$ and below a critical value of $\\alpha_D$. We call this\nbehaviour learning transition.\n","authors":["Matteo Negri","Clarissa Lauditi","Gabriele Perugini","Carlo Lucibello","Enrico Malatesta"],"pdf_url":"https://arxiv.org/pdf/2303.16880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16866v1","updated":"2023-03-29T17:24:12Z","published":"2023-03-29T17:24:12Z","title":"ALUM: Adversarial Data Uncertainty Modeling from Latent Model\n  Uncertainty Compensation","summary":"  It is critical that the models pay attention not only to accuracy but also to\nthe certainty of prediction. Uncertain predictions of deep models caused by\nnoisy data raise significant concerns in trustworthy AI areas. To explore and\nhandle uncertainty due to intrinsic data noise, we propose a novel method\ncalled ALUM to simultaneously handle the model uncertainty and data uncertainty\nin a unified scheme. Rather than solely modeling data uncertainty in the\nultimate layer of a deep model based on randomly selected training data, we\npropose to explore mined adversarial triplets to facilitate data uncertainty\nmodeling and non-parametric uncertainty estimations to compensate for the\ninsufficiently trained latent model layers. Thus, the critical data uncertainty\nand model uncertainty caused by noisy data can be readily quantified for\nimproving model robustness. Our proposed ALUM is model-agnostic which can be\neasily implemented into any existing deep model with little extra computation\noverhead. Extensive experiments on various noisy learning tasks validate the\nsuperior robustness and generalization ability of our method. The code is\nreleased at https://github.com/wwzjer/ALUM.\n","authors":["Wei Wei","Jiahuan Zhou","Hongze Li","Ying Wu"],"pdf_url":"https://arxiv.org/pdf/2303.16866v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.16861v1","updated":"2023-03-29T17:18:58Z","published":"2023-03-29T17:18:58Z","title":"Beyond Empirical Risk Minimization: Local Structure Preserving\n  Regularization for Improving Adversarial Robustness","summary":"  It is broadly known that deep neural networks are susceptible to being fooled\nby adversarial examples with perturbations imperceptible by humans. Various\ndefenses have been proposed to improve adversarial robustness, among which\nadversarial training methods are most effective. However, most of these methods\ntreat the training samples independently and demand a tremendous amount of\nsamples to train a robust network, while ignoring the latent structural\ninformation among these samples. In this work, we propose a novel Local\nStructure Preserving (LSP) regularization, which aims to preserve the local\nstructure of the input space in the learned embedding space. In this manner,\nthe attacking effect of adversarial samples lying in the vicinity of clean\nsamples can be alleviated. We show strong empirical evidence that with or\nwithout adversarial training, our method consistently improves the performance\nof adversarial robustness on several image classification datasets compared to\nthe baselines and some state-of-the-art approaches, thus providing promising\ndirection for future research.\n","authors":["Wei Wei","Jiahuan Zhou","Ying Wu"],"pdf_url":"https://arxiv.org/pdf/2303.16861v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.16860v1","updated":"2023-03-29T17:17:59Z","published":"2023-03-29T17:17:59Z","title":"Physical Deep Reinforcement Learning Towards Safety Guarantee","summary":"  Deep reinforcement learning (DRL) has achieved tremendous success in many\ncomplex decision-making tasks of autonomous systems with high-dimensional state\nand/or action spaces. However, the safety and stability still remain major\nconcerns that hinder the applications of DRL to safety-critical autonomous\nsystems. To address the concerns, we proposed the Phy-DRL: a physical deep\nreinforcement learning framework. The Phy-DRL is novel in two architectural\ndesigns: i) Lyapunov-like reward, and ii) residual control (i.e., integration\nof physics-model-based control and data-driven control). The concurrent\nphysical reward and residual control empower the Phy-DRL the (mathematically)\nprovable safety and stability guarantees. Through experiments on the inverted\npendulum, we show that the Phy-DRL features guaranteed safety and stability and\nenhanced robustness, while offering remarkably accelerated training and\nenlarged reward.\n","authors":["Hongpeng Cao","Yanbing Mao","Lui Sha","Marco Caccamo"],"pdf_url":"https://arxiv.org/pdf/2303.16860v1.pdf","comment":"Working Paper"},{"id":"http://arxiv.org/abs/2303.16852v1","updated":"2023-03-29T16:59:22Z","published":"2023-03-29T16:59:22Z","title":"Diffusion Schrödinger Bridge Matching","summary":"  Solving transport problems, i.e. finding a map transporting one given\ndistribution to another, has numerous applications in machine learning. Novel\nmass transport methods motivated by generative modeling have recently been\nproposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models\n(FMMs) implement such a transport through a Stochastic Differential Equation\n(SDE) or an Ordinary Differential Equation (ODE). However, while it is\ndesirable in many applications to approximate the deterministic dynamic Optimal\nTransport (OT) map which admits attractive properties, DDMs and FMMs are not\nguaranteed to provide transports close to the OT map. In contrast,\nSchr\\\"odinger bridges (SBs) compute stochastic dynamic mappings which recover\nentropy-regularized versions of OT. Unfortunately, existing numerical methods\napproximating SBs either scale poorly with dimension or accumulate errors\nacross iterations. In this work, we introduce Iterative Markovian Fitting, a\nnew methodology for solving SB problems, and Diffusion Schr\\\"odinger Bridge\nMatching (DSBM), a novel numerical algorithm for computing IMF iterates. DSBM\nsignificantly improves over previous SB numerics and recovers as\nspecial/limiting cases various recent transport methods. We demonstrate the\nperformance of DSBM on a variety of problems.\n","authors":["Yuyang Shi","Valentin De Bortoli","Andrew Campbell","Arnaud Doucet"],"pdf_url":"https://arxiv.org/pdf/2303.16852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04089v2","updated":"2023-03-29T16:52:08Z","published":"2022-12-08T05:50:53Z","title":"Editing Models with Task Arithmetic","summary":"  Changing how pre-trained models behave -- e.g., improving their performance\non a downstream task or mitigating biases learned during pre-training -- is a\ncommon practice when developing machine learning systems. In this work, we\npropose a new paradigm for steering the behavior of neural networks, centered\naround \\textit{task vectors}. A task vector specifies a direction in the weight\nspace of a pre-trained model, such that movement in that direction improves\nperformance on the task. We build task vectors by subtracting the weights of a\npre-trained model from the weights of the same model after fine-tuning on a\ntask. We show that these task vectors can be modified and combined together\nthrough arithmetic operations such as negation and addition, and the behavior\nof the resulting model is steered accordingly. Negating a task vector decreases\nperformance on the target task, with little change in model behavior on control\ntasks. Moreover, adding task vectors together can improve performance on\nmultiple tasks at once. Finally, when tasks are linked by an analogy\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\nthree of the tasks can improve performance on the fourth, even when no data\nfrom the fourth task is used for training. Overall, our experiments with\nseveral models, modalities and tasks show that task arithmetic is a simple,\nefficient and effective way of editing models.\n","authors":["Gabriel Ilharco","Marco Tulio Ribeiro","Mitchell Wortsman","Suchin Gururangan","Ludwig Schmidt","Hannaneh Hajishirzi","Ali Farhadi"],"pdf_url":"https://arxiv.org/pdf/2212.04089v2.pdf","comment":"In Proceedings of the 11th International Conference on Learning\n  Representations (ICLR 2023)"},{"id":"http://arxiv.org/abs/2303.15214v2","updated":"2023-03-29T16:51:15Z","published":"2023-03-27T13:55:07Z","title":"Generalizable Denoising of Microscopy Images using Generative\n  Adversarial Networks and Contrastive Learning","summary":"  Microscopy images often suffer from high levels of noise, which can hinder\nfurther analysis and interpretation. Content-aware image restoration (CARE)\nmethods have been proposed to address this issue, but they often require large\namounts of training data and suffer from over-fitting. To overcome these\nchallenges, we propose a novel framework for few-shot microscopy image\ndenoising. Our approach combines a generative adversarial network (GAN) trained\nvia contrastive learning (CL) with two structure preserving loss terms\n(Structural Similarity Index and Total Variation loss) to further improve the\nquality of the denoised images using little data. We demonstrate the\neffectiveness of our method on three well-known microscopy imaging datasets,\nand show that we can drastically reduce the amount of training data while\nretaining the quality of the denoising, thus alleviating the burden of\nacquiring paired data and enabling few-shot learning. The proposed framework\ncan be easily extended to other image restoration tasks and has the potential\nto significantly advance the field of microscopy image analysis.\n","authors":["Felix Fuentes-Hurtado","Jean-Baptiste Sibarita","Virgile Viasnoff"],"pdf_url":"https://arxiv.org/pdf/2303.15214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07090v2","updated":"2023-03-29T16:48:49Z","published":"2023-02-14T14:42:54Z","title":"A Complete Expressiveness Hierarchy for Subgraph GNNs via Subgraph\n  Weisfeiler-Lehman Tests","summary":"  Recently, subgraph GNNs have emerged as an important direction for developing\nexpressive graph neural networks (GNNs). While numerous architectures have been\nproposed, so far there is still a limited understanding of how various design\nparadigms differ in terms of expressive power, nor is it clear what design\nprinciple achieves maximal expressiveness with minimal architectural\ncomplexity. To address these fundamental questions, this paper conducts a\nsystematic study of general node-based subgraph GNNs through the lens of\nSubgraph Weisfeiler-Lehman Tests (SWL). Our central result is to build a\ncomplete hierarchy of SWL with strictly growing expressivity. Concretely, we\nprove that any node-based subgraph GNN falls into one of the six SWL\nequivalence classes, among which $\\mathsf{SSWL}$ achieves the maximal\nexpressive power. We also study how these equivalence classes differ in terms\nof their practical expressiveness such as encoding graph distance and\nbiconnectivity. Furthermore, we give a tight expressivity upper bound of all\nSWL algorithms by establishing a close relation with localized versions of WL\nand Folklore WL (FWL) tests. Our results provide insights into the power of\nexisting subgraph GNNs, guide the design of new architectures, and point out\ntheir limitations by revealing an inherent gap with the 2-FWL test. Finally,\nexperiments demonstrate that $\\mathsf{SSWL}$-inspired subgraph GNNs can\nsignificantly outperform prior architectures on multiple benchmarks despite\ngreat simplicity.\n","authors":["Bohang Zhang","Guhao Feng","Yiheng Du","Di He","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2302.07090v2.pdf","comment":"76 pages, 13 figures"},{"id":"http://arxiv.org/abs/2303.16841v1","updated":"2023-03-29T16:47:25Z","published":"2023-03-29T16:47:25Z","title":"Randomly Projected Convex Clustering Model: Motivation, Realization, and\n  Cluster Recovery Guarantees","summary":"  In this paper, we propose a randomly projected convex clustering model for\nclustering a collection of $n$ high dimensional data points in $\\mathbb{R}^d$\nwith $K$ hidden clusters. Compared to the convex clustering model for\nclustering original data with dimension $d$, we prove that, under some mild\nconditions, the perfect recovery of the cluster membership assignments of the\nconvex clustering model, if exists, can be preserved by the randomly projected\nconvex clustering model with embedding dimension $m = O(\\epsilon^{-2}\\log(n))$,\nwhere $0 < \\epsilon < 1$ is some given parameter. We further prove that the\nembedding dimension can be improved to be $O(\\epsilon^{-2}\\log(K))$, which is\nindependent of the number of data points. Extensive numerical experiment\nresults will be presented in this paper to demonstrate the robustness and\nsuperior performance of the randomly projected convex clustering model. The\nnumerical results presented in this paper also demonstrate that the randomly\nprojected convex clustering model can outperform the randomly projected K-means\nmodel in practice.\n","authors":["Ziwen Wang","Yancheng Yuan","Jiaming Ma","Tieyong Zeng","Defeng Sun"],"pdf_url":"https://arxiv.org/pdf/2303.16841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16839v1","updated":"2023-03-29T16:42:30Z","published":"2023-03-29T16:42:30Z","title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks","summary":"  The development of language models have moved from encoder-decoder to\ndecoder-only designs. In addition, the common knowledge has it that the two\nmost popular multimodal tasks, the generative and contrastive tasks, tend to\nconflict with one another, are hard to accommodate in one architecture, and\nfurther need complex adaptations for downstream tasks. We propose a novel\nparadigm of training with a decoder-only model for multimodal tasks, which is\nsurprisingly effective in jointly learning of these disparate vision-language\ntasks. This is done with a simple model, called MaMMUT. It consists of a single\nvision encoder and a text decoder, and is able to accommodate contrastive and\ngenerative learning by a novel two-pass approach on the text decoder. We\ndemonstrate that joint training of these diverse-objective tasks is simple,\neffective, and maximizes the weight-sharing of the model. Furthermore, the same\narchitecture enables straightforward extensions to open-vocabulary object\ndetection and video-language tasks. The model tackles a diverse range of tasks,\nwhile being modest in capacity. Our model achieves the SOTA on image-text and\ntext-image retrieval, video question answering and open-vocabulary detection\ntasks, outperforming much larger and more extensively trained foundational\nmodels. It shows competitive results on VQA and Video Captioning, especially\nconsidering its size. Ablations confirm the flexibility and advantages of our\napproach.\n","authors":["Weicheng Kuo","AJ Piergiovanni","Dahun Kim","Xiyang Luo","Ben Caine","Wei Li","Abhijit Ogale","Luowei Zhou","Andrew Dai","Zhifeng Chen","Claire Cui","Anelia Angelova"],"pdf_url":"https://arxiv.org/pdf/2303.16839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16835v1","updated":"2023-03-29T16:28:43Z","published":"2023-03-29T16:28:43Z","title":"Zero-shot Entailment of Leaderboards for Empirical AI Research","summary":"  We present a large-scale empirical investigation of the zero-shot learning\nphenomena in a specific recognizing textual entailment (RTE) task category,\ni.e. the automated mining of leaderboards for Empirical AI Research. The prior\nreported state-of-the-art models for leaderboards extraction formulated as an\nRTE task, in a non-zero-shot setting, are promising with above 90% reported\nperformances. However, a central research question remains unexamined: did the\nmodels actually learn entailment? Thus, for the experiments in this paper, two\nprior reported state-of-the-art models are tested out-of-the-box for their\nability to generalize or their capacity for entailment, given leaderboard\nlabels that were unseen during training. We hypothesize that if the models\nlearned entailment, their zero-shot performances can be expected to be\nmoderately high as well--perhaps, concretely, better than chance. As a result\nof this work, a zero-shot labeled dataset is created via distant labeling\nformulating the leaderboard extraction RTE task.\n","authors":["Salomon Kabongo","Jennifer D'Souza","Sören Auer"],"pdf_url":"https://arxiv.org/pdf/2303.16835v1.pdf","comment":"5 pages, 1 figure. Accepted for publication at JCDL 2023 - Late\n  Breaking Results and Datasets track\n  (https://2023.jcdl.org/calls/papers/#paper_types), official citation\n  forthcoming"},{"id":"http://arxiv.org/abs/2302.11510v3","updated":"2023-03-29T16:25:43Z","published":"2023-02-22T17:27:03Z","title":"Selective experience replay compression using coresets for lifelong deep\n  reinforcement learning in medical imaging","summary":"  Selective experience replay is a popular strategy for integrating lifelong\nlearning with deep reinforcement learning. Selective experience replay aims to\nrecount selected experiences from previous tasks to avoid catastrophic\nforgetting. Furthermore, selective experience replay based techniques are model\nagnostic and allow experiences to be shared across different models. However,\nstoring experiences from all previous tasks make lifelong learning using\nselective experience replay computationally very expensive and impractical as\nthe number of tasks increase. To that end, we propose a reward\ndistribution-preserving coreset compression technique for compressing\nexperience replay buffers stored for selective experience replay.\n  We evaluated the coreset compression technique on the brain tumor\nsegmentation (BRATS) dataset for the task of ventricle localization and on the\nwhole-body MRI for localization of left knee cap, left kidney, right\ntrochanter, left lung, and spleen. The coreset lifelong learning models trained\non a sequence of 10 different brain MR imaging environments demonstrated\nexcellent performance localizing the ventricle with a mean pixel error distance\nof 12.93 for the compression ratio of 10x. In comparison, the conventional\nlifelong learning model localized the ventricle with a mean pixel distance of\n10.87. Similarly, the coreset lifelong learning models trained on whole-body\nMRI demonstrated no significant difference (p=0.28) between the 10x compressed\ncoreset lifelong learning models and conventional lifelong learning models for\nall the landmarks. The mean pixel distance for the 10x compressed models across\nall the landmarks was 25.30, compared to 19.24 for the conventional lifelong\nlearning models. Our results demonstrate that the potential of the\ncoreset-based ERB compression method for compressing experiences without a\nsignificant drop in performance.\n","authors":["Guangyao Zheng","Samson Zhou","Vishwa S. Parekh","Michael A. Jacobs","Vladimir Braverman"],"pdf_url":"https://arxiv.org/pdf/2302.11510v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16821v1","updated":"2023-03-29T16:12:45Z","published":"2023-03-29T16:12:45Z","title":"Decision Making for Autonomous Driving in Interactive Merge Scenarios\n  via Learning-based Prediction","summary":"  Autonomous agents that drive on roads shared with human drivers must reason\nabout the nuanced interactions among traffic participants. This poses a highly\nchallenging decision making problem since human behavior is influenced by a\nmultitude of factors (e.g., human intentions and emotions) that are hard to\nmodel. This paper presents a decision making approach for autonomous driving,\nfocusing on the complex task of merging into moving traffic where uncertainty\nemanates from the behavior of other drivers and imperfect sensor measurements.\nWe frame the problem as a partially observable Markov decision process (POMDP)\nand solve it online with Monte Carlo tree search. The solution to the POMDP is\na policy that performs high-level driving maneuvers, such as giving way to an\napproaching car, keeping a safe distance from the vehicle in front or merging\ninto traffic. Our method leverages a model learned from data to predict the\nfuture states of traffic while explicitly accounting for interactions among the\nsurrounding agents. From these predictions, the autonomous vehicle can\nanticipate the future consequences of its actions on the environment and\noptimize its trajectory accordingly. We thoroughly test our approach in\nsimulation, showing that the autonomous vehicle can adapt its behavior to\ndifferent situations. We also compare against other methods, demonstrating an\nimprovement with respect to the considered performance metrics.\n","authors":["Salar Arbabi","Davide Tavernini","Saber Fallah","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2303.16821v1.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2303.16816v1","updated":"2023-03-29T16:06:07Z","published":"2023-03-29T16:06:07Z","title":"PAC-Bayesian bounds for learning LTI-ss systems with input from\n  empirical loss","summary":"  In this paper we derive a Probably Approxilmately Correct(PAC)-Bayesian error\nbound for linear time-invariant (LTI) stochastic dynamical systems with inputs.\nSuch bounds are widespread in machine learning, and they are useful for\ncharacterizing the predictive power of models learned from finitely many data\npoints. In particular, with the bound derived in this paper relates future\naverage prediction errors with the prediction error generated by the model on\nthe data used for learning. In turn, this allows us to provide finite-sample\nerror bounds for a wide class of learning/system identification algorithms.\nFurthermore, as LTI systems are a sub-class of recurrent neural networks\n(RNNs), these error bounds could be a first step towards PAC-Bayesian bounds\nfor RNNs.\n","authors":["Deividas Eringis","John Leth","Zheng-Hua Tan","Rafael Wisniewski","Mihaly Petreczky"],"pdf_url":"https://arxiv.org/pdf/2303.16816v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2212.14838"},{"id":"http://arxiv.org/abs/2303.16813v1","updated":"2023-03-29T15:56:43Z","published":"2023-03-29T15:56:43Z","title":"Optimal approximation of $C^k$-functions using shallow complex-valued\n  neural networks","summary":"  We prove a quantitative result for the approximation of functions of\nregularity $C^k$ (in the sense of real variables) defined on the complex cube\n$\\Omega_n := [-1,1]^n +i[-1,1]^n\\subseteq \\mathbb{C}^n$ using shallow\ncomplex-valued neural networks. Precisely, we consider neural networks with a\nsingle hidden layer and $m$ neurons, i.e., networks of the form $z \\mapsto\n\\sum_{j=1}^m \\sigma_j \\cdot \\phi\\big(\\rho_j^T z + b_j\\big)$ and show that one\ncan approximate every function in $C^k \\left( \\Omega_n; \\mathbb{C}\\right)$\nusing a function of that form with error of the order $m^{-k/(2n)}$ as $m \\to\n\\infty$, provided that the activation function $\\phi: \\mathbb{C} \\to\n\\mathbb{C}$ is smooth but not polyharmonic on some non-empty open set.\nFurthermore, we show that the selection of the weights $\\sigma_j, b_j \\in\n\\mathbb{C}$ and $\\rho_j \\in \\mathbb{C}^n$ is continuous with respect to $f$ and\nprove that the derived rate of approximation is optimal under this continuity\nassumption. We also discuss the optimality of the result for a possibly\ndiscontinuous choice of the weights.\n","authors":["Paul Geuchen","Felix Voigtlaender"],"pdf_url":"https://arxiv.org/pdf/2303.16813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07983v4","updated":"2023-03-29T15:55:03Z","published":"2022-10-14T17:27:56Z","title":"Improving Transfer Learning with a Dual Image and Video Transformer for\n  Multi-label Movie Trailer Genre Classification","summary":"  In this paper, we study the transferability of ImageNet spatial and Kinetics\nspatio-temporal representations to multi-label Movie Trailer Genre\nClassification (MTGC). In particular, we present an extensive evaluation of the\ntransferability of ConvNet and Transformer models pretrained on ImageNet and\nKinetics to Trailers12k, a new manually-curated movie trailer dataset composed\nof 12,000 videos labeled with 10 different genres and associated metadata. We\nanalyze different aspects that can influence transferability, such as frame\nrate, input video extension, and spatio-temporal modeling. In order to reduce\nthe spatio-temporal structure gap between ImageNet/Kinetics and Trailers12k, we\npropose Dual Image and Video Transformer Architecture (DIViTA), which performs\nshot detection so as to segment the trailer into highly correlated clips,\nproviding a more cohesive input for pretrained backbones and improving\ntransferability (a 1.83% increase for ImageNet and 3.75% for Kinetics). Our\nresults demonstrate that representations learned on either ImageNet or Kinetics\nare comparatively transferable to Trailers12k. Moreover, both datasets provide\ncomplementary information that can be combined to improve classification\nperformance (a 2.91% gain compared to the top single pretraining).\nInterestingly, using lightweight ConvNets as pretrained backbones resulted in\nonly a 3.46% drop in classification performance compared with the top\nTransformer while requiring only 11.82% of its parameters and 0.81% of its\nFLOPS.\n","authors":["Ricardo Montalvo-Lezama","Berenice Montalvo-Lezama","Gibran Fuentes-Pineda"],"pdf_url":"https://arxiv.org/pdf/2210.07983v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.05737v2","updated":"2023-03-29T15:42:30Z","published":"2022-10-11T19:06:42Z","title":"Context-aware Bayesian Mixed Multinomial Logit Model","summary":"  The mixed multinomial logit model assumes constant preference parameters of a\ndecision-maker throughout different choice situations, which may be considered\ntoo strong for certain choice modelling applications. This paper proposes an\neffective approach to model context-dependent intra-respondent heterogeneity,\nthereby introducing the concept of the Context-aware Bayesian mixed multinomial\nlogit model, where a neural network maps contextual information to\ninterpretable shifts in the preference parameters of each individual in each\nchoice occasion. The proposed model offers several key advantages. First, it\nsupports both continuous and discrete variables, as well as complex non-linear\ninteractions between both types of variables. Secondly, each context\nspecification is considered jointly as a whole by the neural network rather\nthan each variable being considered independently. Finally, since the neural\nnetwork parameters are shared across all decision-makers, it can leverage\ninformation from other decision-makers to infer the effect of a particular\ncontext on a particular decision-maker. Even though the context-aware Bayesian\nmixed multinomial logit model allows for flexible interactions between\nattributes, the increase in computational complexity is minor, compared to the\nmixed multinomial logit model. We illustrate the concept and interpretation of\nthe proposed model in a simulation study. We furthermore present a real-world\ncase study from the travel behaviour domain - a bicycle route choice model,\nbased on a large-scale, crowdsourced dataset of GPS trajectories including\n119,448 trips made by 8,555 cyclists.\n","authors":["Mirosława Łukawska","Anders Fjendbo Jensen","Filipe Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2210.05737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16796v1","updated":"2023-03-29T15:38:25Z","published":"2023-03-29T15:38:25Z","title":"Module-based regularization improves Gaussian graphical models when\n  observing noisy data","summary":"  Researchers often represent relations in multi-variate correlational data\nusing Gaussian graphical models, which require regularization to sparsify the\nmodels. Acknowledging that they often study the modular structure of the\ninferred network, we suggest integrating it in the cross-validation of the\nregularization strength to balance under- and overfitting. Using synthetic and\nreal data, we show that this approach allows us to better recover and infer\nmodular structure in noisy data compared with the graphical lasso, a standard\napproach using the Gaussian log-likelihood when cross-validating the\nregularization strength.\n","authors":["Magnus Neuman","Joaquín Calatayud","Viktor Tasselius","Martin Rosvall"],"pdf_url":"https://arxiv.org/pdf/2303.16796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.09537v3","updated":"2023-03-29T15:33:16Z","published":"2021-11-18T06:11:45Z","title":"The Prominence of Artificial Intelligence in COVID-19","summary":"  In December 2019, a novel virus called COVID-19 had caused an enormous number\nof causalities to date. The battle with the novel Coronavirus is baffling and\nhorrifying after the Spanish Flu 2019. While the front-line doctors and medical\nresearchers have made significant progress in controlling the spread of the\nhighly contiguous virus, technology has also proved its significance in the\nbattle. Moreover, Artificial Intelligence has been adopted in many medical\napplications to diagnose many diseases, even baffling experienced doctors.\nTherefore, this survey paper explores the methodologies proposed that can aid\ndoctors and researchers in early and inexpensive methods of diagnosis of the\ndisease. Most developing countries have difficulties carrying out tests using\nthe conventional manner, but a significant way can be adopted with Machine and\nDeep Learning. On the other hand, the access to different types of medical\nimages has motivated the researchers. As a result, a mammoth number of\ntechniques are proposed. This paper first details the background knowledge of\nthe conventional methods in the Artificial Intelligence domain. Following that,\nwe gather the commonly used datasets and their use cases to date. In addition,\nwe also show the percentage of researchers adopting Machine Learning over Deep\nLearning. Thus we provide a thorough analysis of this scenario. Lastly, in the\nresearch challenges, we elaborate on the problems faced in COVID-19 research,\nand we address the issues with our understanding to build a bright and healthy\nenvironment.\n","authors":["MD Abdullah Al Nasim","Aditi Dhali","Faria Afrin","Noshin Tasnim Zaman","Nazmul Karimm","Md Mahim Anjum Haque"],"pdf_url":"https://arxiv.org/pdf/2111.09537v3.pdf","comment":"63 pages, 3 tables, 17 figures"},{"id":"http://arxiv.org/abs/2303.16781v1","updated":"2023-03-29T15:17:05Z","published":"2023-03-29T15:17:05Z","title":"GRAF: Graph Attention-aware Fusion Networks","summary":"  A large number of real-world networks include multiple types of nodes and\nedges. Graph Neural Network (GNN) emerged as a deep learning framework to\nutilize node features on graph-structured data showing superior performance.\nHowever, popular GNN-based architectures operate on one homogeneous network.\nEnabling them to work on multiple networks brings additional challenges due to\nthe heterogeneity of the networks and the multiplicity of the existing\nassociations. In this study, we present a computational approach named GRAF\nutilizing GNN-based approaches on multiple networks with the help of attention\nmechanisms and network fusion. Using attention-based neighborhood aggregation,\nGRAF learns the importance of each neighbor per node (called node-level\nattention) followed by the importance of association (called association-level\nattention) in a hierarchical way. Then, GRAF processes a network fusion step\nweighing each edge according to learned node- and association-level attention,\nwhich results in a fused enriched network. Considering that the fused network\ncould be a highly dense network with many weak edges depending on the given\ninput networks, we included an edge elimination step with respect to edges'\nweights. Finally, GRAF utilizes Graph Convolutional Network (GCN) on the fused\nnetwork and incorporates the node features on the graph-structured data for the\nprediction task or any other downstream analysis. Our extensive evaluations of\nprediction tasks from different domains showed that GRAF outperformed the\nstate-of-the-art methods. Utilization of learned node-level and\nassociation-level attention allowed us to prioritize the edges properly. The\nsource code for our tool is publicly available at\nhttps://github.com/bozdaglab/GRAF.\n","authors":["Ziynet Nesibe Kesimoglu","Serdar Bozdag"],"pdf_url":"https://arxiv.org/pdf/2303.16781v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2303.16748v1","updated":"2023-03-29T14:54:19Z","published":"2023-03-29T14:54:19Z","title":"Multi-View Clustering via Semi-non-negative Tensor Factorization","summary":"  Multi-view clustering (MVC) based on non-negative matrix factorization (NMF)\nand its variants have received a huge amount of attention in recent years due\nto their advantages in clustering interpretability. However, existing NMF-based\nmulti-view clustering methods perform NMF on each view data respectively and\nignore the impact of between-view. Thus, they can't well exploit the\nwithin-view spatial structure and between-view complementary information. To\nresolve this issue, we present semi-non-negative tensor factorization\n(Semi-NTF) and develop a novel multi-view clustering based on Semi-NTF with\none-side orthogonal constraint. Our model directly performs Semi-NTF on the\n3rd-order tensor which is composed of anchor graphs of views. Thus, our model\ndirectly considers the between-view relationship. Moreover, we use the tensor\nSchatten p-norm regularization as a rank approximation of the 3rd-order tensor\nwhich characterizes the cluster structure of multi-view data and exploits the\nbetween-view complementary information. In addition, we provide an optimization\nalgorithm for the proposed method and prove mathematically that the algorithm\nalways converges to the stationary KKT point. Extensive experiments on various\nbenchmark datasets indicate that our proposed method is able to achieve\nsatisfactory clustering performance.\n","authors":["Jing Li","Quanxue Gao","Qianqian Wang","Wei Xia","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2303.16748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10381v4","updated":"2023-03-29T14:50:13Z","published":"2022-11-18T17:25:14Z","title":"Environmental Sensor Placement with Convolutional Gaussian Neural\n  Processes","summary":"  Environmental sensors are crucial for monitoring weather conditions and the\nimpacts of climate change. However, it is challenging to maximise measurement\ninformativeness and place sensors efficiently, particularly in remote regions\nlike Antarctica. Probabilistic machine learning models can evaluate placement\ninformativeness by predicting the uncertainty reduction provided by a new\nsensor. Gaussian process (GP) models are widely used for this purpose, but they\nstruggle with capturing complex non-stationary behaviour and scaling to large\ndatasets. This paper proposes using a convolutional Gaussian neural process\n(ConvGNP) to address these issues. A ConvGNP uses neural networks to\nparameterise a joint Gaussian distribution at arbitrary target locations,\nenabling flexibility and scalability. Using simulated surface air temperature\nanomaly over Antarctica as ground truth, the ConvGNP learns spatial and\nseasonal non-stationarities, outperforming a non-stationary GP baseline. In a\nsimulated sensor placement experiment, the ConvGNP better predicts the\nperformance boost obtained from new observations than GP baselines, leading to\nmore informative sensor placements. We contrast our approach with physics-based\nsensor placement methods and propose future work towards an operational sensor\nplacement recommendation system. This system could help to realise\nenvironmental digital twins that actively direct measurement sampling to\nimprove the digital representation of reality.\n","authors":["Tom R. Andersson","Wessel P. Bruinsma","Stratis Markou","James Requeima","Alejandro Coca-Castro","Anna Vaughan","Anna-Louise Ellis","Matthew A. Lazzara","Daniel C. Jones","J. Scott Hosking","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2211.10381v4.pdf","comment":"In review for the Climate Informatics 2023 special issue of\n  Environmental Data Science"},{"id":"http://arxiv.org/abs/2303.16741v1","updated":"2023-03-29T14:48:51Z","published":"2023-03-29T14:48:51Z","title":"Who You Play Affects How You Play: Predicting Sports Performance Using\n  Graph Attention Networks With Temporal Convolution","summary":"  This study presents a novel deep learning method, called GATv2-GCN, for\npredicting player performance in sports. To construct a dynamic player\ninteraction graph, we leverage player statistics and their interactions during\ngameplay. We use a graph attention network to capture the attention that each\nplayer pays to each other, allowing for more accurate modeling of the dynamic\nplayer interactions. To handle the multivariate player statistics time series,\nwe incorporate a temporal convolution layer, which provides the model with\ntemporal predictive power. We evaluate the performance of our model using\nreal-world sports data, demonstrating its effectiveness in predicting player\nperformance. Furthermore, we explore the potential use of our model in a sports\nbetting context, providing insights into profitable strategies that leverage\nour predictive power. The proposed method has the potential to advance the\nstate-of-the-art in player performance prediction and to provide valuable\ninsights for sports analytics and betting industries.\n","authors":["Rui Luo","Vikram Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2303.16741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16737v1","updated":"2023-03-29T14:41:03Z","published":"2023-03-29T14:41:03Z","title":"Multi-Agent Reinforcement Learning with Action Masking for UAV-enabled\n  Mobile Communications","summary":"  Unmanned Aerial Vehicles (UAVs) are increasingly used as aerial base stations\nto provide ad hoc communications infrastructure. Building upon prior research\nefforts which consider either static nodes, 2D trajectories or single UAV\nsystems, this paper focuses on the use of multiple UAVs for providing wireless\ncommunication to mobile users in the absence of terrestrial communications\ninfrastructure. In particular, we jointly optimize UAV 3D trajectory and NOMA\npower allocation to maximize system throughput. Firstly, a weighted\nK-means-based clustering algorithm establishes UAV-user associations at regular\nintervals. The efficacy of training a novel Shared Deep Q-Network (SDQN) with\naction masking is then explored. Unlike training each UAV separately using DQN,\nthe SDQN reduces training time by using the experiences of multiple UAVs\ninstead of a single agent. We also show that SDQN can be used to train a\nmulti-agent system with differing action spaces. Simulation results confirm\nthat: 1) training a shared DQN outperforms a conventional DQN in terms of\nmaximum system throughput (+20%) and training time (-10%); 2) it can converge\nfor agents with different action spaces, yielding a 9% increase in throughput\ncompared to mutual learning algorithms; and 3) combining NOMA with an SDQN\narchitecture enables the network to achieve a better sum rate compared with\nexisting baseline schemes.\n","authors":["Danish Rizvi","David Boyle"],"pdf_url":"https://arxiv.org/pdf/2303.16737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16727v1","updated":"2023-03-29T14:28:41Z","published":"2023-03-29T14:28:41Z","title":"VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking","summary":"  Scale is the primary factor for building a powerful foundation model that\ncould well generalize to a variety of downstream tasks. However, it is still\nchallenging to train video foundation models with billions of parameters. This\npaper shows that video masked autoencoder (VideoMAE) is a scalable and general\nself-supervised pre-trainer for building video foundation models. We scale the\nVideoMAE in both model and data with a core design. Specifically, we present a\ndual masking strategy for efficient pre-training, with an encoder operating on\na subset of video tokens and a decoder processing another subset of video\ntokens. Although VideoMAE is very efficient due to high masking ratio in\nencoder, masking decoder can still further reduce the overall computational\ncost. This enables the efficient pre-training of billion-level models in video.\nWe also use a progressive training paradigm that involves an initial\npre-training on a diverse multi-sourced unlabeled dataset, followed by a\npost-pre-training on a mixed labeled dataset. Finally, we successfully train a\nvideo ViT model with a billion parameters, which achieves a new\nstate-of-the-art performance on the datasets of Kinetics (90.0% on K400 and\n89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). In\naddition, we extensively verify the pre-trained video ViT models on a variety\nof downstream tasks, demonstrating its effectiveness as a general video\nrepresentation learner.\n","authors":["Limin Wang","Bingkun Huang","Zhiyu Zhao","Zhan Tong","Yinan He","Yi Wang","Yali Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.16727v1.pdf","comment":"CVPR 2023 camera-ready version"},{"id":"http://arxiv.org/abs/2303.16725v1","updated":"2023-03-29T14:22:08Z","published":"2023-03-29T14:22:08Z","title":"Machine Learning for Uncovering Biological Insights in Spatial\n  Transcriptomics Data","summary":"  Development and homeostasis in multicellular systems both require exquisite\ncontrol over spatial molecular pattern formation and maintenance. Advances in\nspatially-resolved and high-throughput molecular imaging methods such as\nmultiplexed immunofluorescence and spatial transcriptomics (ST) provide\nexciting new opportunities to augment our fundamental understanding of these\nprocesses in health and disease. The large and complex datasets resulting from\nthese techniques, particularly ST, have led to rapid development of innovative\nmachine learning (ML) tools primarily based on deep learning techniques. These\nML tools are now increasingly featured in integrated experimental and\ncomputational workflows to disentangle signals from noise in complex biological\nsystems. However, it can be difficult to understand and balance the different\nimplicit assumptions and methodologies of a rapidly expanding toolbox of\nanalytical tools in ST. To address this, we summarize major ST analysis goals\nthat ML can help address and current analysis trends. We also describe four\nmajor data science concepts and related heuristics that can help guide\npractitioners in their choices of the right tools for the right biological\nquestions.\n","authors":["Alex J. Lee","Robert Cahill","Reza Abbasi-Asl"],"pdf_url":"https://arxiv.org/pdf/2303.16725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12095v4","updated":"2023-03-29T14:21:51Z","published":"2023-02-22T11:01:20Z","title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution\n  Perspective","summary":"  ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance to\nunexpected inputs, is still unclear to the public. Robustness is of particular\nconcern in responsible AI, especially for safety-critical applications. In this\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\nseveral popular foundation models as baselines. Results show that ChatGPT shows\nconsistent advantages on most adversarial and OOD classification and\ntranslation tasks. However, the absolute performance is far from perfection,\nwhich suggests that adversarial and OOD robustness remains a significant threat\nto foundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.\n","authors":["Jindong Wang","Xixu Hu","Wenxin Hou","Hao Chen","Runkai Zheng","Yidong Wang","Linyi Yang","Haojun Huang","Wei Ye","Xiubo Geng","Binxin Jiao","Yue Zhang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2302.12095v4.pdf","comment":"Technical report; code is at:\n  https://github.com/microsoft/robustlearn"},{"id":"http://arxiv.org/abs/2303.16721v1","updated":"2023-03-29T14:17:21Z","published":"2023-03-29T14:17:21Z","title":"Maximum likelihood method revisited: Gauge symmetry in Kullback --\n  Leibler divergence and performance-guaranteed regularization","summary":"  The maximum likelihood method is the best-known method for estimating the\nprobabilities behind the data. However, the conventional method obtains the\nprobability model closest to the empirical distribution, resulting in\noverfitting. Then regularization methods prevent the model from being\nexcessively close to the wrong probability, but little is known systematically\nabout their performance. The idea of regularization is similar to\nerror-correcting codes, which obtain optimal decoding by mixing suboptimal\nsolutions with an incorrectly received code. The optimal decoding in\nerror-correcting codes is achieved based on gauge symmetry. We propose a\ntheoretically guaranteed regularization in the maximum likelihood method by\nfocusing on a gauge symmetry in Kullback -- Leibler divergence. In our\napproach, we obtain the optimal model without the need to search for\nhyperparameters frequently appearing in regularization.\n","authors":["Akihisa Ichiki"],"pdf_url":"https://arxiv.org/pdf/2303.16721v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.16716v1","updated":"2023-03-29T14:15:38Z","published":"2023-03-29T14:15:38Z","title":"Topological Point Cloud Clustering","summary":"  We present Topological Point Cloud Clustering (TPCC), a new method to cluster\npoints in an arbitrary point cloud based on their contribution to global\ntopological features. TPCC synthesizes desirable features from spectral\nclustering and topological data analysis and is based on considering the\nspectral properties of a simplicial complex associated to the considered point\ncloud. As it is based on considering sparse eigenvector computations, TPCC is\nsimilarly easy to interpret and implement as spectral clustering. However, by\nfocusing not just on a single matrix associated to a graph created from the\npoint cloud data, but on a whole set of Hodge-Laplacians associated to an\nappropriately constructed simplicial complex, we can leverage a far richer set\nof topological features to characterize the data points within the point cloud\nand benefit from the relative robustness of topological techniques against\nnoise. We test the performance of TPCC on both synthetic and real-world data\nand compare it with classical spectral clustering.\n","authors":["Vincent P. Grande","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2303.16716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.13726v4","updated":"2023-03-29T13:57:28Z","published":"2020-06-24T13:41:37Z","title":"Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial\n  Robustness","summary":"  Evaluating the robustness of a defense model is a challenging task in\nadversarial robustness research. Obfuscated gradients have previously been\nfound to exist in many defense methods and cause a false signal of robustness.\nIn this paper, we identify a more subtle situation called Imbalanced Gradients\nthat can also cause overestimated adversarial robustness. The phenomenon of\nimbalanced gradients occurs when the gradient of one term of the margin loss\ndominates and pushes the attack towards to a suboptimal direction. To exploit\nimbalanced gradients, we formulate a Margin Decomposition (MD) attack that\ndecomposes a margin loss into individual terms and then explores the\nattackability of these terms separately via a two-stage process. We also\npropose a multi-targeted and ensemble version of our MD attack. By\ninvestigating 24 defense models proposed since 2018, we find that 11 models are\nsusceptible to a certain degree of imbalanced gradients and our MD attack can\ndecrease their robustness evaluated by the best standalone baseline attack by\nmore than 1%. We also provide an in-depth investigation on the likely causes of\nimbalanced gradients and effective countermeasures. Our code is available at\nhttps://github.com/HanxunH/MDAttack.\n","authors":["Xingjun Ma","Linxi Jiang","Hanxun Huang","Zejia Weng","James Bailey","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2006.13726v4.pdf","comment":"To appear in Machine Learning"},{"id":"http://arxiv.org/abs/2303.00859v2","updated":"2023-03-29T13:54:36Z","published":"2023-03-01T23:14:09Z","title":"FuNVol: A Multi-Asset Implied Volatility Market Simulator using\n  Functional Principal Components and Neural SDEs","summary":"  Here, we introduce a new approach for generating sequences of implied\nvolatility (IV) surfaces across multiple assets that is faithful to historical\nprices. We do so using a combination of functional data analysis and neural\nstochastic differential equations (SDEs) combined with a probability integral\ntransform penalty to reduce model misspecification. We demonstrate that\nlearning the joint dynamics of IV surfaces and prices produces market scenarios\nthat are consistent with historical features and lie within the sub-manifold of\nsurfaces that are essentially free of static arbitrage. Finally, we demonstrate\nthat delta hedging using the simulated surfaces generates profit and loss (P&L)\ndistributions that are consistent with realised P&Ls.\n","authors":["Vedant Choudhary","Sebastian Jaimungal","Maxime Bergeron"],"pdf_url":"https://arxiv.org/pdf/2303.00859v2.pdf","comment":"30 pages, 12 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.16704v1","updated":"2023-03-29T13:54:32Z","published":"2023-03-29T13:54:32Z","title":"TraVaG: Differentially Private Trace Variant Generation Using GANs","summary":"  Process mining is rapidly growing in the industry. Consequently, privacy\nconcerns regarding sensitive and private information included in event data,\nused by process mining algorithms, are becoming increasingly relevant.\nState-of-the-art research mainly focuses on providing privacy guarantees, e.g.,\ndifferential privacy, for trace variants that are used by the main process\nmining techniques, e.g., process discovery. However, privacy preservation\ntechniques for releasing trace variants still do not fulfill all the\nrequirements of industry-scale usage. Moreover, providing privacy guarantees\nwhen there exists a high rate of infrequent trace variants is still a\nchallenge. In this paper, we introduce TraVaG as a new approach for releasing\ndifferentially private trace variants based on \\text{Generative Adversarial\nNetworks} (GANs) that provides industry-scale benefits and enhances the level\nof privacy guarantees when there exists a high ratio of infrequent variants.\nMoreover, TraVaG overcomes shortcomings of conventional privacy preservation\ntechniques such as bounding the length of variants and introducing fake\nvariants. Experimental results on real-life event data show that our approach\noutperforms state-of-the-art techniques in terms of privacy guarantees, plain\ndata utility preservation, and result utility preservation.\n","authors":["Majid Rafiei","Frederik Wangelik","Mahsa Pourbafrani","Wil M. P. van der Aalst"],"pdf_url":"https://arxiv.org/pdf/2303.16704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16698v1","updated":"2023-03-29T13:51:06Z","published":"2023-03-29T13:51:06Z","title":"Probabilistic inverse optimal control with local linearization for\n  non-linear partially observable systems","summary":"  Inverse optimal control methods can be used to characterize behavior in\nsequential decision-making tasks. Most existing work, however, requires the\ncontrol signals to be known, or is limited to fully-observable or linear\nsystems. This paper introduces a probabilistic approach to inverse optimal\ncontrol for stochastic non-linear systems with missing control signals and\npartial observability that unifies existing approaches. By using an explicit\nmodel of the noise characteristics of the sensory and control systems of the\nagent in conjunction with local linearization techniques, we derive an\napproximate likelihood for the model parameters, which can be computed within a\nsingle forward pass. We evaluate our proposed method on stochastic and\npartially observable version of classic control tasks, a navigation task, and a\nmanual reaching task. The proposed method has broad applicability, ranging from\nimitation learning to sensorimotor neuroscience.\n","authors":["Dominik Straub","Matthias Schultheis","Heinz Koeppl","Constantin A. Rothkopf"],"pdf_url":"https://arxiv.org/pdf/2303.16698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.17406v7","updated":"2023-03-29T13:29:51Z","published":"2022-10-31T15:43:57Z","title":"Emergent Linguistic Structures in Neural Networks are Fragile","summary":"  Large Language Models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structure. In this paper, focusing on the\nability of language models to represent syntax, we propose a framework to\nassess the consistency and robustness of linguistic representations. To this\nend, we introduce measures of robustness of neural network models that leverage\nrecent advances in extracting linguistic constructs from LLMs via probing\ntasks, i.e., simple tasks used to extract meaningful information about a single\nfacet of a language model, such as syntax reconstruction and root\nidentification. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures by analysing their\nperformance and robustness with respect to syntax-preserving perturbations. We\nprovide evidence that context-free representation (e.g., GloVe) are in some\ncases competitive with context-dependent representations from modern LLMs\n(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our key\nobservation is that emergent syntactic representations in neural networks are\nbrittle. We make the code, trained models and logs available to the community\nas a contribution to the debate about the capabilities of LLMs.\n","authors":["Emanuele La Malfa","Matthew Wicker","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2210.17406v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16674v1","updated":"2023-03-29T13:27:14Z","published":"2023-03-29T13:27:14Z","title":"Neuro-symbolic Rule Learning in Real-world Classification Tasks","summary":"  Neuro-symbolic rule learning has attracted lots of attention as it offers\nbetter interpretability than pure neural models and scales better than symbolic\nrule learning. A recent approach named pix2rule proposes a neural Disjunctive\nNormal Form (neural DNF) module to learn symbolic rules with feed-forward\nlayers. Although proved to be effective in synthetic binary classification,\npix2rule has not been applied to more challenging tasks such as multi-label and\nmulti-class classifications over real-world data. In this paper, we address\nthis limitation by extending the neural DNF module to (i) support rule learning\nin real-world multi-class and multi-label classification tasks, (ii) enforce\nthe symbolic property of mutual exclusivity (i.e. predicting exactly one class)\nin multi-class classification, and (iii) explore its scalability over large\ninputs and outputs. We train a vanilla neural DNF model similar to pix2rule's\nneural DNF module for multi-label classification, and we propose a novel\nextended model called neural DNF-EO (Exactly One) which enforces mutual\nexclusivity in multi-class classification. We evaluate the classification\nperformance, scalability and interpretability of our neural DNF-based models,\nand compare them against pure neural models and a state-of-the-art symbolic\nrule learner named FastLAS. We demonstrate that our neural DNF-based models\nperform similarly to neural networks, but provide better interpretability by\nenabling the extraction of logical rules. Our models also scale well when the\nrule search space grows in size, in contrast to FastLAS, which fails to learn\nin multi-class classification tasks with 200 classes and in all multi-label\nsettings.\n","authors":["Kexin Gu Baugh","Nuri Cingillioglu","Alessandra Russo"],"pdf_url":"https://arxiv.org/pdf/2303.16674v1.pdf","comment":"Accepted at AAAI-MAKE 2023"},{"id":"http://arxiv.org/abs/2303.16668v1","updated":"2023-03-29T13:22:20Z","published":"2023-03-29T13:22:20Z","title":"A Byzantine-Resilient Aggregation Scheme for Federated Learning via\n  Matrix Autoregression on Client Updates","summary":"  In this work, we propose FLANDERS, a novel federated learning (FL)\naggregation scheme robust to Byzantine attacks. FLANDERS considers the local\nmodel updates sent by clients at each FL round as a matrix-valued time series.\nThen, it identifies malicious clients as outliers of this time series by\ncomparing actual observations with those estimated by a matrix autoregressive\nforecasting model. Experiments conducted on several datasets under different FL\nsettings demonstrate that FLANDERS matches the robustness of the most powerful\nbaselines against Byzantine clients. Furthermore, FLANDERS remains highly\neffective even under extremely severe attack scenarios, as opposed to existing\ndefense strategies.\n","authors":["Gabriele Tolomei","Edoardo Gabrielli","Dimitri Belli","Vittorio Miori"],"pdf_url":"https://arxiv.org/pdf/2303.16668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16656v1","updated":"2023-03-29T13:04:04Z","published":"2023-03-29T13:04:04Z","title":"Learning Flow Functions from Data with Applications to Nonlinear\n  Oscillators","summary":"  We describe a recurrent neural network (RNN) based architecture to learn the\nflow function of a causal, time-invariant and continuous-time control system\nfrom trajectory data. By restricting the class of control inputs to piecewise\nconstant functions, we show that learning the flow function is equivalent to\nlearning the input-to-state map of a discrete-time dynamical system. This\nmotivates the use of an RNN together with encoder and decoder networks which\nmap the state of the system to the hidden state of the RNN and back. We show\nthat the proposed architecture is able to approximate the flow function by\nexploiting the system's causality and time-invariance. The output of the\nlearned flow function model can be queried at any time instant. We\nexperimentally validate the proposed method using models of the Van der Pol and\nFitzHugh Nagumo oscillators. In both cases, the results demonstrate that the\narchitecture is able to closely reproduce the trajectories of these two\nsystems. For the Van der Pol oscillator, we further show that the trained model\ngeneralises to the system's response with a prolonged prediction time horizon\nas well as control inputs outside the training distribution. For the\nFitzHugh-Nagumo oscillator, we show that the model accurately captures the\ninput-dependent phenomena of excitability.\n","authors":["Miguel Aguiar","Amritam Das","Karl H. Johansson"],"pdf_url":"https://arxiv.org/pdf/2303.16656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16633v1","updated":"2023-03-29T12:43:36Z","published":"2023-03-29T12:43:36Z","title":"Targeted Adversarial Attacks on Wind Power Forecasts","summary":"  In recent years, researchers proposed a variety of deep learning models for\nwind power forecasting. These models predict the wind power generation of wind\nfarms or entire regions more accurately than traditional machine learning\nalgorithms or physical models. However, latest research has shown that deep\nlearning models can often be manipulated by adversarial attacks. Since wind\npower forecasts are essential for the stability of modern power systems, it is\nimportant to protect them from this threat. In this work, we investigate the\nvulnerability of two different forecasting models to targeted, semitargeted,\nand untargeted adversarial attacks. We consider a Long Short-Term Memory (LSTM)\nnetwork for predicting the power generation of a wind farm and a Convolutional\nNeural Network (CNN) for forecasting the wind power generation throughout\nGermany. Moreover, we propose the Total Adversarial Robustness Score (TARS), an\nevaluation metric for quantifying the robustness of regression models to\ntargeted and semi-targeted adversarial attacks. It assesses the impact of\nattacks on the model's performance, as well as the extent to which the\nattacker's goal was achieved, by assigning a score between 0 (very vulnerable)\nand 1 (very robust). In our experiments, the LSTM forecasting model was fairly\nrobust and achieved a TARS value of over 0.81 for all adversarial attacks\ninvestigated. The CNN forecasting model only achieved TARS values below 0.06\nwhen trained ordinarily, and was thus very vulnerable. Yet, its robustness\ncould be significantly improved by adversarial training, which always resulted\nin a TARS above 0.46.\n","authors":["René Heinrich","Christoph Scholz","Stephan Vogt","Malte Lehna"],"pdf_url":"https://arxiv.org/pdf/2303.16633v1.pdf","comment":"20 pages, including appendix, 12 figures"},{"id":"http://arxiv.org/abs/2303.08863v2","updated":"2023-03-29T12:42:48Z","published":"2023-03-15T18:32:52Z","title":"Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield\n  Images with Class Labels","summary":"  Image-to-image reconstruction problems with free or inexpensive metadata in\nthe form of class labels appear often in biological and medical image domains.\nExisting text-guided or style-transfer image-to-image approaches do not\ntranslate to datasets where additional information is provided as discrete\nclasses. We introduce and implement a model which combines image-to-image and\nclass-guided denoising diffusion probabilistic models. We train our model on a\nreal-world dataset of microscopy images used for drug discovery, with and\nwithout incorporating metadata labels. By exploring the properties of\nimage-to-image diffusion with relevant labels, we show that class-guided\nimage-to-image diffusion can improve the meaningful content of the\nreconstructed images and outperform the unguided model in useful downstream\ntasks.\n","authors":["Jan Oscar Cross-Zamirski","Praveen Anand","Guy Williams","Elizabeth Mouchet","Yinhai Wang","Carola-Bibiane Schönlieb"],"pdf_url":"https://arxiv.org/pdf/2303.08863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14961v2","updated":"2023-03-29T12:31:06Z","published":"2023-03-27T07:52:58Z","title":"Diffusion Denoised Smoothing for Certified and Adversarial Robust\n  Out-Of-Distribution Detection","summary":"  As the use of machine learning continues to expand, the importance of\nensuring its safety cannot be overstated. A key concern in this regard is the\nability to identify whether a given sample is from the training distribution,\nor is an \"Out-Of-Distribution\" (OOD) sample. In addition, adversaries can\nmanipulate OOD samples in ways that lead a classifier to make a confident\nprediction. In this study, we present a novel approach for certifying the\nrobustness of OOD detection within a $\\ell_2$-norm around the input, regardless\nof network architecture and without the need for specific components or\nadditional training. Further, we improve current techniques for detecting\nadversarial attacks on OOD samples, while providing high levels of certified\nand adversarial robustness on in-distribution samples. The average of all OOD\ndetection metrics on CIFAR10/100 shows an increase of $\\sim 13 \\% / 5\\%$\nrelative to previous approaches.\n","authors":["Nicola Franco","Daniel Korth","Jeanette Miriam Lorenz","Karsten Roscher","Stephan Guennemann"],"pdf_url":"https://arxiv.org/pdf/2303.14961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16626v1","updated":"2023-03-29T12:28:49Z","published":"2023-03-29T12:28:49Z","title":"Fairlearn: Assessing and Improving Fairness of AI Systems","summary":"  Fairlearn is an open source project to help practitioners assess and improve\nfairness of artificial intelligence (AI) systems. The associated Python\nlibrary, also named fairlearn, supports evaluation of a model's output across\naffected populations and includes several algorithms for mitigating fairness\nissues. Grounded in the understanding that fairness is a sociotechnical\nchallenge, the project integrates learning resources that aid practitioners in\nconsidering a system's broader societal context.\n","authors":["Hilde Weerts","Miroslav Dudík","Richard Edgar","Adrin Jalali","Roman Lutz","Michael Madaio"],"pdf_url":"https://arxiv.org/pdf/2303.16626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.10838v3","updated":"2023-03-29T12:26:32Z","published":"2022-01-26T09:44:13Z","title":"Privacy-Preserving Logistic Regression Training with A Faster Gradient\n  Variant","summary":"  Logistic regression training over encrypted data has been an attractive idea\nto security concerns for years. In this paper, we propose a faster gradient\nvariant called $\\texttt{quadratic gradient}$ to implement logistic regression\ntraining in a homomorphic encryption domain, the core of which can be seen as\nan extension of the simplified fixed Hessian.\n  We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient\nAlgorithm (Adagrad) respectively with this gradient variant and evaluate the\nenhanced algorithms on several datasets. Experimental results show that the\nenhanced methods have a state-of-the-art performance in convergence speed\ncompared to the naive first-order gradient methods. We then adopt the enhanced\nNAG method to implement homomorphic logistic regression training and obtain a\ncomparable result by only $3$ iterations.\n","authors":["John Chiang"],"pdf_url":"https://arxiv.org/pdf/2201.10838v3.pdf","comment":"The basic work of this paper, $\\texttt{quadratic gradient}$ and the\n  enhanced full batch NAG, was nearly finished in September 2019. The initial\n  version of this paper was written in April 2020, rejected by ICANN 2020. The\n  enhanced mini-batch NAG was introduced into this paper in September 2020 and\n  later rejected by a special issue on the journal FGCS 2020"},{"id":"http://arxiv.org/abs/2208.12815v2","updated":"2023-03-29T12:19:49Z","published":"2022-08-26T15:45:20Z","title":"What Does the Gradient Tell When Attacking the Graph Structure","summary":"  Recent research has revealed that Graph Neural Networks (GNNs) are\nsusceptible to adversarial attacks targeting the graph structure. A malicious\nattacker can manipulate a limited number of edges, given the training labels,\nto impair the victim model's performance. Previous empirical studies indicate\nthat gradient-based attackers tend to add edges rather than remove them. In\nthis paper, we present a theoretical demonstration revealing that attackers\ntend to increase inter-class edges due to the message passing mechanism of\nGNNs, which explains some previous empirical observations. By connecting\ndissimilar nodes, attackers can more effectively corrupt node features, making\nsuch attacks more advantageous. However, we demonstrate that the inherent\nsmoothness of GNN's message passing tends to blur node dissimilarity in the\nfeature space, leading to the loss of crucial information during the forward\nprocess. To address this issue, we propose a novel surrogate model with\nmulti-level propagation that preserves the node dissimilarity information. This\nmodel parallelizes the propagation of unaggregated raw features and multi-hop\naggregated features, while introducing batch normalization to enhance the\ndissimilarity in node representations and counteract the smoothness resulting\nfrom topological aggregation. Our experiments show significant improvement with\nour approach.Furthermore, both theoretical and experimental evidence suggest\nthat adding inter-class edges constitutes an easily observable attack pattern.\nWe propose an innovative attack loss that balances attack effectiveness and\nimperceptibility, sacrificing some attack effectiveness to attain greater\nimperceptibility. We also provide experiments to validate the compromise\nperformance achieved through this attack loss.\n","authors":["Zihan Liu","Ge Wang","Yun Luo","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2208.12815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16618v1","updated":"2023-03-29T12:19:23Z","published":"2023-03-29T12:19:23Z","title":"Personalised Language Modelling of Screen Characters Using Rich Metadata\n  Annotations","summary":"  Personalisation of language models for dialogue sensitises them to better\ncapture the speaking patterns of people of specific characteristics, and/or in\nspecific environments. However, rich character annotations are difficult to\ncome by and to successfully leverage. In this work, we release and describe a\nnovel set of manual annotations for 863 speakers from the popular Cornell Movie\nDialog Corpus, including features like characteristic quotes and character\ndescriptions, and a set of six automatically extracted metadata for over 95% of\nthe featured films. We perform extensive experiments on two corpora and show\nthat such annotations can be effectively used to personalise language models,\nreducing perplexity by up to 8.5%. Our method can be applied even zero-shot for\nspeakers for whom no prior training data is available, by relying on\ncombinations of characters' demographic characteristics. Since collecting such\nmetadata is costly, we also contribute a cost-benefit analysis to highlight\nwhich annotations were most cost-effective relative to the reduction in\nperplexity.\n","authors":["Sebastian Vincent","Rowanne Sumner","Alice Dowek","Charlotte Blundell","Emily Preston","Chris Bayliss","Chris Oakley","Carolina Scarton"],"pdf_url":"https://arxiv.org/pdf/2303.16618v1.pdf","comment":"9 pages; 4 figures; 6 tables. Preprint"},{"id":"http://arxiv.org/abs/2208.06828v2","updated":"2023-03-29T12:10:09Z","published":"2022-08-14T11:00:27Z","title":"Multinomial Logistic Regression Algorithms via Quadratic Gradient","summary":"  Multinomial logistic regression, also known by other names such as multiclass\nlogistic regression and softmax regression, is a fundamental classification\nmethod that generalizes binary logistic regression to multiclass problems. A\nrecently work proposed a faster gradient called $\\texttt{quadratic gradient}$\nthat can accelerate the binary logistic regression training, and presented an\nenhanced Nesterov's accelerated gradient (NAG) method for binary logistic\nregression.\n  In this paper, we extend this work to multiclass logistic regression and\npropose an enhanced Adaptive Gradient Algorithm (Adagrad) that can accelerate\nthe original Adagrad method. We test the enhanced NAG method and the enhanced\nAdagrad method on some multiclass-problem datasets. Experimental results show\nthat both enhanced methods converge faster than their original ones\nrespectively.\n","authors":["John Chiang"],"pdf_url":"https://arxiv.org/pdf/2208.06828v2.pdf","comment":"There is a good chance that the enhanced gradient methods for\n  multiclass LR could be used in the classisation neural-network training via\n  the softmax activation and the cross-entropy loss"},{"id":"http://arxiv.org/abs/2209.03282v2","updated":"2023-03-29T12:05:23Z","published":"2022-09-03T05:07:34Z","title":"Quadratic Gradient: Combining Gradient Algorithms and Newton's Method as\n  One","summary":"  It might be inadequate for the line search technique for Newton's method to\nuse only one floating point number. A column vector of the same size as the\ngradient might be better than a mere float number to accelerate each of the\ngradient elements with different rates. Moreover, a square matrix of the same\norder as the Hessian matrix might be helpful to correct the Hessian matrix.\nChiang applied something between a column vector and a square matrix, namely a\ndiagonal matrix, to accelerate the gradient and further proposed a faster\ngradient variant called quadratic gradient. In this paper, we present a new way\nto build a new version of the quadratic gradient. This new quadratic gradient\ndoesn't satisfy the convergence conditions of the fixed Hessian Newton's\nmethod. However, experimental results show that it sometimes has a better\nperformance than the original one in convergence rate. Also, Chiang speculates\nthat there might be a relation between the Hessian matrix and the learning rate\nfor the first-order gradient descent method. We prove that the floating number\n$\\frac{1}{\\epsilon + \\max \\{| \\lambda_i | \\}}$ can be a good learning rate of\nthe gradient methods, where $\\epsilon$ is a number to avoid division by zero\nand $\\lambda_i$ the eigenvalues of the Hessian matrix.\n","authors":["John Chiang"],"pdf_url":"https://arxiv.org/pdf/2209.03282v2.pdf","comment":"In this work, we proposed an enhanced Adam method via quadratic\n  gradient and applied the quadratic gradient to the general numerical\n  optimization problems. The quadratic gradient can indeed be used to build\n  enhanced gradient methods for general optimization problems. There is a good\n  chance that quadratic gradient can also be applied to quasi-Newton methods,\n  such as the famous BFGS method"},{"id":"http://arxiv.org/abs/2210.14914v2","updated":"2023-03-29T11:46:27Z","published":"2022-10-26T17:15:44Z","title":"Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias\n  Boost Machine Abstract Reasoning Ability","summary":"  Great endeavors have been made to study AI's ability in abstract reasoning,\nalong with which different versions of RAVEN's progressive matrices (RPM) are\nproposed as benchmarks. Previous works give inkling that without sophisticated\ndesign or extra meta-data containing semantic information, neural networks may\nstill be indecisive in making decisions regarding RPM problems, after\nrelentless training. Evidenced by thorough experiments and ablation studies, we\nshowcase that end-to-end neural networks embodied with felicitous inductive\nbias, intentionally design or serendipitously match, can solve RPM problems\nelegantly, without the augment of any extra meta-data or preferences of any\nspecific backbone. Our work also reveals that multi-viewpoint with\nmulti-evaluation is a key learning strategy for successful reasoning. Finally,\npotential explanations for the failure of connectionist models in\ngeneralization are provided. We hope that these results will serve as\ninspections of AI's ability beyond perception and toward abstract reasoning.\nSource code can be found in https://github.com/QinglaiWeiCASIA/RavenSolver.\n","authors":["Qinglai Wei","Diancheng Chen","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2210.14914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15975v2","updated":"2023-03-29T11:46:22Z","published":"2023-03-28T13:47:16Z","title":"Large-scale Pre-trained Models are Surprisingly Strong in Incremental\n  Novel Class Discovery","summary":"  Discovering novel concepts from unlabelled data and in a continuous manner is\nan important desideratum of lifelong learners. In the literature such problems\nhave been partially addressed under very restricted settings, where either\naccess to labelled data is provided for discovering novel concepts (e.g., NCD)\nor learning occurs for a limited number of incremental steps (e.g.,\nclass-iNCD). In this work we challenge the status quo and propose a more\nchallenging and practical learning paradigm called MSc-iNCD, where learning\noccurs continuously and unsupervisedly, while exploiting the rich priors from\nlarge-scale pre-trained models. To this end, we propose simple baselines that\nare not only resilient under longer learning scenarios, but are surprisingly\nstrong when compared with sophisticated state-of-the-art methods. We conduct\nextensive empirical evaluation on a multitude of benchmarks and show the\neffectiveness of our proposed baselines, which significantly raises the bar.\n","authors":["Mingxuan Liu","Subhankar Roy","Zhun Zhong","Nicu Sebe","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2303.15975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16604v1","updated":"2023-03-29T11:37:41Z","published":"2023-03-29T11:37:41Z","title":"Bi-directional Training for Composed Image Retrieval via Text Prompt\n  Learning","summary":"  Composed image retrieval searches for a target image based on a multi-modal\nuser query comprised of a reference image and modification text describing the\ndesired changes. Existing approaches to solving this challenging task learn a\nmapping from the (reference image, modification text)-pair to an image\nembedding that is then matched against a large image corpus. One area that has\nnot yet been explored is the reverse direction, which asks the question, what\nreference image when modified as describe by the text would produce the given\ntarget image? In this work we propose a bi-directional training scheme that\nleverages such reversed queries and can be applied to existing composed image\nretrieval architectures. To encode the bi-directional query we prepend a\nlearnable token to the modification text that designates the direction of the\nquery and then finetune the parameters of the text embedding module. We make no\nother changes to the network architecture. Experiments on two standard datasets\nshow that our novel approach achieves improved performance over a baseline\nBLIP-based model that itself already achieves state-of-the-art performance.\n","authors":["Zheyuan Liu","Weixuan Sun","Yicong Hong","Damien Teney","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.16604v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2109.05472v2","updated":"2023-03-29T11:34:30Z","published":"2021-09-12T09:40:18Z","title":"Compute and Energy Consumption Trends in Deep Learning Inference","summary":"  The progress of some AI paradigms such as deep learning is said to be linked\nto an exponential growth in the number of parameters. There are many studies\ncorroborating these trends, but does this translate into an exponential\nincrease in energy consumption? In order to answer this question we focus on\ninference costs rather than training costs, as the former account for most of\nthe computing effort, solely because of the multiplicative factors. Also, apart\nfrom algorithmic innovations, we account for more specific and powerful\nhardware (leading to higher FLOPS) that is usually accompanied with important\nenergy efficiency optimisations. We also move the focus from the first\nimplementation of a breakthrough paper towards the consolidated version of the\ntechniques one or two year later. Under this distinctive and comprehensive\nperspective, we study relevant models in the areas of computer vision and\nnatural language processing: for a sustained increase in performance we see a\nmuch softer growth in energy consumption than previously anticipated. The only\ncaveat is, yet again, the multiplicative factor, as future AI increases\npenetration and becomes more pervasive.\n","authors":["Radosvet Desislavov","Fernando Martínez-Plumed","José Hernández-Orallo"],"pdf_url":"https://arxiv.org/pdf/2109.05472v2.pdf","comment":"For a revised version and its published version refer to: Desislavov,\n  Radosvet, Fernando Mart\\'inez-Plumed, and Jos\\'e Hern\\'andez-Orallo. Trends\n  in AI inference energy consumption: Beyond the performance-vs-parameter laws\n  of deep learning. Sustainable Computing: Informatics and Systems, Volume 38,\n  April 2023. (https://doi.org/10.1016/j.suscom.2023.100857)"},{"id":"http://arxiv.org/abs/2303.16603v1","updated":"2023-03-29T11:33:51Z","published":"2023-03-29T11:33:51Z","title":"Federated Learning in MIMO Satellite Broadcast System","summary":"  Federated learning (FL) is a type of distributed machine learning at the\nwireless edge that preserves the privacy of clients' data from adversaries and\neven the central server. Existing federated learning approaches either use (i)\nsecure multiparty computation (SMC) which is vulnerable to inference or (ii)\ndifferential privacy which may decrease the test accuracy given a large number\nof parties with relatively small amounts of data each. To tackle the problem\nwith the existing methods in the literature, In this paper, we introduce\nincorporate federated learning in the inner-working of MIMO systems.\n","authors":["Raphael Pinard","Mitra Hassani","Wayne Lemieux"],"pdf_url":"https://arxiv.org/pdf/2303.16603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.04475v2","updated":"2023-03-29T11:11:31Z","published":"2022-07-10T14:36:04Z","title":"Finite-time High-probability Bounds for Polyak-Ruppert Averaged Iterates\n  of Linear Stochastic Approximation","summary":"  This paper provides a finite-time analysis of linear stochastic approximation\n(LSA) algorithms with fixed step size, a core method in statistics and machine\nlearning. LSA is used to compute approximate solutions of a $d$-dimensional\nlinear system $\\bar{\\mathbf{A}} \\theta = \\bar{\\mathbf{b}}$ for which\n$(\\bar{\\mathbf{A}}, \\bar{\\mathbf{b}})$ can only be estimated by\n(asymptotically) unbiased observations\n$\\{(\\mathbf{A}(Z_n),\\mathbf{b}(Z_n))\\}_{n \\in \\mathbb{N}}$. We consider here\nthe case where $\\{Z_n\\}_{n \\in \\mathbb{N}}$ is an i.i.d. sequence or a\nuniformly geometrically ergodic Markov chain. We derive $p$-th moment and\nhigh-probability deviation bounds for the iterates defined by LSA and its\nPolyak-Ruppert-averaged version. Our finite-time instance-dependent bounds for\nthe averaged LSA iterates are sharp in the sense that the leading term we\nobtain coincides with the local asymptotic minimax limit. Moreover, the\nremainder terms of our bounds admit a tight dependence on the mixing time\n$t_{\\operatorname{mix}}$ of the underlying chain and the norm of the noise\nvariables. We emphasize that our result requires the SA step size to scale only\nwith logarithm of the problem dimension $d$.\n","authors":["Alain Durmus","Eric Moulines","Alexey Naumov","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2207.04475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16589v1","updated":"2023-03-29T10:49:31Z","published":"2023-03-29T10:49:31Z","title":"Poster: Link between Bias, Node Sensitivity and Long-Tail Distribution\n  in trained DNNs","summary":"  Owing to their remarkable learning (and relearning) capabilities, deep neural\nnetworks (DNNs) find use in numerous real-world applications. However, the\nlearning of these data-driven machine learning models is generally as good as\nthe data available to them for training. Hence, training datasets with\nlong-tail distribution pose a challenge for DNNs, since the DNNs trained on\nthem may provide a varying degree of classification performance across\ndifferent output classes. While the overall bias of such networks is already\nhighlighted in existing works, this work identifies the node bias that leads to\na varying sensitivity of the nodes for different output classes. To the best of\nour knowledge, this is the first work highlighting this unique challenge in\nDNNs, discussing its probable causes, and providing open challenges for this\nnew research direction. We support our reasoning using an empirical case study\nof the networks trained on a real-world dataset.\n","authors":["Mahum Naseer","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.16589v1.pdf","comment":"To appear at the 16th IEEE International Conference on Software\n  Testing, Verification and Validation (ICST 2023), Dublin, Ireland"},{"id":"http://arxiv.org/abs/2303.16585v1","updated":"2023-03-29T10:42:50Z","published":"2023-03-29T10:42:50Z","title":"Quantum Deep Hedging","summary":"  Quantum machine learning has the potential for a transformative impact across\nindustry sectors and in particular in finance. In our work we look at the\nproblem of hedging where deep reinforcement learning offers a powerful\nframework for real markets. We develop quantum reinforcement learning methods\nbased on policy-search and distributional actor-critic algorithms that use\nquantum neural network architectures with orthogonal and compound layers for\nthe policy and value functions. We prove that the quantum neural networks we\nuse are trainable, and we perform extensive simulations that show that quantum\nmodels can reduce the number of trainable parameters while achieving comparable\nperformance and that the distributional approach obtains better performance\nthan other standard approaches, both classical and quantum. We successfully\nimplement the proposed models on a trapped-ion quantum processor, utilizing\ncircuits with up to $16$ qubits, and observe performance that agrees well with\nnoiseless simulation. Our quantum techniques are general and can be applied to\nother reinforcement learning problems beyond hedging.\n","authors":["El Amine Cherrat","Snehal Raj","Iordanis Kerenidis","Abhishek Shekhar","Ben Wood","Jon Dee","Shouvanik Chakrabarti","Richard Chen","Dylan Herman","Shaohan Hu","Pierre Minssen","Ruslan Shaydulin","Yue Sun","Romina Yalovetzky","Marco Pistoia"],"pdf_url":"https://arxiv.org/pdf/2303.16585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12989v2","updated":"2023-03-29T10:37:40Z","published":"2022-12-26T02:32:20Z","title":"Improved Kernel Alignment Regret Bound for Online Kernel Learning","summary":"  In this paper, we improve the kernel alignment regret bound for online kernel\nlearning in the regime of the Hinge loss function. Previous algorithm achieves\na regret of $O((\\mathcal{A}_TT\\ln{T})^{\\frac{1}{4}})$ at a computational\ncomplexity (space and per-round time) of $O(\\sqrt{\\mathcal{A}_TT\\ln{T}})$,\nwhere $\\mathcal{A}_T$ is called \\textit{kernel alignment}. We propose an\nalgorithm whose regret bound and computational complexity are better than\nprevious results. Our results depend on the decay rate of eigenvalues of the\nkernel matrix. If the eigenvalues of the kernel matrix decay exponentially,\nthen our algorithm enjoys a regret of $O(\\sqrt{\\mathcal{A}_T})$ at a\ncomputational complexity of $O(\\ln^2{T})$. Otherwise, our algorithm enjoys a\nregret of $O((\\mathcal{A}_TT)^{\\frac{1}{4}})$ at a computational complexity of\n$O(\\sqrt{\\mathcal{A}_TT})$. We extend our algorithm to batch learning and\nobtain a $O(\\frac{1}{T}\\sqrt{\\mathbb{E}[\\mathcal{A}_T]})$ excess risk bound\nwhich improves the previous $O(1/\\sqrt{T})$ bound.\n","authors":["Junfan Li","Shizhong Liao"],"pdf_url":"https://arxiv.org/pdf/2212.12989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.06424v4","updated":"2023-03-29T10:11:26Z","published":"2022-06-13T19:08:36Z","title":"Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual\n  Correspondence","summary":"  Next generation cellular networks will implement radio sensing functions\nalongside customary communications, thereby enabling unprecedented worldwide\nsensing coverage outdoors. Deep learning has revolutionised computer vision but\nhas had limited application to radio perception tasks, in part due to lack of\nsystematic datasets and benchmarks dedicated to the study of the performance\nand promise of radio sensing. To address this gap, we present MaxRay: a\nsynthetic radio-visual dataset and benchmark that facilitate precise target\nlocalisation in radio. We further propose to learn to localise targets in radio\nwithout supervision by extracting self-coordinates from radio-visual\ncorrespondence. We use such self-supervised coordinates to train a radio\nlocaliser network. We characterise our performance against a number of\nstate-of-the-art baselines. Our results indicate that accurate radio target\nlocalisation can be automatically learned from paired radio-visual data without\nlabels, which is important for empirical data. This opens the door for vast\ndata scalability and may prove key to realising the promise of robust radio\nsensing atop a unified communication-perception cellular infrastructure.\nDataset will be hosted on IEEE DataPort.\n","authors":["Mohammed Alloulah","Maximilian Arnold"],"pdf_url":"https://arxiv.org/pdf/2206.06424v4.pdf","comment":"To appear in IEEE/CVF CVPR '23"},{"id":"http://arxiv.org/abs/2210.04428v2","updated":"2023-03-29T10:05:04Z","published":"2022-10-10T04:19:53Z","title":"A Simple Baseline that Questions the Use of Pretrained-Models in\n  Continual Learning","summary":"  With the success of pretraining techniques in representation learning, a\nnumber of continual learning methods based on pretrained models have been\nproposed. Some of these methods design continual learning mechanisms on the\npre-trained representations and only allow minimum updates or even no updates\nof the backbone models during the training of continual learning. In this\npaper, we question whether the complexity of these models is needed to achieve\ngood performance by comparing them to a simple baseline that we designed. We\nargue that the pretrained feature extractor itself can be strong enough to\nachieve a competitive or even better continual learning performance on\nSplit-CIFAR100 and CoRe 50 benchmarks. To validate this, we conduct a very\nsimple baseline that 1) use the frozen pretrained model to extract image\nfeatures for every class encountered during the continual learning stage and\ncompute their corresponding mean features on training data, and 2) predict the\nclass of the input based on the nearest neighbor distance between test samples\nand mean features of the classes; i.e., Nearest Mean Classifier (NMC). This\nbaseline is single-headed, exemplar-free, and can be task-free (by updating the\nmeans continually). This baseline achieved 88.53% on 10-Split-CIFAR-100,\nsurpassing most state-of-the-art continual learning methods that are all\ninitialized using the same pretrained transformer model. We hope our baseline\nmay encourage future progress in designing learning systems that can\ncontinually add quality to the learning representations even if they started\nfrom some pretrained weights.\n","authors":["Paul Janson","Wenxuan Zhang","Rahaf Aljundi","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2210.04428v2.pdf","comment":"6 pages, Workshop on Distribution Shifts 2022 , Code available at\n  https://github.com/Pauljanson002/pretrained-cl.git"},{"id":"http://arxiv.org/abs/2206.14649v2","updated":"2023-03-29T10:01:09Z","published":"2022-06-28T03:41:50Z","title":"Cooperative Retriever and Ranker in Deep Recommenders","summary":"  Deep recommender systems (DRS) are intensively applied in modern web\nservices. To deal with the massive web contents, DRS employs a two-stage\nworkflow: retrieval and ranking, to generate its recommendation results. The\nretriever aims to select a small set of relevant candidates from the entire\nitems with high efficiency; while the ranker, usually more precise but\ntime-consuming, is supposed to further refine the best items from the retrieved\ncandidates. Traditionally, the two components are trained either independently\nor within a simple cascading pipeline, which is prone to poor collaboration\neffect. Though some latest works suggested to train retriever and ranker\njointly, there still exist many severe limitations: item distribution shift\nbetween training and inference, false negative, and misalignment of ranking\norder. As such, it remains to explore effective collaborations between\nretriever and ranker.\n","authors":["Xu Huang","Defu Lian","Jin Chen","Zheng Liu","Xing Xie","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2206.14649v2.pdf","comment":"12pages, 4 figures, WWW'23"},{"id":"http://arxiv.org/abs/2303.16565v1","updated":"2023-03-29T09:47:48Z","published":"2023-03-29T09:47:48Z","title":"PMAA: A Progressive Multi-scale Attention Autoencoder Model for\n  High-Performance Cloud Removal from Multi-temporal Satellite Imagery","summary":"  Satellite imagery analysis plays a vital role in remote sensing, but the\ninformation loss caused by cloud cover seriously hinders its application. This\nstudy presents a high-performance cloud removal architecture called Progressive\nMulti-scale Attention Autoencoder (PMAA), which simultaneously leverages global\nand local information. It mainly consists of a cloud detection backbone and a\ncloud removal module. The cloud detection backbone uses cloud masks to\nreinforce cloudy areas to prompt the cloud removal module. The cloud removal\nmodule mainly comprises a novel Multi-scale Attention Module (MAM) and a Local\nInteraction Module (LIM). PMAA establishes the long-range dependency of\nmulti-scale features using MAM and modulates the reconstruction of the\nfine-grained details using LIM, allowing for the simultaneous representation of\nfine- and coarse-grained features at the same level. With the help of diverse\nand multi-scale feature representation, PMAA outperforms the previous\nstate-of-the-art model CTGAN consistently on the Sen2_MTC_Old and Sen2_MTC_New\ndatasets. Furthermore, PMAA has a considerable efficiency advantage, with only\n0.5% and 14.6% of the parameters and computational complexity of CTGAN,\nrespectively. These extensive results highlight the potential of PMAA as a\nlightweight cloud removal network suitable for deployment on edge devices. We\nwill release the code and trained models to facilitate the study in this\ndirection.\n","authors":["Xuechao Zou","Kai Li","Junliang Xing","Pin Tao","Yachao Cui"],"pdf_url":"https://arxiv.org/pdf/2303.16565v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.16563v1","updated":"2023-03-29T09:45:50Z","published":"2023-03-29T09:45:50Z","title":"Plan4MC: Skill Reinforcement Learning and Planning for Open-World\n  Minecraft Tasks","summary":"  We study building a multi-task agent in Minecraft. Without human\ndemonstrations, solving long-horizon tasks in this open-ended environment with\nreinforcement learning (RL) is extremely sample inefficient. To tackle the\nchallenge, we decompose solving Minecraft tasks into learning basic skills and\nplanning over the skills. We propose three types of fine-grained basic skills\nin Minecraft, and use RL with intrinsic rewards to accomplish basic skills with\nhigh success rates. For skill planning, we use Large Language Models to find\nthe relationships between skills and build a skill graph in advance. When the\nagent is solving a task, our skill search algorithm walks on the skill graph\nand generates the proper skill plans for the agent. In experiments, our method\naccomplishes 24 diverse Minecraft tasks, where many tasks require sequentially\nexecuting for more than 10 skills. Our method outperforms baselines in most\ntasks by a large margin. The project's website and code can be found at\nhttps://sites.google.com/view/plan4mc.\n","authors":["Haoqi Yuan","Chi Zhang","Hongcheng Wang","Feiyang Xie","Penglin Cai","Hao Dong","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2303.16563v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2209.11908v5","updated":"2023-03-29T09:22:21Z","published":"2022-09-24T02:48:02Z","title":"Fast Lifelong Adaptive Inverse Reinforcement Learning from\n  Demonstrations","summary":"  Learning from Demonstration (LfD) approaches empower end-users to teach\nrobots novel tasks via demonstrations of the desired behaviors, democratizing\naccess to robotics. However, current LfD frameworks are not capable of fast\nadaptation to heterogeneous human demonstrations nor the large-scale deployment\nin ubiquitous robotics applications. In this paper, we propose a novel LfD\nframework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Our\napproach (1) leverages learned strategies to construct policy mixtures for fast\nadaptation to new demonstrations, allowing for quick end-user personalization,\n(2) distills common knowledge across demonstrations, achieving accurate task\ninference; and (3) expands its model only when needed in lifelong deployments,\nmaintaining a concise set of prototypical strategies that can approximate all\nbehaviors via policy mixtures. We empirically validate that FLAIR achieves\nadaptability (i.e., the robot adapts to heterogeneous, user-specific task\npreferences), efficiency (i.e., the robot achieves sample-efficient\nadaptation), and scalability (i.e., the model grows sublinearly with the number\nof demonstrations while maintaining high performance). FLAIR surpasses\nbenchmarks across three control tasks with an average 57% improvement in policy\nreturns and an average 78% fewer episodes required for demonstration modeling\nusing policy mixtures. Finally, we demonstrate the success of FLAIR in a table\ntennis task and find users rate FLAIR as having higher task (p<.05) and\npersonalization (p<.05) performance.\n","authors":["Letian Chen","Sravan Jayanthi","Rohan Paleja","Daniel Martin","Viacheslav Zakharov","Matthew Gombolay"],"pdf_url":"https://arxiv.org/pdf/2209.11908v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02992v4","updated":"2023-03-29T09:18:05Z","published":"2022-10-06T15:34:29Z","title":"COVID-19 Detection Using Segmentation, Region Extraction and\n  Classification Pipeline","summary":"  The main purpose of this study is to develop a pipeline for COVID-19\ndetection from a big and challenging database of Computed Tomography (CT)\nimages. The proposed pipeline includes a segmentation part, a lung extraction\npart, and a classifier part. Optional slice removal techniques after UNet-based\nsegmentation of slices were also tried. The methodologies tried in the\nsegmentation part are traditional segmentation methods as well as UNet-based\nmethods. In the classification part, a Convolutional Neural Network (CNN) was\nused to take the final diagnosis decisions. In terms of the results: in the\nsegmentation part, the proposed segmentation methods show high dice scores on a\npublicly available dataset. In the classification part, the results were\ncompared at slice-level and at patient-level as well. At slice-level, methods\nwere compared and showed high validation accuracy indicating efficiency in\npredicting 2D slices. At patient level, the proposed methods were also compared\nin terms of validation accuracy and macro F1 score on the validation set. The\ndataset used for classification is COV-19CT Database. The method proposed here\nshowed improvement from our precious results on the same dataset. In\nConclusion, the improved work in this paper has potential clinical usages for\nCOVID-19 detection and diagnosis via CT images. The code is on github at\nhttps://github.com/IDU-CVLab/COV19D_3rd\n","authors":["Kenan Morani"],"pdf_url":"https://arxiv.org/pdf/2210.02992v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16548v1","updated":"2023-03-29T09:14:38Z","published":"2023-03-29T09:14:38Z","title":"Policy Gradient Methods for Discrete Time Linear Quadratic Regulator\n  With Random Parameters","summary":"  This paper studies an infinite horizon optimal control problem for\ndiscrete-time linear system and quadratic criteria, both with random parameters\nwhich are independent and identically distributed with respect to time. In this\ngeneral setting, we apply the policy gradient method, a reinforcement learning\ntechnique, to search for the optimal control without requiring knowledge of\nstatistical information of the parameters. We investigate the sub-Gaussianity\nof the state process and establish global linear convergence guarantee for this\napproach based on assumptions that are weaker and easier to verify compared to\nexisting results. Numerical experiments are presented to illustrate our result.\n","authors":["Deyue Li"],"pdf_url":"https://arxiv.org/pdf/2303.16548v1.pdf","comment":"55 pages, 3 figures"},{"id":"http://arxiv.org/abs/2211.09702v2","updated":"2023-03-29T09:10:16Z","published":"2022-10-10T09:37:53Z","title":"Deep Reinforcement Learning Based Joint Downlink Beamforming and RIS\n  Configuration in RIS-aided MU-MISO Systems Under Hardware Impairments and\n  Imperfect CSI","summary":"  We introduce a novel deep reinforcement learning (DRL) approach to jointly\noptimize transmit beamforming and reconfigurable intelligent surface (RIS)\nphase shifts in a multiuser multiple input single output (MU-MISO) system to\nmaximize the sum downlink rate under the phase-dependent reflection amplitude\nmodel. Our approach addresses the challenge of imperfect channel state\ninformation (CSI) and hardware impairments by considering a practical RIS\namplitude model. We compare the performance of our approach against a vanilla\nDRL agent in two scenarios: perfect CSI and phase-dependent RIS amplitudes, and\nmismatched CSI and ideal RIS reflections. The results demonstrate that the\nproposed framework significantly outperforms the vanilla DRL agent under\nmismatch and approaches the golden standard. Our contributions include\nmodifications to the DRL approach to address the joint design of transmit\nbeamforming and phase shifts and the phase-dependent amplitude model. To the\nbest of our knowledge, our method is the first DRL-based approach for the\nphase-dependent reflection amplitude model in RIS-aided MU-MISO systems. Our\nfindings in this study highlight the potential of our approach as a promising\nsolution to overcome hardware impairments in RIS-aided wireless communication\nsystems.\n","authors":["Baturay Saglam","Doga Gurgunoglu","Suleyman S. Kozat"],"pdf_url":"https://arxiv.org/pdf/2211.09702v2.pdf","comment":"2023 IEEE International Conference on Communications Workshops (ICC\n  Workshops)"},{"id":"http://arxiv.org/abs/2210.15659v3","updated":"2023-03-29T08:54:29Z","published":"2022-10-27T17:59:09Z","title":"A Primal-dual Approach for Solving Variational Inequalities with\n  General-form Constraints","summary":"  Yang et al. (2023) recently addressed the open problem of solving Variational\nInequalities (VIs) with equality and inequality constraints through a\nfirst-order gradient method. However, the proposed primal-dual method called\nACVI is applicable when we can compute analytic solutions of its subproblems;\nthus, the general case remains an open problem. In this paper, we adopt a\nwarm-starting technique where we solve the subproblems approximately at each\niteration and initialize the variables with the approximate solution found at\nthe previous iteration. We prove its convergence and show that the gap function\nof the last iterate of this inexact-ACVI method decreases at a rate of\n$\\mathcal{O}(\\frac{1}{\\sqrt{K}})$ when the operator is $L$-Lipschitz and\nmonotone, provided that the errors decrease at appropriate rates.\nInterestingly, we show that often in numerical experiments, this technique\nconverges faster than its exact counterpart. Furthermore, for the cases when\nthe inequality constraints are simple, we propose a variant of ACVI named\nP-ACVI and prove its convergence for the same setting. We further demonstrate\nthe efficacy of the proposed methods through numerous experiments. We also\nrelax the assumptions in Yang et al., yielding, to our knowledge, the first\nconvergence result that does not rely on the assumption that the operator is\n$L$-Lipschitz. Our source code is provided at\n$\\texttt{https://github.com/mpagli/Revisiting-ACVI}$.\n","authors":["Tatjana Chavdarova","Matteo Pagliardini","Tong Yang","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2210.15659v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2206.10575"},{"id":"http://arxiv.org/abs/2303.16535v1","updated":"2023-03-29T08:51:28Z","published":"2023-03-29T08:51:28Z","title":"Nonlinear Independent Component Analysis for Principled Disentanglement\n  in Unsupervised Deep Learning","summary":"  A central problem in unsupervised deep learning is how to find useful\nrepresentations of high-dimensional data, sometimes called \"disentanglement\".\nMost approaches are heuristic and lack a proper theoretical foundation. In\nlinear representation learning, independent component analysis (ICA) has been\nsuccessful in many applications areas, and it is principled, i.e. based on a\nwell-defined probabilistic model. However, extension of ICA to the nonlinear\ncase has been problematic due to the lack of identifiability, i.e. uniqueness\nof the representation. Recently, nonlinear extensions that utilize temporal\nstructure or some auxiliary information have been proposed. Such models are in\nfact identifiable, and consequently, an increasing number of algorithms have\nbeen developed. In particular, some self-supervised algorithms can be shown to\nestimate nonlinear ICA, even though they have initially been proposed from\nheuristic perspectives. This paper reviews the state-of-the-art of nonlinear\nICA theory and algorithms.\n","authors":["Aapo Hyvarinen","Ilyes Khemakhem","Hiroshi Morioka"],"pdf_url":"https://arxiv.org/pdf/2303.16535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16532v1","updated":"2023-03-29T08:39:36Z","published":"2023-03-29T08:39:36Z","title":"Futures Quantitative Investment with Heterogeneous Continual Graph\n  Neural Network","summary":"  It is a challenging problem to predict trends of futures prices with\ntraditional econometric models as one needs to consider not only futures'\nhistorical data but also correlations among different futures. Spatial-temporal\ngraph neural networks (STGNNs) have great advantages in dealing with such kind\nof spatial-temporal data. However, we cannot directly apply STGNNs to\nhigh-frequency future data because future investors have to consider both the\nlong-term and short-term characteristics when doing decision-making. To capture\nboth the long-term and short-term features, we exploit more label information\nby designing four heterogeneous tasks: price regression, price moving average\nregression, price gap regression (within a short interval), and change-point\ndetection, which involve both long-term and short-term scenes. To make full use\nof these labels, we train our model in a continual manner. Traditional\ncontinual GNNs define the gradient of prices as the parameter important to\novercome catastrophic forgetting (CF). Unfortunately, the losses of the four\nheterogeneous tasks lie in different spaces. Hence it is improper to calculate\nthe parameter importance with their losses. We propose to calculate parameter\nimportance with mutual information between original observations and the\nextracted features. The empirical results based on 49 commodity futures\ndemonstrate that our model has higher prediction performance on capturing\nlong-term or short-term dynamic change.\n","authors":["Zhizhong Tan","Min Hu","Yixuan Wang","Lu Wei","Bin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.16532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16529v1","updated":"2023-03-29T08:35:11Z","published":"2023-03-29T08:35:11Z","title":"Importance Sampling for Stochastic Gradient Descent in Deep Neural\n  Networks","summary":"  Stochastic gradient descent samples uniformly the training set to build an\nunbiased gradient estimate with a limited number of samples. However, at a\ngiven step of the training process, some data are more helpful than others to\ncontinue learning. Importance sampling for training deep neural networks has\nbeen widely studied to propose sampling schemes yielding better performance\nthan the uniform sampling scheme. After recalling the theory of importance\nsampling for deep learning, this paper reviews the challenges inherent to this\nresearch area. In particular, we propose a metric allowing the assessment of\nthe quality of a given sampling scheme; and we study the interplay between the\nsampling scheme and the optimizer used.\n","authors":["Thibault Lahire"],"pdf_url":"https://arxiv.org/pdf/2303.16529v1.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.16524v1","updated":"2023-03-29T08:26:26Z","published":"2023-03-29T08:26:26Z","title":"Ensemble Learning Model on Artificial Neural Network-Backpropagation\n  (ANN-BP) Architecture for Coal Pillar Stability Classification","summary":"  Pillars are important structural units used to ensure mining safety in\nunderground hard rock mines. Therefore, precise predictions regarding the\nstability of underground pillars are required. One common index that is often\nused to assess pillar stability is the Safety Factor (SF). Unfortunately, such\ncrisp boundaries in pillar stability assessment using SF are unreliable. This\npaper presents a novel application of Artificial Neural Network-Backpropagation\n(ANN-BP) and Deep Ensemble Learning for pillar stability classification. There\nare three types of ANN-BP used for the classification of pillar stability\ndistinguished by their activation functions: ANN-BP ReLU, ANN-BP ELU, and\nANN-BP GELU. This research also presents a new labeling alternative for pillar\nstability by considering its suitability with the SF. Thus, pillar stability is\nexpanded into four categories: failed with a suitable safety factor, intact\nwith a suitable safety factor, failed without a suitable safety factor, and\nintact without a suitable safety factor. There are five inputs used for each\nmodel: pillar width, mining height, bord width, depth to floor, and ratio. The\nresults showed that the ANN-BP model with Ensemble Learning could improve\nANN-BP performance with an average accuracy of 86.48% and an F_2-score of\n96.35% for the category of failed with a suitable safety factor.\n","authors":["G. Aileen Mendrofa","Gatot Fatwanto Hertono","Bevina Desjwiandara Handari"],"pdf_url":"https://arxiv.org/pdf/2303.16524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16521v1","updated":"2023-03-29T08:23:26Z","published":"2023-03-29T08:23:26Z","title":"Hard Regularization to Prevent Collapse in Online Deep Clustering\n  without Data Augmentation","summary":"  Online deep clustering refers to the joint use of a feature extraction\nnetwork and a clustering model to assign cluster labels to each new data point\nor batch as it is processed. While faster and more versatile than offline\nmethods, online clustering can easily reach the collapsed solution where the\nencoder maps all inputs to the same point and all are put into a single\ncluster. Successful existing models have employed various techniques to avoid\nthis problem, most of which require data augmentation or which aim to make the\naverage soft assignment across the dataset the same for each cluster. We\npropose a method that does not require data augmentation, and that, differently\nfrom existing methods, regularizes the hard assignments. Using a Bayesian\nframework, we derive an intuitive optimization objective that can be\nstraightforwardly included in the training of the encoder network. Tested on\nfour image datasets, we show that it consistently avoids collapse more robustly\nthan other methods and that it leads to more accurate clustering. We also\nconduct further experiments and analyses justifying our choice to regularize\nthe hard cluster assignments.\n","authors":["Louis Mahon","Thomas Lukasiewicz"],"pdf_url":"https://arxiv.org/pdf/2303.16521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16520v1","updated":"2023-03-29T08:21:54Z","published":"2023-03-29T08:21:54Z","title":"Fair Federated Medical Image Segmentation via Client Contribution\n  Estimation","summary":"  How to ensure fairness is an important topic in federated learning (FL).\nRecent studies have investigated how to reward clients based on their\ncontribution (collaboration fairness), and how to achieve uniformity of\nperformance across clients (performance fairness). Despite achieving progress\non either one, we argue that it is critical to consider them together, in order\nto engage and motivate more diverse clients joining FL to derive a high-quality\nglobal model. In this work, we propose a novel method to optimize both types of\nfairness simultaneously. Specifically, we propose to estimate client\ncontribution in gradient and data space. In gradient space, we monitor the\ngradient direction differences of each client with respect to others. And in\ndata space, we measure the prediction error on client data using an auxiliary\nmodel. Based on this contribution estimation, we propose a FL method, federated\ntraining via contribution estimation (FedCE), i.e., using estimation as global\nmodel aggregation weights. We have theoretically analyzed our method and\nempirically evaluated it on two real-world medical datasets. The effectiveness\nof our approach has been validated with significant performance improvements,\nbetter collaboration fairness, better performance fairness, and comprehensive\nanalytical studies.\n","authors":["Meirui Jiang","Holger R Roth","Wenqi Li","Dong Yang","Can Zhao","Vishwesh Nath","Daguang Xu","Qi Dou","Ziyue Xu"],"pdf_url":"https://arxiv.org/pdf/2303.16520v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15364v2","updated":"2023-03-29T08:08:52Z","published":"2023-03-13T13:36:16Z","title":"Inflation forecasting with attention based transformer neural networks","summary":"  Inflation is a major determinant for allocation decisions and its forecast is\na fundamental aim of governments and central banks. However, forecasting\ninflation is not a trivial task, as its prediction relies on low frequency,\nhighly fluctuating data with unclear explanatory variables. While classical\nmodels show some possibility of predicting inflation, reliably beating the\nrandom walk benchmark remains difficult. Recently, (deep) neural networks have\nshown impressive results in a multitude of applications, increasingly setting\nthe new state-of-the-art. This paper investigates the potential of the\ntransformer deep neural network architecture to forecast different inflation\nrates. The results are compared to a study on classical time series and machine\nlearning models. We show that our adapted transformer, on average, outperforms\nthe baseline in 6 out of 16 experiments, showing best scores in two out of four\ninvestigated inflation rates. Our results demonstrate that a transformer based\nneural network can outperform classical regression and machine learning models\nin certain inflation rates and forecasting horizons.\n","authors":["Maximilian Tschuchnig","Petra Tschuchnig","Cornelia Ferner","Michael Gadermayr"],"pdf_url":"https://arxiv.org/pdf/2303.15364v2.pdf","comment":"Paper was rejected and we want to switch to a new dataset. So there\n  will not be a simple resubmit with minor changes but some bigger changes in\n  1. Dataset and 2. Discussion. We would later resubmit again. Thank you!"},{"id":"http://arxiv.org/abs/2303.16510v1","updated":"2023-03-29T07:36:54Z","published":"2023-03-29T07:36:54Z","title":"Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms\n  for Optimization under Orthogonality Constraints","summary":"  Orthogonality constraints naturally appear in many machine learning problems,\nfrom Principal Components Analysis to robust neural network training. They are\nusually solved using Riemannian optimization algorithms, which minimize the\nobjective function while enforcing the constraint. However, enforcing the\northogonality constraint can be the most time-consuming operation in such\nalgorithms. Recently, Ablin & Peyr\\'e (2022) proposed the Landing algorithm, a\nmethod with cheap iterations that does not enforce the orthogonality constraint\nbut is attracted towards the manifold in a smooth manner. In this article, we\nprovide new practical and theoretical developments for the landing algorithm.\nFirst, the method is extended to the Stiefel manifold, the set of rectangular\northogonal matrices. We also consider stochastic and variance reduction\nalgorithms when the cost function is an average of many functions. We\ndemonstrate that all these methods have the same rate of convergence as their\nRiemannian counterparts that exactly enforce the constraint. Finally, our\nexperiments demonstrate the promise of our approach to an array of\nmachine-learning problems that involve orthogonality constraints.\n","authors":["Pierre Ablin","Simon Vary","Bin Gao","P. -A. Absil"],"pdf_url":"https://arxiv.org/pdf/2303.16510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16506v1","updated":"2023-03-29T07:32:01Z","published":"2023-03-29T07:32:01Z","title":"Local Interpretability of Random Forests for Multi-Target Regression","summary":"  Multi-target regression is useful in a plethora of applications. Although\nrandom forest models perform well in these tasks, they are often difficult to\ninterpret. Interpretability is crucial in machine learning, especially when it\ncan directly impact human well-being. Although model-agnostic techniques exist\nfor multi-target regression, specific techniques tailored to random forest\nmodels are not available. To address this issue, we propose a technique that\nprovides rule-based interpretations for instances made by a random forest model\nfor multi-target regression, influenced by a recent model-specific technique\nfor random forest interpretability. The proposed technique was evaluated\nthrough extensive experiments and shown to offer competitive interpretations\ncompared to state-of-the-art techniques.\n","authors":["Avraam Bardos","Nikolaos Mylonas","Ioannis Mollas","Grigorios Tsoumakas"],"pdf_url":"https://arxiv.org/pdf/2303.16506v1.pdf","comment":"8 pages, 1 figure, 2 tables, to be submitted to XAI conference 2023\n  as an extended abstract"},{"id":"http://arxiv.org/abs/2303.16504v1","updated":"2023-03-29T07:29:07Z","published":"2023-03-29T07:29:07Z","title":"An Over-parameterized Exponential Regression","summary":"  Over the past few years, there has been a significant amount of research\nfocused on studying the ReLU activation function, with the aim of achieving\nneural network convergence through over-parametrization. However, recent\ndevelopments in the field of Large Language Models (LLMs) have sparked interest\nin the use of exponential activation functions, specifically in the attention\nmechanism.\n  Mathematically, we define the neural function $F: \\mathbb{R}^{d \\times m}\n\\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ using an exponential activation\nfunction. Given a set of data points with labels $\\{(x_1, y_1), (x_2, y_2),\n\\dots, (x_n, y_n)\\} \\subset \\mathbb{R}^d \\times \\mathbb{R}$ where $n$ denotes\nthe number of the data. Here $F(W(t),x)$ can be expressed as $F(W(t),x) :=\n\\sum_{r=1}^m a_r \\exp(\\langle w_r, x \\rangle)$, where $m$ represents the number\nof neurons, and $w_r(t)$ are weights at time $t$. It's standard in literature\nthat $a_r$ are the fixed weights and it's never changed during the training. We\ninitialize the weights $W(0) \\in \\mathbb{R}^{d \\times m}$ with random Gaussian\ndistributions, such that $w_r(0) \\sim \\mathcal{N}(0, I_d)$ and initialize $a_r$\nfrom random sign distribution for each $r \\in [m]$.\n  Using the gradient descent algorithm, we can find a weight $W(T)$ such that\n$\\| F(W(T), X) - y \\|_2 \\leq \\epsilon$ holds with probability $1-\\delta$, where\n$\\epsilon \\in (0,0.1)$ and $m = \\Omega(n^{2+o(1)}\\log(n/\\delta))$. To optimize\nthe over-parameterization bound $m$, we employ several tight analysis\ntechniques from previous studies [Song and Yang arXiv 2019, Munteanu, Omlor,\nSong and Woodruff ICML 2022].\n","authors":["Yeqi Gao","Sridhar Mahadevan","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2303.16504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14202v2","updated":"2023-03-29T07:27:28Z","published":"2023-02-27T23:37:03Z","title":"Mixtures of All Trees","summary":"  Tree-shaped graphical models are widely used for their tractability. However,\nthey unfortunately lack expressive power as they require committing to a\nparticular sparse dependency structure. We propose a novel class of generative\nmodels called mixtures of all trees: that is, a mixture over all possible\n($n^{n-2}$) tree-shaped graphical models over $n$ variables. We show that it is\npossible to parameterize this Mixture of All Trees (MoAT) model compactly\n(using a polynomial-size representation) in a way that allows for tractable\nlikelihood computation and optimization via stochastic gradient descent.\nFurthermore, by leveraging the tractability of tree-shaped models, we devise\nfast-converging conditional sampling algorithms for approximate inference, even\nthough our theoretical analysis suggests that exact computation of marginals in\nthe MoAT model is NP-hard. Empirically, MoAT achieves state-of-the-art\nperformance on density estimation benchmarks when compared against powerful\nprobabilistic models including hidden Chow-Liu Trees.\n","authors":["Nikil Roashan Selvam","Honghua Zhang","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2302.14202v2.pdf","comment":"Accepted to AISTATS 2023"},{"id":"http://arxiv.org/abs/2204.08467v2","updated":"2023-03-29T07:25:54Z","published":"2022-04-16T08:26:19Z","title":"IOP-FL: Inside-Outside Personalization for Federated Medical Image\n  Segmentation","summary":"  Federated learning (FL) allows multiple medical institutions to\ncollaboratively learn a global model without centralizing client data. It is\ndifficult, if possible at all, for such a global model to commonly achieve\noptimal performance for each individual client, due to the heterogeneity of\nmedical images from various scanners and patient demographics. This problem\nbecomes even more significant when deploying the global model to unseen clients\noutside the FL with unseen distributions not presented during federated\ntraining. To optimize the prediction accuracy of each individual client for\nmedical imaging tasks, we propose a novel unified framework for both\n\\textit{Inside and Outside model Personalization in FL} (IOP-FL). Our inside\npersonalization uses a lightweight gradient-based approach that exploits the\nlocal adapted model for each client, by accumulating both the global gradients\nfor common knowledge and the local gradients for client-specific optimization.\nMoreover, and importantly, the obtained local personalized models and the\nglobal model can form a diverse and informative routing space to personalize an\nadapted model for outside FL clients. Hence, we design a new test-time routing\nscheme using the consistency loss with a shape constraint to dynamically\nincorporate the models, given the distribution information conveyed by the test\ndata. Our extensive experimental results on two medical image segmentation\ntasks present significant improvements over SOTA methods on both inside and\noutside personalization, demonstrating the potential of our IOP-FL scheme for\nclinical practice.\n","authors":["Meirui Jiang","Hongzheng Yang","Chen Cheng","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2204.08467v2.pdf","comment":"Accepted by IEEE TMI special issue on federated learning for medical\n  imaging"},{"id":"http://arxiv.org/abs/2303.16502v1","updated":"2023-03-29T07:25:03Z","published":"2023-03-29T07:25:03Z","title":"Unified analysis of SGD-type methods","summary":"  This note focuses on a simple approach to the unified analysis of SGD-type\nmethods from (Gorbunov et al., 2020) for strongly convex smooth optimization\nproblems. The similarities in the analyses of different stochastic first-order\nmethods are discussed along with the existing extensions of the framework. The\nlimitations of the analysis and several alternative approaches are mentioned as\nwell.\n","authors":["Eduard Gorbunov"],"pdf_url":"https://arxiv.org/pdf/2303.16502v1.pdf","comment":"Part of the Encyclopedia of Optimization. 8 pages"},{"id":"http://arxiv.org/abs/2303.02304v2","updated":"2023-03-29T06:50:55Z","published":"2023-03-04T03:06:47Z","title":"Coupled Multiwavelet Neural Operator Learning for Coupled Partial\n  Differential Equations","summary":"  Coupled partial differential equations (PDEs) are key tasks in modeling the\ncomplex dynamics of many physical processes. Recently, neural operators have\nshown the ability to solve PDEs by learning the integral kernel directly in\nFourier/Wavelet space, so the difficulty for solving the coupled PDEs depends\non dealing with the coupled mappings between the functions. Towards this end,\nwe propose a \\textit{coupled multiwavelets neural operator} (CMWNO) learning\nscheme by decoupling the coupled integral kernels during the multiwavelet\ndecomposition and reconstruction procedures in the Wavelet space. The proposed\nmodel achieves significantly higher accuracy compared to previous\nlearning-based solvers in solving the coupled PDEs including Gray-Scott (GS)\nequations and the non-local mean field game (MFG) problem. According to our\nexperimental results, the proposed model exhibits a $2\\times \\sim 4\\times$\nimprovement relative $L$2 error compared to the best results from the\nstate-of-the-art models.\n","authors":["Xiongye Xiao","Defu Cao","Ruochen Yang","Gaurav Gupta","Gengshuo Liu","Chenzhong Yin","Radu Balan","Paul Bogdan"],"pdf_url":"https://arxiv.org/pdf/2303.02304v2.pdf","comment":"Accepted to ICLR 2023"},{"id":"http://arxiv.org/abs/2210.12350v2","updated":"2023-03-29T06:35:48Z","published":"2022-10-22T04:38:00Z","title":"Instance-Aware Image Completion","summary":"  Image completion is a task that aims to fill in the missing region of a\nmasked image with plausible contents. However, existing image completion\nmethods tend to fill in the missing region with the surrounding texture instead\nof hallucinating a visual instance that is suitable in accordance with the\ncontext of the scene. In this work, we propose a novel image completion model,\ndubbed ImComplete, that hallucinates the missing instance that harmonizes well\nwith - and thus preserves - the original context. ImComplete first adopts a\ntransformer architecture that considers the visible instances and the location\nof the missing region. Then, ImComplete completes the semantic segmentation\nmasks within the missing region, providing pixel-level semantic and structural\nguidance. Finally, the image synthesis blocks generate photo-realistic content.\nWe perform a comprehensive evaluation of the results in terms of visual quality\n(LPIPS and FID) and contextual preservation scores (CLIPscore and object\ndetection accuracy) with COCO-panoptic and Visual Genome datasets. Experimental\nresults show the superiority of ImComplete on various natural images.\n","authors":["Jinoh Cho","Minguk Kang","Vibhav Vineet","Jaesik Park"],"pdf_url":"https://arxiv.org/pdf/2210.12350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16464v1","updated":"2023-03-29T05:33:53Z","published":"2023-03-29T05:33:53Z","title":"Lipschitzness Effect of a Loss Function on Generalization Performance of\n  Deep Neural Networks Trained by Adam and AdamW Optimizers","summary":"  The generalization performance of deep neural networks with regard to the\noptimization algorithm is one of the major concerns in machine learning. This\nperformance can be affected by various factors. In this paper, we theoretically\nprove that the Lipschitz constant of a loss function is an important factor to\ndiminish the generalization error of the output model obtained by Adam or\nAdamW. The results can be used as a guideline for choosing the loss function\nwhen the optimization algorithm is Adam or AdamW. In addition, to evaluate the\ntheoretical bound in a practical setting, we choose the human age estimation\nproblem in computer vision. For assessing the generalization better, the\ntraining and test datasets are drawn from different distributions. Our\nexperimental evaluation shows that the loss function with lower Lipschitz\nconstant and maximum value improves the generalization of the model trained by\nAdam or AdamW.\n","authors":["Mohammad Lashkari","Amin Gheibi"],"pdf_url":"https://arxiv.org/pdf/2303.16464v1.pdf","comment":"13 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.01082v2","updated":"2023-03-29T05:25:48Z","published":"2023-03-02T09:04:35Z","title":"GBMST: An Efficient Minimum Spanning Tree Clustering Based on\n  Granular-Ball Computing","summary":"  Most of the existing clustering methods are based on a single granularity of\ninformation, such as the distance and density of each data. This most\nfine-grained based approach is usually inefficient and susceptible to noise.\nTherefore, we propose a clustering algorithm that combines multi-granularity\nGranular-Ball and minimum spanning tree (MST). We construct coarsegrained\ngranular-balls, and then use granular-balls and MST to implement the clustering\nmethod based on \"large-scale priority\", which can greatly avoid the influence\nof outliers and accelerate the construction process of MST. Experimental\nresults on several data sets demonstrate the power of the algorithm. All codes\nhave been released at https://github.com/xjnine/GBMST.\n","authors":["Jiang Xie","Shuyin Xia","Guoyin Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2303.01082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16459v1","updated":"2023-03-29T05:08:21Z","published":"2023-03-29T05:08:21Z","title":"GNNBuilder: An Automated Framework for Generic Graph Neural Network\n  Accelerator Generation, Simulation, and Optimization","summary":"  There are plenty of graph neural network (GNN) accelerators being proposed.\nHowever, they highly rely on users' hardware expertise and are usually\noptimized for one specific GNN model, making them challenging for practical use\n. Therefore, in this work, we propose GNNBuilder, the first automated, generic,\nend-to-end GNN accelerator generation framework. It features four advantages:\n(1) GNNBuilder can automatically generate GNN accelerators for a wide range of\nGNN models arbitrarily defined by users; (2) GNNBuilder takes standard PyTorch\nprogramming interface, introducing zero overhead for algorithm developers; (3)\nGNNBuilder supports end-to-end code generation, simulation, accelerator\noptimization, and hardware deployment, realizing a push-button fashion for GNN\naccelerator design; (4) GNNBuilder is equipped with accurate performance models\nof its generated accelerator, enabling fast and flexible design space\nexploration (DSE). In the experiments, first, we show that our accelerator\nperformance model has errors within $36\\%$ for latency prediction and $18\\%$\nfor BRAM count prediction. Second, we show that our generated accelerators can\noutperform CPU by $6.33\\times$ and GPU by $6.87\\times$. This framework is\nopen-source, and the code is available at\nhttps://anonymous.4open.science/r/gnn-builder-83B4/.\n","authors":["Stefan Abi-Karam","Cong Hao"],"pdf_url":"https://arxiv.org/pdf/2303.16459v1.pdf","comment":"10 pages, 7 figures, 4 tables, 3 listings"},{"id":"http://arxiv.org/abs/2303.16458v1","updated":"2023-03-29T05:05:02Z","published":"2023-03-29T05:05:02Z","title":"When to Pre-Train Graph Neural Networks? An Answer from Data Generation\n  Perspective!","summary":"  Recently, graph pre-training has attracted wide research attention, which\naims to learn transferable knowledge from unlabeled graph data so as to improve\ndownstream performance. Despite these recent attempts, the negative transfer is\na major issue when applying graph pre-trained models to downstream tasks.\nExisting works made great efforts on the issue of what to pre-train and how to\npre-train by designing a number of graph pre-training and fine-tuning\nstrategies. However, there are indeed cases where no matter how advanced the\nstrategy is, the \"pre-train and fine-tune\" paradigm still cannot achieve clear\nbenefits. This paper introduces a generic framework W2PGNN to answer the\ncrucial question of when to pre-train (i.e., in what situations could we take\nadvantage of graph pre-training) before performing effortful pre-training or\nfine-tuning. We start from a new perspective to explore the complex generative\nmechanisms from the pre-training data to downstream data. In particular, W2PGNN\nfirst fits the pre-training data into graphon bases, each element of graphon\nbasis (i.e., a graphon) identifies a fundamental transferable pattern shared by\na collection of pre-training graphs. All convex combinations of graphon bases\ngive rise to a generator space, from which graphs generated form the solution\nspace for those downstream data that can benefit from pre-training. In this\nmanner, the feasibility of pre-training can be quantified as the generation\nprobability of the downstream data from any generator in the generator space.\nW2PGNN provides three broad applications, including providing the application\nscope of graph pre-trained models, quantifying the feasibility of performing\npre-training, and helping select pre-training data to enhance downstream\nperformance. We give a theoretically sound solution for the first application\nand extensive empirical justifications for the latter two applications.\n","authors":["Yuxuan Cao","Jiarong Xu","Carl Yang","Jiaan Wang","Yunchao Zhang","Chunping Wang","Lei Chen","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.16458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15430v2","updated":"2023-03-29T04:49:46Z","published":"2023-03-27T17:54:32Z","title":"TextMI: Textualize Multimodal Information for Integrating Non-verbal\n  Cues in Pre-trained Language Models","summary":"  Pre-trained large language models have recently achieved ground-breaking\nperformance in a wide variety of language understanding tasks. However, the\nsame model can not be applied to multimodal behavior understanding tasks (e.g.,\nvideo sentiment/humor detection) unless non-verbal features (e.g., acoustic and\nvisual) can be integrated with language. Jointly modeling multiple modalities\nsignificantly increases the model complexity, and makes the training process\ndata-hungry. While an enormous amount of text data is available via the web,\ncollecting large-scale multimodal behavioral video datasets is extremely\nexpensive, both in terms of time and money. In this paper, we investigate\nwhether large language models alone can successfully incorporate non-verbal\ninformation when they are presented in textual form. We present a way to\nconvert the acoustic and visual information into corresponding textual\ndescriptions and concatenate them with the spoken text. We feed this augmented\ninput to a pre-trained BERT model and fine-tune it on three downstream\nmultimodal tasks: sentiment, humor, and sarcasm detection. Our approach,\nTextMI, significantly reduces model complexity, adds interpretability to the\nmodel's decision, and can be applied for a diverse set of tasks while achieving\nsuperior (multimodal sarcasm detection) or near SOTA (multimodal sentiment\nanalysis and multimodal humor detection) performance. We propose TextMI as a\ngeneral, competitive baseline for multimodal behavioral analysis tasks,\nparticularly in a low-resource setting.\n","authors":["Md Kamrul Hasan","Md Saiful Islam","Sangwu Lee","Wasifur Rahman","Iftekhar Naim","Mohammed Ibrahim Khan","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2303.15430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16454v1","updated":"2023-03-29T04:43:03Z","published":"2023-03-29T04:43:03Z","title":"Conductivity Imaging from Internal Measurements with Mixed Least-Squares\n  Deep Neural Networks","summary":"  In this work we develop a novel approach using deep neural networks to\nreconstruct the conductivity distribution in elliptic problems from one\ninternal measurement. The approach is based on a mixed reformulation of the\ngoverning equation and utilizes the standard least-squares objective to\napproximate the conductivity and flux simultaneously, with deep neural networks\nas ansatz functions. We provide a thorough analysis of the neural network\napproximations for both continuous and empirical losses, including rigorous\nerror estimates that are explicit in terms of the noise level, various penalty\nparameters and neural network architectural parameters (depth, width and\nparameter bound). We also provide extensive numerical experiments in two- and\nmulti-dimensions to illustrate distinct features of the approach, e.g.,\nexcellent stability with respect to data noise and capability of solving\nhigh-dimensional problems.\n","authors":["Bangti Jin","Xiyao Li","Qimeng Quan","Zhi Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.16454v1.pdf","comment":"28 pages. 12 figures"},{"id":"http://arxiv.org/abs/2303.16452v1","updated":"2023-03-29T04:35:50Z","published":"2023-03-29T04:35:50Z","title":"ProtFIM: Fill-in-Middle Protein Sequence Design via Protein Language\n  Models","summary":"  Protein language models (pLMs), pre-trained via causal language modeling on\nprotein sequences, have been a promising tool for protein sequence design. In\nreal-world protein engineering, there are many cases where the amino acids in\nthe middle of a protein sequence are optimized while maintaining other\nresidues. Unfortunately, because of the left-to-right nature of pLMs, existing\npLMs modify suffix residues by prompting prefix residues, which are\ninsufficient for the infilling task that considers the whole surrounding\ncontext. To find the more effective pLMs for protein engineering, we design a\nnew benchmark, Secondary structureE InFilling rEcoveRy, SEIFER, which\napproximates infilling sequence design scenarios. With the evaluation of\nexisting models on the benchmark, we reveal the weakness of existing language\nmodels and show that language models trained via fill-in-middle transformation,\ncalled ProtFIM, are more appropriate for protein engineering. Also, we prove\nthat ProtFIM generates protein sequences with decent protein representations\nthrough exhaustive experiments and visualizations.\n","authors":["Youhan Lee","Hasun Yu"],"pdf_url":"https://arxiv.org/pdf/2303.16452v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2211.10890v2","updated":"2023-03-29T04:04:55Z","published":"2022-11-20T07:18:56Z","title":"Single-Pass Contrastive Learning Can Work for Both Homophilic and\n  Heterophilic Graph","summary":"  Existing graph contrastive learning (GCL) techniques typically require two\nforward passes for a single instance to construct the contrastive loss, which\nis effective for capturing the low-frequency signals of node features. Such a\ndual-pass design has shown empirical success on homophilic graphs, but its\neffectiveness on heterophilic graphs, where directly connected nodes typically\nhave different labels, is unknown. In addition, existing GCL approaches fail to\nprovide strong performance guarantees. Coupled with the unpredictability of GCL\napproaches on heterophilic graphs, their applicability in real-world contexts\nis limited. Then, a natural question arises: Can we design a GCL method that\nworks for both homophilic and heterophilic graphs with a performance guarantee?\nTo answer this question, we theoretically study the concentration property of\nfeatures obtained by neighborhood aggregation on homophilic and heterophilic\ngraphs, introduce the single-pass graph contrastive learning loss based on the\nproperty, and provide performance guarantees for the minimizer of the loss on\ndownstream tasks. As a direct consequence of our analysis, we implement the\nSingle-Pass Graph Contrastive Learning method (SP-GCL). Empirically, on 14\nbenchmark datasets with varying degrees of homophily, the features learned by\nthe SP-GCL can match or outperform existing strong baselines with significantly\nless computational overhead, which demonstrates the usefulness of our findings\nin real-world cases.\n","authors":["Haonan Wang","Jieyu Zhang","Qi Zhu","Wei Huang","Kenji Kawaguchi","Xiaokui Xiao"],"pdf_url":"https://arxiv.org/pdf/2211.10890v2.pdf","comment":"21 pages, 5 figures, 8 tables. arXiv admin note: substantial text\n  overlap with arXiv:2204.04874. The code is available at\n  https://github.com/haonan3/SPGCL"},{"id":"http://arxiv.org/abs/2211.13123v2","updated":"2023-03-29T04:01:17Z","published":"2022-11-22T02:03:27Z","title":"Motif-aware temporal GCN for fraud detection in signed cryptocurrency\n  trust networks","summary":"  Graph convolutional networks (GCNs) is a class of artificial neural networks\nfor processing data that can be represented as graphs. Since financial\ntransactions can naturally be constructed as graphs, GCNs are widely applied in\nthe financial industry, especially for financial fraud detection. In this\npaper, we focus on fraud detection on cryptocurrency truct networks. In the\nliterature, most works focus on static networks. Whereas in this study, we\nconsider the evolving nature of cryptocurrency networks, and use local\nstructural as well as the balance theory to guide the training process. More\nspecifically, we compute motif matrices to capture the local topological\ninformation, then use them in the GCN aggregation process. The generated\nembedding at each snapshot is a weighted average of embeddings within a time\nwindow, where the weights are learnable parameters. Since the trust networks is\nsigned on each edge, balance theory is used to guide the training process.\nExperimental results on bitcoin-alpha and bitcoin-otc datasets show that the\nproposed model outperforms those in the literature.\n","authors":["Song Li","Jiandong Zhou","Chong MO","Jin LI","Geoffrey K. F. Tso","Yuxing Tian"],"pdf_url":"https://arxiv.org/pdf/2211.13123v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.13814v2","updated":"2023-03-29T03:56:52Z","published":"2022-05-27T08:00:13Z","title":"Global Convergence of Over-parameterized Deep Equilibrium Models","summary":"  A deep equilibrium model (DEQ) is implicitly defined through an equilibrium\npoint of an infinite-depth weight-tied model with an input-injection. Instead\nof infinite computations, it solves an equilibrium point directly with\nroot-finding and computes gradients with implicit differentiation. The training\ndynamics of over-parameterized DEQs are investigated in this study. By\nsupposing a condition on the initial equilibrium point, we show that the unique\nequilibrium point always exists during the training process, and the gradient\ndescent is proved to converge to a globally optimal solution at a linear\nconvergence rate for the quadratic loss function. In order to show that the\nrequired initial condition is satisfied via mild over-parameterization, we\nperform a fine-grained analysis on random DEQs. We propose a novel\nprobabilistic framework to overcome the technical difficulty in the\nnon-asymptotic analysis of infinite-depth weight-tied models.\n","authors":["Zenan Ling","Xingyu Xie","Qiuhao Wang","Zongpeng Zhang","Zhouchen Lin"],"pdf_url":"https://arxiv.org/pdf/2205.13814v2.pdf","comment":"Accepted by AISTATS 2023"},{"id":"http://arxiv.org/abs/2301.10350v2","updated":"2023-03-29T03:16:44Z","published":"2023-01-24T23:34:13Z","title":"Parameterizing the cost function of Dynamic Time Warping with\n  application to time series classification","summary":"  Dynamic Time Warping (DTW) is a popular time series distance measure that\naligns the points in two series with one another. These alignments support\nwarping of the time dimension to allow for processes that unfold at differing\nrates. The distance is the minimum sum of costs of the resulting alignments\nover any allowable warping of the time dimension. The cost of an alignment of\ntwo points is a function of the difference in the values of those points. The\noriginal cost function was the absolute value of this difference. Other cost\nfunctions have been proposed. A popular alternative is the square of the\ndifference. However, to our knowledge, this is the first investigation of both\nthe relative impacts of using different cost functions and the potential to\ntune cost functions to different tasks. We do so in this paper by using a\ntunable cost function {\\lambda}{\\gamma} with parameter {\\gamma}. We show that\nhigher values of {\\gamma} place greater weight on larger pairwise differences,\nwhile lower values place greater weight on smaller pairwise differences. We\ndemonstrate that training {\\gamma} significantly improves the accuracy of both\nthe DTW nearest neighbor and Proximity Forest classifiers.\n","authors":["Matthieu Herrmann","Chang Wei Tan","Geoffrey I. Webb"],"pdf_url":"https://arxiv.org/pdf/2301.10350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.12096v2","updated":"2023-03-29T03:12:54Z","published":"2021-06-22T23:10:25Z","title":"Learning Identity-Preserving Transformations on Data Manifolds","summary":"  Many machine learning techniques incorporate identity-preserving\ntransformations into their models to generalize their performance to previously\nunseen data. These transformations are typically selected from a set of\nfunctions that are known to maintain the identity of an input when applied\n(e.g., rotation, translation, flipping, and scaling). However, there are many\nnatural variations that cannot be labeled for supervision or defined through\nexamination of the data. As suggested by the manifold hypothesis, many of these\nnatural variations live on or near a low-dimensional, nonlinear manifold.\nSeveral techniques represent manifold variations through a set of learned Lie\ngroup operators that define directions of motion on the manifold. However,\nthese approaches are limited because they require transformation labels when\ntraining their models and they lack a method for determining which regions of\nthe manifold are appropriate for applying each specific operator. We address\nthese limitations by introducing a learning strategy that does not require\ntransformation labels and developing a method that learns the local regions\nwhere each operator is likely to be used while preserving the identity of\ninputs. Experiments on MNIST and Fashion MNIST highlight our model's ability to\nlearn identity-preserving transformations on multi-class datasets.\nAdditionally, we train on CelebA to showcase our model's ability to learn\nsemantically meaningful transformations on complex datasets in an unsupervised\nmanner.\n","authors":["Marissa Connor","Kion Fallah","Christopher Rozell"],"pdf_url":"https://arxiv.org/pdf/2106.12096v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16424v1","updated":"2023-03-29T03:10:09Z","published":"2023-03-29T03:10:09Z","title":"ProductAE: Toward Deep Learning Driven Error-Correction Codes of Large\n  Dimensions","summary":"  While decades of theoretical research have led to the invention of several\nclasses of error-correction codes, the design of such codes is an extremely\nchallenging task, mostly driven by human ingenuity. Recent studies demonstrate\nthat such designs can be effectively automated and accelerated via tools from\nmachine learning (ML), thus enabling ML-driven classes of error-correction\ncodes with promising performance gains compared to classical designs. A\nfundamental challenge, however, is that it is prohibitively complex, if not\nimpossible, to design and train fully ML-driven encoder and decoder pairs for\nlarge code dimensions. In this paper, we propose Product Autoencoder\n(ProductAE) -- a computationally-efficient family of deep learning driven\n(encoder, decoder) pairs -- aimed at enabling the training of relatively large\ncodes (both encoder and decoder) with a manageable training complexity. We\nbuild upon ideas from classical product codes and propose constructing large\nneural codes using smaller code components. ProductAE boils down the complex\nproblem of training the encoder and decoder for a large code dimension $k$ and\nblocklength $n$ to less-complex sub-problems of training encoders and decoders\nfor smaller dimensions and blocklengths. Our training results show successful\ntraining of ProductAEs of dimensions as large as $k = 300$ bits with meaningful\nperformance gains compared to state-of-the-art classical and neural designs.\nMoreover, we demonstrate excellent robustness and adaptivity of ProductAEs to\nchannel models different than the ones used for training.\n","authors":["Mohammad Vahid Jamali","Hamid Saber","Homayoon Hatami","Jung Hyun Bae"],"pdf_url":"https://arxiv.org/pdf/2303.16424v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2110.04466"},{"id":"http://arxiv.org/abs/2210.14347v2","updated":"2023-03-29T03:04:44Z","published":"2022-10-25T21:19:50Z","title":"Interpolating Discriminant Functions in High-Dimensional Gaussian Latent\n  Mixtures","summary":"  This paper considers binary classification of high-dimensional features under\na postulated model with a low-dimensional latent Gaussian mixture structure and\nnon-vanishing noise. A generalized least squares estimator is used to estimate\nthe direction of the optimal separating hyperplane. The estimated hyperplane is\nshown to interpolate on the training data. While the direction vector can be\nconsistently estimated as could be expected from recent results in linear\nregression, a naive plug-in estimate fails to consistently estimate the\nintercept. A simple correction, that requires an independent hold-out sample,\nrenders the procedure minimax optimal in many scenarios. The interpolation\nproperty of the latter procedure can be retained, but surprisingly depends on\nthe way the labels are encoded.\n","authors":["Xin Bing","Marten Wegkamp"],"pdf_url":"https://arxiv.org/pdf/2210.14347v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16417v1","updated":"2023-03-29T02:50:59Z","published":"2023-03-29T02:50:59Z","title":"Problems and shortcuts in deep learning for screening mammography","summary":"  This work reveals undiscovered challenges in the performance and\ngeneralizability of deep learning models. We (1) identify spurious shortcuts\nand evaluation issues that can inflate performance and (2) propose training and\nanalysis methods to address them.\n  We trained an AI model to classify cancer on a retrospective dataset of\n120,112 US exams (3,467 cancers) acquired from 2008 to 2017 and 16,693 UK exams\n(5,655 cancers) acquired from 2011 to 2015.\n  We evaluated on a screening mammography test set of 11,593 US exams (102\ncancers; 7,594 women; age 57.1 \\pm 11.0) and 1,880 UK exams (590 cancers; 1,745\nwomen; age 63.3 \\pm 7.2). A model trained on images of only view markers (no\nbreast) achieved a 0.691 AUC. The original model trained on both datasets\nachieved a 0.945 AUC on the combined US+UK dataset but paradoxically only 0.838\nand 0.892 on the US and UK datasets, respectively. Sampling cancers equally\nfrom both datasets during training mitigated this shortcut. A similar AUC\nparadox (0.903) occurred when evaluating diagnostic exams vs screening exams\n(0.862 vs 0.861, respectively). Removing diagnostic exams during training\nalleviated this bias. Finally, the model did not exhibit the AUC paradox over\nscanner models but still exhibited a bias toward Selenia Dimension (SD) over\nHologic Selenia (HS) exams. Analysis showed that this AUC paradox occurred when\na dataset attribute had values with a higher cancer prevalence (dataset bias)\nand the model consequently assigned a higher probability to these attribute\nvalues (model bias). Stratification and balancing cancer prevalence can\nmitigate shortcuts during evaluation.\n  Dataset and model bias can introduce shortcuts and the AUC paradox,\npotentially pervasive issues within the healthcare AI space. Our methods can\nverify and mitigate shortcuts while providing a clear understanding of\nperformance.\n","authors":["Trevor Tsue","Brent Mombourquette","Ahmed Taha","Thomas Paul Matthews","Yen Nhi Truong Vu","Jason Su"],"pdf_url":"https://arxiv.org/pdf/2303.16417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16412v1","updated":"2023-03-29T02:42:17Z","published":"2023-03-29T02:42:17Z","title":"A Comprehensive and Versatile Multimodal Deep Learning Approach for\n  Predicting Diverse Properties of Advanced Materials","summary":"  We present a multimodal deep learning (MDL) framework for predicting physical\nproperties of a 10-dimensional acrylic polymer composite material by merging\nphysical attributes and chemical data. Our MDL model comprises four modules,\nincluding three generative deep learning models for material structure\ncharacterization and a fourth model for property prediction. Our approach\nhandles an 18-dimensional complexity, with 10 compositional inputs and 8\nproperty outputs, successfully predicting 913,680 property data points across\n114,210 composition conditions. This level of complexity is unprecedented in\ncomputational materials science, particularly for materials with undefined\nstructures. We propose a framework to analyze the high-dimensional information\nspace for inverse material design, demonstrating flexibility and adaptability\nto various materials and scales, provided sufficient data is available. This\nstudy advances future research on different materials and the development of\nmore sophisticated models, drawing us closer to the ultimate goal of predicting\nall properties of all materials.\n","authors":["Shun Muroga","Yasuaki Miki","Kenji Hata"],"pdf_url":"https://arxiv.org/pdf/2303.16412v1.pdf","comment":"38 pages, 17 figures, 1 table"},{"id":"http://arxiv.org/abs/2212.04692v2","updated":"2023-03-29T02:36:58Z","published":"2022-12-09T06:52:36Z","title":"Attention in a family of Boltzmann machines emerging from modern\n  Hopfield networks","summary":"  Hopfield networks and Boltzmann machines (BMs) are fundamental energy-based\nneural network models. Recent studies on modern Hopfield networks have broaden\nthe class of energy functions and led to a unified perspective on general\nHopfield networks including an attention module. In this letter, we consider\nthe BM counterparts of modern Hopfield networks using the associated energy\nfunctions, and study their salient properties from a trainability perspective.\nIn particular, the energy function corresponding to the attention module\nnaturally introduces a novel BM, which we refer to as the attentional BM\n(AttnBM). We verify that AttnBM has a tractable likelihood function and\ngradient for certain special cases and is easy to train. Moreover, we reveal\nthe hidden connections between AttnBM and some single-layer models, namely the\nGaussian--Bernoulli restricted BM and the denoising autoencoder with softmax\nunits coming from denoising score matching. We also investigate BMs introduced\nby other energy functions and show that the energy function of dense\nassociative memory models gives BMs belonging to Exponential Family Harmoniums.\n","authors":["Toshihiro Ota","Ryo Karakida"],"pdf_url":"https://arxiv.org/pdf/2212.04692v2.pdf","comment":"15 pages, 3 figures. v2: added figures and various\n  corrections/improvements especially in Introduction and Section 3. Published\n  version"},{"id":"http://arxiv.org/abs/2303.16407v1","updated":"2023-03-29T02:35:02Z","published":"2023-03-29T02:35:02Z","title":"LMDA-Net:A lightweight multi-dimensional attention network for general\n  EEG-based brain-computer interface paradigms and interpretability","summary":"  EEG-based recognition of activities and states involves the use of prior\nneuroscience knowledge to generate quantitative EEG features, which may limit\nBCI performance. Although neural network-based methods can effectively extract\nfeatures, they often encounter issues such as poor generalization across\ndatasets, high predicting volatility, and low model interpretability. Hence, we\npropose a novel lightweight multi-dimensional attention network, called\nLMDA-Net. By incorporating two novel attention modules designed specifically\nfor EEG signals, the channel attention module and the depth attention module,\nLMDA-Net can effectively integrate features from multiple dimensions, resulting\nin improved classification performance across various BCI tasks. LMDA-Net was\nevaluated on four high-impact public datasets, including motor imagery (MI) and\nP300-Speller paradigms, and was compared with other representative models. The\nexperimental results demonstrate that LMDA-Net outperforms other representative\nmethods in terms of classification accuracy and predicting volatility,\nachieving the highest accuracy in all datasets within 300 training epochs.\nAblation experiments further confirm the effectiveness of the channel attention\nmodule and the depth attention module. To facilitate an in-depth understanding\nof the features extracted by LMDA-Net, we propose class-specific neural network\nfeature interpretability algorithms that are suitable for event-related\npotentials (ERPs) and event-related desynchronization/synchronization\n(ERD/ERS). By mapping the output of the specific layer of LMDA-Net to the time\nor spatial domain through class activation maps, the resulting feature\nvisualizations can provide interpretable analysis and establish connections\nwith EEG time-spatial analysis in neuroscience. In summary, LMDA-Net shows\ngreat potential as a general online decoding model for various EEG tasks.\n","authors":["Zhengqing Miao","Xin Zhang","Meirong Zhao","Dong Ming"],"pdf_url":"https://arxiv.org/pdf/2303.16407v1.pdf","comment":"20 pages, 7 Figures"},{"id":"http://arxiv.org/abs/2303.16406v1","updated":"2023-03-29T02:33:54Z","published":"2023-03-29T02:33:54Z","title":"Hierarchical Video-Moment Retrieval and Step-Captioning","summary":"  There is growing interest in searching for information from large video\ncorpora. Prior works have studied relevant tasks, such as text-based video\nretrieval, moment retrieval, video summarization, and video captioning in\nisolation, without an end-to-end setup that can jointly search from video\ncorpora and generate summaries. Such an end-to-end setup would allow for many\ninteresting applications, e.g., a text-based search that finds a relevant video\nfrom a video corpus, extracts the most relevant moment from that video, and\nsegments the moment into important steps with captions. To address this, we\npresent the HiREST (HIerarchical REtrieval and STep-captioning) dataset and\npropose a new benchmark that covers hierarchical information retrieval and\nvisual/textual stepwise summarization from an instructional video corpus.\nHiREST consists of 3.4K text-video pairs from an instructional video dataset,\nwhere 1.1K videos have annotations of moment spans relevant to text query and\nbreakdown of each moment into key instruction steps with caption and timestamps\n(totaling 8.6K step captions). Our hierarchical benchmark consists of video\nretrieval, moment retrieval, and two novel moment segmentation and step\ncaptioning tasks. In moment segmentation, models break down a video moment into\ninstruction steps and identify start-end boundaries. In step captioning, models\ngenerate a textual summary for each step. We also present starting point\ntask-specific and end-to-end joint baseline models for our new benchmark. While\nthe baseline models show some promising results, there still exists large room\nfor future improvement by the community. Project website:\nhttps://hirest-cvpr2023.github.io\n","authors":["Abhay Zala","Jaemin Cho","Satwik Kottur","Xilun Chen","Barlas Oğuz","Yasher Mehdad","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2303.16406v1.pdf","comment":"CVPR 2023 (15 pages; the first two authors contributed equally;\n  Project website: https://hirest-cvpr2023.github.io)"},{"id":"http://arxiv.org/abs/2211.13866v2","updated":"2023-03-29T02:25:24Z","published":"2022-11-25T02:43:54Z","title":"Minimal Width for Universal Property of Deep RNN","summary":"  A recurrent neural network (RNN) is a widely used deep-learning network for\ndealing with sequential data. Imitating a dynamical system, an infinite-width\nRNN can approximate any open dynamical system in a compact domain. In general,\ndeep networks with bounded widths are more effective than wide networks in\npractice; however, the universal approximation theorem for deep narrow\nstructures has yet to be extensively studied. In this study, we prove the\nuniversality of deep narrow RNNs and show that the upper bound of the minimum\nwidth for universality can be independent of the length of the data.\nSpecifically, we show that a deep RNN with ReLU activation can approximate any\ncontinuous function or $L^p$ function with the widths $d_x+d_y+2$ and\n$\\max\\{d_x+1,d_y\\}$, respectively, where the target function maps a finite\nsequence of vectors in $\\mathbb{R}^{d_x}$ to a finite sequence of vectors in\n$\\mathbb{R}^{d_y}$. We also compute the additional width required if the\nactivation function is $\\tanh$ or more. In addition, we prove the universality\nof other recurrent networks, such as bidirectional RNNs. Bridging a multi-layer\nperceptron and an RNN, our theory and proof technique can be an initial step\ntoward further research on deep RNNs.\n","authors":["Chang hoon Song","Geonho Hwang","Jun ho Lee","Myungjoo Kang"],"pdf_url":"https://arxiv.org/pdf/2211.13866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16396v1","updated":"2023-03-29T02:17:28Z","published":"2023-03-29T02:17:28Z","title":"Using Connected Vehicle Trajectory Data to Evaluate the Effects of\n  Speeding","summary":"  Speeding has been and continues to be a major contributing factor to traffic\nfatalities. Various transportation agencies have proposed speed management\nstrategies to reduce the amount of speeding on arterials. While there have been\nvarious studies done on the analysis of speeding proportions above the speed\nlimit, few studies have considered the effect on the individual's journey. Many\nstudies utilized speed data from detectors, which is limited in that there is\nno information of the route that the driver took. This study aims to explore\nthe effects of various roadway features an individual experiences for a given\njourney on speeding proportions. Connected vehicle trajectory data was utilized\nto identify the path that a driver took, along with the vehicle related\nvariables. The level of speeding proportion is predicted using multiple\nlearning models. The model with the best performance, Extreme Gradient\nBoosting, achieved an accuracy of 0.756. The proposed model can be used to\nunderstand how the environment and vehicle's path effects the drivers' speeding\nbehavior, as well as predict the areas with high levels of speeding\nproportions. The results suggested that features related to an individual\ndriver's trip, i.e., total travel time, has a significant contribution towards\nspeeding. Features that are related to the environment of the individual\ndriver's trip, i.e., proportion of residential area, also had a significant\neffect on reducing speeding proportions. It is expected that the findings could\nhelp inform transportation agencies more on the factors related to speeding for\nan individual driver's trip.\n","authors":["Jorge Ugan","Mohamed Abdel-Aty","Zubayer Islam"],"pdf_url":"https://arxiv.org/pdf/2303.16396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.06994v2","updated":"2023-03-29T02:15:18Z","published":"2021-07-10T00:14:41Z","title":"Systematic human learning and generalization from a brief tutorial with\n  explanatory feedback","summary":"  Neural networks have long been used to model human intelligence, capturing\nelements of behavior and cognition, and their neural basis. Recent advancements\nin deep learning have enabled neural network models to reach and even surpass\nhuman levels of intelligence in many respects, yet unlike humans, their ability\nto learn new tasks quickly remains a challenge. People can reason not only in\nfamiliar domains, but can also rapidly learn to reason through novel problems\nand situations, raising the question of how well modern neural network models\ncapture human intelligence and in which ways they diverge. In this work, we\nexplore this gap by investigating human adults' ability to learn an abstract\nreasoning task based on Sudoku from a brief instructional tutorial with\nexplanatory feedback for incorrect responses using a narrow range of training\nexamples. We find that participants who master the task do so within a small\nnumber of trials and generalize well to puzzles outside of the training range.\nWe also find that most of those who master the task can describe a valid\nsolution strategy, and such participants perform better on transfer puzzles\nthan those whose strategy descriptions are vague or incomplete. Interestingly,\nfewer than half of our human participants were successful in acquiring a valid\nsolution strategy, and this ability is associated with high school mathematics\neducation. We consider the challenges these findings pose for building\ncomputational models that capture all aspects of our findings and point toward\na possible role for learning to engage in explanation-based reasoning to\nsupport rapid learning and generalization.\n","authors":["Andrew J. Nam","James L. McClelland"],"pdf_url":"https://arxiv.org/pdf/2107.06994v2.pdf","comment":"27 pages, 108 references, 8 Figures, and one Table, plus\n  Supplementary Materials"},{"id":"http://arxiv.org/abs/2203.02194v5","updated":"2023-03-29T02:13:23Z","published":"2022-03-04T09:04:55Z","title":"Rethinking Reconstruction Autoencoder-Based Out-of-Distribution\n  Detection","summary":"  In some scenarios, classifier requires detecting out-of-distribution samples\nfar from its training data. With desirable characteristics, reconstruction\nautoencoder-based methods deal with this problem by using input reconstruction\nerror as a metric of novelty vs. normality. We formulate the essence of such\napproach as a quadruplet domain translation with an intrinsic bias to only\nquery for a proxy of conditional data uncertainty. Accordingly, an improvement\ndirection is formalized as maximumly compressing the autoencoder's latent space\nwhile ensuring its reconstructive power for acting as a described domain\ntranslator. From it, strategies are introduced including semantic\nreconstruction, data certainty decomposition and normalized L2 distance to\nsubstantially improve original methods, which together establish\nstate-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of\nCIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method\nworks without any additional data, hard-to-implement structure, time-consuming\npipeline, and even harming the classification accuracy of known classes.\n","authors":["Yibo Zhou"],"pdf_url":"https://arxiv.org/pdf/2203.02194v5.pdf","comment":"Accepted('Poster' presentation) as main conference paper of CVPR2022"},{"id":"http://arxiv.org/abs/2303.16390v1","updated":"2023-03-29T02:02:08Z","published":"2023-03-29T02:02:08Z","title":"Are Data-driven Explanations Robust against Out-of-distribution Data?","summary":"  As black-box models increasingly power high-stakes applications, a variety of\ndata-driven explanation methods have been introduced. Meanwhile, machine\nlearning models are constantly challenged by distributional shifts. A question\nnaturally arises: Are data-driven explanations robust against\nout-of-distribution data? Our empirical results show that even though predict\ncorrectly, the model might still yield unreliable explanations under\ndistributional shifts. How to develop robust explanations against\nout-of-distribution data? To address this problem, we propose an end-to-end\nmodel-agnostic learning framework Distributionally Robust Explanations (DRE).\nThe key idea is, inspired by self-supervised learning, to fully utilizes the\ninter-distribution information to provide supervisory signals for the learning\nof explanations without human annotation. Can robust explanations benefit the\nmodel's generalization capability? We conduct extensive experiments on a wide\nrange of tasks and data types, including classification and regression on image\nand scientific tabular data. Our results demonstrate that the proposed method\nsignificantly improves the model's performance in terms of explanation and\nprediction robustness against distributional shifts.\n","authors":["Tang Li","Fengchun Qiao","Mengmeng Ma","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2303.16390v1.pdf","comment":"In Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2023"},{"id":"http://arxiv.org/abs/2302.04062v2","updated":"2023-03-29T01:59:46Z","published":"2023-02-08T13:59:31Z","title":"Machine Learning for Synthetic Data Generation: A Review","summary":"  Data plays a crucial role in machine learning. However, in real-world\napplications, there are several problems with data, e.g., data are of low\nquality; a limited number of data points lead to under-fitting of the machine\nlearning model; it is hard to access the data due to privacy, safety and\nregulatory concerns. Synthetic data generation offers a promising new avenue,\nas it can be shared and used in ways that real-world data cannot. This paper\nsystematically reviews the existing works that leverage machine learning models\nfor synthetic data generation. Specifically, we discuss the synthetic data\ngeneration works from several perspectives: (i) applications, including\ncomputer vision, speech, natural language, healthcare, and business; (ii)\nmachine learning methods, particularly neural network architectures and deep\ngenerative models; (iii) privacy and fairness issue. In addition, we identify\nthe challenges and opportunities in this emerging field and suggest future\nresearch directions.\n","authors":["Yingzhou Lu","Huazheng Wang","Wenqi Wei"],"pdf_url":"https://arxiv.org/pdf/2302.04062v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14157v2","updated":"2023-03-29T01:54:49Z","published":"2023-03-24T17:12:38Z","title":"Efficient Scale-Invariant Generator with Column-Row Entangled Pixel\n  Synthesis","summary":"  Any-scale image synthesis offers an efficient and scalable solution to\nsynthesize photo-realistic images at any scale, even going beyond 2K\nresolution. However, existing GAN-based solutions depend excessively on\nconvolutions and a hierarchical architecture, which introduce inconsistency and\nthe $``$texture sticking$\"$ issue when scaling the output resolution. From\nanother perspective, INR-based generators are scale-equivariant by design, but\ntheir huge memory footprint and slow inference hinder these networks from being\nadopted in large-scale or real-time systems. In this work, we propose\n$\\textbf{C}$olumn-$\\textbf{R}$ow $\\textbf{E}$ntangled $\\textbf{P}$ixel\n$\\textbf{S}$ynthesis ($\\textbf{CREPS}$), a new generative model that is both\nefficient and scale-equivariant without using any spatial convolutions or\ncoarse-to-fine design. To save memory footprint and make the system scalable,\nwe employ a novel bi-line representation that decomposes layer-wise feature\nmaps into separate $``$thick$\"$ column and row encodings. Experiments on\nvarious datasets, including FFHQ, LSUN-Church, MetFaces, and Flickr-Scenery,\nconfirm CREPS' ability to synthesize scale-consistent and alias-free images at\nany arbitrary resolution with proper training and inference speed. Code is\navailable at https://github.com/VinAIResearch/CREPS.\n","authors":["Thuan Hoang Nguyen","Thanh Van Le","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2303.14157v2.pdf","comment":"Accepted to CVPR 2023; Project Page:\n  https://thuanz123.github.io/creps/"},{"id":"http://arxiv.org/abs/2303.16110v2","updated":"2023-03-29T01:52:56Z","published":"2023-03-28T16:26:37Z","title":"Invariant preservation in machine learned PDE solvers via error\n  correction","summary":"  Machine learned partial differential equation (PDE) solvers trade the\nreliability of standard numerical methods for potential gains in accuracy\nand/or speed. The only way for a solver to guarantee that it outputs the exact\nsolution is to use a convergent method in the limit that the grid spacing\n$\\Delta x$ and timestep $\\Delta t$ approach zero. Machine learned solvers,\nwhich learn to update the solution at large $\\Delta x$ and/or $\\Delta t$, can\nnever guarantee perfect accuracy. Some amount of error is inevitable, so the\nquestion becomes: how do we constrain machine learned solvers to give us the\nsorts of errors that we are willing to tolerate? In this paper, we design more\nreliable machine learned PDE solvers by preserving discrete analogues of the\ncontinuous invariants of the underlying PDE. Examples of such invariants\ninclude conservation of mass, conservation of energy, the second law of\nthermodynamics, and/or non-negative density. Our key insight is simple: to\npreserve invariants, at each timestep apply an error-correcting algorithm to\nthe update rule. Though this strategy is different from how standard solvers\npreserve invariants, it is necessary to retain the flexibility that allows\nmachine learned solvers to be accurate at large $\\Delta x$ and/or $\\Delta t$.\nThis strategy can be applied to any autoregressive solver for any\ntime-dependent PDE in arbitrary geometries with arbitrary boundary conditions.\nAlthough this strategy is very general, the specific error-correcting\nalgorithms need to be tailored to the invariants of the underlying equations as\nwell as to the solution representation and time-stepping scheme of the solver.\nThe error-correcting algorithms we introduce have two key properties. First, by\npreserving the right invariants they guarantee numerical stability. Second, in\nclosed or periodic systems they do so without degrading the accuracy of an\nalready-accurate solver.\n","authors":["Nick McGreivy","Ammar Hakim"],"pdf_url":"https://arxiv.org/pdf/2303.16110v2.pdf","comment":"41 pages, 10 figures"},{"id":"http://arxiv.org/abs/2303.03634v3","updated":"2023-03-29T01:22:48Z","published":"2023-03-07T03:46:53Z","title":"PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation","summary":"  Fall accidents are critical issues in an aging and aged society. Recently,\nmany researchers developed pre-impact fall detection systems using deep\nlearning to support wearable-based fall protection systems for preventing\nsevere injuries. However, most works only employed simple neural network models\ninstead of complex models considering the usability in resource-constrained\nmobile devices and strict latency requirements. In this work, we propose a\nnovel pre-impact fall detection via CNN-ViT knowledge distillation, namely\nPreFallKD, to strike a balance between detection performance and computational\ncomplexity. The proposed PreFallKD transfers the detection knowledge from the\npre-trained teacher model (vision transformer) to the student model\n(lightweight convolutional neural networks). Additionally, we apply data\naugmentation techniques to tackle issues of data imbalance. We conduct the\nexperiment on the KFall public dataset and compare PreFallKD with other\nstate-of-the-art models. The experiment results show that PreFallKD could boost\nthe student model during the testing phase and achieves reliable F1-score\n(92.66%) and lead time (551.3 ms).\n","authors":["Tin-Han Chi","Kai-Chun Liu","Chia-Yeh Hsieh","Yu Tsao","Chia-Tai Chan"],"pdf_url":"https://arxiv.org/pdf/2303.03634v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16376v1","updated":"2023-03-29T00:58:18Z","published":"2023-03-29T00:58:18Z","title":"A Unified Single-stage Learning Model for Estimating Fiber Orientation\n  Distribution Functions on Heterogeneous Multi-shell Diffusion-weighted MRI","summary":"  Diffusion-weighted (DW) MRI measures the direction and scale of the local\ndiffusion process in every voxel through its spectrum in q-space, typically\nacquired in one or more shells. Recent developments in micro-structure imaging\nand multi-tissue decomposition have sparked renewed attention to the radial\nb-value dependence of the signal. Applications in tissue classification and\nmicro-architecture estimation, therefore, require a signal representation that\nextends over the radial as well as angular domain. Multiple approaches have\nbeen proposed that can model the non-linear relationship between the DW-MRI\nsignal and biological microstructure. In the past few years, many deep\nlearning-based methods have been developed towards faster inference speed and\nhigher inter-scan consistency compared with traditional model-based methods\n(e.g., multi-shell multi-tissue constrained spherical deconvolution). However,\na multi-stage learning strategy is typically required since the learning\nprocess relied on various middle representations, such as simple harmonic\noscillator reconstruction (SHORE) representation. In this work, we present a\nunified dynamic network with a single-stage spherical convolutional neural\nnetwork, which allows efficient fiber orientation distribution function (fODF)\nestimation through heterogeneous multi-shell diffusion MRI sequences. We study\nthe Human Connectome Project (HCP) young adults with test-retest scans. From\nthe experimental results, the proposed single-stage method outperforms prior\nmulti-stage approaches in repeated fODF estimation with shell dropoff and\nsingle-shell DW-MRI sequences.\n","authors":["Tianyuan Yao","Nancy Newlin","Praitayini Kanakaraj","Vishwesh nath","Leon Y Cai","Karthik Ramadass","Kurt Schilling","Bennett A. Landman","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2303.16376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16372v1","updated":"2023-03-29T00:49:38Z","published":"2023-03-29T00:49:38Z","title":"Non-Asymptotic Lower Bounds For Training Data Reconstruction","summary":"  We investigate semantic guarantees of private learning algorithms for their\nresilience to training Data Reconstruction Attacks (DRAs) by informed\nadversaries. To this end, we derive non-asymptotic minimax lower bounds on the\nadversary's reconstruction error against learners that satisfy differential\nprivacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate\nthat our lower bound analysis for the latter also covers the high dimensional\nregime, wherein, the input data dimensionality may be larger than the\nadversary's query budget. Motivated by the theoretical improvements conferred\nby metric DP, we extend the privacy analysis of popular deep learning\nalgorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion\nof metric differential privacy.\n","authors":["Prateeti Mukherjee","Satya Lokam"],"pdf_url":"https://arxiv.org/pdf/2303.16372v1.pdf","comment":"19 Pages, 2 Figures"},{"id":"http://arxiv.org/abs/2303.16364v1","updated":"2023-03-29T00:08:26Z","published":"2023-03-29T00:08:26Z","title":"Maximum likelihood smoothing estimation in state-space models: An\n  incomplete-information based approach","summary":"  This paper revisits classical works of Rauch (1963, et al. 1965) and develops\na novel method for maximum likelihood (ML) smoothing estimation from incomplete\ninformation/data of stochastic state-space systems. Score function and\nconditional observed information matrices of incomplete data are introduced and\ntheir distributional identities are established. Using these identities, the ML\nsmoother $\\widehat{x}_{k\\vert n}^s =\\argmax_{x_k} \\log\nf(x_k,\\widehat{x}_{k+1\\vert n}^s, y_{0:n}\\vert\\theta)$, $k\\leq n-1$, is\npresented. The result shows that the ML smoother gives an estimate of state\n$x_k$ with more adherence of loglikehood having less standard errors than that\nof the ML state estimator $\\widehat{x}_k=\\argmax_{x_k} \\log\nf(x_k,y_{0:k}\\vert\\theta)$, with $\\widehat{x}_{n\\vert n}^s=\\widehat{x}_n$.\nRecursive estimation is given in terms of an EM-gradient-particle algorithm\nwhich extends the work of \\cite{Lange} for ML smoothing estimation. The\nalgorithm has an explicit iteration update which lacks in (\\cite{Ramadan})\nEM-algorithm for smoothing. A sequential Monte Carlo method is developed for\nvaluation of the score function and observed information matrices. A recursive\nequation for the covariance matrix of estimation error is developed to\ncalculate the standard errors. In the case of linear systems, the method shows\nthat the Rauch-Tung-Striebel (RTS) smoother is a fully efficient smoothing\nstate-estimator whose covariance matrix coincides with the Cram\\'er-Rao lower\nbound, the inverse of expected information matrix. Furthermore, the RTS\nsmoother coincides with the Kalman filter having less covariance matrix.\nNumerical studies are performed, confirming the accuracy of the main results.\n","authors":["Budhi Arta Surya"],"pdf_url":"https://arxiv.org/pdf/2303.16364v1.pdf","comment":"3 figures"},{"id":"http://arxiv.org/abs/2205.01265v3","updated":"2023-03-29T00:07:18Z","published":"2022-05-03T01:32:47Z","title":"From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based\n  Visual Programming Tasks","summary":"  Block-based visual programming environments are increasingly used to\nintroduce computing concepts to beginners. Given that programming tasks are\nopen-ended and conceptual, novice students often struggle when learning in\nthese environments. AI-driven programming tutors hold great promise in\nautomatically assisting struggling students, and need several components to\nrealize this potential. We investigate the crucial component of student\nmodeling, in particular, the ability to automatically infer students'\nmisconceptions for predicting (synthesizing) their behavior. We introduce a\nnovel benchmark, StudentSyn, centered around the following challenge: For a\ngiven student, synthesize the student's attempt on a new target task after\nobserving the student's attempt on a fixed reference task. This challenge is\nakin to that of program synthesis; however, instead of synthesizing a\n{solution} (i.e., program an expert would write), the goal here is to\nsynthesize a {student attempt} (i.e., program that a given student would\nwrite). We first show that human experts (TutorSS) can achieve high performance\non the benchmark, whereas simple baselines perform poorly. Then, we develop two\nneuro/symbolic techniques (NeurSS and SymSS) in a quest to close this gap with\nTutorSS.\n","authors":["Adish Singla","Nikitas Theodoropoulos"],"pdf_url":"https://arxiv.org/pdf/2205.01265v3.pdf","comment":"Longer version of EDM 2022 paper"},{"id":"http://arxiv.org/abs/2303.17062v1","updated":"2023-03-29T23:31:32Z","published":"2023-03-29T23:31:32Z","title":"Ideal Abstractions for Decision-Focused Learning","summary":"  We present a methodology for formulating simplifying abstractions in machine\nlearning systems by identifying and harnessing the utility structure of\ndecisions. Machine learning tasks commonly involve high-dimensional output\nspaces (e.g., predictions for every pixel in an image or node in a graph), even\nthough a coarser output would often suffice for downstream decision-making\n(e.g., regions of an image instead of pixels). Developers often hand-engineer\nabstractions of the output space, but numerous abstractions are possible and it\nis unclear how the choice of output space for a model impacts its usefulness in\ndownstream decision-making. We propose a method that configures the output\nspace automatically in order to minimize the loss of decision-relevant\ninformation. Taking a geometric perspective, we formulate a step of the\nalgorithm as a projection of the probability simplex, termed fold, that\nminimizes the total loss of decision-related information in the H-entropy\nsense. Crucially, learning in the abstracted outcome space requires less data,\nleading to a net improvement in decision quality. We demonstrate the method in\ntwo domains: data acquisition for deep neural network training and a\nclosed-loop wildfire management task.\n","authors":["Michael Poli","Stefano Massaroli","Stefano Ermon","Bryan Wilder","Eric Horvitz"],"pdf_url":"https://arxiv.org/pdf/2303.17062v1.pdf","comment":"AISTATS 2023"},{"id":"http://arxiv.org/abs/2303.17056v1","updated":"2023-03-29T22:58:55Z","published":"2023-03-29T22:58:55Z","title":"Audio-Visual Grouping Network for Sound Localization from Mixtures","summary":"  Sound source localization is a typical and challenging task that predicts the\nlocation of sound sources in a video. Previous single-source methods mainly\nused the audio-visual association as clues to localize sounding objects in each\nimage. Due to the mixed property of multiple sound sources in the original\nspace, there exist rare multi-source approaches to localizing multiple sources\nsimultaneously, except for one recent work using a contrastive random walk in\nthe graph with images and separated sound as nodes. Despite their promising\nperformance, they can only handle a fixed number of sources, and they cannot\nlearn compact class-aware representations for individual sources. To alleviate\nthis shortcoming, in this paper, we propose a novel audio-visual grouping\nnetwork, namely AVGN, that can directly learn category-wise semantic features\nfor each source from the input audio mixture and image to localize multiple\nsources simultaneously. Specifically, our AVGN leverages learnable audio-visual\nclass tokens to aggregate class-aware source features. Then, the aggregated\nsemantic features for each source can be used as guidance to localize the\ncorresponding visual regions. Compared to existing multi-source methods, our\nnew framework can localize a flexible number of sources and disentangle\ncategory-aware audio-visual representations for individual sound sources. We\nconduct extensive experiments on MUSIC, VGGSound-Instruments, and VGG-Sound\nSources benchmarks. The results demonstrate that the proposed AVGN can achieve\nstate-of-the-art sounding object localization performance on both single-source\nand multi-source scenarios. Code is available at\n\\url{https://github.com/stoneMo/AVGN}.\n","authors":["Shentong Mo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2303.17056v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2007.08926v8","updated":"2023-03-29T22:34:28Z","published":"2020-07-17T12:13:16Z","title":"Smart Choices and the Selection Monad","summary":"  Describing systems in terms of choices and their resulting costs and rewards\noffers the promise of freeing algorithm designers and programmers from\nspecifying how those choices should be made; in implementations, the choices\ncan be realized by optimization techniques and, increasingly, by\nmachine-learning methods. We study this approach from a programming-language\nperspective. We define two small languages that support decision-making\nabstractions: one with choices and rewards, and the other additionally with\nprobabilities. We give both operational and denotational semantics.\n  In the case of the second language we consider three denotational semantics,\nwith varying degrees of correlation between possible program values and\nexpected rewards. The operational semantics combine the usual semantics of\nstandard constructs with optimization over spaces of possible execution\nstrategies. The denotational semantics, which are compositional, rely on the\nselection monad, to handle choice, augmented with an auxiliary monad to handle\nother effects, such as rewards or probability.\n  We establish adequacy theorems that the two semantics coincide in all cases.\nWe also prove full abstraction at base types, with varying notions of\nobservation in the probabilistic case corresponding to the various degrees of\ncorrelation. We present axioms for choice combined with rewards and\nprobability, establishing completeness at base types for the case of rewards\nwithout probability.\n","authors":["Martin Abadi","Gordon Plotkin"],"pdf_url":"https://arxiv.org/pdf/2007.08926v8.pdf","comment":"Final revision for LMCS publication"},{"id":"http://arxiv.org/abs/2303.17046v1","updated":"2023-03-29T22:18:47Z","published":"2023-03-29T22:18:47Z","title":"Have it your way: Individualized Privacy Assignment for DP-SGD","summary":"  When training a machine learning model with differential privacy, one sets a\nprivacy budget. This budget represents a maximal privacy violation that any\nuser is willing to face by contributing their data to the training set. We\nargue that this approach is limited because different users may have different\nprivacy expectations. Thus, setting a uniform privacy budget across all points\nmay be overly conservative for some users or, conversely, not sufficiently\nprotective for others. In this paper, we capture these preferences through\nindividualized privacy budgets. To demonstrate their practicality, we introduce\na variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which\nsupports such individualized budgets. DP-SGD is the canonical approach to\ntraining models with differential privacy. We modify its data sampling and\ngradient noising mechanisms to arrive at our approach, which we call\nIndividualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees\ntailored to the preferences of individual users and their data points, we find\nit empirically improves privacy-utility trade-offs.\n","authors":["Franziska Boenisch","Christopher Mühl","Adam Dziedzic","Roy Rinberg","Nicolas Papernot"],"pdf_url":"https://arxiv.org/pdf/2303.17046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17045v1","updated":"2023-03-29T22:16:52Z","published":"2023-03-29T22:16:52Z","title":"Training Neural Networks is NP-Hard in Fixed Dimension","summary":"  We study the parameterized complexity of training two-layer neural networks\nwith respect to the dimension of the input data and the number of hidden\nneurons, considering ReLU and linear threshold activation functions. Albeit the\ncomputational complexity of these problems has been studied numerous times in\nrecent years, several questions are still open. We answer questions by Arora et\nal. [ICLR '18] and Khalife and Basu [IPCO '22] showing that both problems are\nNP-hard for two dimensions, which excludes any polynomial-time algorithm for\nconstant dimension. We also answer a question by Froese et al. [JAIR '22]\nproving W[1]-hardness for four ReLUs (or two linear threshold neurons) with\nzero training error. Finally, in the ReLU case, we show fixed-parameter\ntractability for the combined parameter number of dimensions and number of\nReLUs if the network is assumed to compute a convex map. Our results settle the\ncomplexity status regarding these parameters almost completely.\n","authors":["Vincent Froese","Christoph Hertrich"],"pdf_url":"https://arxiv.org/pdf/2303.17045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17043v1","updated":"2023-03-29T22:06:24Z","published":"2023-03-29T22:06:24Z","title":"Federated Stochastic Bandit Learning with Unobserved Context","summary":"  We study the problem of federated stochastic multi-arm contextual bandits\nwith unknown contexts, in which M agents are faced with different bandits and\ncollaborate to learn. The communication model consists of a central server and\nthe agents share their estimates with the central server periodically to learn\nto choose optimal actions in order to minimize the total regret. We assume that\nthe exact contexts are not observable and the agents observe only a\ndistribution of the contexts. Such a situation arises, for instance, when the\ncontext itself is a noisy measurement or based on a prediction mechanism. Our\ngoal is to develop a distributed and federated algorithm that facilitates\ncollaborative learning among the agents to select a sequence of optimal actions\nso as to maximize the cumulative reward. By performing a feature vector\ntransformation, we propose an elimination-based algorithm and prove the regret\nbound for linearly parametrized reward functions. Finally, we validated the\nperformance of our algorithm and compared it with another baseline approach\nusing numerical simulations on synthetic data and on the real-world movielens\ndataset.\n","authors":["Jiabin Lin","Shana Moothedath"],"pdf_url":"https://arxiv.org/pdf/2303.17043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17041v1","updated":"2023-03-29T22:02:15Z","published":"2023-03-29T22:02:15Z","title":"The secret of immersion: actor driven camera movement generation for\n  auto-cinematography","summary":"  Immersion plays a vital role when designing cinematic creations, yet the\ndifficulty in immersive shooting prevents designers to create satisfactory\noutputs. In this work, we analyze the specific components that contribute to\ncinematographic immersion considering spatial, emotional, and aesthetic level,\nwhile these components are then combined into a high-level evaluation\nmechanism. Guided by such a immersion mechanism, we propose a GAN-based camera\ncontrol system that is able to generate actor-driven camera movements in the 3D\nvirtual environment to obtain immersive film sequences. The proposed\nencoder-decoder architecture in the generation flow transfers character motion\ninto camera trajectory conditioned on an emotion factor. This ensures spatial\nand emotional immersion by performing actor-camera synchronization physically\nand psychologically. The emotional immersion is further strengthened by\nincorporating regularization that controls camera shakiness for expressing\ndifferent mental statuses. To achieve aesthetic immersion, we make effort to\nimprove aesthetic frame compositions by modifying the synthesized camera\ntrajectory. Based on a self-supervised adjustor, the adjusted camera placements\ncan project the character to the appropriate on-frame locations following\naesthetic rules. The experimental results indicate that our proposed camera\ncontrol system can efficiently offer immersive cinematic videos, both\nquantitatively and qualitatively, based on a fine-grained immersive shooting.\nLive examples are shown in the supplementary video.\n","authors":["Xinyi Wu","Haohong Wang","Aggelos K. Katsaggelos"],"pdf_url":"https://arxiv.org/pdf/2303.17041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09516v3","updated":"2023-03-29T21:33:16Z","published":"2022-03-17T17:59:54Z","title":"AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation","summary":"  Powerful priors allow us to perform inference with insufficient information.\nIn this paper, we propose an autoregressive prior for 3D shapes to solve\nmultimodal 3D tasks such as shape completion, reconstruction, and generation.\nWe model the distribution over 3D shapes as a non-sequential autoregressive\ndistribution over a discretized, low-dimensional, symbolic grid-like latent\nrepresentation of 3D shapes. This enables us to represent distributions over 3D\nshapes conditioned on information from an arbitrary set of spatially anchored\nquery locations and thus perform shape completion in such arbitrary settings\n(e.g., generating a complete chair given only a view of the back leg). We also\nshow that the learned autoregressive prior can be leveraged for conditional\ntasks such as single-view reconstruction and language-based generation. This is\nachieved by learning task-specific naive conditionals which can be approximated\nby light-weight models trained on minimal paired data. We validate the\neffectiveness of the proposed method using both quantitative and qualitative\nevaluation and show that the proposed method outperforms the specialized\nstate-of-the-art methods trained for individual tasks. The project page with\ncode and video visualizations can be found at\nhttps://yccyenchicheng.github.io/AutoSDF/.\n","authors":["Paritosh Mittal","Yen-Chi Cheng","Maneesh Singh","Shubham Tulsiani"],"pdf_url":"https://arxiv.org/pdf/2203.09516v3.pdf","comment":"In CVPR 2022. The first two authors contributed equally to this work.\n  Project: https://yccyenchicheng.github.io/AutoSDF/. Add Supp"},{"id":"http://arxiv.org/abs/2208.09016v2","updated":"2023-03-29T21:20:00Z","published":"2022-08-18T18:32:48Z","title":"Improving Small Molecule Generation using Mutual Information Machine","summary":"  We address the task of controlled generation of small molecules, which\nentails finding novel molecules with desired properties under certain\nconstraints (e.g., similarity to a reference molecule). Here we introduce\nMolMIM, a probabilistic auto-encoder for small molecule drug discovery that\nlearns an informative and clustered latent space. MolMIM is trained with Mutual\nInformation Machine (MIM) learning, and provides a fixed length representation\nof variable length SMILES strings. Since encoder-decoder models can learn\nrepresentations with ``holes'' of invalid samples, here we propose a novel\nextension to the training procedure which promotes a dense latent space, and\nallows the model to sample valid molecules from random perturbations of latent\ncodes. We provide a thorough comparison of MolMIM to several variable-size and\nfixed-size encoder-decoder models, demonstrating MolMIM's superior generation\nas measured in terms of validity, uniqueness, and novelty. We then utilize\nCMA-ES, a naive black-box and gradient free search algorithm, over MolMIM's\nlatent space for the task of property guided molecule optimization. We achieve\nstate-of-the-art results in several constrained single property optimization\ntasks as well as in the challenging task of multi-objective optimization,\nimproving over previous success rate SOTA by more than 5\\% . We attribute the\nstrong results to MolMIM's latent representation which clusters similar\nmolecules in the latent space, whereas CMA-ES is often used as a baseline\noptimization method. We also demonstrate MolMIM to be favourable in a compute\nlimited regime, making it an attractive model for such cases.\n","authors":["Danny Reidenbach","Micha Livne","Rajesh K. Ilango","Michelle Gill","Johnny Israeli"],"pdf_url":"https://arxiv.org/pdf/2208.09016v2.pdf","comment":"Published at the MLDD workshop, ICLR 2023. version 2. 8 pages, 4\n  figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.17027v1","updated":"2023-03-29T21:14:05Z","published":"2023-03-29T21:14:05Z","title":"EPG-MGCN: Ego-Planning Guided Multi-Graph Convolutional Network for\n  Heterogeneous Agent Trajectory Prediction","summary":"  To drive safely in complex traffic environments, autonomous vehicles need to\nmake an accurate prediction of the future trajectories of nearby heterogeneous\ntraffic agents (i.e., vehicles, pedestrians, bicyclists, etc). Due to the\ninteractive nature, human drivers are accustomed to infer what the future\nsituations will become if they are going to execute different maneuvers. To\nfully exploit the impacts of interactions, this paper proposes a ego-planning\nguided multi-graph convolutional network (EPG-MGCN) to predict the trajectories\nof heterogeneous agents using both historical trajectory information and ego\nvehicle's future planning information. The EPG-MGCN first models the social\ninteractions by employing four graph topologies, i.e., distance graphs,\nvisibility graphs, planning graphs and category graphs. Then, the planning\ninformation of the ego vehicle is encoded by both the planning graph and the\nsubsequent planning-guided prediction module to reduce uncertainty in the\ntrajectory prediction. Finally, a category-specific gated recurrent unit\n(CS-GRU) encoder-decoder is designed to generate future trajectories for each\nspecific type of agents. Our network is evaluated on two real-world trajectory\ndatasets: ApolloScape and NGSIM. The experimental results show that the\nproposed EPG-MGCN achieves state-of-the-art performance compared to existing\nmethods.\n","authors":["Zihao Sheng","Zilin Huang","Sikai Chen"],"pdf_url":"https://arxiv.org/pdf/2303.17027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17015v1","updated":"2023-03-29T20:44:42Z","published":"2023-03-29T20:44:42Z","title":"HyperDiffusion: Generating Implicit Neural Fields with Weight-Space\n  Diffusion","summary":"  Implicit neural fields, typically encoded by a multilayer perceptron (MLP)\nthat maps from coordinates (e.g., xyz) to signals (e.g., signed distances),\nhave shown remarkable promise as a high-fidelity and compact representation.\nHowever, the lack of a regular and explicit grid structure also makes it\nchallenging to apply generative modeling directly on implicit neural fields in\norder to synthesize new data. To this end, we propose HyperDiffusion, a novel\napproach for unconditional generative modeling of implicit neural fields.\nHyperDiffusion operates directly on MLP weights and generates new neural\nimplicit fields encoded by synthesized MLP parameters. Specifically, a\ncollection of MLPs is first optimized to faithfully represent individual data\nsamples. Subsequently, a diffusion process is trained in this MLP weight space\nto model the underlying distribution of neural implicit fields. HyperDiffusion\nenables diffusion modeling over a implicit, compact, and yet high-fidelity\nrepresentation of complex signals across 3D shapes and 4D mesh animations\nwithin one single unified framework.\n","authors":["Ziya Erkoç","Fangchang Ma","Qi Shan","Matthias Nießner","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2303.17015v1.pdf","comment":"Project page: https://ziyaerkoc.com/hyperdiffusion/ Video:\n  https://www.youtube.com/watch?v=wjFpsKdo-II"},{"id":"http://arxiv.org/abs/2203.07490v3","updated":"2023-03-29T20:35:22Z","published":"2022-03-14T20:53:35Z","title":"Geometric Repair for Fair Classification at Any Decision Threshold","summary":"  We study the problem of post-processing a supervised machine-learned\nregressor to maximize fair binary classification at all decision thresholds.\nSpecifically, we show that by decreasing the statistical distance between each\ngroup's score distributions, we can increase fair performance across all\nthresholds at once, and that we can do so without a significant decrease in\naccuracy. To this end, we introduce a formal measure of distributional parity,\nwhich captures the degree of similarity in the distributions of classifications\nfor different protected groups. In contrast to prior work, which has been\nlimited to studies of demographic parity across all thresholds, our measure\napplies to a large class of fairness metrics. Our main result is to put forward\na novel post-processing algorithm based on optimal transport, which provably\nmaximizes distributional parity. We support this result with experiments on\nseveral fairness benchmarks.\n","authors":["Kweku Kwegyir-Aggrey","Jessica Dai","A. Feder Cooper","John Dickerson","Keegan Hines"],"pdf_url":"https://arxiv.org/pdf/2203.07490v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17010v1","updated":"2023-03-29T20:29:26Z","published":"2023-03-29T20:29:26Z","title":"Specification-Guided Data Aggregation for Semantically Aware Imitation\n  Learning","summary":"  Advancements in simulation and formal methods-guided environment sampling\nhave enabled the rigorous evaluation of machine learning models in a number of\nsafety-critical scenarios, such as autonomous driving. Application of these\nenvironment sampling techniques towards improving the learned models themselves\nhas yet to be fully exploited. In this work, we introduce a novel method for\nimproving imitation-learned models in a semantically aware fashion by\nleveraging specification-guided sampling techniques as a means of aggregating\nexpert data in new environments. Specifically, we create a set of formal\nspecifications as a means of partitioning the space of possible environments\ninto semantically similar regions, and identify elements of this partition\nwhere our learned imitation behaves most differently from the expert. We then\naggregate expert data on environments in these identified regions, leading to\nmore accurate imitation of the expert's behavior semantics. We instantiate our\napproach in a series of experiments in the CARLA driving simulator, and\ndemonstrate that our approach leads to models that are more accurate than those\nlearned with other environment sampling methods.\n","authors":["Ameesh Shah","Jonathan DeCastro","John Gideon","Beyazit Yalcinkaya","Guy Rosman","Sanjit A. Seshia"],"pdf_url":"https://arxiv.org/pdf/2303.17010v1.pdf","comment":"8 pages, under review"},{"id":"http://arxiv.org/abs/2303.17009v1","updated":"2023-03-29T20:27:49Z","published":"2023-03-29T20:27:49Z","title":"A comparative evaluation of image-to-image translation methods for stain\n  transfer in histopathology","summary":"  Image-to-image translation (I2I) methods allow the generation of artificial\nimages that share the content of the original image but have a different style.\nWith the advances in Generative Adversarial Networks (GANs)-based methods, I2I\nmethods enabled the generation of artificial images that are indistinguishable\nfrom natural images. Recently, I2I methods were also employed in histopathology\nfor generating artificial images of in silico stained tissues from a different\ntype of staining. We refer to this process as stain transfer. The number of I2I\nvariants is constantly increasing, which makes a well justified choice of the\nmost suitable I2I methods for stain transfer challenging. In our work, we\ncompare twelve stain transfer approaches, three of which are based on\ntraditional and nine on GAN-based image processing methods. The analysis relies\non complementary quantitative measures for the quality of image translation,\nthe assessment of the suitability for deep learning-based tissue grading, and\nthe visual evaluation by pathologists. Our study highlights the strengths and\nweaknesses of the stain transfer approaches, thereby allowing a rational choice\nof the underlying I2I algorithms. Code, data, and trained models for stain\ntransfer between H&E and Masson's Trichrome staining will be made available\nonline.\n","authors":["Igor Zingman","Sergio Frayle","Ivan Tankoyeu","Segrey Sukhanov","Fabian Heinemann"],"pdf_url":"https://arxiv.org/pdf/2303.17009v1.pdf","comment":"17 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.17003v1","updated":"2023-03-29T20:10:13Z","published":"2023-03-29T20:10:13Z","title":"Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission\n  Exams","summary":"  The present study aims to explore the capabilities of Language Models (LMs)\nin tackling high-stakes multiple-choice tests, represented here by the Exame\nNacional do Ensino M\\'edio (ENEM), a multidisciplinary entrance examination\nwidely adopted by Brazilian universities. This exam poses challenging tasks for\nLMs, since its questions may span into multiple fields of knowledge, requiring\nunderstanding of information from diverse domains. For instance, a question may\nrequire comprehension of both statistics and biology to be solved. This work\nanalyzed responses generated by GPT-3.5 and GPT-4 models for questions\npresented in the 2009-2017 exams, as well as for questions of the 2022 exam,\nwhich were made public after the training of the models was completed.\nFurthermore, different prompt strategies were tested, including the use of\nChain-of-Thought (CoT) prompts to generate explanations for answers. On the\n2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy\nof 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on\nexperiments are available at https://github.com/piresramon/gpt-4-enem.\n","authors":["Desnes Nunes","Ricardo Primi","Ramon Pires","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2303.17003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17001v1","updated":"2023-03-29T20:07:07Z","published":"2023-03-29T20:07:07Z","title":"The G-invariant graph Laplacian","summary":"  Graph Laplacian based algorithms for data lying on a manifold have been\nproven effective for tasks such as dimensionality reduction, clustering, and\ndenoising. In this work, we consider data sets whose data point not only lie on\na manifold, but are also closed under the action of a continuous group. An\nexample of such data set is volumes that line on a low dimensional manifold,\nwhere each volume may be rotated in three-dimensional space. We introduce the\nG-invariant graph Laplacian that generalizes the graph Laplacian by accounting\nfor the action of the group on the data set. We show that like the standard\ngraph Laplacian, the G-invariant graph Laplacian converges to the\nLaplace-Beltrami operator on the data manifold, but with a significantly\nimproved convergence rate. Furthermore, we show that the eigenfunctions of the\nG-invariant graph Laplacian admit the form of tensor products between the group\nelements and eigenvectors of certain matrices, which can be computed\nefficiently using FFT-type algorithms. We demonstrate our construction and its\nadvantages on the problem of filtering data on a noisy manifold closed under\nthe action of the special unitary group SU(2).\n","authors":["Eitan Rosen","Yoel Shkolnisky"],"pdf_url":"https://arxiv.org/pdf/2303.17001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16999v1","updated":"2023-03-29T20:00:19Z","published":"2023-03-29T20:00:19Z","title":"PopSparse: Accelerated block sparse matrix multiplication on IPU","summary":"  Reducing the computational cost of running large scale neural networks using\nsparsity has attracted great attention in the deep learning community. While\nmuch success has been achieved in reducing FLOP and parameter counts while\nmaintaining acceptable task performance, achieving actual speed improvements\nhas typically been much more difficult, particularly on general purpose\naccelerators (GPAs) such as NVIDIA GPUs using low precision number formats. In\nthis work we introduce PopSparse, a library that enables fast sparse operations\non Graphcore IPUs by leveraging both the unique hardware characteristics of\nIPUs as well as any block structure defined in the data. We target two\ndifferent types of sparsity: static, where the sparsity pattern is fixed at\ncompile-time; and dynamic, where it can change each time the model is run. We\npresent benchmark results for matrix multiplication for both of these modes on\nIPU with a range of block sizes, matrix sizes and densities. Results indicate\nthat the PopSparse implementations are faster than dense matrix multiplications\non IPU at a range of sparsity levels with large matrix size and block size.\nFurthermore, static sparsity in general outperforms dynamic sparsity. While\nprevious work on GPAs has shown speedups only for very high sparsity (typically\n99\\% and above), the present work demonstrates that our static sparse\nimplementation outperforms equivalent dense calculations in FP16 at lower\nsparsity (around 90%).\n","authors":["Zhiyi Li","Douglas Orr","Valeriu Ohan","Godfrey Da costa","Tom Murray","Adam Sanders","Deniz Beker","Dominic Masters"],"pdf_url":"https://arxiv.org/pdf/2303.16999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16998v1","updated":"2023-03-29T19:58:39Z","published":"2023-03-29T19:58:39Z","title":"Does Sparsity Help in Learning Misspecified Linear Bandits?","summary":"  Recently, the study of linear misspecified bandits has generated intriguing\nimplications of the hardness of learning in bandits and reinforcement learning\n(RL). In particular, Du et al. (2020) show that even if a learner is given\nlinear features in $\\mathbb{R}^d$ that approximate the rewards in a bandit or\nRL with a uniform error of $\\varepsilon$, searching for an\n$O(\\varepsilon)$-optimal action requires pulling at least $\\Omega(\\exp(d))$\nqueries. Furthermore, Lattimore et al. (2020) show that a degraded\n$O(\\varepsilon\\sqrt{d})$-optimal solution can be learned within\n$\\operatorname{poly}(d/\\varepsilon)$ queries. Yet it is unknown whether a\nstructural assumption on the ground-truth parameter, such as sparsity, could\nbreak the $\\varepsilon\\sqrt{d}$ barrier. In this paper, we address this\nquestion by showing that algorithms can obtain $O(\\varepsilon)$-optimal actions\nby querying $O(\\varepsilon^{-s}d^s)$ actions, where $s$ is the sparsity\nparameter, removing the $\\exp(d)$-dependence. We then establish\ninformation-theoretical lower bounds, i.e., $\\Omega(\\exp(s))$, to show that our\nupper bound on sample complexity is nearly tight if one demands an error $\nO(s^{\\delta}\\varepsilon)$ for $0<\\delta<1$. For $\\delta\\geq 1$, we further show\nthat $\\operatorname{poly}(s/\\varepsilon)$ queries are possible when the linear\nfeatures are \"good\" and even in general settings. These results provide a\nnearly complete picture of how sparsity can help in misspecified bandit\nlearning and provide a deeper understanding of when linear features are\n\"useful\" for bandit and reinforcement learning with misspecification.\n","authors":["Jialin Dong","Lin F. Yang"],"pdf_url":"https://arxiv.org/pdf/2303.16998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.03336v2","updated":"2023-03-29T19:46:59Z","published":"2021-05-07T15:40:23Z","title":"Neural network architectures using min-plus algebra for solving certain\n  high dimensional optimal control problems and Hamilton-Jacobi PDEs","summary":"  Solving high dimensional optimal control problems and corresponding\nHamilton-Jacobi PDEs are important but challenging problems in control\nengineering. In this paper, we propose two abstract neural network\narchitectures which are respectively used to compute the value function and the\noptimal control for certain class of high dimensional optimal control problems.\nWe provide the mathematical analysis for the two abstract architectures. We\nalso show several numerical results computed using the deep neural network\nimplementations of these abstract architectures. A preliminary implementation\nof our proposed neural network architecture on FPGAs shows promising speed up\ncompared to CPUs. This work paves the way to leverage efficient dedicated\nhardware designed for neural networks to solve high dimensional optimal control\nproblems and Hamilton-Jacobi PDEs.\n","authors":["Jérôme Darbon","Peter M. Dower","Tingwei Meng"],"pdf_url":"https://arxiv.org/pdf/2105.03336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.03486v2","updated":"2023-03-29T19:11:56Z","published":"2022-05-06T22:02:01Z","title":"Clustered Graph Matching for Label Recovery and Graph Classification","summary":"  Given a collection of vertex-aligned networks and an additional\nlabel-shuffled network, we propose procedures for leveraging the signal in the\nvertex-aligned collection to recover the labels of the shuffled network. We\nconsider matching the shuffled network to averages of the networks in the\nvertex-aligned collection at different levels of granularity. We demonstrate\nboth in theory and practice that if the graphs come from different network\nclasses, then clustering the networks into classes followed by matching the new\ngraph to cluster-averages can yield higher fidelity matching performance than\nmatching to the global average graph. Moreover, by minimizing the graph\nmatching objective function with respect to each cluster average, this approach\nsimultaneously classifies and recovers the vertex labels for the shuffled\ngraph. These theoretical developments are further reinforced via an\nilluminating real data experiment matching human connectomes.\n","authors":["Zhirui Li","Jesus Arroyo","Konstantinos Pantazis","Vince Lyzinski"],"pdf_url":"https://arxiv.org/pdf/2205.03486v2.pdf","comment":"22 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.16971v1","updated":"2023-03-29T19:10:24Z","published":"2023-03-29T19:10:24Z","title":"Sparse joint shift in multinomial classification","summary":"  Sparse joint shift (SJS) was recently proposed as a tractable model for\ngeneral dataset shift which may cause changes to the marginal distributions of\nfeatures and labels as well as the posterior probabilities and the\nclass-conditional feature distributions. Fitting SJS for a target dataset\nwithout label observations may produce valid predictions of labels and\nestimates of class prior probabilities. We present new results on the\ntransmission of SJS from sets of features to larger sets of features, a\nconditional correction formula for the class posterior probabilities under the\ntarget distribution, identifiability of SJS, and the relationship between SJS\nand covariate shift. In addition, we point out inconsistencies in the\nalgorithms which were proposed for estimating the characteristics of SJS, as\nthey could hamper the search for optimal solutions.\n","authors":["Dirk Tasche"],"pdf_url":"https://arxiv.org/pdf/2303.16971v1.pdf","comment":"19 pages, 3 appendices"},{"id":"http://arxiv.org/abs/2303.16963v1","updated":"2023-03-29T18:51:13Z","published":"2023-03-29T18:51:13Z","title":"Fairness-Aware Data Valuation for Supervised Learning","summary":"  Data valuation is a ML field that studies the value of training instances\ntowards a given predictive task. Although data bias is one of the main sources\nof downstream model unfairness, previous work in data valuation does not\nconsider how training instances may influence both performance and fairness of\nML models. Thus, we propose Fairness-Aware Data vauatiOn (FADO), a data\nvaluation framework that can be used to incorporate fairness concerns into a\nseries of ML-related tasks (e.g., data pre-processing, exploratory data\nanalysis, active learning). We propose an entropy-based data valuation metric\nsuited to address our two-pronged goal of maximizing both performance and\nfairness, which is more computationally efficient than existing metrics. We\nthen show how FADO can be applied as the basis for unfairness mitigation\npre-processing techniques. Our methods achieve promising results -- up to a 40\np.p. improvement in fairness at a less than 1 p.p. loss in performance compared\nto a baseline -- and promote fairness in a data-centric way, where a deeper\nunderstanding of data quality takes center stage.\n","authors":["José Pombal","Pedro Saleiro","Mário A. T. Figueiredo","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2303.16963v1.pdf","comment":"ICLR 2023 Workshop Trustworthy ML"},{"id":"http://arxiv.org/abs/2303.16954v1","updated":"2023-03-29T18:18:44Z","published":"2023-03-29T18:18:44Z","title":"Leveraging joint sparsity in hierarchical Bayesian learning","summary":"  We present a hierarchical Bayesian learning approach to infer jointly sparse\nparameter vectors from multiple measurement vectors. Our model uses separate\nconditionally Gaussian priors for each parameter vector and common\ngamma-distributed hyper-parameters to enforce joint sparsity. The resulting\njoint-sparsity-promoting priors are combined with existing Bayesian inference\nmethods to generate a new family of algorithms. Our numerical experiments,\nwhich include a multi-coil magnetic resonance imaging application, demonstrate\nthat our new approach consistently outperforms commonly used hierarchical\nBayesian methods.\n","authors":["Jan Glaubitz","Anne Gelb"],"pdf_url":"https://arxiv.org/pdf/2303.16954v1.pdf","comment":"24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2303.16952v1","updated":"2023-03-29T18:17:41Z","published":"2023-03-29T18:17:41Z","title":"Meta-Learning Parameterized First-Order Optimizers using Differentiable\n  Convex Optimization","summary":"  Conventional optimization methods in machine learning and controls rely\nheavily on first-order update rules. Selecting the right method and\nhyperparameters for a particular task often involves trial-and-error or\npractitioner intuition, motivating the field of meta-learning. We generalize a\nbroad family of preexisting update rules by proposing a meta-learning framework\nin which the inner loop optimization step involves solving a differentiable\nconvex optimization (DCO). We illustrate the theoretical appeal of this\napproach by showing that it enables one-step optimization of a family of linear\nleast squares problems, given that the meta-learner has sufficient exposure to\nsimilar tasks. Various instantiations of the DCO update rule are compared to\nconventional optimizers on a range of illustrative experimental settings.\n","authors":["Tanmay Gautam","Samuel Pfrommer","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2303.16952v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2301.07609v2","updated":"2023-03-29T18:10:15Z","published":"2023-01-18T15:40:19Z","title":"Physics-informed Information Field Theory for Modeling Physical Systems\n  with Uncertainty Quantification","summary":"  Data-driven approaches coupled with physical knowledge are powerful\ntechniques to model systems. The goal of such models is to efficiently solve\nfor the underlying field by combining measurements with known physical laws. As\nmany systems contain unknown elements, such as missing parameters, noisy data,\nor incomplete physical laws, this is widely approached as an uncertainty\nquantification problem. The common techniques to handle all the variables\ntypically depend on the numerical scheme used to approximate the posterior, and\nit is desirable to have a method which is independent of any such\ndiscretization. Information field theory (IFT) provides the tools necessary to\nperform statistics over fields that are not necessarily Gaussian. We extend IFT\nto physics-informed IFT (PIFT) by encoding the functional priors with\ninformation about the physical laws which describe the field. The posteriors\nderived from this PIFT remain independent of any numerical scheme and can\ncapture multiple modes, allowing for the solution of problems which are\nill-posed. We demonstrate our approach through an analytical example involving\nthe Klein-Gordon equation. We then develop a variant of stochastic gradient\nLangevin dynamics to draw samples from the joint posterior over the field and\nmodel parameters. We apply our method to numerical examples with various\ndegrees of model-form error and to inverse problems involving nonlinear\ndifferential equations. As an addendum, the method is equipped with a metric\nwhich allows the posterior to automatically quantify model-form uncertainty.\nBecause of this, our numerical experiments show that the method remains robust\nto even an incorrect representation of the physics given sufficient data. We\nnumerically demonstrate that the method correctly identifies when the physics\ncannot be trusted, in which case it automatically treats learning the field as\na regression problem.\n","authors":["Alex Alberts","Ilias Bilionis"],"pdf_url":"https://arxiv.org/pdf/2301.07609v2.pdf","comment":"32 pages, 8 figures Accepted by Journal of Computational Physics"},{"id":"http://arxiv.org/abs/2303.16947v1","updated":"2023-03-29T18:07:25Z","published":"2023-03-29T18:07:25Z","title":"De-coupling and De-positioning Dense Self-supervised Learning","summary":"  Dense Self-Supervised Learning (SSL) methods address the limitations of using\nimage-level feature representations when handling images with multiple objects.\nAlthough the dense features extracted by employing segmentation maps and\nbounding boxes allow networks to perform SSL for each object, we show that they\nsuffer from coupling and positional bias, which arise from the receptive field\nincreasing with layer depth and zero-padding. We address this by introducing\nthree data augmentation strategies, and leveraging them in (i) a decoupling\nmodule that aims to robustify the network to variations in the object's\nsurroundings, and (ii) a de-positioning module that encourages the network to\ndiscard positional object information. We demonstrate the benefits of our\nmethod on COCO and on a new challenging benchmark, OpenImage-MINI, for object\nclassification, semantic segmentation, and object detection. Our extensive\nexperiments evidence the better generalization of our method compared to the\nSOTA dense SSL methods\n","authors":["Congpei Qiu","Tong Zhang","Wei Ke","Mathieu Salzmann","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2303.16947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16938v1","updated":"2023-03-29T18:03:28Z","published":"2023-03-29T18:03:28Z","title":"Are Neural Architecture Search Benchmarks Well Designed? A Deeper Look\n  Into Operation Importance","summary":"  Neural Architecture Search (NAS) benchmarks significantly improved the\ncapability of developing and comparing NAS methods while at the same time\ndrastically reduced the computational overhead by providing meta-information\nabout thousands of trained neural networks. However, tabular benchmarks have\nseveral drawbacks that can hinder fair comparisons and provide unreliable\nresults. These usually focus on providing a small pool of operations in heavily\nconstrained search spaces -- usually cell-based neural networks with\npre-defined outer-skeletons. In this work, we conducted an empirical analysis\nof the widely used NAS-Bench-101, NAS-Bench-201 and TransNAS-Bench-101\nbenchmarks in terms of their generability and how different operations\ninfluence the performance of the generated architectures. We found that only a\nsubset of the operation pool is required to generate architectures close to the\nupper-bound of the performance range. Also, the performance distribution is\nnegatively skewed, having a higher density of architectures in the upper-bound\nrange. We consistently found convolution layers to have the highest impact on\nthe architecture's performance, and that specific combination of operations\nfavors top-scoring architectures. These findings shed insights on the correct\nevaluation and comparison of NAS methods using NAS benchmarks, showing that\ndirectly searching on NAS-Bench-201, ImageNet16-120 and TransNAS-Bench-101\nproduces more reliable results than searching only on CIFAR-10. Furthermore,\nwith this work we provide suggestions for future benchmark evaluations and\ndesign. The code used to conduct the evaluations is available at\nhttps://github.com/VascoLopes/NAS-Benchmark-Evaluation.\n","authors":["Vasco Lopes","Bruno Degardin","Luís A. Alexandre"],"pdf_url":"https://arxiv.org/pdf/2303.16938v1.pdf","comment":"15 pages; 11 figues; 10 tables"},{"id":"http://arxiv.org/abs/2303.16914v1","updated":"2023-03-29T16:44:13Z","published":"2023-03-29T16:44:13Z","title":"A New Deep Learning and XAI-Based Algorithm for Features Selection in\n  Genomics","summary":"  In the field of functional genomics, the analysis of gene expression profiles\nthrough Machine and Deep Learning is increasingly providing meaningful insight\ninto a number of diseases. The paper proposes a novel algorithm to perform\nFeature Selection on genomic-scale data, which exploits the reconstruction\ncapabilities of autoencoders and an ad-hoc defined Explainable Artificial\nIntelligence-based score in order to select the most informative genes for\ndiagnosis, prognosis, and precision medicine. Results of the application on a\nChronic Lymphocytic Leukemia dataset evidence the effectiveness of the\nalgorithm, by identifying and suggesting a set of meaningful genes for further\nmedical investigation.\n","authors":["Carlo Adornetto","Gianluigi Greco"],"pdf_url":"https://arxiv.org/pdf/2303.16914v1.pdf","comment":"8 pages, 5 figures, Best Doctoral Consortium Paper AIxIA2022 (Udine,\n  Italy)"}],"Multimedia":[{"id":"http://arxiv.org/abs/1903.01192v3","updated":"2023-03-29T17:30:25Z","published":"2019-03-04T11:56:53Z","title":"STEFANN: Scene Text Editor using Font Adaptive Neural Network","summary":"  Textual information in a captured scene plays an important role in scene\ninterpretation and decision making. Though there exist methods that can\nsuccessfully detect and interpret complex text regions present in a scene, to\nthe best of our knowledge, there is no significant prior work that aims to\nmodify the textual information in an image. The ability to edit text directly\non images has several advantages including error correction, text restoration\nand image reusability. In this paper, we propose a method to modify text in an\nimage at character-level. We approach the problem in two stages. At first, the\nunobserved character (target) is generated from an observed character (source)\nbeing modified. We propose two different neural network architectures - (a)\nFANnet to achieve structural consistency with source font and (b) Colornet to\npreserve source color. Next, we replace the source character with the generated\ncharacter maintaining both geometric and visual consistency with neighboring\ncharacters. Our method works as a unified platform for modifying text in\nimages. We present the effectiveness of our method on COCO-Text and ICDAR\ndatasets both qualitatively and quantitatively.\n","authors":["Prasun Roy","Saumik Bhattacharya","Subhankar Ghosh","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/1903.01192v3.pdf","comment":"Accepted in The IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2020"},{"id":"http://arxiv.org/abs/2303.13397v2","updated":"2023-03-29T02:33:18Z","published":"2023-03-23T16:15:18Z","title":"DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh\n  Recovery from a Video","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications such as gaming, human-computer interaction, and virtual\nreality. Compared to single image-based methods, video-based methods can\nutilize temporal information to further improve performance by incorporating\nhuman body motion priors. However, many-to-many approaches such as VIBE suffer\nfrom motion smoothness and temporal inconsistency. While many-to-one approaches\nsuch as TCMR and MPS-Net rely on the future frames, which is non-causal and\ntime inefficient during inference. To address these challenges, a novel\nDiffusion-Driven Transformer-based framework (DDT) for video-based HMR is\npresented. DDT is designed to decode specific motion patterns from the input\nsequence, enhancing motion smoothness and temporal consistency. As a\nmany-to-many approach, the decoder of our DDT outputs the human mesh of all the\nframes, making DDT more viable for real-world applications where time\nefficiency is crucial and a causal model is desired. Extensive experiments are\nconducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),\nwhich demonstrated the effectiveness and efficiency of our DDT.\n","authors":["Ce Zheng","Guo-Jun Qi","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17056v1","updated":"2023-03-29T22:58:55Z","published":"2023-03-29T22:58:55Z","title":"Audio-Visual Grouping Network for Sound Localization from Mixtures","summary":"  Sound source localization is a typical and challenging task that predicts the\nlocation of sound sources in a video. Previous single-source methods mainly\nused the audio-visual association as clues to localize sounding objects in each\nimage. Due to the mixed property of multiple sound sources in the original\nspace, there exist rare multi-source approaches to localizing multiple sources\nsimultaneously, except for one recent work using a contrastive random walk in\nthe graph with images and separated sound as nodes. Despite their promising\nperformance, they can only handle a fixed number of sources, and they cannot\nlearn compact class-aware representations for individual sources. To alleviate\nthis shortcoming, in this paper, we propose a novel audio-visual grouping\nnetwork, namely AVGN, that can directly learn category-wise semantic features\nfor each source from the input audio mixture and image to localize multiple\nsources simultaneously. Specifically, our AVGN leverages learnable audio-visual\nclass tokens to aggregate class-aware source features. Then, the aggregated\nsemantic features for each source can be used as guidance to localize the\ncorresponding visual regions. Compared to existing multi-source methods, our\nnew framework can localize a flexible number of sources and disentangle\ncategory-aware audio-visual representations for individual sound sources. We\nconduct extensive experiments on MUSIC, VGGSound-Instruments, and VGG-Sound\nSources benchmarks. The results demonstrate that the proposed AVGN can achieve\nstate-of-the-art sounding object localization performance on both single-source\nand multi-source scenarios. Code is available at\n\\url{https://github.com/stoneMo/AVGN}.\n","authors":["Shentong Mo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2303.17056v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17041v1","updated":"2023-03-29T22:02:15Z","published":"2023-03-29T22:02:15Z","title":"The secret of immersion: actor driven camera movement generation for\n  auto-cinematography","summary":"  Immersion plays a vital role when designing cinematic creations, yet the\ndifficulty in immersive shooting prevents designers to create satisfactory\noutputs. In this work, we analyze the specific components that contribute to\ncinematographic immersion considering spatial, emotional, and aesthetic level,\nwhile these components are then combined into a high-level evaluation\nmechanism. Guided by such a immersion mechanism, we propose a GAN-based camera\ncontrol system that is able to generate actor-driven camera movements in the 3D\nvirtual environment to obtain immersive film sequences. The proposed\nencoder-decoder architecture in the generation flow transfers character motion\ninto camera trajectory conditioned on an emotion factor. This ensures spatial\nand emotional immersion by performing actor-camera synchronization physically\nand psychologically. The emotional immersion is further strengthened by\nincorporating regularization that controls camera shakiness for expressing\ndifferent mental statuses. To achieve aesthetic immersion, we make effort to\nimprove aesthetic frame compositions by modifying the synthesized camera\ntrajectory. Based on a self-supervised adjustor, the adjusted camera placements\ncan project the character to the appropriate on-frame locations following\naesthetic rules. The experimental results indicate that our proposed camera\ncontrol system can efficiently offer immersive cinematic videos, both\nquantitatively and qualitatively, based on a fine-grained immersive shooting.\nLive examples are shown in the supplementary video.\n","authors":["Xinyi Wu","Haohong Wang","Aggelos K. Katsaggelos"],"pdf_url":"https://arxiv.org/pdf/2303.17041v1.pdf","comment":null}]},"2023-03-30T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.17590v1","updated":"2023-03-30T17:57:43Z","published":"2023-03-30T17:57:43Z","title":"Going Beyond Nouns With Vision & Language Models Using Synthetic Data","summary":"  Large-scale pre-trained Vision & Language (VL) models have shown remarkable\nperformance in many applications, enabling replacing a fixed set of supported\nclasses with zero-shot open vocabulary reasoning over (almost arbitrary)\nnatural language prompts. However, recent works have uncovered a fundamental\nweakness of these models. For example, their difficulty to understand Visual\nLanguage Concepts (VLC) that go 'beyond nouns' such as the meaning of\nnon-object words (e.g., attributes, actions, relations, states, etc.), or\ndifficulty in performing compositional reasoning such as understanding the\nsignificance of the order of the words in a sentence. In this work, we\ninvestigate to which extent purely synthetic data could be leveraged to teach\nthese models to overcome such shortcomings without compromising their zero-shot\ncapabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale\nsynthetic dataset and data generation codebase allowing to generate additional\nsuitable data to improve VLC understanding and compositional reasoning of VL\nmodels. Additionally, we propose a general VL finetuning strategy for\neffectively leveraging SyViC towards achieving these improvements. Our\nextensive experiments and ablations on VL-Checklist, Winoground, and ARO\nbenchmarks demonstrate that it is possible to adapt strong pre-trained VL\nmodels with synthetic data significantly enhancing their VLC understanding\n(e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their\nzero-shot accuracy.\n","authors":["Paola Cascante-Bonilla","Khaled Shehada","James Seale Smith","Sivan Doveh","Donghyun Kim","Rameswar Panda","Gül Varol","Aude Oliva","Vicente Ordonez","Rogerio Feris","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2303.17590v1.pdf","comment":"Project page: https://synthetic-vic.github.io/"},{"id":"http://arxiv.org/abs/2303.17580v1","updated":"2023-03-30T17:48:28Z","published":"2023-03-30T17:48:28Z","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace","summary":"  Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence (AGI). While there are abundant AI\nmodels available for different domains and modalities, they cannot handle\ncomplicated AI tasks. Considering large language models (LLMs) have exhibited\nexceptional ability in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks and language could be a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, a\nsystem that leverages LLMs (e.g., ChatGPT) to connect various AI models in\nmachine learning communities (e.g., HuggingFace) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHuggingFace, execute each subtask with the selected AI model, and summarize the\nresponse according to the execution results. By leveraging the strong language\ncapability of ChatGPT and abundant AI models in HuggingFace, HuggingGPT is able\nto cover numerous sophisticated AI tasks in different modalities and domains\nand achieve impressive results in language, vision, speech, and other\nchallenging tasks, which paves a new way towards AGI.\n","authors":["Yongliang Shen","Kaitao Song","Xu Tan","Dongsheng Li","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2303.17580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17574v1","updated":"2023-03-30T17:40:30Z","published":"2023-03-30T17:40:30Z","title":"Elastic Weight Removal for Faithful and Abstractive Dialogue Generation","summary":"  Ideally, dialogue systems should generate responses that are faithful to the\nknowledge contained in relevant documents. However, many models generate\nhallucinated responses instead that contradict it or contain unverifiable\ninformation. To mitigate such undesirable behaviour, it has been proposed to\nfine-tune a `negative expert' on negative examples and subtract its parameters\nfrom those of a pre-trained model. However, intuitively, this does not take\ninto account that some parameters are more responsible than others in causing\nhallucinations. Thus, we propose to weigh their individual importance via (an\napproximation of) the Fisher Information matrix, which measures the uncertainty\nof their estimate. We call this method Elastic Weight Removal (EWR). We\nevaluate our method -- using different variants of Flan-T5 as a backbone\nlanguage model -- on multiple datasets for information-seeking dialogue\ngeneration and compare our method with state-of-the-art techniques for\nfaithfulness, such as CTRL, Quark, DExperts, and Noisy Channel reranking.\nExtensive automatic and human evaluation shows that EWR systematically\nincreases faithfulness at minor costs in terms of other metrics. However, we\nnotice that only discouraging hallucinations may increase extractiveness, i.e.\nshallow copy-pasting of document spans, which can be undesirable. Hence, as a\nsecond main contribution, we show that our method can be extended to\nsimultaneously discourage hallucinations and extractive responses. We publicly\nrelease the code for reproducing EWR and all baselines.\n","authors":["Nico Daheim","Nouha Dziri","Mrinmaya Sachan","Iryna Gurevych","Edoardo M. Ponti"],"pdf_url":"https://arxiv.org/pdf/2303.17574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17564v1","updated":"2023-03-30T17:30:36Z","published":"2023-03-30T17:30:36Z","title":"BloombergGPT: A Large Language Model for Finance","summary":"  The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. As a next step, we plan to release\ntraining logs (Chronicles) detailing our experience in training BloombergGPT.\n","authors":["Shijie Wu","Ozan Irsoy","Steven Lu","Vadim Dabravolski","Mark Dredze","Sebastian Gehrmann","Prabhanjan Kambadur","David Rosenberg","Gideon Mann"],"pdf_url":"https://arxiv.org/pdf/2303.17564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17557v1","updated":"2023-03-30T17:26:16Z","published":"2023-03-30T17:26:16Z","title":"Recognition, recall, and retention of few-shot memories in large\n  language models","summary":"  The training of modern large language models (LLMs) takes place in a regime\nwhere most training examples are seen only a few times by the model during the\ncourse of training. What does a model remember about such examples seen only a\nfew times during training and how long does that memory persist in the face of\ncontinuous training with new examples? Here, we investigate these questions\nthrough simple recognition, recall, and retention experiments with LLMs. In\nrecognition experiments, we ask if the model can distinguish the seen example\nfrom a novel example; in recall experiments, we ask if the model can correctly\nrecall the seen example when cued by a part of it; and in retention\nexperiments, we periodically probe the model's memory for the original examples\nas the model is trained continuously with new examples. We find that a single\nexposure is generally sufficient for a model to achieve near perfect accuracy\neven in very challenging recognition experiments. We estimate that the\nrecognition performance of even small language models easily exceeds human\nrecognition performance reported in similar experiments with humans (Shepard,\n1967). Achieving near perfect recall takes more exposures, but most models can\ndo it in just 3 exposures. The flip side of this remarkable capacity for fast\nlearning is that precise memories are quickly overwritten: recall performance\nfor the original examples drops steeply over the first 10 training updates with\nnew examples, followed by a more gradual decline. Even after 100K updates,\nhowever, some of the original examples are still recalled near perfectly. A\nqualitatively similar retention pattern has been observed in human long-term\nmemory retention studies before (Bahrick, 1984). Finally, recognition is much\nmore robust to interference than recall and memory for natural language\nsentences is generally superior to memory for stimuli without structure.\n","authors":["A. Emin Orhan"],"pdf_url":"https://arxiv.org/pdf/2303.17557v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.17548v1","updated":"2023-03-30T17:17:08Z","published":"2023-03-30T17:17:08Z","title":"Whose Opinions Do Language Models Reflect?","summary":"  Language models (LMs) are increasingly being used in open-ended contexts,\nwhere the opinions reflected by LMs in response to subjective queries can have\na profound impact, both on user satisfaction, as well as shaping the views of\nsociety at large. In this work, we put forth a quantitative framework to\ninvestigate the opinions reflected by LMs -- by leveraging high-quality public\nopinion polls and their associated human responses. Using this framework, we\ncreate OpinionsQA, a new dataset for evaluating the alignment of LM opinions\nwith those of 60 US demographic groups over topics ranging from abortion to\nautomation. Across topics, we find substantial misalignment between the views\nreflected by current LMs and those of US demographic groups: on par with the\nDemocrat-Republican divide on climate change. Notably, this misalignment\npersists even after explicitly steering the LMs towards particular demographic\ngroups. Our analysis not only confirms prior observations about the\nleft-leaning tendencies of some human feedback-tuned LMs, but also surfaces\ngroups whose opinions are poorly reflected by current LMs (e.g., 65+ and\nwidowed individuals). Our code and data are available at\nhttps://github.com/tatsu-lab/opinions_qa.\n","authors":["Shibani Santurkar","Esin Durmus","Faisal Ladhak","Cinoo Lee","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2303.17548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12186v2","updated":"2023-03-30T17:15:42Z","published":"2022-05-24T16:41:30Z","title":"Enhancing Continual Learning with Global Prototypes: Counteracting\n  Negative Representation Drift","summary":"  Continual learning (CL) aims to learn a sequence of tasks over time, with\ndata distributions shifting from one task to another. When training on new task\ndata, data representations from old tasks may drift. Some negative\nrepresentation drift can result in catastrophic forgetting, by causing the\nlocally learned class prototypes and data representations to correlate poorly\nacross tasks. To mitigate such representation drift, we propose a method that\nfinds global prototypes to guide the learning, and learns data representations\nwith the regularization of the self-supervised information. Specifically, for\nNLP tasks, we formulate each task in a masked language modeling style, and\nlearn the task via a neighbor attention mechanism over a pre-trained language\nmodel. Experimental results show that our proposed method can learn fairly\nconsistent representations with less representation drift, and significantly\nreduce catastrophic forgetting in CL without resampling data from past tasks.\n","authors":["Xueying Bai","Jinghuan Shang","Yifan Sun","Niranjan Balasubramanian"],"pdf_url":"https://arxiv.org/pdf/2205.12186v2.pdf","comment":"version 2"},{"id":"http://arxiv.org/abs/2303.17517v1","updated":"2023-03-30T16:34:10Z","published":"2023-03-30T16:34:10Z","title":"Hindi as a Second Language: Improving Visually Grounded Speech with\n  Semantically Similar Samples","summary":"  The objective of this work is to explore the learning of visually grounded\nspeech models (VGS) from multilingual perspective. Bilingual VGS models are\ngenerally trained with an equal number of spoken captions from both languages.\nHowever, in reality, there can be an imbalance among the languages for the\navailable spoken captions. Our key contribution in this work is to leverage the\npower of a high-resource language in a bilingual visually grounded speech model\nto improve the performance of a low-resource language. We introduce two methods\nto distill the knowledge of high-resource language into low-resource languages:\n(1) incorporating a strong pre-trained high-resource language encoder and (2)\nusing semantically similar spoken captions. Our experiments show that combining\nthese two approaches effectively enables the low-resource language to surpass\nthe performances of monolingual and bilingual counterparts for cross-modal\nretrieval tasks.\n","authors":["Hyeonggon Ryu","Arda Senocak","In So Kweon","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2303.17517v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.17491v1","updated":"2023-03-30T16:01:52Z","published":"2023-03-30T16:01:52Z","title":"Language Models can Solve Computer Tasks","summary":"  Agents capable of carrying out general tasks on a computer can improve\nefficiency and productivity by automating repetitive tasks and assisting in\ncomplex problem-solving. Ideally, such agents should be able to solve new\ncomputer tasks presented to them through natural language commands. However,\nprevious approaches to this problem require large amounts of expert\ndemonstrations and task-specific reward functions, both of which are\nimpractical for new tasks. In this work, we show that a pre-trained large\nlanguage model (LLM) agent can execute computer tasks guided by natural\nlanguage using a simple prompting scheme where the agent recursively criticizes\nand improves its output (RCI). The RCI approach significantly outperforms\nexisting LLM methods for automating computer tasks and surpasses supervised\nlearning (SL) and reinforcement learning (RL) approaches on the MiniWoB++\nbenchmark. RCI is competitive with the state-of-the-art SL+RL method, using\nonly a handful of demonstrations per task rather than tens of thousands, and\nwithout a task-specific reward function. Furthermore, we demonstrate RCI\nprompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of\nnatural language reasoning tasks, outperforming chain of thought (CoT)\nprompting. We find that RCI combined with CoT performs better than either\nseparately.\n","authors":["Geunwoo Kim","Pierre Baldi","Stephen McAleer"],"pdf_url":"https://arxiv.org/pdf/2303.17491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17475v1","updated":"2023-03-30T15:48:26Z","published":"2023-03-30T15:48:26Z","title":"Efficient distributed representations beyond negative sampling","summary":"  This article describes an efficient method to learn distributed\nrepresentations, also known as embeddings. This is accomplished minimizing an\nobjective function similar to the one introduced in the Word2Vec algorithm and\nlater adopted in several works. The optimization computational bottleneck is\nthe calculation of the softmax normalization constants for which a number of\noperations scaling quadratically with the sample size is required. This\ncomplexity is unsuited for large datasets and negative sampling is a popular\nworkaround, allowing one to obtain distributed representations in linear time\nwith respect to the sample size. Negative sampling consists, however, in a\nchange of the loss function and hence solves a different optimization problem\nfrom the one originally proposed. Our contribution is to show that the sotfmax\nnormalization constants can be estimated in linear time, allowing us to design\nan efficient optimization strategy to learn distributed representations. We\ntest our approximation on two popular applications related to word and node\nembeddings. The results evidence competing performance in terms of accuracy\nwith respect to negative sampling with a remarkably lower computational time.\n","authors":["Lorenzo Dall'Amico","Enrico Maria Belliardo"],"pdf_url":"https://arxiv.org/pdf/2303.17475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17466v1","updated":"2023-03-30T15:43:39Z","published":"2023-03-30T15:43:39Z","title":"Assessing Cross-Cultural Alignment between ChatGPT and Human Societies:\n  An Empirical Study","summary":"  The recent release of ChatGPT has garnered widespread recognition for its\nexceptional ability to generate human-like responses in dialogue. Given its\nusage by users from various nations and its training on a vast multilingual\ncorpus that incorporates diverse cultural and societal norms, it is crucial to\nevaluate its effectiveness in cultural adaptation. In this paper, we\ninvestigate the underlying cultural background of ChatGPT by analyzing its\nresponses to questions designed to quantify human cultural differences. Our\nfindings suggest that, when prompted with American context, ChatGPT exhibits a\nstrong alignment with American culture, but it adapts less effectively to other\ncultural contexts. Furthermore, by using different prompts to probe the model,\nwe show that English prompts reduce the variance in model responses, flattening\nout cultural differences and biasing them towards American culture. This study\nprovides valuable insights into the cultural implications of ChatGPT and\nhighlights the necessity of greater diversity and cultural awareness in\nlanguage technologies.\n","authors":["Yong Cao","Li Zhou","Seolhwa Lee","Laura Cabello","Min Chen","Daniel Hershcovich"],"pdf_url":"https://arxiv.org/pdf/2303.17466v1.pdf","comment":"C3NLP@EACL"},{"id":"http://arxiv.org/abs/2211.07717v3","updated":"2023-03-30T15:03:17Z","published":"2022-10-28T18:31:52Z","title":"Deep Temporal Modelling of Clinical Depression through Social Media Text","summary":"  We describe the development of a model to detect user-level clinical\ndepression based on a user's temporal social media posts. Our model uses a\nDepression Symptoms Detection (DSD) classifier, which is trained on the largest\nexisting samples of clinician annotated tweets for clinical depression\nsymptoms. We subsequently use our DSD model to extract clinically relevant\nfeatures, e.g., depression scores and their consequent temporal patterns, as\nwell as user posting activity patterns, e.g., quantifying their ``no activity''\nor ``silence.'' Furthermore, to evaluate the efficacy of these extracted\nfeatures, we create three kinds of datasets including a test dataset, from two\nexisting well-known benchmark datasets for user-level depression detection. We\nthen provide accuracy measures based on single features, baseline features and\nfeature ablation tests, at several different levels of temporal granularity.\nThe relevant data distributions and clinical depression detection related\nsettings can be exploited to draw a complete picture of the impact of different\nfeatures across our created datasets. Finally, we show that, in general, only\nsemantic oriented representation models perform well. However, clinical\nfeatures may enhance overall performance provided that the training and testing\ndistribution is similar, and there is more data in a user's timeline. The\nconsequence is that the predictive capability of depression scores increase\nsignificantly while used in a more sensitive clinical depression detection\nsettings.\n","authors":["Nawshad Farruque","Randy Goebel","Sudhakar Sivapalan","Osmar R. Zaïane"],"pdf_url":"https://arxiv.org/pdf/2211.07717v3.pdf","comment":"Tables are properly oriented and some more typos were fixed"},{"id":"http://arxiv.org/abs/2302.09419v2","updated":"2023-03-30T14:44:09Z","published":"2023-02-18T20:51:09Z","title":"A Comprehensive Survey on Pretrained Foundation Models: A History from\n  BERT to ChatGPT","summary":"  Pretrained Foundation Models (PFMs) are regarded as the foundation for\nvarious downstream tasks with different data modalities. A PFM (e.g., BERT,\nChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable\nparameter initialization for a wide range of downstream applications. BERT\nlearns bidirectional encoder representations from Transformers, which are\ntrained on large datasets as contextual language models. Similarly, the\ngenerative pretrained transformer (GPT) method employs Transformers as the\nfeature extractor and is trained using an autoregressive paradigm on large\ndatasets. Recently, ChatGPT shows promising success on large language models,\nwhich applies an autoregressive language model with zero shot or few shot\nprompting. The remarkable achievements of PFM have brought significant\nbreakthroughs to various fields of AI. Numerous studies have proposed different\nmethods, raising the demand for an updated survey. This study provides a\ncomprehensive review of recent research advancements, challenges, and\nopportunities for PFMs in text, image, graph, as well as other data modalities.\nThe review covers the basic components and existing pretraining methods used in\nnatural language processing, computer vision, and graph learning. Additionally,\nit explores advanced PFMs used for different data modalities and unified PFMs\nthat consider data quality and quantity. The review also discusses research\nrelated to the fundamentals of PFMs, such as model efficiency and compression,\nsecurity, and privacy. Finally, the study provides key implications, future\nresearch directions, challenges, and open problems in the field of PFMs.\nOverall, this survey aims to shed light on the research of the PFMs on\nscalability, security, logical reasoning ability, cross-domain learning\nability, and the user-friendly interactive ability for artificial general\nintelligence.\n","authors":["Ce Zhou","Qian Li","Chen Li","Jun Yu","Yixin Liu","Guangjing Wang","Kai Zhang","Cheng Ji","Qiben Yan","Lifang He","Hao Peng","Jianxin Li","Jia Wu","Ziwei Liu","Pengtao Xie","Caiming Xiong","Jian Pei","Philip S. Yu","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2302.09419v2.pdf","comment":"99 pages, 16 figures"},{"id":"http://arxiv.org/abs/2303.17408v1","updated":"2023-03-30T14:25:44Z","published":"2023-03-30T14:25:44Z","title":"Medical Intervention Duration Estimation Using Language-enhanced\n  Transformer Encoder with Medical Prompts","summary":"  In recent years, estimating the duration of medical intervention based on\nelectronic health records (EHRs) has gained significant attention in the filed\nof clinical decision support. However, current models largely focus on\nstructured data, leaving out information from the unstructured clinical\nfree-text data. To address this, we present a novel language-enhanced\ntransformer-based framework, which projects all relevant clinical data\nmodalities (continuous, categorical, binary, and free-text features) into a\nharmonized language latent space using a pre-trained sentence encoder with the\nhelp of medical prompts. The proposed method enables the integration of\ninformation from different modalities within the cell transformer encoder and\nleads to more accurate duration estimation for medical intervention. Our\nexperimental results on both US-based (length of stay in ICU estimation) and\nAsian (surgical duration prediction) medical datasets demonstrate the\neffectiveness of our proposed framework, which outperforms tailored baseline\napproaches and exhibits robustness to data corruption in EHRs.\n","authors":["Yucheng Ruan","Xiang Lan","Daniel J. Tan","Hairil Rizal Abdullah","Mengling Feng"],"pdf_url":"https://arxiv.org/pdf/2303.17408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02970v3","updated":"2023-03-30T14:22:55Z","published":"2022-09-07T07:32:37Z","title":"Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence","summary":"  Nowadays, foundation models become one of fundamental infrastructures in\nartificial intelligence, paving ways to the general intelligence. However, the\nreality presents two urgent challenges: existing foundation models are\ndominated by the English-language community; users are often given limited\nresources and thus cannot always use foundation models. To support the\ndevelopment of the Chinese-language community, we introduce an open-source\nproject, called Fengshenbang, which leads by the research center for Cognitive\nComputing and Natural Language (CCNL). Our project has comprehensive\ncapabilities, including large pre-trained models, user-friendly APIs,\nbenchmarks, datasets, and others. We wrap all these in three sub-projects: the\nFengshenbang Model, the Fengshen Framework, and the Fengshen Benchmark. An\nopen-source roadmap, Fengshenbang, aims to re-evaluate the open-source\ncommunity of Chinese pre-trained large-scale models, prompting the development\nof the entire Chinese large-scale model community. We also want to build a\nuser-centered open-source ecosystem to allow individuals to access the desired\nmodels to match their computing resources. Furthermore, we invite companies,\ncolleges, and research institutions to collaborate with us to build the\nlarge-scale open-source model-based ecosystem. We hope that this project will\nbe the foundation of Chinese cognitive intelligence.\n","authors":["Jiaxing Zhang","Ruyi Gan","Junjie Wang","Yuxiang Zhang","Lin Zhang","Ping Yang","Xinyu Gao","Ziwei Wu","Xiaoqun Dong","Junqing He","Jianheng Zhuo","Qi Yang","Yongfeng Huang","Xiayu Li","Yanghan Wu","Junyu Lu","Xinyu Zhu","Weifeng Chen","Ting Han","Kunhao Pan","Rui Wang","Hao Wang","Xiaojun Wu","Zhongshen Zeng","Chongpei Chen"],"pdf_url":"https://arxiv.org/pdf/2209.02970v3.pdf","comment":"Added the Chinese version and is now a bilingual paper"},{"id":"http://arxiv.org/abs/2303.17395v1","updated":"2023-03-30T14:07:47Z","published":"2023-03-30T14:07:47Z","title":"WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for\n  Audio-Language Multimodal Research","summary":"  The advancement of audio-language (AL) multimodal learning tasks has been\nsignificant in recent years. However, researchers face challenges due to the\ncostly and time-consuming collection process of existing audio-language\ndatasets, which are limited in size. To address this data scarcity issue, we\nintroduce WavCaps, the first large-scale weakly-labelled audio captioning\ndataset, comprising approximately 400k audio clips with paired captions. We\nsourced audio clips and their raw descriptions from web sources and a sound\nevent detection dataset. However, the online-harvested raw descriptions are\nhighly noisy and unsuitable for direct use in tasks such as automated audio\ncaptioning. To overcome this issue, we propose a three-stage processing\npipeline for filtering noisy data and generating high-quality captions, where\nChatGPT, a large language model, is leveraged to filter and transform raw\ndescriptions automatically. We conduct a comprehensive analysis of the\ncharacteristics of WavCaps dataset and evaluate it on multiple downstream\naudio-language multimodal learning tasks. The systems trained on WavCaps\noutperform previous state-of-the-art (SOTA) models by a significant margin. Our\naspiration is for the WavCaps dataset we have proposed to facilitate research\nin audio-language multimodal learning and demonstrate the potential of\nutilizing ChatGPT to enhance academic research. Our dataset and codes are\navailable at https://github.com/XinhaoMei/WavCaps.\n","authors":["Xinhao Mei","Chutong Meng","Haohe Liu","Qiuqiang Kong","Tom Ko","Chengqi Zhao","Mark D. Plumbley","Yuexian Zou","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2303.17395v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2303.14956v2","updated":"2023-03-30T13:41:54Z","published":"2023-03-27T07:39:05Z","title":"Unified Text Structuralization with Instruction-tuned Language Models","summary":"  Text structuralization is one of the important fields of natural language\nprocessing (NLP) consists of information extraction (IE) and structure\nformalization. However, current studies of text structuralization suffer from a\nshortage of manually annotated high-quality datasets from different domains and\nlanguages, which require specialized professional knowledge. In addition, most\nIE methods are designed for a specific type of structured data, e.g., entities,\nrelations, and events, making them hard to generalize to others. In this work,\nwe propose a simple and efficient approach to instruct large language model\n(LLM) to extract a variety of structures from texts. More concretely, we add a\nprefix and a suffix instruction to indicate the desired IE task and structure\ntype, respectively, before feeding the text into a LLM. Experiments on two LLMs\nshow that this approach can enable language models to perform comparable with\nother state-of-the-art methods on datasets of a variety of languages and\nknowledge, and can generalize to other IE sub-tasks via changing the content of\ninstruction. Another benefit of our approach is that it can help researchers to\nbuild datasets in low-source and domain-specific scenarios, e.g., fields in\nfinance and law, with low cost.\n","authors":["Xuanfan Ni","Piji Li","Huayang Li"],"pdf_url":"https://arxiv.org/pdf/2303.14956v2.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17367v1","updated":"2023-03-30T13:29:49Z","published":"2023-03-30T13:29:49Z","title":"A BERT-based Unsupervised Grammatical Error Correction Framework","summary":"  Grammatical error correction (GEC) is a challenging task of natural language\nprocessing techniques. While more attempts are being made in this approach for\nuniversal languages like English or Chinese, relatively little work has been\ndone for low-resource languages for the lack of large annotated corpora. In\nlow-resource languages, the current unsupervised GEC based on language model\nscoring performs well. However, the pre-trained language model is still to be\nexplored in this context. This study proposes a BERT-based unsupervised GEC\nframework, where GEC is viewed as multi-class classification task. The\nframework contains three modules: data flow construction module, sentence\nperplexity scoring module, and error detecting and correcting module. We\npropose a novel scoring method for pseudo-perplexity to evaluate a sentence's\nprobable correctness and construct a Tagalog corpus for Tagalog GEC research.\nIt obtains competitive performance on the Tagalog corpus we construct and\nopen-source Indonesian corpus and it demonstrates that our framework is\ncomplementary to baseline method for low-resource GEC task.\n","authors":["Nankai Lin","Hongbin Zhang","Menglan Shen","Yu Wang","Shengyi Jiang","Aimin Yang"],"pdf_url":"https://arxiv.org/pdf/2303.17367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17324v1","updated":"2023-03-30T12:24:25Z","published":"2023-03-30T12:24:25Z","title":"Topics in the Haystack: Extracting and Evaluating Topics beyond\n  Coherence","summary":"  Extracting and identifying latent topics in large text corpora has gained\nincreasing importance in Natural Language Processing (NLP). Most models,\nwhether probabilistic models similar to Latent Dirichlet Allocation (LDA) or\nneural topic models, follow the same underlying approach of topic\ninterpretability and topic extraction. We propose a method that incorporates a\ndeeper understanding of both sentence and document themes, and goes beyond\nsimply analyzing word frequencies in the data. This allows our model to detect\nlatent topics that may include uncommon words or neologisms, as well as words\nnot present in the documents themselves. Additionally, we propose several new\nevaluation metrics based on intruder words and similarity measures in the\nsemantic space. We present correlation coefficients with human identification\nof intruder words and achieve near-human level results at the word-intrusion\ntask. We demonstrate the competitive performance of our method with a large\nbenchmark study, and achieve superior results compared to state-of-the-art\ntopic modeling and document clustering models.\n","authors":["Anton Thielmann","Quentin Seifert","Arik Reuter","Elisabeth Bergherr","Benjamin Säfken"],"pdf_url":"https://arxiv.org/pdf/2303.17324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17322v1","updated":"2023-03-30T12:23:39Z","published":"2023-03-30T12:23:39Z","title":"Yes but.. Can ChatGPT Identify Entities in Historical Documents?","summary":"  Large language models (LLMs) have been leveraged for several years now,\nobtaining state-of-the-art performance in recognizing entities from modern\ndocuments. For the last few months, the conversational agent ChatGPT has\n\"prompted\" a lot of interest in the scientific community and public due to its\ncapacity of generating plausible-sounding answers. In this paper, we explore\nthis ability by probing it in the named entity recognition and classification\n(NERC) task in primary sources (e.g., historical newspapers and classical\ncommentaries) in a zero-shot manner and by comparing it with state-of-the-art\nLM-based systems. Our findings indicate several shortcomings in identifying\nentities in historical text that range from the consistency of entity\nannotation guidelines, entity complexity, and code-switching, to the\nspecificity of prompting. Moreover, as expected, the inaccessibility of\nhistorical archives to the public (and thus on the Internet) also impacts its\nperformance.\n","authors":["Carlos-Emiliano González-Gallardo","Emanuela Boros","Nancy Girdhar","Ahmed Hamdi","Jose G. Moreno","Antoine Doucet"],"pdf_url":"https://arxiv.org/pdf/2303.17322v1.pdf","comment":"5 pages, accepted to JCDL2023"},{"id":"http://arxiv.org/abs/2303.17276v1","updated":"2023-03-30T10:32:18Z","published":"2023-03-30T10:32:18Z","title":"Humans in Humans Out: On GPT Converging Toward Common Sense in both\n  Success and Failure","summary":"  Increase in computational scale and fine-tuning has seen a dramatic\nimprovement in the quality of outputs of large language models (LLMs) like GPT.\nGiven that both GPT-3 and GPT-4 were trained on large quantities of\nhuman-generated text, we might ask to what extent their outputs reflect\npatterns of human thinking, both for correct and incorrect cases. The Erotetic\nTheory of Reason (ETR) provides a symbolic generative model of both human\nsuccess and failure in thinking, across propositional, quantified, and\nprobabilistic reasoning, as well as decision-making. We presented GPT-3,\nGPT-3.5, and GPT-4 with 61 central inference and judgment problems from a\nrecent book-length presentation of ETR, consisting of experimentally verified\ndata-points on human judgment and extrapolated data-points predicted by ETR,\nwith correct inference patterns as well as fallacies and framing effects (the\nETR61 benchmark). ETR61 includes classics like Wason's card task, illusory\ninferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3\nshowed evidence of ETR-predicted outputs for 59% of these examples, rising to\n77% in GPT-3.5 and 75% in GPT-4. Remarkably, the production of human-like\nfallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in\nGPT-4. This suggests that larger and more advanced LLMs may develop a tendency\ntoward more human-like mistakes, as relevant thought patterns are inherent in\nhuman-produced training data. According to ETR, the same fundamental patterns\nare involved both in successful and unsuccessful ordinary reasoning, so that\nthe \"bad\" cases could paradoxically be learned from the \"good\" cases. We\nfurther present preliminary evidence that ETR-inspired prompt engineering could\nreduce instances of these mistakes.\n","authors":["Philipp Koralus","Vincent Wang-Maścianica"],"pdf_url":"https://arxiv.org/pdf/2303.17276v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2206.08317v3","updated":"2023-03-30T07:00:38Z","published":"2022-06-16T17:24:14Z","title":"Paraformer: Fast and Accurate Parallel Transformer for\n  Non-autoregressive End-to-End Speech Recognition","summary":"  Transformers have recently dominated the ASR field. Although able to yield\ngood performance, they involve an autoregressive (AR) decoder to generate\ntokens one by one, which is computationally inefficient. To speed up inference,\nnon-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to\nenable parallel generation. However, due to an independence assumption within\nthe output tokens, performance of single-step NAR is inferior to that of AR\nmodels, especially with a large-scale corpus. There are two challenges to\nimproving single-step NAR: Firstly to accurately predict the number of output\ntokens and extract hidden variables; secondly, to enhance modeling of\ninterdependence between output tokens. To tackle both challenges, we propose a\nfast and accurate parallel transformer, termed Paraformer. This utilizes a\ncontinuous integrate-and-fire based predictor to predict the number of tokens\nand generate hidden variables. A glancing language model (GLM) sampler then\ngenerates semantic embeddings to enhance the NAR decoder's ability to model\ncontext interdependence. Finally, we design a strategy to generate negative\nsamples for minimum word error rate training to further improve performance.\nExperiments using the public AISHELL-1, AISHELL-2 benchmark, and an\nindustrial-level 20,000 hour task demonstrate that the proposed Paraformer can\nattain comparable performance to the state-of-the-art AR transformer, with more\nthan 10x speedup.\n","authors":["Zhifu Gao","Shiliang Zhang","Ian McLoughlin","Zhijie Yan"],"pdf_url":"https://arxiv.org/pdf/2206.08317v3.pdf","comment":"5 pages, 3 figures, accepted by INTERSPEECH 2022"},{"id":"http://arxiv.org/abs/2303.17183v1","updated":"2023-03-30T06:42:22Z","published":"2023-03-30T06:42:22Z","title":"The Nordic Pile: A 1.2TB Nordic Dataset for Language Modeling","summary":"  Pre-training Large Language Models (LLMs) require massive amounts of text\ndata, and the performance of the LLMs typically correlates with the scale and\nquality of the datasets. This means that it may be challenging to build LLMs\nfor smaller languages such as Nordic ones, where the availability of text\ncorpora is limited. In order to facilitate the development of the LLMS in the\nNordic languages, we curate a high-quality dataset consisting of 1.2TB of text,\nin all of the major North Germanic languages (Danish, Icelandic, Norwegian, and\nSwedish), as well as some high-quality English data. This paper details our\nconsiderations and processes for collecting, cleaning, and filtering the\ndataset.\n","authors":["Joey Öhman","Severine Verlinden","Ariel Ekgren","Amaru Cuba Gyllensten","Tim Isbister","Evangelia Gogoulou","Fredrik Carlsson","Magnus Sahlgren"],"pdf_url":"https://arxiv.org/pdf/2303.17183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16839v2","updated":"2023-03-30T05:44:47Z","published":"2023-03-29T16:42:30Z","title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks","summary":"  The development of language models have moved from encoder-decoder to\ndecoder-only designs. In addition, the common knowledge has it that the two\nmost popular multimodal tasks, the generative and contrastive tasks, tend to\nconflict with one another, are hard to accommodate in one architecture, and\nfurther need complex adaptations for downstream tasks. We propose a novel\nparadigm of training with a decoder-only model for multimodal tasks, which is\nsurprisingly effective in jointly learning of these disparate vision-language\ntasks. This is done with a simple model, called MaMMUT. It consists of a single\nvision encoder and a text decoder, and is able to accommodate contrastive and\ngenerative learning by a novel two-pass approach on the text decoder. We\ndemonstrate that joint learning of these diverse objectives is simple,\neffective, and maximizes the weight-sharing of the model across these tasks.\nFurthermore, the same architecture enables straightforward extensions to\nopen-vocabulary object detection and video-language tasks. The model tackles a\ndiverse range of tasks, while being modest in capacity. Our model achieves the\nstate of the art on image-text and text-image retrieval, video question\nanswering and open-vocabulary detection tasks, outperforming much larger and\nmore extensively trained foundational models. It shows very competitive results\non VQA and Video Captioning, especially considering its capacity. Ablations\nconfirm the flexibility and advantages of our approach.\n","authors":["Weicheng Kuo","AJ Piergiovanni","Dahun Kim","Xiyang Luo","Ben Caine","Wei Li","Abhijit Ogale","Luowei Zhou","Andrew Dai","Zhifeng Chen","Claire Cui","Anelia Angelova"],"pdf_url":"https://arxiv.org/pdf/2303.16839v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17161v1","updated":"2023-03-30T05:44:44Z","published":"2023-03-30T05:44:44Z","title":"TreePiece: Faster Semantic Parsing via Tree Tokenization","summary":"  Autoregressive (AR) encoder-decoder neural networks have proved successful in\nmany NLP problems, including Semantic Parsing -- a task that translates natural\nlanguage to machine-readable parse trees. However, the sequential prediction\nprocess of AR models can be slow. To accelerate AR for semantic parsing, we\nintroduce a new technique called TreePiece that tokenizes a parse tree into\nsubtrees and generates one subtree per decoding step. On TopV2 benchmark,\nTreePiece shows 4.6 times faster decoding speed than standard AR, and\ncomparable speed but significantly higher accuracy compared to\nNon-Autoregressive (NAR).\n","authors":["Sid Wang","Akshat Shrivastava","Sasha Livshits"],"pdf_url":"https://arxiv.org/pdf/2303.17161v1.pdf","comment":"4 pages main body + 4 pages appendices"},{"id":"http://arxiv.org/abs/2212.04088v3","updated":"2023-03-30T04:50:44Z","published":"2022-12-08T05:46:32Z","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models","summary":"  This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n","authors":["Chan Hee Song","Jiaman Wu","Clayton Washington","Brian M. Sadler","Wei-Lun Chao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2212.04088v3.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17119v1","updated":"2023-03-30T03:10:28Z","published":"2023-03-30T03:10:28Z","title":"TLAG: An Informative Trigger and Label-Aware Knowledge Guided Model for\n  Dialogue-based Relation Extraction","summary":"  Dialogue-based Relation Extraction (DRE) aims to predict the relation type of\nargument pairs that are mentioned in dialogue. The latest trigger-enhanced\nmethods propose trigger prediction tasks to promote DRE. However, these methods\nare not able to fully leverage the trigger information and even bring noise to\nrelation extraction. To solve these problems, we propose TLAG, which fully\nleverages the trigger and label-aware knowledge to guide the relation\nextraction. First, we design an adaptive trigger fusion module to fully\nleverage the trigger information. Then, we introduce label-aware knowledge to\nfurther promote our model's performance. Experimental results on the DialogRE\ndataset show that our TLAG outperforms the baseline models, and detailed\nanalyses demonstrate the effectiveness of our approach.\n","authors":["Hao An","Dongsheng Chen","Weiyuan Xu","Zhihong Zhu","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2303.17119v1.pdf","comment":"Accepted by CSCWD 2023"},{"id":"http://arxiv.org/abs/2212.13738v2","updated":"2023-03-30T01:42:53Z","published":"2022-12-28T08:10:31Z","title":"TempCLR: Temporal Alignment Representation with Contrastive Learning","summary":"  Video representation learning has been successful in video-text pre-training\nfor zero-shot transfer, where each sentence is trained to be close to the\npaired video clips in a common feature space. For long videos, given a\nparagraph of description where the sentences describe different segments of the\nvideo, by matching all sentence-clip pairs, the paragraph and the full video\nare aligned implicitly. However, such unit-level comparison may ignore global\ntemporal context, which inevitably limits the generalization ability. In this\npaper, we propose a contrastive learning framework TempCLR to compare the full\nvideo and the paragraph explicitly. As the video/paragraph is formulated as a\nsequence of clips/sentences, under the constraint of their temporal order, we\nuse dynamic time warping to compute the minimum cumulative cost over\nsentence-clip pairs as the sequence-level distance. To explore the temporal\ndynamics, we break the consistency of temporal succession by shuffling video\nclips w.r.t. temporal granularity. Then, we obtain the representations for\nclips/sentences, which perceive the temporal information and thus facilitate\nthe sequence alignment. In addition to pre-training on the video and paragraph,\nour approach can also generalize on the matching between video instances. We\nevaluate our approach on video retrieval, action step localization, and\nfew-shot action recognition, and achieve consistent performance gain over all\nthree tasks. Detailed ablation studies are provided to justify the approach\ndesign.\n","authors":["Yuncong Yang","Jiawei Ma","Shiyuan Huang","Long Chen","Xudong Lin","Guangxing Han","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2212.13738v2.pdf","comment":"ICLR 2023 Camera Ready. Code Link:\n  https://github.com/yyuncong/TempCLR"},{"id":"http://arxiv.org/abs/2303.17071v1","updated":"2023-03-30T00:30:19Z","published":"2023-03-30T00:30:19Z","title":"DERA: Enhancing Large Language Model Completions with Dialog-Enabled\n  Resolving Agents","summary":"  Large language models (LLMs) have emerged as valuable tools for many natural\nlanguage understanding tasks. In safety-critical applications such as\nhealthcare, the utility of these models is governed by their ability to\ngenerate outputs that are factually accurate and complete. In this work, we\npresent dialog-enabled resolving agents (DERA). DERA is a paradigm made\npossible by the increased conversational abilities of LLMs, namely GPT-4. It\nprovides a simple, interpretable forum for models to communicate feedback and\niteratively improve output. We frame our dialog as a discussion between two\nagent types - a Researcher, who processes information and identifies crucial\nproblem components, and a Decider, who has the autonomy to integrate the\nResearcher's information and makes judgments on the final output.\n  We test DERA against three clinically-focused tasks. For medical conversation\nsummarization and care plan generation, DERA shows significant improvement over\nthe base GPT-4 performance in both human expert preference evaluations and\nquantitative metrics. In a new finding, we also show that GPT-4's performance\n(70%) on an open-ended version of the MedQA question-answering (QA) dataset\n(Jin et al. 2021, USMLE) is well above the passing level (60%), with DERA\nshowing similar performance. We release the open-ended MEDQA dataset at\nhttps://github.com/curai/curai-research/tree/main/DERA.\n","authors":["Varun Nair","Elliot Schumacher","Geoffrey Tso","Anitha Kannan"],"pdf_url":"https://arxiv.org/pdf/2303.17071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17728v1","updated":"2023-03-30T22:06:10Z","published":"2023-03-30T22:06:10Z","title":"Evaluation of GPT and BERT-based models on identifying protein-protein\n  interactions in biomedical text","summary":"  Detecting protein-protein interactions (PPIs) is crucial for understanding\ngenetic mechanisms, disease pathogenesis, and drug design. However, with the\nfast-paced growth of biomedical literature, there is a growing need for\nautomated and accurate extraction of PPIs to facilitate scientific knowledge\ndiscovery. Pre-trained language models, such as generative pre-trained\ntransformer (GPT) and bidirectional encoder representations from transformers\n(BERT), have shown promising results in natural language processing (NLP)\ntasks. We evaluated the PPI identification performance of various GPT and BERT\nmodels using a manually curated benchmark corpus of 164 PPIs in 77 sentences\nfrom learning language in logic (LLL). BERT-based models achieved the best\noverall performance, with PubMedBERT achieving the highest precision (85.17%)\nand F1-score (86.47%) and BioM-ALBERT achieving the highest recall (93.83%).\nDespite not being explicitly trained for biomedical texts, GPT-4 achieved\ncomparable performance to the best BERT models with 83.34% precision, 76.57%\nrecall, and 79.18% F1-score. These findings suggest that GPT models can\neffectively detect PPIs from text data and have the potential for use in\nbiomedical literature mining tasks.\n","authors":["Hasin Rehana","Nur Bengisu Çam","Mert Basmaci","Yongqun He","Arzucan Özgür","Junguk Hur"],"pdf_url":"https://arxiv.org/pdf/2303.17728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08447v3","updated":"2023-03-30T21:59:01Z","published":"2022-11-15T19:00:20Z","title":"SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic\n  Specialisation for Chinese Sexism Detection in Social Media","summary":"  The goal of sexism detection is to mitigate negative online content targeting\ncertain gender groups of people. However, the limited availability of labeled\nsexism-related datasets makes it problematic to identify online sexism for\nlow-resource languages. In this paper, we address the task of automatic sexism\ndetection in social media for one low-resource language -- Chinese. Rather than\ncollecting new sexism data or building cross-lingual transfer learning models,\nwe develop a cross-lingual domain-aware semantic specialisation system in order\nto make the most of existing data. Semantic specialisation is a technique for\nretrofitting pre-trained distributional word vectors by integrating external\nlinguistic knowledge (such as lexico-semantic relations) into the specialised\nfeature space. To do this, we leverage semantic resources for sexism from a\nhigh-resource language (English) to specialise pre-trained word vectors in the\ntarget language (Chinese) to inject domain knowledge. We demonstrate the\nbenefit of our sexist word embeddings (SexWEs) specialised by our framework via\nintrinsic evaluation of word similarity and extrinsic evaluation of sexism\ndetection. Compared with other specialisation approaches and Chinese baseline\nword vectors, our SexWEs shows an average score improvement of 0.033 and 0.064\nin both intrinsic and extrinsic evaluations, respectively. The ablative results\nand visualisation of SexWEs also prove the effectiveness of our framework on\nretrofitting word vectors in low-resource languages.\n","authors":["Aiqi Jiang","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2211.08447v3.pdf","comment":"accepted at ICWSM 2023"},{"id":"http://arxiv.org/abs/2303.17710v1","updated":"2023-03-30T21:05:22Z","published":"2023-03-30T21:05:22Z","title":"What Types of Questions Require Conversation to Answer? A Case Study of\n  AskReddit Questions","summary":"  The proliferation of automated conversational systems such as chatbots,\nspoken-dialogue systems, and smart speakers, has significantly impacted modern\ndigital life. However, these systems are primarily designed to provide answers\nto well-defined questions rather than to support users in exploring complex,\nill-defined questions. In this paper, we aim to push the boundaries of\nconversational systems by examining the types of nebulous, open-ended questions\nthat can best be answered through conversation. We first sampled 500 questions\nfrom one million open-ended requests posted on AskReddit, and then recruited\nonline crowd workers to answer eight inquiries about these questions. We also\nperformed open coding to categorize the questions into 27 different domains. We\nfound that the issues people believe require conversation to resolve\nsatisfactorily are highly social and personal. Our work provides insights into\nhow future research could be geared to align with users' needs.\n","authors":["Shih-Hong Huang","Chieh-Yang Huang","Ya-Fang Lin","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2303.17710v1.pdf","comment":"To appear in CHI 2023 Late-Breaking Work"},{"id":"http://arxiv.org/abs/2202.12107v3","updated":"2023-03-30T21:00:17Z","published":"2022-02-24T14:01:50Z","title":"From Natural Language to Simulations: Applying GPT-3 Codex to Automate\n  Simulation Modeling of Logistics Systems","summary":"  Our work is the first attempt to apply Natural Language Processing to\nautomate the development of simulation models of systems vitally important for\nlogistics. We demonstrated that the framework built on top of the fine-tuned\nGPT-3 Codex, a Transformer-based language model, could produce functionally\nvalid simulations of queuing and inventory control systems given the verbal\ndescription. In conducted experiments, GPT-3 Codex demonstrated convincing\nexpertise in Python as well as an understanding of the domain-specific\nvocabulary. As a result, the language model could produce simulations of a\nsingle-product inventory-control system and single-server queuing system given\nthe domain-specific context, a detailed description of the process, and a list\nof variables with the corresponding values. The demonstrated results, along\nwith the rapid improvement of language models, open the door for significant\nsimplification of the workflow behind the simulation model development, which\nwill allow experts to focus on the high-level consideration of the problem and\nholistic thinking.\n","authors":["Ilya Jackson","Maria Jesus Saenz"],"pdf_url":"https://arxiv.org/pdf/2202.12107v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17695v1","updated":"2023-03-30T20:23:49Z","published":"2023-03-30T20:23:49Z","title":"Task Oriented Conversational Modelling With Subjective Knowledge","summary":"  Existing conversational models are handled by a database(DB) and API based\nsystems. However, very often users' questions require information that cannot\nbe handled by such systems. Nonetheless, answers to these questions are\navailable in the form of customer reviews and FAQs. DSTC-11 proposes a three\nstage pipeline consisting of knowledge seeking turn detection, knowledge\nselection and response generation to create a conversational model grounded on\nthis subjective knowledge. In this paper, we focus on improving the knowledge\nselection module to enhance the overall system performance. In particular, we\npropose entity retrieval methods which result in an accurate and faster\nknowledge search. Our proposed Named Entity Recognition (NER) based entity\nretrieval method results in 7X faster search compared to the baseline model.\nAdditionally, we also explore a potential keyword extraction method which can\nimprove the accuracy of knowledge selection. Preliminary results show a 4 \\%\nimprovement in exact match score on knowledge selection task. The code is\navailable https://github.com/raja-kumar/knowledge-grounded-TODS\n","authors":["Raja Kumar"],"pdf_url":"https://arxiv.org/pdf/2303.17695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17683v1","updated":"2023-03-30T19:51:18Z","published":"2023-03-30T19:51:18Z","title":"Fine-Tuning BERT with Character-Level Noise for Zero-Shot Transfer to\n  Dialects and Closely-Related Languages","summary":"  In this work, we induce character-level noise in various forms when\nfine-tuning BERT to enable zero-shot cross-lingual transfer to unseen dialects\nand languages. We fine-tune BERT on three sentence-level classification tasks\nand evaluate our approach on an assortment of unseen dialects and languages. We\nfind that character-level noise can be an extremely effective agent of\ncross-lingual transfer under certain conditions, while it is not as helpful in\nothers. Specifically, we explore these differences in terms of the nature of\nthe task and the relationships between source and target languages, finding\nthat introduction of character-level noise during fine-tuning is particularly\nhelpful when a task draws on surface level cues and the source-target\ncross-lingual pair has a relatively high lexical overlap with shorter (i.e.,\nless meaningful) unseen tokens on average.\n","authors":["Aarohi Srivastava","David Chiang"],"pdf_url":"https://arxiv.org/pdf/2303.17683v1.pdf","comment":"Accepted for publication at VarDial 2023"},{"id":"http://arxiv.org/abs/2303.17651v1","updated":"2023-03-30T18:30:01Z","published":"2023-03-30T18:30:01Z","title":"Self-Refine: Iterative Refinement with Self-Feedback","summary":"  Like people, LLMs do not always generate the best text for a given generation\nproblem on their first try (e.g., summaries, answers, explanations). Just as\npeople then refine their text, we introduce SELF-REFINE, a framework for\nsimilarly improving initial outputs from LLMs through iterative feedback and\nrefinement. The main idea is to generate an output using an LLM, then allow the\nsame model to provide multi-aspect feedback for its own output; finally, the\nsame model refines its previously generated output given its own feedback.\nUnlike earlier work, our iterative refinement framework does not require\nsupervised training data or reinforcement learning, and works with a single\nLLM. We experiment with 7 diverse tasks, ranging from review rewriting to math\nreasoning, demonstrating that our approach outperforms direct generation. In\nall tasks, outputs generated with SELF-REFINE are preferred by humans and by\nautomated metrics over those generated directly with GPT-3.5 and GPT-4,\nimproving on average by absolute 20% across tasks.\n","authors":["Aman Madaan","Niket Tandon","Prakhar Gupta","Skyler Hallinan","Luyu Gao","Sarah Wiegreffe","Uri Alon","Nouha Dziri","Shrimai Prabhumoye","Yiming Yang","Sean Welleck","Bodhisattwa Prasad Majumder","Shashank Gupta","Amir Yazdanbakhsh","Peter Clark"],"pdf_url":"https://arxiv.org/pdf/2303.17651v1.pdf","comment":"Code, data, and demo at https://selfrefine.info/"},{"id":"http://arxiv.org/abs/2303.17650v1","updated":"2023-03-30T18:28:33Z","published":"2023-03-30T18:28:33Z","title":"Comparing Abstractive Summaries Generated by ChatGPT to Real Summaries\n  Through Blinded Reviewers and Text Classification Algorithms","summary":"  Large Language Models (LLMs) have gathered significant attention due to their\nimpressive performance on a variety of tasks. ChatGPT, developed by OpenAI, is\na recent addition to the family of language models and is being called a\ndisruptive technology by a few, owing to its human-like text-generation\ncapabilities. Although, many anecdotal examples across the internet have\nevaluated ChatGPT's strength and weakness, only a few systematic research\nstudies exist. To contribute to the body of literature of systematic research\non ChatGPT, we evaluate the performance of ChatGPT on Abstractive Summarization\nby the means of automated metrics and blinded human reviewers. We also build\nautomatic text classifiers to detect ChatGPT generated summaries. We found that\nwhile text classification algorithms can distinguish between real and generated\nsummaries, humans are unable to distinguish between real summaries and those\nproduced by ChatGPT.\n","authors":["Mayank Soni","Vincent Wade"],"pdf_url":"https://arxiv.org/pdf/2303.17650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06135v2","updated":"2023-03-30T18:28:05Z","published":"2023-03-10T18:53:52Z","title":"Rewarding Chatbots for Real-World Engagement with Millions of Users","summary":"  The emergence of pretrained large language models has led to the deployment\nof a range of social chatbots for chitchat. Although these chatbots demonstrate\nlanguage ability and fluency, they are not guaranteed to be engaging and can\nstruggle to retain users. This work investigates the development of social\nchatbots that prioritize user engagement to enhance retention, specifically\nexamining the use of human feedback to efficiently develop highly engaging\nchatbots. The proposed approach uses automatic pseudo-labels collected from\nuser interactions to train a reward model that can be used to reject\nlow-scoring sample responses generated by the chatbot model at inference time.\nIntuitive evaluation metrics, such as mean conversation length (MCL), are\nintroduced as proxies to measure the level of engagement of deployed chatbots.\nA/B testing on groups of 10,000 new daily chatbot users on the Chai Research\nplatform shows that this approach increases the MCL by up to 70%, which\ntranslates to a more than 30% increase in user retention for a GPT-J 6B model.\nFuture work aims to use the reward model to realise a data fly-wheel, where the\nlatest user conversations can be used to alternately fine-tune the language\nmodel and the reward model.\n","authors":["Robert Irvine","Douglas Boubert","Vyas Raina","Adian Liusie","Ziyi Zhu","Vineet Mudupalli","Aliaksei Korshuk","Zongyi Liu","Fritz Cremer","Valentin Assassi","Christie-Carol Beauchamp","Xiaoding Lu","Thomas Rialan","William Beauchamp"],"pdf_url":"https://arxiv.org/pdf/2303.06135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17649v1","updated":"2023-03-30T18:27:15Z","published":"2023-03-30T18:27:15Z","title":"Aligning a medium-size GPT model in English to a small closed domain in\n  Spanish using reinforcement learning","summary":"  In this paper, we propose a methodology to align a medium-sized GPT model,\noriginally trained in English for an open domain, to a small closed domain in\nSpanish. The application for which the model is finely tuned is the question\nanswering task. To achieve this we also needed to train and implement another\nneural network (which we called the reward model) that could score and\ndetermine whether an answer is appropriate for a given question. This component\nserved to improve the decoding and generation of the answers of the system.\nNumerical metrics such as BLEU and perplexity were used to evaluate the model,\nand human judgment was also used to compare the decoding technique with others.\nFinally, the results favored the proposed method, and it was determined that it\nis feasible to use a reward model to align the generation of responses.\n","authors":["Oscar R. Navarrete-Parra","Victor Uc-Cetina","Jorge Reyes-Magana"],"pdf_url":"https://arxiv.org/pdf/2303.17649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17647v1","updated":"2023-03-30T18:24:06Z","published":"2023-03-30T18:24:06Z","title":"Detecting and Grounding Important Characters in Visual Stories","summary":"  Characters are essential to the plot of any story. Establishing the\ncharacters before writing a story can improve the clarity of the plot and the\noverall flow of the narrative. However, previous work on visual storytelling\ntends to focus on detecting objects in images and discovering relationships\nbetween them. In this approach, characters are not distinguished from other\nobjects when they are fed into the generation pipeline. The result is a\ncoherent sequence of events rather than a character-centric story. In order to\naddress this limitation, we introduce the VIST-Character dataset, which\nprovides rich character-centric annotations, including visual and textual\nco-reference chains and importance ratings for characters. Based on this\ndataset, we propose two new tasks: important character detection and character\ngrounding in visual stories. For both tasks, we develop simple, unsupervised\nmodels based on distributional similarity and pre-trained vision-and-language\nmodels. Our new dataset, together with these models, can serve as the\nfoundation for subsequent work on analysing and generating stories from a\ncharacter-centric perspective.\n","authors":["Danyang Liu","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2303.17647v1.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.13592v2","updated":"2023-03-30T14:59:26Z","published":"2023-03-23T18:16:30Z","title":"Prompting Multilingual Large Language Models to Generate Code-Mixed\n  Texts: The Case of South East Asian Languages","summary":"  While code-mixing is a common linguistic practice in many parts of the world,\ncollecting high-quality and low-cost code-mixed data remains a challenge for\nnatural language processing (NLP) research. The proliferation of Large Language\nModels (LLMs) in recent times compels one to ask: can these systems be used for\ndata generation? In this article, we explore prompting multilingual LLMs in a\nzero-shot manner to create code-mixed data for five languages in South East\nAsia (SEA) -- Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the\ncreole language Singlish. We find that ChatGPT shows the most potential,\ncapable of producing code-mixed text 68% of the time when the term\n\"code-mixing\" is explicitly defined. Moreover, both ChatGPT's and InstructGPT's\n(davinci-003) performances in generating Singlish texts are noteworthy,\naveraging a 96% success rate across a variety of prompts. Their code-mixing\nproficiency, however, is dampened by word choice errors that lead to semantic\ninaccuracies. Other multilingual models such as BLOOMZ and Flan-T5-XXL are\nunable to produce code-mixed texts altogether. By highlighting the limited\npromises of LLMs in a specific form of low-resource data generation, we call\nfor a measured approach when applying similar techniques to other data-scarce\nNLP contexts.\n","authors":["Zheng-Xin Yong","Ruochen Zhang","Jessica Zosa Forde","Skyler Wang","Samuel Cahyawijaya","Holy Lovenia","Genta Indra Winata","Lintang Sutawika","Jan Christian Blaise Cruz","Long Phan","Yin Lin Tan","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2303.13592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18062v1","updated":"2023-03-30T12:36:46Z","published":"2023-03-30T12:36:46Z","title":"Solving morphological analogies: from retrieval to generation","summary":"  Analogical inference is a remarkable capability of human reasoning, and has\nbeen used to solve hard reasoning tasks. Analogy based reasoning (AR) has\ngained increasing interest from the artificial intelligence community and has\nshown its potential in multiple machine learning tasks such as classification,\ndecision making and recommendation with competitive results. We propose a deep\nlearning (DL) framework to address and tackle two key tasks in AR: analogy\ndetection and solving. The framework is thoroughly tested on the Siganalogies\ndataset of morphological analogical proportions (APs) between words, and shown\nto outperform symbolic approaches in many languages. Previous work have\nexplored the behavior of the Analogy Neural Network for classification (ANNc)\non analogy detection and of the Analogy Neural Network for retrieval (ANNr) on\nanalogy solving by retrieval, as well as the potential of an autoencoder (AE)\nfor analogy solving by generating the solution word. In this article we\nsummarize these findings and we extend them by combining ANNr and the AE\nembedding model, and checking the performance of ANNc as an retrieval method.\nThe combination of ANNr and AE outperforms the other approaches in almost all\ncases, and ANNc as a retrieval method achieves competitive or better\nperformance than 3CosMul. We conclude with general guidelines on using our\nframework to tackle APs with DL.\n","authors":["Esteban Marquer","Miguel Couceiro"],"pdf_url":"https://arxiv.org/pdf/2303.18062v1.pdf","comment":"Preprint submitted to Springer special Issue in Annals of Mathematics\n  and Artificial Intelligence: Mathematical Foundations of analogical reasoning\n  and applications"},{"id":"http://arxiv.org/abs/2303.17612v1","updated":"2023-03-30T01:37:19Z","published":"2023-03-30T01:37:19Z","title":"oBERTa: Improving Sparse Transfer Learning via improved initialization,\n  distillation, and pruning regimes","summary":"  In this paper, we introduce the range of oBERTa language models, an\neasy-to-use set of language models, which allows Natural Language Processing\n(NLP) practitioners to obtain between 3.8 and 24.3 times faster models without\nexpertise in model compression. Specifically, oBERTa extends existing work on\npruning, knowledge distillation, and quantization and leverages frozen\nembeddings to improve knowledge distillation, and improved model initialization\nto deliver higher accuracy on a a broad range of transfer tasks. In generating\noBERTa, we explore how the highly optimized RoBERTa differs from the BERT with\nrespect to pruning during pre-training and fine-tuning and find it less\namenable to compression during fine-tuning. We explore the use of oBERTa on a\nbroad seven representative NLP tasks and find that the improved compression\ntechniques allow a pruned oBERTa model to match the performance of BERTBASE and\nexceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering\ndataset, despite being 8x and 2x, respectively, faster in inference. We release\nour code, training regimes, and associated model for broad usage to encourage\nusage and experimentation.\n","authors":["Daniel Campos","Alexandre Marques","Mark Kurtz","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.17612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00993v1","updated":"2023-03-30T17:59:46Z","published":"2023-03-30T17:59:46Z","title":"Unsupervised Word Segmentation Using Temporal Gradient Pseudo-Labels","summary":"  Unsupervised word segmentation in audio utterances is challenging as, in\nspeech, there is typically no gap between words. In a preliminary experiment,\nwe show that recent deep self-supervised features are very effective for word\nsegmentation but require supervision for training the classification head. To\nextend their effectiveness to unsupervised word segmentation, we propose a\npseudo-labeling strategy. Our approach relies on the observation that the\ntemporal gradient magnitude of the embeddings (i.e. the distance between the\nembeddings of subsequent frames) is typically minimal far from the boundaries\nand higher nearer the boundaries. We use a thresholding function on the\ntemporal gradient magnitude to define a psuedo-label for wordness. We train a\nlinear classifier, mapping the embedding of a single frame to the pseudo-label.\nFinally, we use the classifier score to predict whether a frame is a word or a\nboundary. In an empirical investigation, our method, despite its simplicity and\nfast run time, is shown to significantly outperform all previous methods on two\ndatasets.\n","authors":["Tzeviya Sylvia Fuchs","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2304.00993v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2304.01005v1","updated":"2023-03-30T15:31:49Z","published":"2023-03-30T15:31:49Z","title":"Federated Learning Based Multilingual Emoji Prediction In Clean and\n  Attack Scenarios","summary":"  Federated learning is a growing field in the machine learning community due\nto its decentralized and private design. Model training in federated learning\nis distributed over multiple clients giving access to lots of client data while\nmaintaining privacy. Then, a server aggregates the training done on these\nmultiple clients without access to their data, which could be emojis widely\nused in any social media service and instant messaging platforms to express\nusers' sentiments. This paper proposes federated learning-based multilingual\nemoji prediction in both clean and attack scenarios. Emoji prediction data have\nbeen crawled from both Twitter and SemEval emoji datasets. This data is used to\ntrain and evaluate different transformer model sizes including a sparsely\nactivated transformer with either the assumption of clean data in all clients\nor poisoned data via label flipping attack in some clients. Experimental\nresults on these models show that federated learning in either clean or\nattacked scenarios performs similarly to centralized training in multilingual\nemoji prediction on seen and unseen languages under different data sources and\ndistributions. Our trained transformers perform better than other techniques on\nthe SemEval emoji dataset in addition to the privacy as well as distributed\nbenefits of federated learning.\n","authors":["Karim Gamal","Ahmed Gaber","Hossam Amer"],"pdf_url":"https://arxiv.org/pdf/2304.01005v1.pdf","comment":"5 pages, 1 figure, conference"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.17606v1","updated":"2023-03-30T17:59:59Z","published":"2023-03-30T17:59:59Z","title":"AvatarCraft: Transforming Text into Neural Human Avatars with\n  Parameterized Shape and Pose Control","summary":"  Neural implicit fields are powerful for representing 3D scenes and generating\nhigh-quality novel views, but it remains challenging to use such implicit\nrepresentations for creating a 3D human avatar with a specific identity and\nartistic style that can be easily animated. Our proposed method, AvatarCraft,\naddresses this challenge by using diffusion models to guide the learning of\ngeometry and texture for a neural avatar based on a single text prompt. We\ncarefully design the optimization framework of neural implicit fields,\nincluding a coarse-to-fine multi-bounding box training strategy, shape\nregularization, and diffusion-based constraints, to produce high-quality\ngeometry and texture. Additionally, we make the human avatar animatable by\ndeforming the neural implicit field with an explicit warping field that maps\nthe target human mesh to a template human mesh, both represented using\nparametric human models. This simplifies animation and reshaping of the\ngenerated avatar by controlling pose and shape parameters. Extensive\nexperiments on various text descriptions show that AvatarCraft is effective and\nrobust in creating human avatars and rendering novel views, poses, and shapes.\nOur project page is: \\url{https://avatar-craft.github.io/}.\n","authors":["Ruixiang Jiang","Can Wang","Jingbo Zhang","Menglei Chai","Mingming He","Dongdong Chen","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2303.17606v1.pdf","comment":"Project page is: https://avatar-craft.github.io/"},{"id":"http://arxiv.org/abs/2303.17603v1","updated":"2023-03-30T17:59:58Z","published":"2023-03-30T17:59:58Z","title":"NeRF-Supervised Deep Stereo","summary":"  We introduce a novel framework for training deep stereo networks effortlessly\nand without any ground-truth. By leveraging state-of-the-art neural rendering\nsolutions, we generate stereo training data from image sequences collected with\na single handheld camera. On top of them, a NeRF-supervised training procedure\nis carried out, from which we exploit rendered stereo triplets to compensate\nfor occlusions and depth maps as proxy labels. This results in stereo networks\ncapable of predicting sharp and detailed disparity maps. Experimental results\nshow that models trained under this regime yield a 30-40% improvement over\nexisting self-supervised methods on the challenging Middlebury dataset, filling\nthe gap to supervised models and, most times, outperforming them at zero-shot\ngeneralization.\n","authors":["Fabio Tosi","Alessio Tonioni","Daniele De Gregorio","Matteo Poggi"],"pdf_url":"https://arxiv.org/pdf/2303.17603v1.pdf","comment":"CVPR 2023. Project page: https://nerfstereo.github.io/ Code:\n  https://github.com/fabiotosi92/NeRF-Supervised-Deep-Stereo"},{"id":"http://arxiv.org/abs/2303.17604v1","updated":"2023-03-30T17:59:58Z","published":"2023-03-30T17:59:58Z","title":"Token Merging for Fast Stable Diffusion","summary":"  The landscape of image generation has been forever changed by open vocabulary\ndiffusion models. However, at their core these models use transformers, which\nmakes generation slow. Better implementations to increase the throughput of\nthese transformers have emerged, but they still evaluate the entire model. In\nthis paper, we instead speed up diffusion models by exploiting natural\nredundancy in generated images by merging redundant tokens. After making some\ndiffusion-specific improvements to Token Merging (ToMe), our ToMe for Stable\nDiffusion can reduce the number of tokens in an existing Stable Diffusion model\nby up to 60% while still producing high quality images without any extra\ntraining. In the process, we speed up image generation by up to 2x and reduce\nmemory consumption by up to 5.6x. Furthermore, this speed-up stacks with\nefficient implementations such as xFormers, minimally impacting quality while\nbeing up to 5.4x faster for large images. Code is available at\nhttps://github.com/dbolya/tomesd.\n","authors":["Daniel Bolya","Judy Hoffman"],"pdf_url":"https://arxiv.org/pdf/2303.17604v1.pdf","comment":"Check out the code at https://github.com/dbolya/tomesd"},{"id":"http://arxiv.org/abs/2303.17605v1","updated":"2023-03-30T17:59:58Z","published":"2023-03-30T17:59:58Z","title":"SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution\n  Vision Transformer","summary":"  High-resolution images enable neural networks to learn richer visual\nrepresentations. However, this improved performance comes at the cost of\ngrowing computational complexity, hindering their usage in latency-sensitive\napplications. As not all pixels are equal, skipping computations for\nless-important regions offers a simple and effective measure to reduce the\ncomputation. This, however, is hard to be translated into actual speedup for\nCNNs since it breaks the regularity of the dense convolution workload. In this\npaper, we introduce SparseViT that revisits activation sparsity for recent\nwindow-based vision transformers (ViTs). As window attentions are naturally\nbatched over blocks, actual speedup with window activation pruning becomes\npossible: i.e., ~50% latency reduction with 60% sparsity. Different layers\nshould be assigned with different pruning ratios due to their diverse\nsensitivities and computational costs. We introduce sparsity-aware adaptation\nand apply the evolutionary search to efficiently find the optimal layerwise\nsparsity configuration within the vast search space. SparseViT achieves\nspeedups of 1.5x, 1.4x, and 1.3x compared to its dense counterpart in monocular\n3D object detection, 2D instance segmentation, and 2D semantic segmentation,\nrespectively, with negligible to no loss of accuracy.\n","authors":["Xuanyao Chen","Zhijian Liu","Haotian Tang","Li Yi","Hang Zhao","Song Han"],"pdf_url":"https://arxiv.org/pdf/2303.17605v1.pdf","comment":"CVPR 2023. The first two authors contributed equally to this work.\n  Project page: https://sparsevit.mit.edu"},{"id":"http://arxiv.org/abs/2303.17602v1","updated":"2023-03-30T17:59:39Z","published":"2023-03-30T17:59:39Z","title":"Beyond Appearance: a Semantic Controllable Self-Supervised Learning\n  Framework for Human-Centric Visual Tasks","summary":"  Human-centric visual tasks have attracted increasing research attention due\nto their widespread applications. In this paper, we aim to learn a general\nhuman representation from massive unlabeled human images which can benefit\ndownstream human-centric tasks to the maximum extent. We call this method\nSOLIDER, a Semantic cOntrollable seLf-supervIseD lEaRning framework. Unlike the\nexisting self-supervised learning methods, prior knowledge from human images is\nutilized in SOLIDER to build pseudo semantic labels and import more semantic\ninformation into the learned representation. Meanwhile, we note that different\ndownstream tasks always require different ratios of semantic information and\nappearance information. For example, human parsing requires more semantic\ninformation, while person re-identification needs more appearance information\nfor identification purpose. So a single learned representation cannot fit for\nall requirements. To solve this problem, SOLIDER introduces a conditional\nnetwork with a semantic controller. After the model is trained, users can send\nvalues to the controller to produce representations with different ratios of\nsemantic information, which can fit different needs of downstream tasks.\nFinally, SOLIDER is verified on six downstream human-centric visual tasks. It\noutperforms state of the arts and builds new baselines for these tasks. The\ncode is released in https://github.com/tinyvision/SOLIDER.\n","authors":["Weihua Chen","Xianzhe Xu","Jian Jia","Hao luo","Yaohua Wang","Fan Wang","Rong Jin","Xiuyu Sun"],"pdf_url":"https://arxiv.org/pdf/2303.17602v1.pdf","comment":"accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.17600v1","updated":"2023-03-30T17:59:26Z","published":"2023-03-30T17:59:26Z","title":"When Learning Is Out of Reach, Reset: Generalization in Autonomous\n  Visuomotor Reinforcement Learning","summary":"  Episodic training, where an agent's environment is reset after every success\nor failure, is the de facto standard when training embodied reinforcement\nlearning (RL) agents. The underlying assumption that the environment can be\neasily reset is limiting both practically, as resets generally require human\neffort in the real world and can be computationally expensive in simulation,\nand philosophically, as we'd expect intelligent agents to be able to\ncontinuously learn without intervention. Work in learning without any resets,\ni.e{.} Reset-Free RL (RF-RL), is promising but is plagued by the problem of\nirreversible transitions (e.g{.} an object breaking) which halt learning.\nMoreover, the limited state diversity and instrument setup encountered during\nRF-RL means that works studying RF-RL largely do not require their models to\ngeneralize to new environments. In this work, we instead look to minimize,\nrather than completely eliminate, resets while building visual agents that can\nmeaningfully generalize. As studying generalization has previously not been a\nfocus of benchmarks designed for RF-RL, we propose a new Stretch Pick-and-Place\nbenchmark designed for evaluating generalizations across goals, cosmetic\nvariations, and structural changes. Moreover, towards building performant\nreset-minimizing RL agents, we propose unsupervised metrics to detect\nirreversible transitions and a single-policy training mechanism to enable\ngeneralization. Our proposed approach significantly outperforms prior episodic,\nreset-free, and reset-minimizing approaches achieving higher success rates with\nfewer resets in Stretch-P\\&P and another popular RF-RL benchmark. Finally, we\nfind that our proposed approach can dramatically reduce the number of resets\nrequired for training other embodied tasks, in particular for RoboTHOR\nObjectNav we obtain higher success rates than episodic approaches using 99.97\\%\nfewer resets.\n","authors":["Zichen Zhang","Luca Weihs"],"pdf_url":"https://arxiv.org/pdf/2303.17600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17599v1","updated":"2023-03-30T17:59:25Z","published":"2023-03-30T17:59:25Z","title":"Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models","summary":"  Large-scale text-to-image diffusion models achieve unprecedented success in\nimage generation and editing. However, how to extend such success to video\nediting is unclear. Recent initial attempts at video editing require\nsignificant text-to-video data and computation resources for training, which is\noften not accessible. In this work, we propose vid2vid-zero, a simple yet\neffective method for zero-shot video editing. Our vid2vid-zero leverages\noff-the-shelf image diffusion models, and doesn't require training on any\nvideo. At the core of our method is a null-text inversion module for\ntext-to-video alignment, a cross-frame modeling module for temporal\nconsistency, and a spatial regularization module for fidelity to the original\nvideo. Without any training, we leverage the dynamic nature of the attention\nmechanism to enable bi-directional temporal modeling at test time. Experiments\nand analyses show promising results in editing attributes, subjects, places,\netc., in real-world videos. Code will be made available at\n\\url{https://github.com/baaivision/vid2vid-zero}.\n","authors":["Wen Wang","Kangyang Xie","Zide Liu","Hao Chen","Yue Cao","Xinlong Wang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2303.17599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17598v1","updated":"2023-03-30T17:59:22Z","published":"2023-03-30T17:59:22Z","title":"Consistent View Synthesis with Pose-Guided Diffusion Models","summary":"  Novel view synthesis from a single image has been a cornerstone problem for\nmany Virtual Reality applications that provide immersive experiences. However,\nmost existing techniques can only synthesize novel views within a limited range\nof camera motion or fail to generate consistent and high-quality novel views\nunder significant camera movement. In this work, we propose a pose-guided\ndiffusion model to generate a consistent long-term video of novel views from a\nsingle image. We design an attention layer that uses epipolar lines as\nconstraints to facilitate the association between different viewpoints.\nExperimental results on synthetic and real-world datasets demonstrate the\neffectiveness of the proposed diffusion model against state-of-the-art\ntransformer-based and GAN-based approaches.\n","authors":["Hung-Yu Tseng","Qinbo Li","Changil Kim","Suhib Alsisan","Jia-Bin Huang","Johannes Kopf"],"pdf_url":"https://arxiv.org/pdf/2303.17598v1.pdf","comment":"CVPR 2023. Project page: https://poseguided-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2303.17597v1","updated":"2023-03-30T17:59:17Z","published":"2023-03-30T17:59:17Z","title":"Robo3D: Towards Robust and Reliable 3D Perception against Corruptions","summary":"  The robustness of 3D perception systems under natural corruptions from\nenvironments and sensors is pivotal for safety-critical applications. Existing\nlarge-scale 3D perception datasets often contain data that are meticulously\ncleaned. Such configurations, however, cannot reflect the reliability of\nperception models during the deployment stage. In this work, we present Robo3D,\nthe first comprehensive benchmark heading toward probing the robustness of 3D\ndetectors and segmentors under out-of-distribution scenarios against natural\ncorruptions that occur in real-world environments. Specifically, we consider\neight corruption types stemming from adversarial weather conditions, external\ndisturbances, and internal sensor failure. We uncover that, although promising\nresults have been progressively achieved on standard benchmarks,\nstate-of-the-art 3D perception models are at risk of being vulnerable to\ncorruptions. We draw key observations on the use of data representations,\naugmentation schemes, and training strategies, that could severely affect the\nmodel's performance. To pursue better robustness, we propose a\ndensity-insensitive training framework along with a simple flexible\nvoxelization strategy to enhance the model resiliency. We hope our benchmark\nand approach could inspire future research in designing more robust and\nreliable 3D perception models. Our robustness benchmark suite is publicly\navailable.\n","authors":["Lingdong Kong","Youquan Liu","Xin Li","Runnan Chen","Wenwei Zhang","Jiawei Ren","Liang Pan","Kai Chen","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.17597v1.pdf","comment":"33 pages, 26 figures, 26 tables; code at\n  https://github.com/ldkong1205/Robo3D; project page at\n  https://ldkong.com/Robo3D"},{"id":"http://arxiv.org/abs/2211.09790v2","updated":"2023-03-30T17:59:16Z","published":"2022-11-17T18:57:03Z","title":"ConStruct-VL: Data-Free Continual Structured VL Concepts Learning","summary":"  Recently, large-scale pre-trained Vision-and-Language (VL) foundation models\nhave demonstrated remarkable capabilities in many zero-shot downstream tasks,\nachieving competitive results for recognizing objects defined by as little as\nshort text prompts. However, it has also been shown that VL models are still\nbrittle in Structured VL Concept (SVLC) reasoning, such as the ability to\nrecognize object attributes, states, and inter-object relations. This leads to\nreasoning mistakes, which need to be corrected as they occur by teaching VL\nmodels the missing SVLC skills; often this must be done using private data\nwhere the issue was found, which naturally leads to a data-free continual (no\ntask-id) VL learning setting. In this work, we introduce the first Continual\nData-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it\nis challenging for many existing data-free CL strategies. We, therefore,\npropose a data-free method comprised of a new approach of Adversarial\nPseudo-Replay (APR) which generates adversarial reminders of past tasks from\npast task models. To use this method efficiently, we also propose a continual\nparameter-efficient Layered-LoRA (LaLo) neural architecture allowing\nno-memory-cost access to all past models at train time. We show this approach\noutperforms all data-free methods by as much as ~7% while even matching some\nlevels of experience-replay (prohibitive for applications where data-privacy\nmust be preserved). Our code is publicly available at\nhttps://github.com/jamessealesmith/ConStruct-VL\n","authors":["James Seale Smith","Paola Cascante-Bonilla","Assaf Arbelle","Donghyun Kim","Rameswar Panda","David Cox","Diyi Yang","Zsolt Kira","Rogerio Feris","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2211.09790v2.pdf","comment":"Accepted by the 2023 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR 2023)"},{"id":"http://arxiv.org/abs/2303.17594v1","updated":"2023-03-30T17:59:02Z","published":"2023-03-30T17:59:02Z","title":"MobileInst: Video Instance Segmentation on the Mobile","summary":"  Although recent approaches aiming for video instance segmentation have\nachieved promising results, it is still difficult to employ those approaches\nfor real-world applications on mobile devices, which mainly suffer from (1)\nheavy computation and memory cost and (2) complicated heuristics for tracking\nobjects. To address those issues, we present MobileInst, a lightweight and\nmobile-friendly framework for video instance segmentation on mobile devices.\nFirstly, MobileInst adopts a mobile vision transformer to extract multi-level\nsemantic features and presents an efficient query-based dual-transformer\ninstance decoder for mask kernels and a semantic-enhanced mask decoder to\ngenerate instance segmentation per frame. Secondly, MobileInst exploits simple\nyet effective kernel reuse and kernel association to track objects for video\ninstance segmentation. Further, we propose temporal query passing to enhance\nthe tracking ability for kernels. We conduct experiments on COCO and\nYouTube-VIS datasets to demonstrate the superiority of MobileInst and evaluate\nthe inference latency on a mobile CPU core of Qualcomm Snapdragon-778G, without\nother methods of acceleration. On the COCO dataset, MobileInst achieves 30.5\nmask AP and 176 ms on the mobile CPU, which reduces the latency by 50% compared\nto the previous SOTA. For video instance segmentation, MobileInst achieves 35.0\nAP on YouTube-VIS 2019 and 30.1 AP on YouTube-VIS 2021. Code will be available\nto facilitate real-world applications and future research.\n","authors":["Renhong Zhang","Tianheng Cheng","Shusheng Yang","Haoyi Jiang","Shuai Zhang","Jiancheng Lyu","Xin Li","Xiaowen Ying","Dashan Gao","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.17594v1.pdf","comment":"Preprint. 12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.17595v1","updated":"2023-03-30T17:59:02Z","published":"2023-03-30T17:59:02Z","title":"Neglected Free Lunch -- Learning Image Classifiers Using Annotation\n  Byproducts","summary":"  Supervised learning of image classifiers distills human knowledge into a\nparametric model through pairs of images and corresponding labels (X,Y). We\nargue that this simple and widely used representation of human knowledge\nneglects rich auxiliary information from the annotation procedure, such as the\ntime-series of mouse traces and clicks left after image selection. Our insight\nis that such annotation byproducts Z provide approximate human attention that\nweakly guides the model to focus on the foreground cues, reducing spurious\ncorrelations and discouraging shortcut learning. To verify this, we create\nImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with\nsample-wise annotation byproducts, collected by replicating the respective\noriginal annotation tasks. We refer to the new paradigm of training models with\nannotation byproducts as learning using annotation byproducts (LUAB). We show\nthat a simple multitask loss for regressing Z together with Y already improves\nthe generalisability and robustness of the learned models. Compared to the\noriginal supervised learning, LUAB does not require extra annotation costs.\nImageNet-AB and COCO-AB are at https://github.com/naver-ai/NeglectedFreeLunch.\n","authors":["Dongyoon Han","Junsuk Choe","Seonghyeok Chun","John Joon Young Chung","Minsuk Chang","Sangdoo Yun","Jean Y. Song","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2303.17595v1.pdf","comment":"Code at https://github.com/naver-ai/NeglectedFreeLunch"},{"id":"http://arxiv.org/abs/2211.13218v2","updated":"2023-03-30T17:58:59Z","published":"2022-11-23T18:57:11Z","title":"CODA-Prompt: COntinual Decomposed Attention-based Prompting for\n  Rehearsal-Free Continual Learning","summary":"  Computer vision models suffer from a phenomenon known as catastrophic\nforgetting when learning novel concepts from continuously shifting training\ndata. Typical solutions for this continual learning problem require extensive\nrehearsal of previously seen data, which increases memory costs and may violate\ndata privacy. Recently, the emergence of large-scale pre-trained vision\ntransformer models has enabled prompting approaches as an alternative to\ndata-rehearsal. These approaches rely on a key-query mechanism to generate\nprompts and have been found to be highly resistant to catastrophic forgetting\nin the well-established rehearsal-free continual learning setting. However, the\nkey mechanism of these methods is not trained end-to-end with the task\nsequence. Our experiments show that this leads to a reduction in their\nplasticity, hence sacrificing new task accuracy, and inability to benefit from\nexpanded parameter capacity. We instead propose to learn a set of prompt\ncomponents which are assembled with input-conditioned weights to produce\ninput-conditioned prompts, resulting in a novel attention-based end-to-end\nkey-query scheme. Our experiments show that we outperform the current SOTA\nmethod DualPrompt on established benchmarks by as much as 4.5% in average final\naccuracy. We also outperform the state of art by as much as 4.4% accuracy on a\ncontinual learning benchmark which contains both class-incremental and\ndomain-incremental task shifts, corresponding to many practical settings. Our\ncode is available at https://github.com/GT-RIPL/CODA-Prompt\n","authors":["James Seale Smith","Leonid Karlinsky","Vyshnavi Gutta","Paola Cascante-Bonilla","Donghyun Kim","Assaf Arbelle","Rameswar Panda","Rogerio Feris","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2211.13218v2.pdf","comment":"Accepted by the 2023 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR 2023)"},{"id":"http://arxiv.org/abs/2303.17593v1","updated":"2023-03-30T17:58:52Z","published":"2023-03-30T17:58:52Z","title":"Anatomically aware dual-hop learning for pulmonary embolism detection in\n  CT pulmonary angiograms","summary":"  Pulmonary Embolisms (PE) represent a leading cause of cardiovascular death.\nWhile medical imaging, through computed tomographic pulmonary angiography\n(CTPA), represents the gold standard for PE diagnosis, it is still susceptible\nto misdiagnosis or significant diagnosis delays, which may be fatal for\ncritical cases. Despite the recently demonstrated power of deep learning to\nbring a significant boost in performance in a wide range of medical imaging\ntasks, there are still very few published researches on automatic pulmonary\nembolism detection. Herein we introduce a deep learning based approach, which\nefficiently combines computer vision and deep neural networks for pulmonary\nembolism detection in CTPA. Our method features novel improvements along three\northogonal axes: 1) automatic detection of anatomical structures; 2) anatomical\naware pretraining, and 3) a dual-hop deep neural net for PE detection. We\nobtain state-of-the-art results on the publicly available multicenter\nlarge-scale RSNA dataset.\n","authors":["Florin Condrea","Saikiran Rapaka","Lucian Itu","Puneet Sharma","Jonathan Sperl","A Mohamed Ali","Marius Leordeanu"],"pdf_url":"https://arxiv.org/pdf/2303.17593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17592v1","updated":"2023-03-30T17:58:36Z","published":"2023-03-30T17:58:36Z","title":"Learning Human-to-Robot Handovers from Point Clouds","summary":"  We propose the first framework to learn control policies for vision-based\nhuman-to-robot handovers, a critical task for human-robot interaction. While\nresearch in Embodied AI has made significant progress in training robot agents\nin simulated environments, interacting with humans remains challenging due to\nthe difficulties of simulating humans. Fortunately, recent research has\ndeveloped realistic simulated environments for human-to-robot handovers.\nLeveraging this result, we introduce a method that is trained with a\nhuman-in-the-loop via a two-stage teacher-student framework that uses motion\nand grasp planning, reinforcement learning, and self-supervision. We show\nsignificant performance gains over baselines on a simulation benchmark,\nsim-to-sim transfer and sim-to-real transfer.\n","authors":["Sammy Christen","Wei Yang","Claudia Pérez-D'Arpino","Otmar Hilliges","Dieter Fox","Yu-Wei Chao"],"pdf_url":"https://arxiv.org/pdf/2303.17592v1.pdf","comment":"Accepted at CVPR 2023 as highlight. Project page at\n  https://handover-sim2real.github.io"},{"id":"http://arxiv.org/abs/2303.17591v1","updated":"2023-03-30T17:58:11Z","published":"2023-03-30T17:58:11Z","title":"Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models","summary":"  The unlearning problem of deep learning models, once primarily an academic\nconcern, has become a prevalent issue in the industry. The significant advances\nin text-to-image generation techniques have prompted global discussions on\nprivacy, copyright, and safety, as numerous unauthorized personal IDs, content,\nartistic creations, and potentially harmful materials have been learned by\nthese models and later utilized to generate and distribute uncontrolled\ncontent. To address this challenge, we propose \\textbf{Forget-Me-Not}, an\nefficient and low-cost solution designed to safely remove specified IDs,\nobjects, or styles from a well-configured text-to-image model in as little as\n30 seconds, without impairing its ability to generate other content. Alongside\nour method, we introduce the \\textbf{Memorization Score (M-Score)} and\n\\textbf{ConceptBench} to measure the models' capacity to generate general\nconcepts, grouped into three primary categories: ID, object, and style. Using\nM-Score and ConceptBench, we demonstrate that Forget-Me-Not can effectively\neliminate targeted concepts while maintaining the model's performance on other\nconcepts. Furthermore, Forget-Me-Not offers two practical extensions: a)\nremoval of potentially harmful or NSFW content, and b) enhancement of model\naccuracy, inclusion and diversity through \\textbf{concept correction and\ndisentanglement}. It can also be adapted as a lightweight model patch for\nStable Diffusion, allowing for concept manipulation and convenient\ndistribution. To encourage future research in this critical area and promote\nthe development of safe and inclusive generative models, we will open-source\nour code and ConceptBench at\n\\href{https://github.com/SHI-Labs/Forget-Me-Not}{https://github.com/SHI-Labs/Forget-Me-Not}.\n","authors":["Eric Zhang","Kai Wang","Xingqian Xu","Zhangyang Wang","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2303.17591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17590v1","updated":"2023-03-30T17:57:43Z","published":"2023-03-30T17:57:43Z","title":"Going Beyond Nouns With Vision & Language Models Using Synthetic Data","summary":"  Large-scale pre-trained Vision & Language (VL) models have shown remarkable\nperformance in many applications, enabling replacing a fixed set of supported\nclasses with zero-shot open vocabulary reasoning over (almost arbitrary)\nnatural language prompts. However, recent works have uncovered a fundamental\nweakness of these models. For example, their difficulty to understand Visual\nLanguage Concepts (VLC) that go 'beyond nouns' such as the meaning of\nnon-object words (e.g., attributes, actions, relations, states, etc.), or\ndifficulty in performing compositional reasoning such as understanding the\nsignificance of the order of the words in a sentence. In this work, we\ninvestigate to which extent purely synthetic data could be leveraged to teach\nthese models to overcome such shortcomings without compromising their zero-shot\ncapabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale\nsynthetic dataset and data generation codebase allowing to generate additional\nsuitable data to improve VLC understanding and compositional reasoning of VL\nmodels. Additionally, we propose a general VL finetuning strategy for\neffectively leveraging SyViC towards achieving these improvements. Our\nextensive experiments and ablations on VL-Checklist, Winoground, and ARO\nbenchmarks demonstrate that it is possible to adapt strong pre-trained VL\nmodels with synthetic data significantly enhancing their VLC understanding\n(e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their\nzero-shot accuracy.\n","authors":["Paola Cascante-Bonilla","Khaled Shehada","James Seale Smith","Sivan Doveh","Donghyun Kim","Rameswar Panda","Gül Varol","Aude Oliva","Vicente Ordonez","Rogerio Feris","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2303.17590v1.pdf","comment":"Project page: https://synthetic-vic.github.io/"},{"id":"http://arxiv.org/abs/2303.17583v1","updated":"2023-03-30T17:51:07Z","published":"2023-03-30T17:51:07Z","title":"TiDy-PSFs: Computational Imaging with Time-Averaged Dynamic\n  Point-Spread-Functions","summary":"  Point-spread-function (PSF) engineering is a powerful computational imaging\ntechniques wherein a custom phase mask is integrated into an optical system to\nencode additional information into captured images. Used in combination with\ndeep learning, such systems now offer state-of-the-art performance at monocular\ndepth estimation, extended depth-of-field imaging, lensless imaging, and other\ntasks. Inspired by recent advances in spatial light modulator (SLM) technology,\nthis paper answers a natural question: Can one encode additional information\nand achieve superior performance by changing a phase mask dynamically over\ntime? We first prove that the set of PSFs described by static phase masks is\nnon-convex and that, as a result, time-averaged PSFs generated by dynamic phase\nmasks are fundamentally more expressive. We then demonstrate, in simulation,\nthat time-averaged dynamic (TiDy) phase masks can offer substantially improved\nmonocular depth estimation and extended depth-of-field imaging performance.\n","authors":["Sachin Shah","Sakshum Kulshrestha","Christopher A. Metzler"],"pdf_url":"https://arxiv.org/pdf/2303.17583v1.pdf","comment":"13 pages, 16 figures"},{"id":"http://arxiv.org/abs/2303.17580v1","updated":"2023-03-30T17:48:28Z","published":"2023-03-30T17:48:28Z","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace","summary":"  Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence (AGI). While there are abundant AI\nmodels available for different domains and modalities, they cannot handle\ncomplicated AI tasks. Considering large language models (LLMs) have exhibited\nexceptional ability in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks and language could be a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, a\nsystem that leverages LLMs (e.g., ChatGPT) to connect various AI models in\nmachine learning communities (e.g., HuggingFace) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHuggingFace, execute each subtask with the selected AI model, and summarize the\nresponse according to the execution results. By leveraging the strong language\ncapability of ChatGPT and abundant AI models in HuggingFace, HuggingGPT is able\nto cover numerous sophisticated AI tasks in different modalities and domains\nand achieve impressive results in language, vision, speech, and other\nchallenging tasks, which paves a new way towards AGI.\n","authors":["Yongliang Shen","Kaitao Song","Xu Tan","Dongsheng Li","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2303.17580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17569v1","updated":"2023-03-30T17:37:14Z","published":"2023-03-30T17:37:14Z","title":"Iterative Prompt Learning for Unsupervised Backlit Image Enhancement","summary":"  We propose a novel unsupervised backlit image enhancement method, abbreviated\nas CLIP-LIT, by exploring the potential of Contrastive Language-Image\nPre-Training (CLIP) for pixel-level image enhancement. We show that the\nopen-world CLIP prior not only aids in distinguishing between backlit and\nwell-lit images, but also in perceiving heterogeneous regions with different\nluminance, facilitating the optimization of the enhancement network. Unlike\nhigh-level and image manipulation tasks, directly applying CLIP to enhancement\ntasks is non-trivial, owing to the difficulty in finding accurate prompts. To\nsolve this issue, we devise a prompt learning framework that first learns an\ninitial prompt pair by constraining the text-image similarity between the\nprompt (negative/positive sample) and the corresponding image (backlit\nimage/well-lit image) in the CLIP latent space. Then, we train the enhancement\nnetwork based on the text-image similarity between the enhanced result and the\ninitial prompt pair. To further improve the accuracy of the initial prompt\npair, we iteratively fine-tune the prompt learning framework to reduce the\ndistribution gaps between the backlit images, enhanced results, and well-lit\nimages via rank learning, boosting the enhancement performance. Our method\nalternates between updating the prompt learning framework and enhancement\nnetwork until visually pleasing results are achieved. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art methods in terms of\nvisual quality and generalization ability, without requiring any paired data.\n","authors":["Zhexin Liang","Chongyi Li","Shangchen Zhou","Ruicheng Feng","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2303.17569v1.pdf","comment":"Project page: https://zhexinliang.github.io/CLIP_LIT_page/"},{"id":"http://arxiv.org/abs/2303.17561v1","updated":"2023-03-30T17:27:22Z","published":"2023-03-30T17:27:22Z","title":"SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger","summary":"  During the preceding biennium, vision-language pre-training has achieved\nnoteworthy success on several downstream tasks. Nevertheless, acquiring\nhigh-quality image-text pairs, where the pairs are entirely exclusive of each\nother, remains a challenging task, and noise exists in the commonly used\ndatasets. To address this issue, we propose SoftCLIP, a novel approach that\nrelaxes the strict one-to-one constraint and achieves a soft cross-modal\nalignment by introducing a softened target, which is generated from the\nfine-grained intra-modal self-similarity. The intra-modal guidance is\nindicative to enable two pairs have some local similarities and model\nmany-to-many relationships between the two modalities. Besides, since the\npositive still dominates in the softened target distribution, we disentangle\nthe negatives in the distribution to further boost the relation alignment with\nthe negatives in the cross-modal learning. Extensive experiments demonstrate\nthe effectiveness of SoftCLIP. In particular, on ImageNet zero-shot\nclassification task, using CC3M/CC12M as pre-training dataset, SoftCLIP brings\na top-1 accuracy improvement of 6.8%/7.2% over the CLIP baseline.\n","authors":["Yuting Gao","Jinfeng Liu","Zihan Xu","Tong Wu","Wei Liu","Jie Yang","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2303.17561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17559v1","updated":"2023-03-30T17:26:50Z","published":"2023-03-30T17:26:50Z","title":"DDP: Diffusion Model for Dense Visual Prediction","summary":"  We propose a simple, efficient, yet powerful framework for dense visual\npredictions based on the conditional diffusion pipeline. Our approach follows a\n\"noise-to-map\" generative paradigm for prediction by progressively removing\nnoise from a random Gaussian distribution, guided by the image. The method,\ncalled DDP, efficiently extends the denoising diffusion process into the modern\nperception pipeline. Without task-specific design and architecture\ncustomization, DDP is easy to generalize to most dense prediction tasks, e.g.,\nsemantic segmentation and depth estimation. In addition, DDP shows attractive\nproperties such as dynamic inference and uncertainty awareness, in contrast to\nprevious single-step discriminative methods. We show top results on three\nrepresentative tasks with six diverse benchmarks, without tricks, DDP achieves\nstate-of-the-art or competitive performance on each task compared to the\nspecialist counterparts. For example, semantic segmentation (83.9 mIoU on\nCityscapes), BEV map segmentation (70.6 mIoU on nuScenes), and depth estimation\n(0.05 REL on KITTI). We hope that our approach will serve as a solid baseline\nand facilitate future research\n","authors":["Yuanfeng Ji","Zhe Chen","Enze Xie","Lanqing Hong","Xihui Liu","Zhaoqiang Liu","Tong Lu","Zhenguo Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2303.17559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17550v1","updated":"2023-03-30T17:18:31Z","published":"2023-03-30T17:18:31Z","title":"DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with\n  Diffusion Autoencoder","summary":"  While recent research has made significant progress in speech-driven talking\nface generation, the quality of the generated video still lags behind that of\nreal recordings. One reason for this is the use of handcrafted intermediate\nrepresentations like facial landmarks and 3DMM coefficients, which are designed\nbased on human knowledge and are insufficient to precisely describe facial\nmovements. Additionally, these methods require an external pretrained model for\nextracting these representations, whose performance sets an upper bound on\ntalking face generation. To address these limitations, we propose a novel\nmethod called DAE-Talker that leverages data-driven latent representations\nobtained from a diffusion autoencoder (DAE). DAE contains an image encoder that\nencodes an image into a latent vector and a DDIM image decoder that\nreconstructs the image from it. We train our DAE on talking face video frames\nand then extract their latent representations as the training target for a\nConformer-based speech2latent model. This allows DAE-Talker to synthesize full\nvideo frames and produce natural head movements that align with the content of\nspeech, rather than relying on a predetermined head pose from a template video.\nWe also introduce pose modelling in speech2latent for pose controllability.\nAdditionally, we propose a novel method for generating continuous video frames\nwith the DDIM image decoder trained on individual frames, eliminating the need\nfor modelling the joint distribution of consecutive frames directly. Our\nexperiments show that DAE-Talker outperforms existing popular methods in\nlip-sync, video fidelity, and pose naturalness. We also conduct ablation\nstudies to analyze the effectiveness of the proposed techniques and demonstrate\nthe pose controllability of DAE-Talker.\n","authors":["Chenpng Du","Qi Chen","Tianyu He","Xu Tan","Xie Chen","Kai Yu","Sheng Zhao","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2303.17550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17546v1","updated":"2023-03-30T17:13:56Z","published":"2023-03-30T17:13:56Z","title":"PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance\n  Paired Diffusion Models","summary":"  Image editing using diffusion models has witnessed extremely fast-paced\ngrowth recently. There are various ways in which previous works enable\ncontrolling and editing images. Some works use high-level conditioning such as\ntext, while others use low-level conditioning. Nevertheless, most of them lack\nfine-grained control over the properties of the different objects present in\nthe image, i.e. object-level image editing. In this work, we consider an image\nas a composition of multiple objects, each defined by various properties. Out\nof these properties, we identify structure and appearance as the most intuitive\nto understand and useful for editing purposes. We propose\nStructure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is\ntrained using structure and appearance information explicitly extracted from\nthe images. The proposed model enables users to inject a reference image's\nappearance into the input image at both the object and global levels.\nAdditionally, PAIR-Diffusion allows editing the structure while maintaining the\nstyle of individual components of the image unchanged. We extensively evaluate\nour method on LSUN datasets and the CelebA-HQ face dataset, and we demonstrate\nfine-grained control over both structure and appearance at the object level. We\nalso applied the method to Stable Diffusion to edit any real image at the\nobject level.\n","authors":["Vidit Goel","Elia Peruzzo","Yifan Jiang","Dejia Xu","Nicu Sebe","Trevor Darrell","Zhangyang Wang","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2303.17546v1.pdf","comment":"20 pages and 16 figures"},{"id":"http://arxiv.org/abs/2301.05609v3","updated":"2023-03-30T17:13:48Z","published":"2023-01-13T15:24:40Z","title":"Co-manipulation of soft-materials estimating deformation from depth\n  images","summary":"  Human-robot co-manipulation of soft materials, such as fabrics, composites,\nand sheets of paper/cardboard, is a challenging operation that presents several\nrelevant industrial applications. Estimating the deformation state of the\nco-manipulated material is one of the main challenges. Viable methods provide\nthe indirect measure by calculating the human-robot relative distance. In this\npaper, we develop a data-driven model to estimate the deformation state of the\nmaterial from a depth image through a Convolutional Neural Network (CNN).\nFirst, we define the deformation state of the material as the relative\nroto-translation from the current robot pose and a human grasping position. The\nmodel estimates the current deformation state through a Convolutional Neural\nNetwork, specifically a DenseNet-121 pretrained on ImageNet.The delta between\nthe current and the desired deformation state is fed to the robot controller\nthat outputs twist commands. The paper describes the developed approach to\nacquire, preprocess the dataset and train the model. The model is compared with\nthe current state-of-the-art method based on a skeletal tracker from cameras.\nResults show that our approach achieves better performances and avoids the\nvarious drawbacks caused by using a skeletal tracker.Finally, we also studied\nthe model performance according to different architectures and dataset\ndimensions to minimize the time required for dataset acquisition\n","authors":["Giorgio Nicola","Enrico Villagrossi","Nicola Pedrocchi"],"pdf_url":"https://arxiv.org/pdf/2301.05609v3.pdf","comment":"Pre-print, submitted to Robotics and Computer Integrated\n  Manufacturing"},{"id":"http://arxiv.org/abs/2303.15343v2","updated":"2023-03-30T17:03:49Z","published":"2023-03-27T15:53:01Z","title":"Sigmoid Loss for Language Image Pre-Training","summary":"  We propose a simple pairwise sigmoid loss for image-text pre-training. Unlike\nstandard contrastive learning with softmax normalization, the sigmoid loss\noperates solely on image-text pairs and does not require a global view of the\npairwise similarities for normalization. The sigmoid loss simultaneously allows\nfurther scaling up the batch size, while also performing better at smaller\nbatch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k\nbatch size and a Large LiT model at 20k batch size, the latter achieves 84.5%\nImageNet zero-shot accuracy in two days. This disentanglement of the batch size\nfrom the loss further allows us to study the impact of examples vs pairs and\nnegative to positive ratio. Finally, we push the batch size to the extreme, up\nto one million, and find that the benefits of growing batch size quickly\ndiminish, with a more reasonable batch size of 32k being sufficient. We hope\nour research motivates further explorations in improving the quality and\nefficiency of language-image pre-training.\n","authors":["Xiaohua Zhai","Basil Mustafa","Alexander Kolesnikov","Lucas Beyer"],"pdf_url":"https://arxiv.org/pdf/2303.15343v2.pdf","comment":"Xiaohua and Lucas contributed equally. arXiv v2: fix typo in\n  pseudocode"},{"id":"http://arxiv.org/abs/2302.14007v2","updated":"2023-03-30T16:55:04Z","published":"2023-02-27T17:56:18Z","title":"Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud\n  Pre-training","summary":"  Masked Autoencoders (MAE) have shown promising performance in self-supervised\nlearning for both 2D and 3D computer vision. However, existing MAE-style\nmethods can only learn from the data of a single modality, i.e., either images\nor point clouds, which neglect the implicit semantic and geometric correlation\nbetween 2D and 3D. In this paper, we explore how the 2D modality can benefit 3D\nmasked autoencoding, and propose Joint-MAE, a 2D-3D joint MAE framework for\nself-supervised 3D point cloud pre-training. Joint-MAE randomly masks an input\n3D point cloud and its projected 2D images, and then reconstructs the masked\ninformation of the two modalities. For better cross-modal interaction, we\nconstruct our JointMAE by two hierarchical 2D-3D embedding modules, a joint\nencoder, and a joint decoder with modal-shared and model-specific decoders. On\ntop of this, we further introduce two cross-modal strategies to boost the 3D\nrepresentation learning, which are local-aligned attention mechanisms for 2D-3D\nsemantic cues, and a cross-reconstruction loss for 2D-3D geometric constraints.\nBy our pre-training paradigm, Joint-MAE achieves superior performance on\nmultiple downstream tasks, e.g., 92.4% accuracy for linear SVM on ModelNet40\nand 86.07% accuracy on the hardest split of ScanObjectNN.\n","authors":["Ziyu Guo","Renrui Zhang","Longtian Qiu","Xianzhi Li","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2302.14007v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17531v1","updated":"2023-03-30T16:53:07Z","published":"2023-03-30T16:53:07Z","title":"Asymmetric Face Recognition with Cross Model Compatible Ensembles","summary":"  The asymmetrical retrieval setting is a well suited solution for resource\nconstrained face recognition. In this setting a large model is used for\nindexing the gallery while a lightweight model is used for querying. The key\nprinciple in such systems is ensuring that both models share the same embedding\nspace. Most methods in this domain are based on knowledge distillation. While\nuseful, they suffer from several drawbacks: they are upper-bounded by the\nperformance of the single best model found and cannot be extended to use an\nensemble of models in a straightforward manner. In this paper we present an\napproach that does not rely on knowledge distillation, rather it utilizes\nembedding transformation models. This allows the use of N independently trained\nand diverse gallery models (e.g., trained on different datasets or having a\ndifferent architecture) and a single query model. As a result, we improve the\noverall accuracy beyond that of any single model while maintaining a low\ncomputational budget for querying. Additionally, we propose a gallery image\nrejection method that utilizes the diversity between multiple transformed\nembeddings to estimate the uncertainty of gallery images.\n","authors":["Ori Linial","Alon Shoshan","Nadav Bhonker","Elad Hirsch","Lior Zamir","Igor Kviatkovsky","Gerard Medioni"],"pdf_url":"https://arxiv.org/pdf/2303.17531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17526v1","updated":"2023-03-30T16:48:28Z","published":"2023-03-30T16:48:28Z","title":"CAusal and collaborative proxy-tasKs lEarning for Semi-Supervised Domain\n  Adaptation","summary":"  Semi-supervised domain adaptation (SSDA) adapts a learner to a new domain by\neffectively utilizing source domain data and a few labeled target samples. It\nis a practical yet under-investigated research topic. In this paper, we analyze\nthe SSDA problem from two perspectives that have previously been overlooked,\nand correspondingly decompose it into two \\emph{key subproblems}: \\emph{robust\ndomain adaptation (DA) learning} and \\emph{maximal cross-domain data\nutilization}. \\textbf{(i)} From a causal theoretical view, a robust DA model\nshould distinguish the invariant ``concept'' (key clue to image label) from the\nnuisance of confounding factors across domains. To achieve this goal, we\npropose to generate \\emph{concept-invariant samples} to enable the model to\nclassify the samples through causal intervention, yielding improved\ngeneralization guarantees; \\textbf{(ii)} Based on the robust DA theory, we aim\nto exploit the maximal utilization of rich source domain data and a few labeled\ntarget samples to boost SSDA further. Consequently, we propose a\ncollaboratively debiasing learning framework that utilizes two complementary\nsemi-supervised learning (SSL) classifiers to mutually exchange their unbiased\nknowledge, which helps unleash the potential of source and target domain\ntraining data, thereby producing more convincing pseudo-labels. Such obtained\nlabels facilitate cross-domain feature alignment and duly improve the invariant\nconcept learning. In our experimental study, we show that the proposed model\nsignificantly outperforms SOTA methods in terms of effectiveness and\ngeneralisability on SSDA datasets.\n","authors":["Wenqiao Zhang","Changshuo Liu","Can Cui","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2303.17526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17517v1","updated":"2023-03-30T16:34:10Z","published":"2023-03-30T16:34:10Z","title":"Hindi as a Second Language: Improving Visually Grounded Speech with\n  Semantically Similar Samples","summary":"  The objective of this work is to explore the learning of visually grounded\nspeech models (VGS) from multilingual perspective. Bilingual VGS models are\ngenerally trained with an equal number of spoken captions from both languages.\nHowever, in reality, there can be an imbalance among the languages for the\navailable spoken captions. Our key contribution in this work is to leverage the\npower of a high-resource language in a bilingual visually grounded speech model\nto improve the performance of a low-resource language. We introduce two methods\nto distill the knowledge of high-resource language into low-resource languages:\n(1) incorporating a strong pre-trained high-resource language encoder and (2)\nusing semantically similar spoken captions. Our experiments show that combining\nthese two approaches effectively enables the low-resource language to surpass\nthe performances of monolingual and bilingual counterparts for cross-modal\nretrieval tasks.\n","authors":["Hyeonggon Ryu","Arda Senocak","In So Kweon","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2303.17517v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.17508v1","updated":"2023-03-30T16:22:10Z","published":"2023-03-30T16:22:10Z","title":"Learning in Factored Domains with Information-Constrained Visual\n  Representations","summary":"  Humans learn quickly even in tasks that contain complex visual information.\nThis is due in part to the efficient formation of compressed representations of\nvisual information, allowing for better generalization and robustness. However,\ncompressed representations alone are insufficient for explaining the high speed\nof human learning. Reinforcement learning (RL) models that seek to replicate\nthis impressive efficiency may do so through the use of factored\nrepresentations of tasks. These informationally simplistic representations of\ntasks are similarly motivated as the use of compressed representations of\nvisual information. Recent studies have connected biological visual perception\nto disentangled and compressed representations. This raises the question of how\nhumans learn to efficiently represent visual information in a manner useful for\nlearning tasks. In this paper we present a model of human factored\nrepresentation learning based on an altered form of a $\\beta$-Variational\nAuto-encoder used in a visual learning task. Modelling results demonstrate a\ntrade-off in the informational complexity of model latent dimension spaces,\nbetween the speed of learning and the accuracy of reconstructions.\n","authors":["Tyler Malloy","Miao Liu","Matthew D. Riemer","Tim Klinger","Gerald Tesauro","Chris R. Sims"],"pdf_url":"https://arxiv.org/pdf/2303.17508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17504v1","updated":"2023-03-30T16:14:48Z","published":"2023-03-30T16:14:48Z","title":"3D Line Mapping Revisited","summary":"  In contrast to sparse keypoints, a handful of line segments can concisely\nencode the high-level scene layout, as they often delineate the main structural\nelements. In addition to offering strong geometric cues, they are also\nomnipresent in urban landscapes and indoor scenes. Despite their apparent\nadvantages, current line-based reconstruction methods are far behind their\npoint-based counterparts. In this paper we aim to close the gap by introducing\nLIMAP, a library for 3D line mapping that robustly and efficiently creates 3D\nline maps from multi-view imagery. This is achieved through revisiting the\ndegeneracy problem of line triangulation, carefully crafted scoring and track\nbuilding, and exploiting structural priors such as line coincidence,\nparallelism, and orthogonality. Our code integrates seamlessly with existing\npoint-based Structure-from-Motion methods and can leverage their 3D points to\nfurther improve the line reconstruction. Furthermore, as a byproduct, the\nmethod is able to recover 3D association graphs between lines and points /\nvanishing points (VPs). In thorough experiments, we show that LIMAP\nsignificantly outperforms existing approaches for 3D line mapping. Our robust\n3D line maps also open up new research directions. We show two example\napplications: visual localization and bundle adjustment, where integrating\nlines alongside points yields the best results. Code is available at\nhttps://github.com/cvg/limap.\n","authors":["Shaohui Liu","Yifan Yu","Rémi Pautrat","Marc Pollefeys","Viktor Larsson"],"pdf_url":"https://arxiv.org/pdf/2303.17504v1.pdf","comment":"27 pages, 18 figures, 18 tables. Highlight in CVPR 2023"},{"id":"http://arxiv.org/abs/2302.08646v3","updated":"2023-03-30T16:10:25Z","published":"2023-02-17T01:31:53Z","title":"AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust\n  Autonomous Driving","summary":"  Object detection with on-board sensors (e.g., lidar, radar, and camera) play\na crucial role in autonomous driving (AD), and these sensors complement each\nother in modalities. While crowdsensing may potentially exploit these sensors\n(of huge quantity) to derive more comprehensive knowledge, \\textit{federated\nlearning} (FL) appears to be the necessary tool to reach this potential: it\nenables autonomous vehicles (AVs) to train machine learning models without\nexplicitly sharing raw sensory data. However, the multimodal sensors introduce\nvarious data heterogeneity across distributed AVs (e.g., label quantity skews\nand varied modalities), posing critical challenges to effective FL. To this\nend, we present AutoFed as a heterogeneity-aware FL framework to fully exploit\nmultimodal sensory data on AVs and thus enable robust AD. Specifically, we\nfirst propose a novel model leveraging pseudo-labeling to avoid mistakenly\ntreating unlabeled objects as the background. We also propose an\nautoencoder-based data imputation method to fill missing data modality (of\ncertain AVs) with the available ones. To further reconcile the heterogeneity,\nwe finally present a client selection mechanism exploiting the similarities\namong client models to improve both training stability and convergence rate.\nOur experiments on benchmark dataset confirm that AutoFed substantially\nimproves over status quo approaches in both precision and recall, while\ndemonstrating strong robustness to adverse weather conditions.\n","authors":["Tianyue Zheng","Ang Li","Zhe Chen","Hongbo Wang","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2302.08646v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17490v1","updated":"2023-03-30T16:01:50Z","published":"2023-03-30T16:01:50Z","title":"Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment","summary":"  How does audio describe the world around us? In this paper, we propose a\nmethod for generating an image of a scene from sound. Our method addresses the\nchallenges of dealing with the large gaps that often exist between sight and\nsound. We design a model that works by scheduling the learning procedure of\neach model component to associate audio-visual modalities despite their\ninformation gaps. The key idea is to enrich the audio features with visual\ninformation by learning to align audio to visual latent space. We translate the\ninput audio to visual features, then use a pre-trained generator to produce an\nimage. To further improve the quality of our generated images, we use sound\nsource localization to select the audio-visual pairs that have strong\ncross-modal correlations. We obtain substantially better results on the VEGAS\nand VGGSound datasets than prior approaches. We also show that we can control\nour model's predictions by applying simple manipulations to the input waveform,\nor to the latent space.\n","authors":["Kim Sung-Bin","Arda Senocak","Hyunwoo Ha","Andrew Owens","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2303.17490v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17472v1","updated":"2023-03-30T15:45:51Z","published":"2023-03-30T15:45:51Z","title":"PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D\n  Human Pose Estimation","summary":"  Recently, transformer-based methods have gained significant success in\nsequential 2D-to-3D lifting human pose estimation. As a pioneering work,\nPoseFormer captures spatial relations of human joints in each video frame and\nhuman dynamics across frames with cascaded transformer layers and has achieved\nimpressive performance. However, in real scenarios, the performance of\nPoseFormer and its follow-ups is limited by two factors: (a) The length of the\ninput joint sequence; (b) The quality of 2D joint detection. Existing methods\ntypically apply self-attention to all frames of the input sequence, causing a\nhuge computational burden when the frame number is increased to obtain advanced\nestimation accuracy, and they are not robust to noise naturally brought by the\nlimited capability of 2D joint detectors. In this paper, we propose\nPoseFormerV2, which exploits a compact representation of lengthy skeleton\nsequences in the frequency domain to efficiently scale up the receptive field\nand boost robustness to noisy 2D joint detection. With minimum modifications to\nPoseFormer, the proposed method effectively fuses features both in the time\ndomain and frequency domain, enjoying a better speed-accuracy trade-off than\nits precursor. Extensive experiments on two benchmark datasets (i.e., Human3.6M\nand MPI-INF-3DHP) demonstrate that the proposed approach significantly\noutperforms the original PoseFormer and other transformer-based variants. Code\nis released at \\url{https://github.com/QitaoZhao/PoseFormerV2}.\n","authors":["Qitao Zhao","Ce Zheng","Mengyuan Liu","Pichao Wang","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.17472v1.pdf","comment":"Accepted to CVPR 2023. Project page:\n  https://qitaozhao.github.io/PoseFormerV2"},{"id":"http://arxiv.org/abs/2303.09483v2","updated":"2023-03-30T15:41:42Z","published":"2023-03-16T17:00:42Z","title":"Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks\n  in Continual Learning","summary":"  In contrast to the natural capabilities of humans to learn new tasks in a\nsequential fashion, neural networks are known to suffer from catastrophic\nforgetting, where the model's performances on old tasks drop dramatically after\nbeing optimized for a new task. Since then, the continual learning (CL)\ncommunity has proposed several solutions aiming to equip the neural network\nwith the ability to learn the current task (plasticity) while still achieving\nhigh accuracy on the previous tasks (stability). Despite remarkable\nimprovements, the plasticity-stability trade-off is still far from being solved\nand its underlying mechanism is poorly understood. In this work, we propose\nAuxiliary Network Continual Learning (ANCL), a novel method that applies an\nadditional auxiliary network which promotes plasticity to the continually\nlearned model which mainly focuses on stability. More concretely, the proposed\nframework materializes in a regularizer that naturally interpolates between\nplasticity and stability, surpassing strong baselines on task incremental and\nclass incremental scenarios. Through extensive analyses on ANCL solutions, we\nidentify some essential principles beneath the stability-plasticity trade-off.\n","authors":["Sanghwan Kim","Lorenzo Noci","Antonio Orvieto","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2303.09483v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2210.06756v2","updated":"2023-03-30T15:27:33Z","published":"2022-10-13T05:49:33Z","title":"Decoding Visual Neural Representations by Multimodal Learning of\n  Brain-Visual-Linguistic Features","summary":"  Decoding human visual neural representations is a challenging task with great\nscientific significance in revealing vision-processing mechanisms and\ndeveloping brain-like intelligent machines. Most existing methods are difficult\nto generalize to novel categories that have no corresponding neural data for\ntraining. The two main reasons are 1) the under-exploitation of the multimodal\nsemantic knowledge underlying the neural data and 2) the small number of paired\n(stimuli-responses) training data. To overcome these limitations, this paper\npresents a generic neural decoding method called BraVL that uses multimodal\nlearning of brain-visual-linguistic features. We focus on modeling the\nrelationships between brain, visual and linguistic features via multimodal deep\ngenerative models. Specifically, we leverage the mixture-of-product-of-experts\nformulation to infer a latent code that enables a coherent joint generation of\nall three modalities. To learn a more consistent joint representation and\nimprove the data efficiency in the case of limited brain activity data, we\nexploit both intra- and inter-modality mutual information maximization\nregularization terms. In particular, our BraVL model can be trained under\nvarious semi-supervised scenarios to incorporate the visual and textual\nfeatures obtained from the extra categories. Finally, we construct three\ntrimodal matching datasets, and the extensive experiments lead to some\ninteresting conclusions and cognitive insights: 1) decoding novel visual\ncategories from human brain activity is practically possible with good\naccuracy; 2) decoding models using the combination of visual and linguistic\nfeatures perform much better than those using either of them alone; 3) visual\nperception may be accompanied by linguistic influences to represent the\nsemantics of visual stimuli. Code and data: https://github.com/ChangdeDu/BraVL.\n","authors":["Changde Du","Kaicheng Fu","Jinpeng Li","Huiguang He"],"pdf_url":"https://arxiv.org/pdf/2210.06756v2.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI)"},{"id":"http://arxiv.org/abs/2303.17448v1","updated":"2023-03-30T15:20:21Z","published":"2023-03-30T15:20:21Z","title":"NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change\n  Detection in Heterogeneous Remote Sensing Images","summary":"  Change detection (CD) in heterogeneous remote sensing images is a practical\nand challenging issue for real-life emergencies. In the past decade, the\nheterogeneous CD problem has significantly benefited from the development of\ndeep neural networks (DNN). However, the data-driven DNNs always perform like a\nblack box where the lack of interpretability limits the trustworthiness and\ncontrollability of DNNs in most practical CD applications. As a strong\nknowledge-driven tool to measure correlation between random variables, Copula\ntheory has been introduced into CD, yet it suffers from non-robust CD\nperformance without manual prior selection for Copula functions. To address the\nabove issues, we propose a knowledge-data-driven heterogeneous CD method\n(NN-Copula-CD) based on the Copula-guided interpretable neural network. In our\nNN-Copula-CD, the mathematical characteristics of Copula are designed as the\nlosses to supervise a simple fully connected neural network to learn the\ncorrelation between bi-temporal image patches, and then the changed regions are\nidentified via binary classification for the correlation coefficients of all\nimage patch pairs of the bi-temporal images. We conduct in-depth experiments on\nthree datasets with multimodal images (e.g., Optical, SAR, and NIR), where the\nquantitative results and visualized analysis demonstrate both the effectiveness\nand interpretability of the proposed NN-Copula-CD.\n","authors":["Weiming Li","Xueqian Wang","Gang Li"],"pdf_url":"https://arxiv.org/pdf/2303.17448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.08884v4","updated":"2023-03-30T15:15:11Z","published":"2022-10-17T09:27:39Z","title":"HyperDomainNet: Universal Domain Adaptation for Generative Adversarial\n  Networks","summary":"  Domain adaptation framework of GANs has achieved great progress in recent\nyears as a main successful approach of training contemporary GANs in the case\nof very limited training data. In this work, we significantly improve this\nframework by proposing an extremely compact parameter space for fine-tuning the\ngenerator. We introduce a novel domain-modulation technique that allows to\noptimize only 6 thousand-dimensional vector instead of 30 million weights of\nStyleGAN2 to adapt to a target domain. We apply this parameterization to the\nstate-of-art domain adaptation methods and show that it has almost the same\nexpressiveness as the full parameter space. Additionally, we propose a new\nregularization loss that considerably enhances the diversity of the fine-tuned\ngenerator. Inspired by the reduction in the size of the optimizing parameter\nspace we consider the problem of multi-domain adaptation of GANs, i.e. setting\nwhen the same model can adapt to several domains depending on the input query.\nWe propose the HyperDomainNet that is a hypernetwork that predicts our\nparameterization given the target domain. We empirically confirm that it can\nsuccessfully learn a number of domains at once and may even generalize to\nunseen domains. Source code can be found at\nhttps://github.com/MACderRu/HyperDomainNet\n","authors":["Aibek Alanov","Vadim Titov","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2210.08884v4.pdf","comment":"Accepted to NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.11162v2","updated":"2023-03-30T15:10:20Z","published":"2023-03-20T14:49:03Z","title":"Picture that Sketch: Photorealistic Image Generation from Abstract\n  Sketches","summary":"  Given an abstract, deformed, ordinary sketch from untrained amateurs like you\nand me, this paper turns it into a photorealistic image - just like those shown\nin Fig. 1(a), all non-cherry-picked. We differ significantly from prior art in\nthat we do not dictate an edgemap-like sketch to start with, but aim to work\nwith abstract free-hand human sketches. In doing so, we essentially democratise\nthe sketch-to-photo pipeline, \"picturing\" a sketch regardless of how good you\nsketch. Our contribution at the outset is a decoupled encoder-decoder training\nparadigm, where the decoder is a StyleGAN trained on photos only. This\nimportantly ensures that generated results are always photorealistic. The rest\nis then all centred around how best to deal with the abstraction gap between\nsketch and photo. For that, we propose an autoregressive sketch mapper trained\non sketch-photo pairs that maps a sketch to the StyleGAN latent space. We\nfurther introduce specific designs to tackle the abstract nature of human\nsketches, including a fine-grained discriminative loss on the back of a trained\nsketch-photo retrieval model, and a partial-aware sketch augmentation strategy.\nFinally, we showcase a few downstream tasks our generation model enables,\namongst them is showing how fine-grained sketch-based image retrieval, a\nwell-studied problem in the sketch community, can be reduced to an image\n(generated) to image retrieval task, surpassing state-of-the-arts. We put\nforward generated results in the supplementary for everyone to scrutinise.\n","authors":["Subhadeep Koley","Ayan Kumar Bhunia","Aneeshan Sain","Pinaki Nath Chowdhury","Tao Xiang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2303.11162v2.pdf","comment":"Accepted in CVPR 2023. Project page available at\n  https://subhadeepkoley.github.io/PictureThatSketch"},{"id":"http://arxiv.org/abs/2303.11502v3","updated":"2023-03-30T15:08:36Z","published":"2023-03-20T23:46:46Z","title":"Sketch2Saliency: Learning to Detect Salient Objects from Human Drawings","summary":"  Human sketch has already proved its worth in various visual understanding\ntasks (e.g., retrieval, segmentation, image-captioning, etc). In this paper, we\nreveal a new trait of sketches - that they are also salient. This is intuitive\nas sketching is a natural attentive process at its core. More specifically, we\naim to study how sketches can be used as a weak label to detect salient objects\npresent in an image. To this end, we propose a novel method that emphasises on\nhow \"salient object\" could be explained by hand-drawn sketches. To accomplish\nthis, we introduce a photo-to-sketch generation model that aims to generate\nsequential sketch coordinates corresponding to a given visual photo through a\n2D attention mechanism. Attention maps accumulated across the time steps give\nrise to salient regions in the process. Extensive quantitative and qualitative\nexperiments prove our hypothesis and delineate how our sketch-based saliency\ndetection model gives a competitive performance compared to the\nstate-of-the-art.\n","authors":["Ayan Kumar Bhunia","Subhadeep Koley","Amandeep Kumar","Aneeshan Sain","Pinaki Nath Chowdhury","Tao Xiang","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2303.11502v3.pdf","comment":"CVPR 2023. Project page available at\n  https://ayankumarbhunia.github.io/Sketch2Saliency/"},{"id":"http://arxiv.org/abs/2302.11510v4","updated":"2023-03-30T15:00:52Z","published":"2023-02-22T17:27:03Z","title":"Selective experience replay compression using coresets for lifelong deep\n  reinforcement learning in medical imaging","summary":"  Selective experience replay is a popular strategy for integrating lifelong\nlearning with deep reinforcement learning. Selective experience replay aims to\nrecount selected experiences from previous tasks to avoid catastrophic\nforgetting. Furthermore, selective experience replay based techniques are model\nagnostic and allow experiences to be shared across different models. However,\nstoring experiences from all previous tasks make lifelong learning using\nselective experience replay computationally very expensive and impractical as\nthe number of tasks increase. To that end, we propose a reward\ndistribution-preserving coreset compression technique for compressing\nexperience replay buffers stored for selective experience replay.\n  We evaluated the coreset compression technique on the brain tumor\nsegmentation (BRATS) dataset for the task of ventricle localization and on the\nwhole-body MRI for localization of left knee cap, left kidney, right\ntrochanter, left lung, and spleen. The coreset lifelong learning models trained\non a sequence of 10 different brain MR imaging environments demonstrated\nexcellent performance localizing the ventricle with a mean pixel error distance\nof 12.93 for the compression ratio of 10x. In comparison, the conventional\nlifelong learning model localized the ventricle with a mean pixel distance of\n10.87. Similarly, the coreset lifelong learning models trained on whole-body\nMRI demonstrated no significant difference (p=0.28) between the 10x compressed\ncoreset lifelong learning models and conventional lifelong learning models for\nall the landmarks. The mean pixel distance for the 10x compressed models across\nall the landmarks was 25.30, compared to 19.24 for the conventional lifelong\nlearning models. Our results demonstrate that the potential of the\ncoreset-based ERB compression method for compressing experiences without a\nsignificant drop in performance.\n","authors":["Guangyao Zheng","Samson Zhou","Vladimir Braverman","Michael A. Jacobs","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2302.11510v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16818v2","updated":"2023-03-30T14:53:14Z","published":"2023-03-29T16:08:59Z","title":"BEVSimDet: Simulated Multi-modal Distillation in Bird's-Eye View for\n  Multi-view 3D Object Detection","summary":"  Multi-view camera-based 3D object detection has gained popularity due to its\nlow cost. But accurately inferring 3D geometry solely from camera data remains\nchallenging, which impacts model performance. One promising approach to address\nthis issue is to distill precise 3D geometry knowledge from LiDAR data.\nHowever, transferring knowledge between different sensor modalities is hindered\nby the significant modality gap. In this paper, we approach this challenge from\nthe perspective of both architecture design and knowledge distillation and\npresent a new simulated multi-modal 3D object detection method named BEVSimDet.\nWe first introduce a novel framework that includes a LiDAR and camera\nfusion-based teacher and a simulated multi-modal student, where the student\nsimulates multi-modal features with image-only input. To facilitate effective\ndistillation, we propose a simulated multi-modal distillation scheme that\nsupports intra-modal, cross-modal, and multi-modal distillation simultaneously.\nBy combining them together, BEVSimDet can learn better feature representations\nfor 3D object detection while enjoying cost-effective camera-only deployment.\nExperimental results on the challenging nuScenes benchmark demonstrate the\neffectiveness and superiority of BEVSimDet over recent representative methods.\nThe source code will be released at\n\\href{https://github.com/ViTAE-Transformer/BEVSimDet}{BEVSimDet}.\n","authors":["Haimei Zhao","Qiming Zhang","Shanshan Zhao","Jing Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.16818v2.pdf","comment":"15 pages; add link"},{"id":"http://arxiv.org/abs/2303.17418v1","updated":"2023-03-30T14:38:53Z","published":"2023-03-30T14:38:53Z","title":"Semantic Image Translation for Repairing the Texture Defects of Building\n  Models","summary":"  The accurate representation of 3D building models in urban environments is\nsignificantly hindered by challenges such as texture occlusion, blurring, and\nmissing details, which are difficult to mitigate through standard\nphotogrammetric texture mapping pipelines. Current image completion methods\noften struggle to produce structured results and effectively handle the\nintricate nature of highly-structured fa\\c{c}ade textures with diverse\narchitectural styles. Furthermore, existing image synthesis methods encounter\ndifficulties in preserving high-frequency details and artificial regular\nstructures, which are essential for achieving realistic fa\\c{c}ade texture\nsynthesis. To address these challenges, we introduce a novel approach for\nsynthesizing fa\\c{c}ade texture images that authentically reflect the\narchitectural style from a structured label map, guided by a ground-truth\nfa\\c{c}ade image. In order to preserve fine details and regular structures, we\npropose a regularity-aware multi-domain method that capitalizes on frequency\ninformation and corner maps. We also incorporate SEAN blocks into our generator\nto enable versatile style transfer. To generate plausible structured images\nwithout undesirable regions, we employ image completion techniques to remove\nocclusions according to semantics prior to image inference. Our proposed method\nis also capable of synthesizing texture images with specific styles for\nfa\\c{c}ades that lack pre-existing textures, using manually annotated labels.\nExperimental results on publicly available fa\\c{c}ade image and 3D model\ndatasets demonstrate that our method yields superior results and effectively\naddresses issues associated with flawed textures. The code and datasets will be\nmade publicly available for further research and development.\n","authors":["Qisen Shang","Han Hu","Haojia Yu","Bo Xu","Qing Zhu","Libin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.17418v1.pdf","comment":"31 pages, 16 figures"},{"id":"http://arxiv.org/abs/2209.09616v6","updated":"2023-03-30T14:29:10Z","published":"2022-09-19T09:16:07Z","title":"Provably Uncertainty-Guided Universal Domain Adaptation","summary":"  Universal domain adaptation (UniDA) aims to transfer the knowledge from a\nlabeled source domain to an unlabeled target domain without any assumptions of\nthe label sets, which requires distinguishing the unknown samples from the\nknown ones in the target domain. A main challenge of UniDA is that the\nnonidentical label sets cause the misalignment between the two domains.\nMoreover, the domain discrepancy and the supervised objectives in the source\ndomain easily lead the whole model to be biased towards the common classes and\nproduce overconfident predictions for unknown samples. To address the above\nchallenging problems, we propose a new uncertainty-guided UniDA framework.\nFirstly, we introduce an empirical estimation of the probability of a target\nsample belonging to the unknown class which fully exploits the distribution of\nthe target samples in the latent space. Then, based on the estimation, we\npropose a novel neighbors searching scheme in a linear subspace with a\n$\\delta$-filter to estimate the uncertainty score of a target sample and\ndiscover unknown samples. It fully utilizes the relationship between a target\nsample and its neighbors in the source domain to avoid the influence of domain\nmisalignment. Secondly, this paper well balances the confidences of predictions\nfor both known and unknown samples through an uncertainty-guided margin loss\nbased on the confidences of discovered unknown samples, which can reduce the\ngap between the intra-class variances of known classes with respect to the\nunknown class. Finally, experiments on three public datasets demonstrate that\nour method significantly outperforms existing state-of-the-art methods.\n","authors":["Yifan Wang","Lin Zhang","Ran Song","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2209.09616v6.pdf","comment":"13 pages. arXiv admin note: text overlap with arXiv:2207.09280"},{"id":"http://arxiv.org/abs/2303.17410v1","updated":"2023-03-30T14:27:42Z","published":"2023-03-30T14:27:42Z","title":"Removing supervision in semantic segmentation with local-global matching\n  and area balancing","summary":"  Removing supervision in semantic segmentation is still tricky. Current\napproaches can deal with common categorical patterns yet resort to multi-stage\narchitectures. We design a novel end-to-end model leveraging local-global patch\nmatching to predict categories, good localization, area and shape of objects\nfor semantic segmentation. The local-global matching is, in turn, compelled by\noptimal transport plans fulfilling area constraints nearing a solution for\nexact shape prediction. Our model attains state-of-the-art in Weakly Supervised\nSemantic Segmentation, only image-level labels, with 75% mIoU on PascalVOC2012\nval set and 46% on MS-COCO2014 val set. Dropping the image-level labels and\nclustering self-supervised learned features to yield pseudo-multi-level labels,\nwe obtain an unsupervised model for semantic segmentation. We also attain\nstate-of-the-art on Unsupervised Semantic Segmentation with 43.6% mIoU on\nPascalVOC2012 val set and 19.4% on MS-COCO2014 val set. The code is available\nat https://github.com/deepplants/PC2M.\n","authors":["Simone Rossetti","Nico Samà","Fiora Pirri"],"pdf_url":"https://arxiv.org/pdf/2303.17410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05171v2","updated":"2023-03-30T14:09:23Z","published":"2022-12-10T01:34:47Z","title":"ULIP: Learning a Unified Representation of Language, Images, and Point\n  Clouds for 3D Understanding","summary":"  The recognition capabilities of current state-of-the-art 3D models are\nlimited by datasets with a small number of annotated data and a pre-defined set\nof categories. In its 2D counterpart, recent advances have shown that similar\nproblems can be significantly alleviated by employing knowledge from other\nmodalities, such as language. Inspired by this, leveraging multimodal\ninformation for 3D modality could be promising to improve 3D understanding\nunder the restricted data regime, but this line of research is not well\nstudied. Therefore, we introduce ULIP to learn a unified representation of\nimage, text, and 3D point cloud by pre-training with object triplets from the\nthree modalities. To overcome the shortage of training triplets, ULIP leverages\na pre-trained vision-language model that has already learned a common visual\nand textual space by training with massive image-text pairs. Then, ULIP learns\na 3D representation space aligned with the common image-text space, using a\nsmall number of automatically synthesized triplets. ULIP is agnostic to 3D\nbackbone networks and can easily be integrated into any 3D architecture.\nExperiments show that ULIP effectively improves the performance of multiple\nrecent 3D backbones by simply pre-training them on ShapeNet55 using our\nframework, achieving state-of-the-art performance in both standard 3D\nclassification and zero-shot 3D classification on ModelNet40 and ScanObjectNN.\nULIP also improves the performance of PointMLP by around 3% in 3D\nclassification on ScanObjectNN, and outperforms PointCLIP by 28.8% on top-1\naccuracy for zero-shot 3D classification on ModelNet40. Our code and\npre-trained models are released at https://github.com/salesforce/ULIP.\n","authors":["Le Xue","Mingfei Gao","Chen Xing","Roberto Martín-Martín","Jiajun Wu","Caiming Xiong","Ran Xu","Juan Carlos Niebles","Silvio Savarese"],"pdf_url":"https://arxiv.org/pdf/2212.05171v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17393v1","updated":"2023-03-30T14:04:39Z","published":"2023-03-30T14:04:39Z","title":"Dynamic Conceptional Contrastive Learning for Generalized Category\n  Discovery","summary":"  Generalized category discovery (GCD) is a recently proposed open-world\nproblem, which aims to automatically cluster partially labeled data. The main\nchallenge is that the unlabeled data contain instances that are not only from\nknown categories of the labeled data but also from novel categories. This leads\ntraditional novel category discovery (NCD) methods to be incapacitated for GCD,\ndue to their assumption of unlabeled data are only from novel categories. One\neffective way for GCD is applying self-supervised learning to learn\ndiscriminate representation for unlabeled data. However, this manner largely\nignores underlying relationships between instances of the same concepts (e.g.,\nclass, super-class, and sub-class), which results in inferior representation\nlearning. In this paper, we propose a Dynamic Conceptional Contrastive Learning\n(DCCL) framework, which can effectively improve clustering accuracy by\nalternately estimating underlying visual conceptions and learning conceptional\nrepresentation. In addition, we design a dynamic conception generation and\nupdate mechanism, which is able to ensure consistent conception learning and\nthus further facilitate the optimization of DCCL. Extensive experiments show\nthat DCCL achieves new state-of-the-art performances on six generic and\nfine-grained visual recognition datasets, especially on fine-grained ones. For\nexample, our method significantly surpasses the best competitor by 16.2% on the\nnew classes for the CUB-200 dataset. Code is available at\nhttps://github.com/TPCD/DCCL.\n","authors":["Nan Pu","Zhun Zhong","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2303.17393v1.pdf","comment":"10 pages, 5 figures, accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.17386v1","updated":"2023-03-30T13:57:21Z","published":"2023-03-30T13:57:21Z","title":"Complementary Random Masking for RGB-Thermal Semantic Segmentation","summary":"  RGB-thermal semantic segmentation is one potential solution to achieve\nreliable semantic scene understanding in adverse weather and lighting\nconditions. However, the previous studies mostly focus on designing a\nmulti-modal fusion module without consideration of the nature of multi-modality\ninputs. Therefore, the networks easily become over-reliant on a single\nmodality, making it difficult to learn complementary and meaningful\nrepresentations for each modality. This paper proposes 1) a complementary\nrandom masking strategy of RGB-T images and 2) self-distillation loss between\nclean and masked input modalities. The proposed masking strategy prevents\nover-reliance on a single modality. It also improves the accuracy and\nrobustness of the neural network by forcing the network to segment and classify\nobjects even when one modality is partially available. Also, the proposed\nself-distillation loss encourages the network to extract complementary and\nmeaningful representations from a single modality or complementary masked\nmodalities. Based on the proposed method, we achieve state-of-the-art\nperformance over three RGB-T semantic segmentation benchmarks. Our source code\nis available at https://github.com/UkcheolShin/CRM_RGBTSeg.\n","authors":["Ukcheol Shin","Kyunghyun Lee","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.17386v1.pdf","comment":"Our source code is available at\n  https://github.com/UkcheolShin/CRM_RGBTSeg"},{"id":"http://arxiv.org/abs/2303.15689v2","updated":"2023-03-30T13:53:11Z","published":"2023-03-28T02:31:57Z","title":"Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and\n  Prototype Alignment","summary":"  The success of existing multi-view clustering relies on the assumption of\nsample integrity across multiple views. However, in real-world scenarios,\nsamples of multi-view are partially available due to data corruption or sensor\nfailure, which leads to incomplete multi-view clustering study (IMVC). Although\nseveral attempts have been proposed to address IMVC, they suffer from the\nfollowing drawbacks: i) Existing methods mainly adopt cross-view contrastive\nlearning forcing the representations of each sample across views to be exactly\nthe same, which might ignore view discrepancy and flexibility in\nrepresentations; ii) Due to the absence of non-observed samples across multiple\nviews, the obtained prototypes of clusters might be unaligned and biased,\nleading to incorrect fusion. To address the above issues, we propose a\nCross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep\nIncomplete Multi-view Clustering. Firstly, unlike existing contrastive-based\nmethods, we adopt pair-observed data alignment as 'proxy supervised signals' to\nguide instance-to-instance correspondence construction among views. Then,\nregarding of the shifted prototypes in IMVC, we further propose a prototype\nalignment module to achieve incomplete distribution calibration across views.\nExtensive experimental results showcase the effectiveness of our proposed\nmodules, attaining noteworthy performance improvements when compared to\nexisting IMVC competitors on benchmark datasets.\n","authors":["Jiaqi Jin","Siwei Wang","Zhibin Dong","Xinwang Liu","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.15689v2.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2023"},{"id":"http://arxiv.org/abs/2205.02830v2","updated":"2023-03-30T13:48:26Z","published":"2022-05-05T17:58:06Z","title":"Interaction Replica: Tracking human-object interaction and scene changes\n  from human motion","summary":"  Humans naturally change their environment through interactions, e.g., by\nopening doors or moving furniture. To reproduce such interactions in virtual\nspaces (e.g., metaverse), we need to capture and model them, including changes\nin the scene geometry, ideally from egocentric input alone (head camera and\nbody-worn inertial sensors). While the head camera can be used to localize the\nperson in the scene, estimating dynamic object pose is much more challenging.\nAs the object is often not visible from the head camera (e.g., a human not\nlooking at a chair while sitting down), we can not rely on visual object pose\nestimation. Instead, our key observation is that human motion tells us a lot\nabout scene changes. Motivated by this, we present iReplica, the first\nhuman-object interaction reasoning method which can track objects and scene\nchanges based solely on human motion. iReplica is an essential first step\ntowards advanced AR/VR applications in immersive virtual universes and can\nprovide human-centric training data to teach machines to interact with their\nsurroundings. Our code, data and model will be available on our project page at\nhttp://virtualhumans.mpi-inf.mpg.de/ireplica/\n","authors":["Vladimir Guzov","Julian Chibane","Riccardo Marin","Torsten Sattler","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2205.02830v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03038v2","updated":"2023-03-30T13:47:25Z","published":"2022-12-06T15:12:53Z","title":"Unifying Short and Long-Term Tracking with Graph Hierarchies","summary":"  Tracking objects over long videos effectively means solving a spectrum of\nproblems, from short-term association for un-occluded objects to long-term\nassociation for objects that are occluded and then reappear in the scene.\nMethods tackling these two tasks are often disjoint and crafted for specific\nscenarios, and top-performing approaches are often a mix of techniques, which\nyields engineering-heavy solutions that lack generality. In this work, we\nquestion the need for hybrid approaches and introduce SUSHI, a unified and\nscalable multi-object tracker. Our approach processes long clips by splitting\nthem into a hierarchy of subclips, which enables high scalability. We leverage\ngraph neural networks to process all levels of the hierarchy, which makes our\nmodel unified across temporal scales and highly general. As a result, we obtain\nsignificant improvements over state-of-the-art on four diverse datasets. Our\ncode and models are available at bit.ly/sushi-mot.\n","authors":["Orcun Cetintas","Guillem Brasó","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2212.03038v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17376v1","updated":"2023-03-30T13:42:58Z","published":"2023-03-30T13:42:58Z","title":"A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision","summary":"  There has been a recent explosion of computer vision models which perform\nmany tasks and are composed of an image encoder (usually a ViT) and an\nautoregressive decoder (usually a Transformer). However, most of this work\nsimply presents one system and its results, leaving many questions regarding\ndesign decisions and trade-offs of such systems unanswered. In this work, we\naim to provide such answers. We take a close look at autoregressive decoders\nfor multi-task learning in multimodal computer vision, including\nclassification, captioning, visual question answering, and optical character\nrecognition. Through extensive systematic experiments, we study the effects of\ntask and data mixture, training and regularization hyperparameters,\nconditioning type and specificity, modality combination, and more. Importantly,\nwe compare these to well-tuned single-task baselines to highlight the cost\nincurred by multi-tasking. A key finding is that a small decoder learned on top\nof a frozen pretrained encoder works surprisingly well. We call this setup\nlocked-image tuning with decoder (LiT-decoder). It can be seen as teaching a\ndecoder to interact with a pretrained vision model via natural language.\n","authors":["Lucas Beyer","Bo Wan","Gagan Madan","Filip Pavetic","Andreas Steiner","Alexander Kolesnikov","André Susano Pinto","Emanuele Bugliarello","Xiao Wang","Qihang Yu","Liang-Chieh Chen","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.17376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.08739v2","updated":"2023-03-30T13:40:17Z","published":"2023-01-20T18:59:57Z","title":"FlatFormer: Flattened Window Attention for Efficient Point Cloud\n  Transformer","summary":"  Transformer, as an alternative to CNN, has been proven effective in many\nmodalities (e.g., texts and images). For 3D point cloud transformers, existing\nefforts focus primarily on pushing their accuracy to the state-of-the-art\nlevel. However, their latency lags behind sparse convolution-based models (3x\nslower), hindering their usage in resource-constrained, latency-sensitive\napplications (such as autonomous driving). This inefficiency comes from point\nclouds' sparse and irregular nature, whereas transformers are designed for\ndense, regular workloads. This paper presents FlatFormer to close this latency\ngap by trading spatial proximity for better computational regularity. We first\nflatten the point cloud with window-based sorting and partition points into\ngroups of equal sizes rather than windows of equal shapes. This effectively\navoids expensive structuring and padding overheads. We then apply\nself-attention within groups to extract local features, alternate sorting axis\nto gather features from different directions, and shift windows to exchange\nfeatures across groups. FlatFormer delivers state-of-the-art accuracy on Waymo\nOpen Dataset with 4.6x speedup over (transformer-based) SST and 1.4x speedup\nover (sparse convolutional) CenterPoint. This is the first point cloud\ntransformer that achieves real-time performance on edge GPUs and is faster than\nsparse convolutional methods while achieving on-par or even superior accuracy\non large-scale benchmarks.\n","authors":["Zhijian Liu","Xinyu Yang","Haotian Tang","Shang Yang","Song Han"],"pdf_url":"https://arxiv.org/pdf/2301.08739v2.pdf","comment":"CVPR 2023. The first two authors contributed equally to this work.\n  Project page: https://flatformer.mit.edu"},{"id":"http://arxiv.org/abs/2302.10764v3","updated":"2023-03-30T13:30:57Z","published":"2023-02-14T13:41:57Z","title":"On The Coherence of Quantitative Evaluation of Visual Explanations","summary":"  Recent years have shown an increased development of methods for justifying\nthe predictions of neural networks through visual explanations. These\nexplanations usually take the form of heatmaps which assign a saliency (or\nrelevance) value to each pixel of the input image that expresses how relevant\nthe pixel is for the prediction of a label.\n  Complementing this development, evaluation methods have been proposed to\nassess the \"goodness\" of such explanations. On the one hand, some of these\nmethods rely on synthetic datasets. However, this introduces the weakness of\nhaving limited guarantees regarding their applicability on more realistic\nsettings. On the other hand, some methods rely on metrics for objective\nevaluation. However the level to which some of these evaluation methods perform\nwith respect to each other is uncertain.\n  Taking this into account, we conduct a comprehensive study on a subset of the\nImageNet-1k validation set where we evaluate a number of different\ncommonly-used explanation methods following a set of evaluation methods. We\ncomplement our study with sanity checks on the studied evaluation methods as a\nmeans to investigate their reliability and the impact of characteristics of the\nexplanations on the evaluation methods.\n  Results of our study suggest that there is a lack of coherency on the grading\nprovided by some of the considered evaluation methods. Moreover, we have\nidentified some characteristics of the explanations, e.g. sparsity, which can\nhave a significant effect on the performance.\n","authors":["Benjamin Vandersmissen","Jose Oramas"],"pdf_url":"https://arxiv.org/pdf/2302.10764v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17368v1","updated":"2023-03-30T13:30:12Z","published":"2023-03-30T13:30:12Z","title":"SynBody: Synthetic Dataset with Layered Human Models for 3D Human\n  Perception and Modeling","summary":"  Synthetic data has emerged as a promising source for 3D human research as it\noffers low-cost access to large-scale human datasets. To advance the diversity\nand annotation quality of human models, we introduce a new synthetic dataset,\nSynbody, with three appealing features: 1) a clothed parametric human model\nthat can generate a diverse range of subjects; 2) the layered human\nrepresentation that naturally offers high-quality 3D annotations to support\nmultiple tasks; 3) a scalable system for producing realistic data to facilitate\nreal-world tasks. The dataset comprises 1.7M images with corresponding accurate\n3D annotations, covering 10,000 human body models, 1000 actions, and various\nviewpoints. The dataset includes two subsets for human mesh recovery as well as\nhuman neural rendering. Extensive experiments on SynBody indicate that it\nsubstantially enhances both SMPL and SMPL-X estimation. Furthermore, the\nincorporation of layered annotations offers a valuable training resource for\ninvestigating the Human Neural Radiance Fields (NeRF).\n","authors":["Zhitao Yang","Zhongang Cai","Haiyi Mei","Shuai Liu","Zhaoxi Chen","Weiye Xiao","Yukun Wei","Zhongfei Qing","Chen Wei","Bo Dai","Wayne Wu","Chen Qian","Dahua Lin","Ziwei Liu","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2303.17368v1.pdf","comment":"Project webpage: https://maoxie.github.io/SynBody/"},{"id":"http://arxiv.org/abs/2303.17361v1","updated":"2023-03-30T13:19:07Z","published":"2023-03-30T13:19:07Z","title":"Invertible Convolution with Symmetric Paddings","summary":"  We show that symmetrically padded convolution can be analytically inverted\nvia DFT. We comprehensively analyze several different symmetric and\nanti-symmetric padding modes and show that multiple cases exist where the\ninversion can be achieved. The implementation is available at\n\\url{https://github.com/prclibo/iconv_dft}.\n","authors":["Bo Li"],"pdf_url":"https://arxiv.org/pdf/2303.17361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17354v1","updated":"2023-03-30T13:11:26Z","published":"2023-03-30T13:11:26Z","title":"Incremental Self-Supervised Learning Based on Transformer for Anomaly\n  Detection and Localization","summary":"  In the machine learning domain, research on anomaly detection and\nlocalization within image data has garnered significant attention, particularly\nin practical applications such as industrial defect detection. While existing\napproaches predominantly rely on Convolutional Neural Networks (CNN) as their\nbackbone network, we propose an innovative method based on the Transformer\nbackbone network. Our approach employs a two-stage incremental learning\nstrategy. In the first stage, we train a Masked Autoencoder (MAE) model\nexclusively on normal images. Subsequently, in the second stage, we implement\npixel-level data augmentation techniques to generate corrupted normal images\nand their corresponding pixel labels. This process enables the model to learn\nhow to repair corrupted regions and classify the state of each pixel.\nUltimately, the model produces a pixel reconstruction error matrix and a pixel\nanomaly probability matrix, which are combined to create an anomaly scoring\nmatrix that effectively identifies abnormal regions. When compared to several\nstate-of-the-art CNN-based techniques, our method demonstrates superior\nperformance on the MVTec AD dataset, achieving an impressive 97.6% AUC.\n","authors":["Wenping Jin","Fei Guo","Li Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.17354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.00728v2","updated":"2023-03-30T13:09:34Z","published":"2022-07-02T03:47:13Z","title":"Multi-scale Attentive Image De-raining Networks via Neural Architecture\n  Search","summary":"  Multi-scale architectures and attention modules have shown effectiveness in\nmany deep learning-based image de-raining methods. However, manually designing\nand integrating these two components into a neural network requires a bulk of\nlabor and extensive expertise. In this article, a high-performance multi-scale\nattentive neural architecture search (MANAS) framework is technically developed\nfor image deraining. The proposed method formulates a new multi-scale attention\nsearch space with multiple flexible modules that are favorite to the image\nde-raining task. Under the search space, multi-scale attentive cells are built,\nwhich are further used to construct a powerful image de-raining network. The\ninternal multiscale attentive architecture of the de-raining network is\nsearched automatically through a gradient-based search algorithm, which avoids\nthe daunting procedure of the manual design to some extent. Moreover, in order\nto obtain a robust image de-raining model, a practical and effective\nmulti-to-one training strategy is also presented to allow the de-raining\nnetwork to get sufficient background information from multiple rainy images\nwith the same background scene, and meanwhile, multiple loss functions\nincluding external loss, internal loss, architecture regularization loss, and\nmodel complexity loss are jointly optimized to achieve robust de-raining\nperformance and controllable model complexity. Extensive experimental results\non both synthetic and realistic rainy images, as well as the down-stream vision\napplications (i.e., objection detection and segmentation) consistently\ndemonstrate the superiority of our proposed method. The code is publicly\navailable at https://github.com/lcai-gz/MANAS.\n","authors":["Lei Cai","Yuli Fu","Wanliang Huo","Youjun Xiang","Tao Zhu","Ying Zhang","Huanqiang Zeng","Delu Zeng"],"pdf_url":"https://arxiv.org/pdf/2207.00728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17342v1","updated":"2023-03-30T12:53:22Z","published":"2023-03-30T12:53:22Z","title":"PMatch: Paired Masked Image Modeling for Dense Geometric Matching","summary":"  Dense geometric matching determines the dense pixel-wise correspondence\nbetween a source and support image corresponding to the same 3D structure.\nPrior works employ an encoder of transformer blocks to correlate the two-frame\nfeatures. However, existing monocular pretraining tasks, e.g., image\nclassification, and masked image modeling (MIM), can not pretrain the\ncross-frame module, yielding less optimal performance. To resolve this, we\nreformulate the MIM from reconstructing a single masked image to reconstructing\na pair of masked images, enabling the pretraining of transformer module.\nAdditionally, we incorporate a decoder into pretraining for improved upsampling\nresults. Further, to be robust to the textureless area, we propose a novel\ncross-frame global matching module (CFGM). Since the most textureless area is\nplanar surfaces, we propose a homography loss to further regularize its\nlearning. Combined together, we achieve the State-of-The-Art (SoTA) performance\non geometric matching. Codes and models are available at\nhttps://github.com/ShngJZ/PMatch.\n","authors":["Shengjie Zhu","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2303.17342v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2303.17338v1","updated":"2023-03-30T12:45:46Z","published":"2023-03-30T12:45:46Z","title":"Local region-learning modules for point cloud classification","summary":"  Data organization via forming local regions is an integral part of deep\nlearning networks that process 3D point clouds in a hierarchical manner. At\neach level, the point cloud is sampled to extract representative points and\nthese points are used to be centers of local regions. The organization of local\nregions is of considerable importance since it determines the location and size\nof the receptive field at a particular layer of feature aggregation. In this\npaper, we present two local region-learning modules: Center Shift Module to\ninfer the appropriate shift for each center point, and Radius Update Module to\nalter the radius of each local region. The parameters of the modules are\nlearned through optimizing the loss associated with the particular task within\nan end-to-end network. We present alternatives for these modules through\nvarious ways of modeling the interactions of the features and locations of 3D\npoints in the point cloud. We integrated both modules independently and\ntogether to the PointNet++ object classification architecture, and demonstrated\nthat the modules contributed to a significant increase in classification\naccuracy for the ScanObjectNN data set.\n","authors":["Kaya Turgut","Helin Dutagaci"],"pdf_url":"https://arxiv.org/pdf/2303.17338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14080v3","updated":"2023-03-30T12:40:35Z","published":"2023-03-24T15:44:42Z","title":"Best of Both Worlds: Multimodal Contrastive Learning with Tabular and\n  Imaging Data","summary":"  Medical datasets and especially biobanks, often contain extensive tabular\ndata with rich clinical information in addition to images. In practice,\nclinicians typically have less data, both in terms of diversity and scale, but\nstill wish to deploy deep learning solutions. Combined with increasing medical\ndataset sizes and expensive annotation costs, the necessity for unsupervised\nmethods that can pretrain multimodally and predict unimodally has risen.\n  To address these needs, we propose the first self-supervised contrastive\nlearning framework that takes advantage of images and tabular data to train\nunimodal encoders. Our solution combines SimCLR and SCARF, two leading\ncontrastive learning strategies, and is simple and effective. In our\nexperiments, we demonstrate the strength of our framework by predicting risks\nof myocardial infarction and coronary artery disease (CAD) using cardiac MR\nimages and 120 clinical features from 40,000 UK Biobank subjects. Furthermore,\nwe show the generalizability of our approach to natural images using the DVM\ncar advertisement dataset.\n  We take advantage of the high interpretability of tabular data and through\nattribution and ablation experiments find that morphometric tabular features,\ndescribing size and shape, have outsized importance during the contrastive\nlearning process and improve the quality of the learned embeddings. Finally, we\nintroduce a novel form of supervised contrastive learning, label as a feature\n(LaaF), by appending the ground truth label as a tabular feature during\nmultimodal pretraining, outperforming all supervised contrastive baselines.\n","authors":["Paul Hager","Martin J. Menten","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2303.14080v3.pdf","comment":"Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2212.09281v2","updated":"2023-03-30T12:32:37Z","published":"2022-12-19T07:39:26Z","title":"Boosting Automatic COVID-19 Detection Performance with Self-Supervised\n  Learning and Batch Knowledge Ensembling","summary":"  Problem: Detecting COVID-19 from chest X-Ray (CXR) images has become one of\nthe fastest and easiest methods for detecting COVID-19. However, the existing\nmethods usually use supervised transfer learning from natural images as a\npretraining process. These methods do not consider the unique features of\nCOVID-19 and the similar features between COVID-19 and other pneumonia. Aim: In\nthis paper, we want to design a novel high-accuracy COVID-19 detection method\nthat uses CXR images, which can consider the unique features of COVID-19 and\nthe similar features between COVID-19 and other pneumonia. Methods: Our method\nconsists of two phases. One is self-supervised learning-based pertaining; the\nother is batch knowledge ensembling-based fine-tuning. Self-supervised\nlearning-based pretraining can learn distinguished representations from CXR\nimages without manually annotated labels. On the other hand, batch knowledge\nensembling-based fine-tuning can utilize category knowledge of images in a\nbatch according to their visual feature similarities to improve detection\nperformance. Unlike our previous implementation, we introduce batch knowledge\nensembling into the fine-tuning phase, reducing the memory used in\nself-supervised learning and improving COVID-19 detection accuracy. Results: On\ntwo public COVID-19 CXR datasets, namely, a large dataset and an unbalanced\ndataset, our method exhibited promising COVID-19 detection performance. Our\nmethod maintains high detection accuracy even when annotated CXR training\nimages are reduced significantly (e.g., using only 10% of the original\ndataset). In addition, our method is insensitive to changes in hyperparameters.\n","authors":["Guang Li","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2212.09281v2.pdf","comment":"Published as a journal paper at Elsevier CIBM"},{"id":"http://arxiv.org/abs/2303.17318v1","updated":"2023-03-30T12:14:07Z","published":"2023-03-30T12:14:07Z","title":"The impact of training dataset size and ensemble inference strategies on\n  head and neck auto-segmentation","summary":"  Convolutional neural networks (CNNs) are increasingly being used to automate\nsegmentation of organs-at-risk in radiotherapy. Since large sets of highly\ncurated data are scarce, we investigated how much data is required to train\naccurate and robust head and neck auto-segmentation models. For this, an\nestablished 3D CNN was trained from scratch with different sized datasets\n(25-1000 scans) to segment the brainstem, parotid glands and spinal cord in\nCTs. Additionally, we evaluated multiple ensemble techniques to improve the\nperformance of these models. The segmentations improved with training set size\nup to 250 scans and the ensemble methods significantly improved performance for\nall organs. The impact of the ensemble methods was most notable in the smallest\ndatasets, demonstrating their potential for use in cases where large training\ndatasets are difficult to obtain.\n","authors":["Edward G. A. Henderson","Marcel van Herk","Eliana M. Vasquez Osorio"],"pdf_url":"https://arxiv.org/pdf/2303.17318v1.pdf","comment":"Accepted in 20th IEEE International Symposium on Biomedical Imaging\n  (ISBI 2023)"},{"id":"http://arxiv.org/abs/2303.17316v1","updated":"2023-03-30T12:09:35Z","published":"2023-03-30T12:09:35Z","title":"Masked Autoencoders as Image Processors","summary":"  Transformers have shown significant effectiveness for various vision tasks\nincluding both high-level vision and low-level vision. Recently, masked\nautoencoders (MAE) for feature pre-training have further unleashed the\npotential of Transformers, leading to state-of-the-art performances on various\nhigh-level vision tasks. However, the significance of MAE pre-training on\nlow-level vision tasks has not been sufficiently explored. In this paper, we\nshow that masked autoencoders are also scalable self-supervised learners for\nimage processing tasks. We first present an efficient Transformer model\nconsidering both channel attention and shifted-window-based self-attention\ntermed CSformer. Then we develop an effective MAE architecture for image\nprocessing (MAEIP) tasks. Extensive experimental results show that with the\nhelp of MAEIP pre-training, our proposed CSformer achieves state-of-the-art\nperformance on various image processing tasks, including Gaussian denoising,\nreal image denoising, single-image motion deblurring, defocus deblurring, and\nimage deraining.\n","authors":["Huiyu Duan","Wei Shen","Xiongkuo Min","Danyang Tu","Long Teng","Jia Wang","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.17316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08685v2","updated":"2023-03-30T11:56:29Z","published":"2023-03-15T15:12:36Z","title":"Making Vision Transformers Efficient from A Token Sparsification View","summary":"  The quadratic computational complexity to the number of tokens limits the\npractical applications of Vision Transformers (ViTs). Several works propose to\nprune redundant tokens to achieve efficient ViTs. However, these methods\ngenerally suffer from (i) dramatic accuracy drops, (ii) application difficulty\nin the local vision transformer, and (iii) non-general-purpose networks for\ndownstream tasks. In this work, we propose a novel Semantic Token ViT (STViT),\nfor efficient global and local vision transformers, which can also be revised\nto serve as backbone for downstream tasks. The semantic tokens represent\ncluster centers, and they are initialized by pooling image tokens in space and\nrecovered by attention, which can adaptively represent global or local semantic\ninformation. Due to the cluster properties, a few semantic tokens can attain\nthe same effect as vast image tokens, for both global and local vision\ntransformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base)\ncan achieve the same accuracy with more than 100% inference speed improvement\nand nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16\nsemantic tokens in each window to further speed it up by around 20% with slight\naccuracy increase. Besides great success in image classification, we also\nextend our method to video recognition. In addition, we design a\nSTViT-R(ecover) network to restore the detailed spatial information based on\nthe STViT, making it work for downstream tasks, which is powerless for previous\ntoken sparsification methods. Experiments demonstrate that our method can\nachieve competitive results compared to the original networks in object\ndetection and instance segmentation, with over 30% FLOPs reduction for\nbackbone. Code is available at http://github.com/changsn/STViT-R\n","authors":["Shuning Chang","Pichao Wang","Ming Lin","Fan Wang","David Junhao Zhang","Rong Jin","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2303.08685v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.16765v2","updated":"2023-03-30T11:42:41Z","published":"2023-03-29T14:57:54Z","title":"MDP: A Generalized Framework for Text-Guided Image Editing by\n  Manipulating the Diffusion Path","summary":"  Image generation using diffusion can be controlled in multiple ways. In this\npaper, we systematically analyze the equations of modern generative diffusion\nnetworks to propose a framework, called MDP, that explains the design space of\nsuitable manipulations. We identify 5 different manipulations, including\nintermediate latent, conditional embedding, cross attention maps, guidance, and\npredicted noise. We analyze the corresponding parameters of these manipulations\nand the manipulation schedule. We show that some previous editing methods fit\nnicely into our framework. Particularly, we identified one specific\nconfiguration as a new type of control by manipulating the predicted noise,\nwhich can perform higher-quality edits than previous work for a variety of\nlocal and global edits.\n","authors":["Qian Wang","Biao Zhang","Michael Birsak","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2303.16765v2.pdf","comment":"Project page: https://github.com/QianWangX/MDP-Diffusion"},{"id":"http://arxiv.org/abs/2212.06458v2","updated":"2023-03-30T11:38:34Z","published":"2022-12-13T10:04:01Z","title":"HS-Diffusion: Learning a Semantic-Mixing Diffusion Model for Head\n  Swapping","summary":"  Image-based head swapping task aims to stitch a source head to another source\nbody flawlessly. This seldom-studied task faces two major challenges: 1)\nPreserving the head and body from various sources while generating a seamless\ntransition region. 2) No paired head swapping dataset and benchmark so far. In\nthis paper, we propose a semantic-mixing diffusion model for head swapping\n(HS-Diffusion) which consists of a latent diffusion model (LDM) and a semantic\nlayout generator. We blend the semantic layouts of source head and source body,\nand then inpaint the transition region by the semantic layout generator,\nachieving a coarse-grained head swapping. Semantic-mixing LDM can further\nimplement a fine-grained head swapping with the inpainted layout as condition\nby a progressive fusion process, while preserving head and body with\nhigh-quality reconstruction. To this end, we propose a semantic calibration\nstrategy for natural inpainting and a neck alignment for geometric realism.\nImportantly, we construct a new image-based head swapping benchmark and design\ntwo tailor-designed metrics (Mask-FID and Focal-FID). Extensive experiments\ndemonstrate the superiority of our framework. The code will be available:\nhttps://github.com/qinghew/HS-Diffusion.\n","authors":["Qinghe Wang","Lijie Liu","Miao Hua","Qian He","Pengfei Zhu","Bing Cao","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2212.06458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11052v2","updated":"2023-03-30T11:28:41Z","published":"2023-03-20T12:06:14Z","title":"ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real\n  Novel View Synthesis via Contrastive Learning","summary":"  Although many recent works have investigated generalizable NeRF-based novel\nview synthesis for unseen scenes, they seldom consider the synthetic-to-real\ngeneralization, which is desired in many practical applications. In this work,\nwe first investigate the effects of synthetic data in synthetic-to-real novel\nview synthesis and surprisingly observe that models trained with synthetic data\ntend to produce sharper but less accurate volume densities. For pixels where\nthe volume densities are correct, fine-grained details will be obtained.\nOtherwise, severe artifacts will be produced. To maintain the advantages of\nusing synthetic data while avoiding its negative effects, we propose to\nintroduce geometry-aware contrastive learning to learn multi-view consistent\nfeatures with geometric constraints. Meanwhile, we adopt cross-view attention\nto further enhance the geometry perception of features by querying features\nacross input views. Experiments demonstrate that under the synthetic-to-real\nsetting, our method can render images with higher quality and better\nfine-grained details, outperforming existing generalizable novel view synthesis\nmethods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our\nmethod also achieves state-of-the-art results.\n","authors":["Hao Yang","Lanqing Hong","Aoxue Li","Tianyang Hu","Zhenguo Li","Gim Hee Lee","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2303.11052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.13824v2","updated":"2023-03-30T11:26:56Z","published":"2022-12-28T13:56:54Z","title":"Multi-Realism Image Compression with a Conditional Generator","summary":"  By optimizing the rate-distortion-realism trade-off, generative compression\napproaches produce detailed, realistic images, even at low bit rates, instead\nof the blurry reconstructions produced by rate-distortion optimized models.\nHowever, previous methods do not explicitly control how much detail is\nsynthesized, which results in a common criticism of these methods: users might\nbe worried that a misleading reconstruction far from the input image is\ngenerated. In this work, we alleviate these concerns by training a decoder that\ncan bridge the two regimes and navigate the distortion-realism trade-off. From\na single compressed representation, the receiver can decide to either\nreconstruct a low mean squared error reconstruction that is close to the input,\na realistic reconstruction with high perceptual quality, or anything in\nbetween. With our method, we set a new state-of-the-art in distortion-realism,\npushing the frontier of achievable distortion-realism pairs, i.e., our method\nachieves better distortions at high realism and better realism at low\ndistortion than ever before.\n","authors":["Eirikur Agustsson","David Minnen","George Toderici","Fabian Mentzer"],"pdf_url":"https://arxiv.org/pdf/2212.13824v2.pdf","comment":"CVPR'23 Camera Ready"},{"id":"http://arxiv.org/abs/2303.16001v2","updated":"2023-03-30T11:20:13Z","published":"2023-03-28T14:16:08Z","title":"Adaptive Voronoi NeRFs","summary":"  Neural Radiance Fields (NeRFs) learn to represent a 3D scene from just a set\nof registered images. Increasing sizes of a scene demands more complex\nfunctions, typically represented by neural networks, to capture all details.\nTraining and inference then involves querying the neural network millions of\ntimes per image, which becomes impractically slow. Since such complex functions\ncan be replaced by multiple simpler functions to improve speed, we show that a\nhierarchy of Voronoi diagrams is a suitable choice to partition the scene. By\nequipping each Voronoi cell with its own NeRF, our approach is able to quickly\nlearn a scene representation. We propose an intuitive partitioning of the space\nthat increases quality gains during training by distributing information evenly\namong the networks and avoids artifacts through a top-down adaptive refinement.\nOur framework is agnostic to the underlying NeRF method and easy to implement,\nwhich allows it to be applied to various NeRF variants for improved learning\nand rendering speeds.\n","authors":["Tim Elsner","Victor Czech","Julia Berger","Zain Selman","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2303.16001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17297v1","updated":"2023-03-30T11:16:58Z","published":"2023-03-30T11:16:58Z","title":"Understanding the Robustness of 3D Object Detection with Bird's-Eye-View\n  Representations in Autonomous Driving","summary":"  3D object detection is an essential perception task in autonomous driving to\nunderstand the environments. The Bird's-Eye-View (BEV) representations have\nsignificantly improved the performance of 3D detectors with camera inputs on\npopular benchmarks. However, there still lacks a systematic understanding of\nthe robustness of these vision-dependent BEV models, which is closely related\nto the safety of autonomous driving systems. In this paper, we evaluate the\nnatural and adversarial robustness of various representative models under\nextensive settings, to fully understand their behaviors influenced by explicit\nBEV features compared with those without BEV. In addition to the classic\nsettings, we propose a 3D consistent patch attack by applying adversarial\npatches in the 3D space to guarantee the spatiotemporal consistency, which is\nmore realistic for the scenario of autonomous driving. With substantial\nexperiments, we draw several findings: 1) BEV models tend to be more stable\nthan previous methods under different natural conditions and common corruptions\ndue to the expressive spatial representations; 2) BEV models are more\nvulnerable to adversarial noises, mainly caused by the redundant BEV features;\n3) Camera-LiDAR fusion models have superior performance under different\nsettings with multi-modal inputs, but BEV fusion model is still vulnerable to\nadversarial noises of both point cloud and image. These findings alert the\nsafety issue in the applications of BEV detectors and could facilitate the\ndevelopment of more robust models.\n","authors":["Zijian Zhu","Yichi Zhang","Hai Chen","Yinpeng Dong","Shu Zhao","Wenbo Ding","Jiachen Zhong","Shibao Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.17297v1.pdf","comment":"8 pages, CVPR2023"},{"id":"http://arxiv.org/abs/2303.17294v1","updated":"2023-03-30T11:09:02Z","published":"2023-03-30T11:09:02Z","title":"JCDNet: Joint of Common and Definite phases Network for Weakly\n  Supervised Temporal Action Localization","summary":"  Weakly-supervised temporal action localization aims to localize action\ninstances in untrimmed videos with only video-level supervision. We witness\nthat different actions record common phases, e.g., the run-up in the HighJump\nand LongJump. These different actions are defined as conjoint actions, whose\nrest parts are definite phases, e.g., leaping over the bar in a HighJump.\nCompared with the common phases, the definite phases are more easily localized\nin existing researches. Most of them formulate this task as a Multiple Instance\nLearning paradigm, in which the common phases are tended to be confused with\nthe background, and affect the localization completeness of the conjoint\nactions. To tackle this challenge, we propose a Joint of Common and Definite\nphases Network (JCDNet) by improving feature discriminability of the conjoint\nactions. Specifically, we design a Class-Aware Discriminative module to enhance\nthe contribution of the common phases in classification by the guidance of the\ncoarse definite-phase features. Besides, we introduce a temporal attention\nmodule to learn robust action-ness scores via modeling temporal dependencies,\ndistinguishing the common phases from the background. Extensive experiments on\nthree datasets (THUMOS14, ActivityNetv1.2, and a conjoint-action subset)\ndemonstrate that JCDNet achieves competitive performance against the\nstate-of-the-art methods. Keywords: weakly-supervised learning, temporal action\nlocalization, conjoint action\n","authors":["Yifu Liu","Xiaoxia Li","Zhiling Luo","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.17294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00875v2","updated":"2023-03-30T10:52:55Z","published":"2022-09-27T12:56:56Z","title":"Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset\n  Copyright Protection","summary":"  Deep neural networks (DNNs) have demonstrated their superiority in practice.\nArguably, the rapid development of DNNs is largely benefited from high-quality\n(open-sourced) datasets, based on which researchers and developers can easily\nevaluate and improve their learning methods. Since the data collection is\nusually time-consuming or even expensive, how to protect their copyrights is of\ngreat significance and worth further exploration. In this paper, we revisit\ndataset ownership verification. We find that existing verification methods\nintroduced new security risks in DNNs trained on the protected dataset, due to\nthe targeted nature of poison-only backdoor watermarks. To alleviate this\nproblem, in this work, we explore the untargeted backdoor watermarking scheme,\nwhere the abnormal model behaviors are not deterministic. Specifically, we\nintroduce two dispersibilities and prove their correlation, based on which we\ndesign the untargeted backdoor watermark under both poisoned-label and\nclean-label settings. We also discuss how to use the proposed untargeted\nbackdoor watermark for dataset ownership verification. Experiments on benchmark\ndatasets verify the effectiveness of our methods and their resistance to\nexisting backdoor defenses. Our codes are available at\n\\url{https://github.com/THUYimingLi/Untargeted_Backdoor_Watermark}.\n","authors":["Yiming Li","Yang Bai","Yong Jiang","Yong Yang","Shu-Tao Xia","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2210.00875v2.pdf","comment":"This work is accepted by the NeurIPS 2022 (selected as Oral paper,\n  TOP 2%). The first two authors contributed equally to this work. 25 pages. We\n  have fixed some typos in the previous version"},{"id":"http://arxiv.org/abs/2003.06777v5","updated":"2023-03-30T10:48:54Z","published":"2020-03-15T08:13:16Z","title":"DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning","summary":"  In this work, we develop methods for few-shot image classification from a new\nperspective of optimal matching between image regions. We employ the Earth\nMover's Distance (EMD) as a metric to compute a structural distance between\ndense image representations to determine image relevance. The EMD generates the\noptimal matching flows between structural elements that have the minimum\nmatching cost, which is used to calculate the image distance for\nclassification. To generate the important weights of elements in the EMD\nformulation, we design a cross-reference mechanism, which can effectively\nalleviate the adverse impact caused by the cluttered background and large\nintra-class appearance variations. To implement k-shot classification, we\npropose to learn a structured fully connected layer that can directly classify\ndense image representations with the EMD. Based on the implicit function\ntheorem, the EMD can be inserted as a layer into the network for end-to-end\ntraining. Our extensive experiments validate the effectiveness of our algorithm\nwhich outperforms state-of-the-art methods by a significant margin on five\nwidely used few-shot classification benchmarks, namely, miniImageNet,\ntieredImageNet, Fewshot-CIFAR100 (FC100), Caltech-UCSD Birds-200-2011 (CUB),\nand CIFAR-FewShot (CIFAR-FS). We also demonstrate the effectiveness of our\nmethod on the image retrieval task in our experiments.\n","authors":["Chi Zhang","Yujun Cai","Guosheng Lin","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2003.06777v5.pdf","comment":"DeepEMD V2, accepted by TPAMI"},{"id":"http://arxiv.org/abs/2303.17285v1","updated":"2023-03-30T10:47:26Z","published":"2023-03-30T10:47:26Z","title":"Decomposed Cross-modal Distillation for RGB-based Temporal Action\n  Detection","summary":"  Temporal action detection aims to predict the time intervals and the classes\nof action instances in the video. Despite the promising performance, existing\ntwo-stream models exhibit slow inference speed due to their reliance on\ncomputationally expensive optical flow. In this paper, we introduce a\ndecomposed cross-modal distillation framework to build a strong RGB-based\ndetector by transferring knowledge of the motion modality. Specifically,\ninstead of direct distillation, we propose to separately learn RGB and motion\nrepresentations, which are in turn combined to perform action localization. The\ndual-branch design and the asymmetric training objectives enable effective\nmotion knowledge transfer while preserving RGB information intact. In addition,\nwe introduce a local attentive fusion to better exploit the multimodal\ncomplementarity. It is designed to preserve the local discriminability of the\nfeatures that is important for action localization. Extensive experiments on\nthe benchmarks verify the effectiveness of the proposed method in enhancing\nRGB-based action detectors. Notably, our framework is agnostic to backbones and\ndetection heads, bringing consistent gains across different model combinations.\n","authors":["Pilhyeon Lee","Taeoh Kim","Minho Shim","Dongyoon Wee","Hyeran Byun"],"pdf_url":"https://arxiv.org/pdf/2303.17285v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2212.11533v3","updated":"2023-03-30T10:09:33Z","published":"2022-12-22T08:20:08Z","title":"Multi Lane Detection","summary":"  Lane detection is a long-standing task and a basic module in autonomous\ndriving. The task is to detect the lane of the current driving road, and\nprovide relevant information such as the ID, direction, curvature, width,\nlength, with visualization. Our work is based on CNN backbone DLA-34, along\nwith Affinity Fields, aims to achieve robust detection of various lanes without\nassuming the number of lanes. Besides, we investigate novel decoding methods to\nachieve more efficient lane detection algorithm.\n","authors":["Fei Wu","Luoyu Chen"],"pdf_url":"https://arxiv.org/pdf/2212.11533v3.pdf","comment":"no any valuable and main part is based on other works"},{"id":"http://arxiv.org/abs/2303.16564v2","updated":"2023-03-30T09:50:43Z","published":"2023-03-29T09:47:35Z","title":"Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a\n  Bayesian Neural Network","summary":"  The fairness of a deep neural network is strongly affected by dataset bias\nand spurious correlations, both of which are usually present in modern\nfeature-rich and complex visual datasets. Due to the difficulty and variability\nof the task, no single de-biasing method has been universally successful. In\nparticular, implicit methods not requiring explicit knowledge of bias variables\nare especially relevant for real-world applications. We propose a novel\nimplicit mitigation method using a Bayesian neural network, allowing us to\nleverage the relationship between epistemic uncertainties and the presence of\nbias or spurious correlations in a sample. Our proposed posterior estimate\nsharpening procedure encourages the network to focus on core features that do\nnot contribute to high uncertainties. Experimental results on three benchmark\ndatasets demonstrate that Bayesian networks with sharpened posterior estimates\nperform comparably to prior existing methods and show potential worthy of\nfurther exploration.\n","authors":["Rebecca S Stone","Nishant Ravikumar","Andrew J Bulpitt","David C Hogg"],"pdf_url":"https://arxiv.org/pdf/2303.16564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17255v1","updated":"2023-03-30T09:34:39Z","published":"2023-03-30T09:34:39Z","title":"Adversarial Attack and Defense for Dehazing Networks","summary":"  The research on single image dehazing task has been widely explored. However,\nas far as we know, no comprehensive study has been conducted on the robustness\nof the well-trained dehazing models. Therefore, there is no evidence that the\ndehazing networks can resist malicious attacks. In this paper, we focus on\ndesigning a group of attack methods based on first order gradient to verify the\nrobustness of the existing dehazing algorithms. By analyzing the general goal\nof image dehazing task, five attack methods are proposed, which are prediction,\nnoise, mask, ground-truth and input attack. The corresponding experiments are\nconducted on six datasets with different scales. Further, the defense strategy\nbased on adversarial training is adopted for reducing the negative effects\ncaused by malicious attacks. In summary, this paper defines a new challenging\nproblem for image dehazing area, which can be called as adversarial attack on\ndehazing networks (AADN). Code is available at\nhttps://github.com/guijiejie/AADN.\n","authors":["Jie Gui","Xiaofeng Cong","Chengwei Peng","Yuan Yan Tang","James Tin-Yau Kwok"],"pdf_url":"https://arxiv.org/pdf/2303.17255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17253v1","updated":"2023-03-30T09:32:29Z","published":"2023-03-30T09:32:29Z","title":"HDR Imaging with Spatially Varying Signal-to-Noise Ratios","summary":"  While today's high dynamic range (HDR) image fusion algorithms are capable of\nblending multiple exposures, the acquisition is often controlled so that the\ndynamic range within one exposure is narrow. For HDR imaging in photon-limited\nsituations, the dynamic range can be enormous and the noise within one exposure\nis spatially varying. Existing image denoising algorithms and HDR fusion\nalgorithms both fail to handle this situation, leading to severe limitations in\nlow-light HDR imaging. This paper presents two contributions. Firstly, we\nidentify the source of the problem. We find that the issue is associated with\nthe co-existence of (1) spatially varying signal-to-noise ratio, especially the\nexcessive noise due to very dark regions, and (2) a wide luminance range within\neach exposure. We show that while the issue can be handled by a bank of\ndenoisers, the complexity is high. Secondly, we propose a new method called the\nspatially varying high dynamic range (SV-HDR) fusion network to simultaneously\ndenoise and fuse images. We introduce a new exposure-shared block within our\ncustom-designed multi-scale transformer framework. In a variety of testing\nconditions, the performance of the proposed SV-HDR is better than the existing\nmethods.\n","authors":["Yiheng Chi","Xingguang Zhang","Stanley H. Chan"],"pdf_url":"https://arxiv.org/pdf/2303.17253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.07610v3","updated":"2023-03-30T09:30:31Z","published":"2022-01-19T14:09:14Z","title":"Nonlinear Unknown Input Observability and Unknown Input Reconstruction:\n  The General Analytical Solution","summary":"  Observability is a fundamental structural property of any dynamic system and\ndescribes the possibility of reconstructing the state that characterizes the\nsystem from observing its inputs and outputs. Despite the huge effort made to\nstudy this property and to introduce analytical criteria able to check whether\na dynamic system satisfies this property or not, there is no general analytical\ncriterion to automatically check the state observability when the dynamics are\nalso driven by unknown inputs. Here, we introduce the general analytical\nsolution of this fundamental problem, often called the unknown input\nobservability problem. This paper provides the general analytical solution of\nthis problem, namely, it provides the systematic procedure, based on automatic\ncomputation (differentiation and matrix rank determination), that allows us to\nautomatically check the state observability even in the presence of unknown\ninputs (Algorithm 6.1). A first solution of this problem was presented in the\nsecond part of the book: \"Observability: A New Theory Based on the Group of\nInvariance\" [45]. The solution presented by this paper completes the previous\nsolution in [45]. In particular, the new solution exhaustively accounts for the\nsystems that do not belong to the category of the systems that are \"canonic\nwith respect to their unknown inputs\". The analytical derivations largely\nexploit several new concepts and analytical results introduced in [45].\nFinally, as a simple consequence of the results here obtained, we also provide\nthe answer to the problem of unknown input reconstruction which is intimately\nrelated to the problem of state observability. We illustrate the implementation\nof the new algorithm by studying the observability properties of a nonlinear\nsystem in the framework of visual-inertial sensor fusion, whose dynamics are\ndriven by two unknown inputs and one known input.\n","authors":["Agostino Martinelli"],"pdf_url":"https://arxiv.org/pdf/2201.07610v3.pdf","comment":"This paper was published by the journal of Information Fusion"},{"id":"http://arxiv.org/abs/2303.17249v1","updated":"2023-03-30T09:29:03Z","published":"2023-03-30T09:29:03Z","title":"Model-agnostic explainable artificial intelligence for object detection\n  in image data","summary":"  Object detection is a fundamental task in computer vision, which has been\ngreatly progressed through developing large and intricate deep learning models.\nHowever, the lack of transparency is a big challenge that may not allow the\nwidespread adoption of these models. Explainable artificial intelligence is a\nfield of research where methods are developed to help users understand the\nbehavior, decision logics, and vulnerabilities of AI-based systems. Black-box\nexplanation refers to explaining decisions of an AI system without having\naccess to its internals. In this paper, we design and implement a black-box\nexplanation method named Black-box Object Detection Explanation by Masking\n(BODEM) through adopting a new masking approach for AI-based object detection\nsystems. We propose local and distant masking to generate multiple versions of\nan input image. Local masks are used to disturb pixels within a target object\nto figure out how the object detector reacts to these changes, while distant\nmasks are used to assess how the detection model's decisions are affected by\ndisturbing pixels outside the object. A saliency map is then created by\nestimating the importance of pixels through measuring the difference between\nthe detection output before and after masking. Finally, a heatmap is created\nthat visualizes how important pixels within the input image are to the detected\nobjects. The experimentations on various object detection datasets and models\nshowed that BODEM can be effectively used to explain the behavior of object\ndetectors and reveal their vulnerabilities. This makes BODEM suitable for\nexplaining and validating AI based object detection systems in black-box\nsoftware testing scenarios. Furthermore, we conducted data augmentation\nexperiments that showed local masks produced by BODEM can be used for further\ntraining the object detectors and improve their detection accuracy and\nrobustness.\n","authors":["Milad Moradi","Ke Yan","David Colwell","Matthias Samwald","Rhona Asgari"],"pdf_url":"https://arxiv.org/pdf/2303.17249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17247v1","updated":"2023-03-30T09:24:17Z","published":"2023-03-30T09:24:17Z","title":"Impact of Video Processing Operations in Deepfake Detection","summary":"  The detection of digital face manipulation in video has attracted extensive\nattention due to the increased risk to public trust. To counteract the\nmalicious usage of such techniques, deep learning-based deepfake detection\nmethods have been developed and have shown impressive results. However, the\nperformance of these detectors is often evaluated using benchmarks that hardly\nreflect real-world situations. For example, the impact of various video\nprocessing operations on detection accuracy has not been systematically\nassessed. To address this gap, this paper first analyzes numerous real-world\ninfluencing factors and typical video processing operations. Then, a more\nsystematic assessment methodology is proposed, which allows for a quantitative\nevaluation of a detector's robustness under the influence of different\nprocessing operations. Moreover, substantial experiments have been carried out\non three popular deepfake detectors, which give detailed analyses on the impact\nof each operation and bring insights to foster future research.\n","authors":["Yuhang Lu","Touradj Ebrahimi"],"pdf_url":"https://arxiv.org/pdf/2303.17247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17245v1","updated":"2023-03-30T09:22:17Z","published":"2023-03-30T09:22:17Z","title":"Investigating and Mitigating the Side Effects of Noisy Views in\n  Multi-view Clustering in Practical Scenarios","summary":"  Multi-view clustering (MvC) aims at exploring the category structure among\nmulti-view data without label supervision. Multiple views provide more\ninformation than single views and thus existing MvC methods can achieve\nsatisfactory performance. However, their performance might seriously degenerate\nwhen the views are noisy in practical scenarios. In this paper, we first\nformally investigate the drawback of noisy views and then propose a\ntheoretically grounded deep MvC method (namely MvCAN) to address this issue.\nSpecifically, we propose a novel MvC objective that enables un-shared\nparameters and inconsistent clustering predictions across multiple views to\nreduce the side effects of noisy views. Furthermore, a non-parametric iterative\nprocess is designed to generate a robust learning target for mining multiple\nviews' useful information. Theoretical analysis reveals that MvCAN works by\nachieving the multi-view consistency, complementarity, and noise robustness.\nFinally, experiments on public datasets demonstrate that MvCAN outperforms\nstate-of-the-art methods and is robust against the existence of noisy views.\n","authors":["Jie Xu","Gang Niu","Xiaolong Wang","Yazhou Ren","Lei Feng","Xiaoshuang Shi","Heng Tao Shen","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.17245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17239v1","updated":"2023-03-30T09:16:13Z","published":"2023-03-30T09:16:13Z","title":"Retrospective Motion Correction in Gradient Echo MRI by Explicit Motion\n  Estimation Using Deep CNNs","summary":"  Magnetic Resonance Imaging allows high resolution data acquisition with the\ndownside of motion sensitivity due to relatively long acquisition times. Even\nduring the acquisition of a single 2D slice, motion can severely corrupt the\nimage. Retrospective motion correction strategies do not interfere during\nacquisition time but operate on the motion affected data. Known methods suited\nto this scenario are compressed sensing (CS), generative adversarial networks\n(GANs), and motion estimation. In this paper we propose a strategy to correct\nfor motion artifacts using Deep Convolutional Neuronal Networks (Deep CNNs) in\na reliable and verifiable manner by explicit motion estimation. The sensitivity\nencoding (SENSE) redundancy that multiple receiver coils provide, has in the\npast been used for acceleration, noise reduction and rigid motion compensation.\nWe show that using Deep CNNs the concepts of rigid motion compensation can be\ngeneralized to more complex motion fields. Using a simulated synthetic data\nset, our proposed supervised network is evaluated on motion corrupted MRIs of\nabdomen and head. We compare our results with rigid motion compensation and\nGANs.\n","authors":["Mathias S. Feinler","Bernadette N. Hahn"],"pdf_url":"https://arxiv.org/pdf/2303.17239v1.pdf","comment":"19 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.13959v2","updated":"2023-03-30T09:09:27Z","published":"2023-03-24T12:33:44Z","title":"StereoScene: BEV-Assisted Stereo Matching Empowers 3D Semantic Scene\n  Completion","summary":"  3D semantic scene completion (SSC) is an ill-posed task that requires\ninferring a dense 3D scene from incomplete observations. Previous methods\neither explicitly incorporate 3D geometric input or rely on learnt 3D prior\nbehind monocular RGB images. However, 3D sensors such as LiDAR are expensive\nand intrusive while monocular cameras face challenges in modeling precise\ngeometry due to the inherent ambiguity. In this work, we propose StereoScene\nfor 3D Semantic Scene Completion (SSC), which explores taking full advantage of\nlight-weight camera inputs without resorting to any external 3D sensors. Our\nkey insight is to leverage stereo matching to resolve geometric ambiguity. To\nimprove its robustness in unmatched areas, we introduce bird's-eye-view (BEV)\nrepresentation to inspire hallucination ability with rich context information.\nOn top of the stereo and BEV representations, a mutual interactive aggregation\n(MIA) module is carefully devised to fully unleash their power. Specifically, a\nBi-directional Interaction Transformer (BIT) augmented with confidence\nre-weighting is used to encourage reliable prediction through mutual guidance\nwhile a Dual Volume Aggregation (DVA) module is designed to facilitate\ncomplementary aggregation. Experimental results on SemanticKITTI demonstrate\nthat the proposed StereoScene outperforms the state-of-the-art camera-based\nmethods by a large margin with a relative improvement of 26.9% in geometry and\n38.6% in semantic.\n","authors":["Bohan Li","Yasheng Sun","Xin Jin","Wenjun Zeng","Zheng Zhu","Xiaoefeng Wang","Yunpeng Zhang","James Okae","Hang Xiao","Dalong Du"],"pdf_url":"https://arxiv.org/pdf/2303.13959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17228v1","updated":"2023-03-30T08:51:49Z","published":"2023-03-30T08:51:49Z","title":"Streaming Video Model","summary":"  Video understanding tasks have traditionally been modeled by two separate\narchitectures, specially tailored for two distinct tasks. Sequence-based video\ntasks, such as action recognition, use a video backbone to directly extract\nspatiotemporal features, while frame-based video tasks, such as multiple object\ntracking (MOT), rely on single fixed-image backbone to extract spatial\nfeatures. In contrast, we propose to unify video understanding tasks into one\nnovel streaming video architecture, referred to as Streaming Vision Transformer\n(S-ViT). S-ViT first produces frame-level features with a memory-enabled\ntemporally-aware spatial encoder to serve the frame-based video tasks. Then the\nframe features are input into a task-related temporal decoder to obtain\nspatiotemporal features for sequence-based tasks. The efficiency and efficacy\nof S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based\naction recognition task and the competitive advantage over conventional\narchitecture in the frame-based MOT task. We believe that the concept of\nstreaming video model and the implementation of S-ViT are solid steps towards a\nunified deep learning architecture for video understanding. Code will be\navailable at https://github.com/yuzhms/Streaming-Video-Model.\n","authors":["Yucheng Zhao","Chong Luo","Chuanxin Tang","Dongdong Chen","Noel Codella","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2303.17228v1.pdf","comment":"Accepted by CVPR'23"},{"id":"http://arxiv.org/abs/2302.10574v3","updated":"2023-03-30T08:51:05Z","published":"2023-02-21T10:00:58Z","title":"MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection\n  and Domain Knowledge-driven Pooling for Whole Slide Image Analysis","summary":"  Whole slide image (WSI) has been widely used to assist automated diagnosis\nunder the deep learning fields. However, most previous works only discuss the\nSINGLE task setting which is not aligned with real clinical setting, where\npathologists often conduct multiple diagnosis tasks simultaneously. Also, it is\ncommonly recognized that the multi-task learning paradigm can improve learning\nefficiency by exploiting commonalities and differences across multiple tasks.\nTo this end, we present a novel multi-task framework (i.e., MulGT) for WSI\nanalysis by the specially designed Graph-Transformer equipped with Task-aware\nKnowledge Injection and Domain Knowledge-driven Graph Pooling modules.\nBasically, with the Graph Neural Network and Transformer as the building\ncommons, our framework is able to learn task-agnostic low-level local\ninformation as well as task-specific high-level global representation.\nConsidering that different tasks in WSI analysis depend on different features\nand properties, we also design a novel Task-aware Knowledge Injection module to\ntransfer the task-shared graph embedding into task-specific feature spaces to\nlearn more accurate representation for different tasks. Further, we elaborately\ndesign a novel Domain Knowledge-driven Graph Pooling module for each task to\nimprove both the accuracy and robustness of different tasks by leveraging\ndifferent diagnosis patterns of multiple tasks. We evaluated our method on two\npublic WSI datasets from TCGA projects, i.e., esophageal carcinoma and kidney\ncarcinoma. Experimental results show that our method outperforms single-task\ncounterparts and the state-of-theart methods on both tumor typing and staging\ntasks.\n","authors":["Weiqin Zhao","Shujun Wang","Maximus Yeung","Tianye Niu","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2302.10574v3.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.17225v1","updated":"2023-03-30T08:42:49Z","published":"2023-03-30T08:42:49Z","title":"FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation","summary":"  Recently, open-vocabulary learning has emerged to accomplish segmentation for\narbitrary categories of text-based descriptions, which popularizes the\nsegmentation system to more general-purpose application scenarios. However,\nexisting methods devote to designing specialized architectures or parameters\nfor specific segmentation tasks. These customized design paradigms lead to\nfragmentation between various segmentation tasks, thus hindering the uniformity\nof segmentation models. Hence in this paper, we propose FreeSeg, a generic\nframework to accomplish Unified, Universal and Open-Vocabulary Image\nSegmentation. FreeSeg optimizes an all-in-one network via one-shot training and\nemploys the same architecture and parameters to handle diverse segmentation\ntasks seamlessly in the inference procedure. Additionally, adaptive prompt\nlearning facilitates the unified model to capture task-aware and\ncategory-sensitive concepts, improving model robustness in multi-task and\nvaried scenarios. Extensive experimental results demonstrate that FreeSeg\nestablishes new state-of-the-art results in performance and generalization on\nthree segmentation tasks, which outperforms the best task-specific\narchitectures by a large margin: 5.5% mIoU on semantic segmentation, 17.6% mAP\non instance segmentation, 20.1% PQ on panoptic segmentation for the unseen\nclass on COCO.\n","authors":["Jie Qin","Jie Wu","Pengxiang Yan","Ming Li","Ren Yuxi","Xuefeng Xiao","Yitong Wang","Rui Wang","Shilei Wen","Xin Pan","Xingang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.17225v1.pdf","comment":"Accepted by CVPR 2023; camera-ready version"},{"id":"http://arxiv.org/abs/2303.17222v1","updated":"2023-03-30T08:36:48Z","published":"2023-03-30T08:36:48Z","title":"LatentForensics: Towards lighter deepfake detection in the StyleGAN\n  latent space","summary":"  The classification of forged videos has been a challenge for the past few\nyears. Deepfake classifiers can now reliably predict whether or not video\nframes have been tampered with. However, their performance is tied to both the\ndataset used for training and the analyst's computational power. We propose a\ndeepfake classification method that operates in the latent space of a\nstate-of-the-art generative adversarial network (GAN) trained on high-quality\nface images. The proposed method leverages the structure of the latent space of\nStyleGAN to learn a lightweight classification model. Experimental results on a\nstandard dataset reveal that the proposed approach outperforms other\nstate-of-the-art deepfake classification methods. To the best of our knowledge,\nthis is the first study showing the interest of the latent space of StyleGAN\nfor deepfake classification. Combined with other recent studies on the\ninterpretation and manipulation of this latent space, we believe that the\nproposed approach can help in developing robust deepfake classification methods\nbased on interpretable high-level properties of face images.\n","authors":["Matthieu Delmas","Amine Kacete","Stephane Paquelet","Simon Leglaive","Renaud Seguier"],"pdf_url":"https://arxiv.org/pdf/2303.17222v1.pdf","comment":"5 pages, 5 figures, 1 tables, submitted to ICIP 2023"},{"id":"http://arxiv.org/abs/2303.17218v1","updated":"2023-03-30T08:25:27Z","published":"2023-03-30T08:25:27Z","title":"HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on\n  FPGA Devices","summary":"  For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks\nhave proven to be highly effective, achieving state-of-the-art results. This\nstudy introduces a novel streaming architecture based toolflow for mapping such\nmodels onto FPGAs considering the model's inherent characteristics and the\nfeatures of the targeted FPGA device. The HARFLOW3D toolflow takes as input a\n3D CNN in ONNX format and a description of the FPGA characteristics, generating\na design that minimizes the latency of the computation. The toolflow is\ncomprised of a number of parts, including i) a 3D CNN parser, ii) a performance\nand resource model, iii) a scheduling algorithm for executing 3D models on the\ngenerated hardware, iv) a resource-aware optimization engine tailored for 3D\nmodels, v) an automated mapping to synthesizable code for FPGAs. The ability of\nthe toolflow to support a broad range of models and devices is shown through a\nnumber of experiments on various 3D CNN and FPGA system pairs. Furthermore, the\ntoolflow has produced high-performing results for 3D CNN models that have not\nbeen mapped to FPGAs before, demonstrating the potential of FPGA-based systems\nin this space. Overall, HARFLOW3D has demonstrated its ability to deliver\ncompetitive latency compared to a range of state-of-the-art hand-tuned\napproaches being able to achieve up to 5$\\times$ better performance compared to\nsome of the existing works.\n","authors":["Petros Toupas","Alexander Montgomerie-Corcoran","Christos-Savvas Bouganis","Dimitrios Tzovaras"],"pdf_url":"https://arxiv.org/pdf/2303.17218v1.pdf","comment":"11 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2303.17216v1","updated":"2023-03-30T08:19:42Z","published":"2023-03-30T08:19:42Z","title":"Few-shot Geometry-Aware Keypoint Localization","summary":"  Supervised keypoint localization methods rely on large manually labeled image\ndatasets, where objects can deform, articulate, or occlude. However, creating\nsuch large keypoint labels is time-consuming and costly, and is often\nerror-prone due to inconsistent labeling. Thus, we desire an approach that can\nlearn keypoint localization with fewer yet consistently annotated images. To\nthis end, we present a novel formulation that learns to localize semantically\nconsistent keypoint definitions, even for occluded regions, for varying object\ncategories. We use a few user-labeled 2D images as input examples, which are\nextended via self-supervision using a larger unlabeled dataset. Unlike\nunsupervised methods, the few-shot images act as semantic shape constraints for\nobject localization. Furthermore, we introduce 3D geometry-aware constraints to\nuplift keypoints, achieving more accurate 2D localization. Our general-purpose\nformulation paves the way for semantically conditioned generative modeling and\nattains competitive or state-of-the-art accuracy on several datasets, including\nhuman faces, eyes, animals, cars, and never-before-seen mouth interior (teeth)\nlocalization tasks, not attempted by the previous few-shot methods. Project\npage:\nhttps://xingzhehe.github.io/FewShot3DKP/}{https://xingzhehe.github.io/FewShot3DKP/\n","authors":["Xingzhe He","Gaurav Bharaj","David Ferman","Helge Rhodin","Pablo Garrido"],"pdf_url":"https://arxiv.org/pdf/2303.17216v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17212v1","updated":"2023-03-30T08:15:18Z","published":"2023-03-30T08:15:18Z","title":"SARGAN: Spatial Attention-based Residuals for Facial Expression\n  Manipulation","summary":"  Encoder-decoder based architecture has been widely used in the generator of\ngenerative adversarial networks for facial manipulation. However, we observe\nthat the current architecture fails to recover the input image color, rich\nfacial details such as skin color or texture and introduces artifacts as well.\nIn this paper, we present a novel method named SARGAN that addresses the\nabove-mentioned limitations from three perspectives. First, we employed spatial\nattention-based residual block instead of vanilla residual blocks to properly\ncapture the expression-related features to be changed while keeping the other\nfeatures unchanged. Second, we exploited a symmetric encoder-decoder network to\nattend facial features at multiple scales. Third, we proposed to train the\ncomplete network with a residual connection which relieves the generator of\npressure to generate the input face image thereby producing the desired\nexpression by directly feeding the input image towards the end of the\ngenerator. Both qualitative and quantitative experimental results show that our\nproposed model performs significantly better than state-of-the-art methods. In\naddition, existing models require much larger datasets for training but their\nperformance degrades on out-of-distribution images. In contrast, SARGAN can be\ntrained on smaller facial expressions datasets, which generalizes well on\nout-of-distribution images including human photographs, portraits, avatars and\nstatues.\n","authors":["Arbish Akram","Nazar Khan"],"pdf_url":"https://arxiv.org/pdf/2303.17212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04180v2","updated":"2023-03-30T08:07:01Z","published":"2022-11-08T11:50:31Z","title":"Exploiting segmentation labels and representation learning to forecast\n  therapy response of PDAC patients","summary":"  The prediction of pancreatic ductal adenocarcinoma therapy response is a\nclinically challenging and important task in this high-mortality tumour entity.\nThe training of neural networks able to tackle this challenge is impeded by a\nlack of large datasets and the difficult anatomical localisation of the\npancreas. Here, we propose a hybrid deep neural network pipeline to predict\ntumour response to initial chemotherapy which is based on the Response\nEvaluation Criteria in Solid Tumors (RECIST) score, a standardised method for\ncancer response evaluation by clinicians as well as tumour markers, and\nclinical evaluation of the patients. We leverage a combination of\nrepresentation transfer from segmentation to classification, as well as\nlocalisation and representation learning. Our approach yields a remarkably\ndata-efficient method able to predict treatment response with a ROC-AUC of\n63.7% using only 477 datasets in total.\n","authors":["Alexander Ziller","Ayhan Can Erdur","Friederike Jungmann","Daniel Rueckert","Rickmer Braren","Georgios Kaissis"],"pdf_url":"https://arxiv.org/pdf/2211.04180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07171v3","updated":"2023-03-30T08:06:56Z","published":"2022-06-14T20:57:49Z","title":"Segmentation in large-scale cellular electron microscopy with deep\n  learning: A literature survey","summary":"  Automated and semi-automated techniques in biomedical electron microscopy\n(EM) enable the acquisition of large datasets at a high rate. Segmentation\nmethods are therefore essential to analyze and interpret these large volumes of\ndata, which can no longer completely be labeled manually. In recent years, deep\nlearning algorithms achieved impressive results in both pixel-level labeling\n(semantic segmentation) and the labeling of separate instances of the same\nclass (instance segmentation). In this review, we examine how these algorithms\nwere adapted to the task of segmenting cellular and sub-cellular structures in\nEM images. The special challenges posed by such images and the network\narchitectures that overcame some of them are described. Moreover, a thorough\noverview is also provided on the notable datasets that contributed to the\nproliferation of deep learning in EM. Finally, an outlook of current trends and\nfuture prospects of EM segmentation is given, especially in the area of\nlabel-free learning.\n","authors":["Anusha Aswath","Ahmad Alsahaf","Ben N. G. Giepmans","George Azzopardi"],"pdf_url":"https://arxiv.org/pdf/2206.07171v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17209v1","updated":"2023-03-30T08:05:59Z","published":"2023-03-30T08:05:59Z","title":"Human from Blur: Human Pose Tracking from Blurry Images","summary":"  We propose a method to estimate 3D human poses from substantially blurred\nimages. The key idea is to tackle the inverse problem of image deblurring by\nmodeling the forward problem with a 3D human model, a texture map, and a\nsequence of poses to describe human motion. The blurring process is then\nmodeled by a temporal image aggregation step. Using a differentiable renderer,\nwe can solve the inverse problem by backpropagating the pixel-wise reprojection\nerror to recover the best human motion representation that explains a single or\nmultiple input images. Since the image reconstruction loss alone is\ninsufficient, we present additional regularization terms. To the best of our\nknowledge, we present the first method to tackle this problem. Our method\nconsistently outperforms other methods on significantly blurry inputs since\nthey lack one or multiple key functionalities that our method unifies, i.e.\nimage deblurring with sub-frame accuracy and explicit 3D modeling of non-rigid\nhuman motion.\n","authors":["Yiming Zhao","Denys Rozumnyi","Jie Song","Otmar Hilliges","Marc Pollefeys","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2303.17209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16181v2","updated":"2023-03-30T08:00:35Z","published":"2023-03-28T17:46:16Z","title":"Learning Federated Visual Prompt in Null Space for MRI Reconstruction","summary":"  Federated Magnetic Resonance Imaging (MRI) reconstruction enables multiple\nhospitals to collaborate distributedly without aggregating local data, thereby\nprotecting patient privacy. However, the data heterogeneity caused by different\nMRI protocols, insufficient local training data, and limited communication\nbandwidth inevitably impair global model convergence and updating. In this\npaper, we propose a new algorithm, FedPR, to learn federated visual prompts in\nthe null space of global prompt for MRI reconstruction. FedPR is a new\nfederated paradigm that adopts a powerful pre-trained model while only learning\nand communicating the prompts with few learnable parameters, thereby\nsignificantly reducing communication costs and achieving competitive\nperformance on limited local data. Moreover, to deal with catastrophic\nforgetting caused by data heterogeneity, FedPR also updates efficient federated\nvisual prompts that project the local prompts into an approximate null space of\nthe global prompt, thereby suppressing the interference of gradients on the\nserver performance. Extensive experiments on federated MRI show that FedPR\nsignificantly outperforms state-of-the-art FL algorithms with <6% of\ncommunication costs when given the limited amount of local training data.\n","authors":["Chun-Mei Feng","Bangjun Li","Xinxing Xu","Yong Liu","Huazhu Fu","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2303.16181v2.pdf","comment":"8 pages, Proceedings of the IEEE/CVF International Conference on\n  Computer Vision"},{"id":"http://arxiv.org/abs/2303.12670v3","updated":"2023-03-30T07:55:43Z","published":"2023-03-22T15:48:23Z","title":"Correlational Image Modeling for Self-Supervised Visual Pre-Training","summary":"  We introduce Correlational Image Modeling (CIM), a novel and surprisingly\neffective approach to self-supervised visual pre-training. Our CIM performs a\nsimple pretext task: we randomly crop image regions (exemplars) from an input\nimage (context) and predict correlation maps between the exemplars and the\ncontext. Three key designs enable correlational image modeling as a nontrivial\nand meaningful self-supervisory task. First, to generate useful\nexemplar-context pairs, we consider cropping image regions with various scales,\nshapes, rotations, and transformations. Second, we employ a bootstrap learning\nframework that involves online and target encoders. During pre-training, the\nformer takes exemplars as inputs while the latter converts the context. Third,\nwe model the output correlation maps via a simple cross-attention block, within\nwhich the context serves as queries and the exemplars offer values and keys. We\nshow that CIM performs on par or better than the current state of the art on\nself-supervised and transfer benchmarks.\n","authors":["Wei Li","Jiahao Xie","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2303.12670v3.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2211.10598v2","updated":"2023-03-30T07:51:03Z","published":"2022-11-19T06:23:08Z","title":"LidarGait: Benchmarking 3D Gait Recognition with Point Clouds","summary":"  Video-based gait recognition has achieved impressive results in constrained\nscenarios. However, visual cameras neglect human 3D structure information,\nwhich limits the feasibility of gait recognition in the 3D wild world. Instead\nof extracting gait features from images, this work explores precise 3D gait\nfeatures from point clouds and proposes a simple yet efficient 3D gait\nrecognition framework, termed LidarGait. Our proposed approach projects sparse\npoint clouds into depth maps to learn the representations with 3D geometry\ninformation, which outperforms existing point-wise and camera-based methods by\na significant margin. Due to the lack of point cloud datasets, we built the\nfirst large-scale LiDAR-based gait recognition dataset, SUSTech1K, collected by\na LiDAR sensor and an RGB camera. The dataset contains 25,239 sequences from\n1,050 subjects and covers many variations, including visibility, views,\nocclusions, clothing, carrying, and scenes. Extensive experiments show that (1)\n3D structure information serves as a significant feature for gait recognition.\n(2) LidarGait outperforms existing point-based and silhouette-based methods by\na significant margin, while it also offers stable cross-view results. (3) The\nLiDAR sensor is superior to the RGB camera for gait recognition in the outdoor\nenvironment. The source code and dataset have been made available at\nhttps://lidargait.github.io.\n","authors":["Chuanfu Shen","Chao Fan","Wei Wu","Rui Wang","George Q. Huang","Shiqi Yu"],"pdf_url":"https://arxiv.org/pdf/2211.10598v2.pdf","comment":"15 pages, 15 figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.17200v1","updated":"2023-03-30T07:43:27Z","published":"2023-03-30T07:43:27Z","title":"SynthVSR: Scaling Up Visual Speech Recognition With Synthetic\n  Supervision","summary":"  Recently reported state-of-the-art results in visual speech recognition (VSR)\noften rely on increasingly large amounts of video data, while the publicly\navailable transcribed video datasets are limited in size. In this paper, for\nthe first time, we study the potential of leveraging synthetic visual data for\nVSR. Our method, termed SynthVSR, substantially improves the performance of VSR\nsystems with synthetic lip movements. The key idea behind SynthVSR is to\nleverage a speech-driven lip animation model that generates lip movements\nconditioned on the input speech. The speech-driven lip animation model is\ntrained on an unlabeled audio-visual dataset and could be further optimized\ntowards a pre-trained VSR model when labeled videos are available. As plenty of\ntranscribed acoustic data and face images are available, we are able to\ngenerate large-scale synthetic data using the proposed lip animation model for\nsemi-supervised VSR training. We evaluate the performance of our approach on\nthe largest public VSR benchmark - Lip Reading Sentences 3 (LRS3). SynthVSR\nachieves a WER of 43.3% with only 30 hours of real labeled data, outperforming\noff-the-shelf approaches using thousands of hours of video. The WER is further\nreduced to 27.9% when using all 438 hours of labeled data from LRS3, which is\non par with the state-of-the-art self-supervised AV-HuBERT method. Furthermore,\nwhen combined with large-scale pseudo-labeled audio-visual data SynthVSR yields\na new state-of-the-art VSR WER of 16.9% using publicly available data only,\nsurpassing the recent state-of-the-art approaches trained with 29 times more\nnon-public machine-transcribed video data (90,000 hours). Finally, we perform\nextensive ablation studies to understand the effect of each component in our\nproposed method.\n","authors":["Xubo Liu","Egor Lakomkin","Konstantinos Vougioukas","Pingchuan Ma","Honglie Chen","Ruiming Xie","Morrie Doulaty","Niko Moritz","Jáchym Kolář","Stavros Petridis","Maja Pantic","Christian Fuegen"],"pdf_url":"https://arxiv.org/pdf/2303.17200v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.08657v2","updated":"2023-03-30T07:36:10Z","published":"2023-03-15T14:41:17Z","title":"Economical Quaternion Extraction from a Human Skeletal Pose Estimate\n  using 2-D Cameras","summary":"  In this paper, we present a novel algorithm to extract a quaternion from a\ntwo dimensional camera frame for estimating a contained human skeletal pose.\nThe problem of pose estimation is usually tackled through the usage of stereo\ncameras and intertial measurement units for obtaining depth and euclidean\ndistance for measurement of points in 3D space. However, the usage of these\ndevices comes with a high signal processing latency as well as a significant\nmonetary cost. By making use of MediaPipe, a framework for building perception\npipelines for human pose estimation, the proposed algorithm extracts a\nquaternion from a 2-D frame capturing an image of a human object at a sub-fifty\nmillisecond latency while also being capable of deployment at edges with a\nsingle camera frame and a generally low computational resource availability,\nespecially for use cases involving last-minute detection and reaction by\nautonomous robots. The algorithm seeks to bypass the funding barrier and\nimprove accessibility for robotics researchers involved in designing control\nsystems.\n","authors":["Sriram Radhakrishna","Adithya Balasubramanyam"],"pdf_url":"https://arxiv.org/pdf/2303.08657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.07685v2","updated":"2023-03-30T07:08:21Z","published":"2023-02-15T14:22:34Z","title":"Video Probabilistic Diffusion Models in Projected Latent Space","summary":"  Despite the remarkable progress in deep generative models, synthesizing\nhigh-resolution and temporally coherent videos still remains a challenge due to\ntheir high-dimensionality and complex temporal dynamics along with large\nspatial variations. Recent works on diffusion models have shown their potential\nto solve this challenge, yet they suffer from severe computation- and\nmemory-inefficiency that limit the scalability. To handle this issue, we\npropose a novel generative model for videos, coined projected latent video\ndiffusion models (PVDM), a probabilistic diffusion model which learns a video\ndistribution in a low-dimensional latent space and thus can be efficiently\ntrained with high-resolution videos under limited resources. Specifically, PVDM\nis composed of two components: (a) an autoencoder that projects a given video\nas 2D-shaped latent vectors that factorize the complex cubic structure of video\npixels and (b) a diffusion model architecture specialized for our new\nfactorized latent space and the training/sampling procedure to synthesize\nvideos of arbitrary length with a single model. Experiments on popular video\ngeneration datasets demonstrate the superiority of PVDM compared with previous\nvideo synthesis methods; e.g., PVDM obtains the FVD score of 639.7 on the\nUCF-101 long video (128 frames) generation benchmark, which improves 1773.4 of\nthe prior state-of-the-art.\n","authors":["Sihyun Yu","Kihyuk Sohn","Subin Kim","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2302.07685v2.pdf","comment":"CVPR 2023. Project page: https://sihyun.me/PVDM"},{"id":"http://arxiv.org/abs/2303.17189v1","updated":"2023-03-30T06:56:12Z","published":"2023-03-30T06:56:12Z","title":"LayoutDiffusion: Controllable Diffusion Model for Layout-to-image\n  Generation","summary":"  Recently, diffusion models have achieved great success in image synthesis.\nHowever, when it comes to the layout-to-image generation where an image often\nhas a complex scene of multiple objects, how to make strong control over both\nthe global layout map and each detailed object remains a challenging task. In\nthis paper, we propose a diffusion model named LayoutDiffusion that can obtain\nhigher generation quality and greater controllability than the previous works.\nTo overcome the difficult multimodal fusion of image and layout, we propose to\nconstruct a structural image patch with region information and transform the\npatched image into a special layout to fuse with the normal layout in a unified\nform. Moreover, Layout Fusion Module (LFM) and Object-aware Cross Attention\n(OaCA) are proposed to model the relationship among multiple objects and\ndesigned to be object-aware and position-sensitive, allowing for precisely\ncontrolling the spatial related information. Extensive experiments show that\nour LayoutDiffusion outperforms the previous SOTA methods on FID, CAS by\nrelatively 46.35%, 26.70% on COCO-stuff and 44.29%, 41.82% on VG. Code is\navailable at https://github.com/ZGCTroy/LayoutDiffusion.\n","authors":["Guangcong Zheng","Xianpan Zhou","Xuewei Li","Zhongang Qi","Ying Shan","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2303.17189v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.07580v2","updated":"2023-03-30T06:54:01Z","published":"2023-03-14T01:56:15Z","title":"Sensitive Region-based Metamorphic Testing Framework using Explainable\n  AI","summary":"  Deep Learning (DL) is one of the most popular research topics in machine\nlearning and DL-driven image recognition systems have developed rapidly. Recent\nresearch has employed metamorphic testing (MT) to detect misclassified images.\nMost of them discuss metamorphic relations (MR), with limited attention given\nto which regions should be transformed. We focus on the fact that there are\nsensitive regions where even small transformations can easily change the\nprediction results and propose an MT framework that efficiently tests for\nregions prone to misclassification by transforming these sensitive regions. Our\nevaluation demonstrated that the sensitive regions can be specified by\nExplainable AI (XAI) and our framework effectively detects faults.\n","authors":["Yuma Torikoshi","Yasuharu Nishi","Juichi Takahashi"],"pdf_url":"https://arxiv.org/pdf/2303.07580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13916v2","updated":"2023-03-30T06:35:24Z","published":"2022-11-25T06:10:57Z","title":"Towards Good Practices for Missing Modality Robust Action Recognition","summary":"  Standard multi-modal models assume the use of the same modalities in training\nand inference stages. However, in practice, the environment in which\nmulti-modal models operate may not satisfy such assumption. As such, their\nperformances degrade drastically if any modality is missing in the inference\nstage. We ask: how can we train a model that is robust to missing modalities?\nThis paper seeks a set of good practices for multi-modal action recognition,\nwith a particular interest in circumstances where some modalities are not\navailable at an inference time. First, we study how to effectively regularize\nthe model during training (e.g., data augmentation). Second, we investigate on\nfusion methods for robustness to missing modalities: we find that\ntransformer-based fusion shows better robustness for missing modality than\nsummation or concatenation. Third, we propose a simple modular network,\nActionMAE, which learns missing modality predictive coding by randomly dropping\nmodality features and tries to reconstruct them with the remaining modality\nfeatures. Coupling these good practices, we build a model that is not only\neffective in multi-modal action recognition but also robust to modality\nmissing. Our model achieves the state-of-the-arts on multiple benchmarks and\nmaintains competitive performances even in missing modality scenarios. Codes\nare available at https://github.com/sangminwoo/ActionMAE.\n","authors":["Sangmin Woo","Sumin Lee","Yeonju Park","Muhammad Adi Nugroho","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2211.13916v2.pdf","comment":"AAAI 2023 (Oral); Code: https://github.com/sangminwoo/ActionMAE"},{"id":"http://arxiv.org/abs/2303.17181v1","updated":"2023-03-30T06:32:55Z","published":"2023-03-30T06:32:55Z","title":"Implicit View-Time Interpolation of Stereo Videos using Multi-Plane\n  Disparities and Non-Uniform Coordinates","summary":"  In this paper, we propose an approach for view-time interpolation of stereo\nvideos. Specifically, we build upon X-Fields that approximates an\ninterpolatable mapping between the input coordinates and 2D RGB images using a\nconvolutional decoder. Our main contribution is to analyze and identify the\nsources of the problems with using X-Fields in our application and propose\nnovel techniques to overcome these challenges. Specifically, we observe that\nX-Fields struggles to implicitly interpolate the disparities for large baseline\ncameras. Therefore, we propose multi-plane disparities to reduce the spatial\ndistance of the objects in the stereo views. Moreover, we propose non-uniform\ntime coordinates to handle the non-linear and sudden motion spikes in videos.\nWe additionally introduce several simple, but important, improvements over\nX-Fields. We demonstrate that our approach is able to produce better results\nthan the state of the art, while running in near real-time rates and having low\nmemory and storage costs.\n","authors":["Avinash Paliwal","Andrii Tsarov","Nima Khademi Kalantari"],"pdf_url":"https://arxiv.org/pdf/2303.17181v1.pdf","comment":"Accepted to CVPR 2023. Project page at\n  https://people.engr.tamu.edu/nimak/Papers/CVPR23StereoVideo/index.html and\n  video at https://www.youtube.com/watch?v=XJa_bf8OCrc"},{"id":"http://arxiv.org/abs/2303.17176v1","updated":"2023-03-30T06:24:54Z","published":"2023-03-30T06:24:54Z","title":"A View From Somewhere: Human-Centric Face Representations","summary":"  Few datasets contain self-identified sensitive attributes, inferring\nattributes risks introducing additional biases, and collecting attributes can\ncarry legal risks. Besides, categorical labels can fail to reflect the\ncontinuous nature of human phenotypic diversity, making it difficult to compare\nthe similarity between same-labeled faces. To address these issues, we present\nA View From Somewhere (AVFS) -- a dataset of 638,180 human judgments of face\nsimilarity. We demonstrate the utility of AVFS for learning a continuous,\nlow-dimensional embedding space aligned with human perception. Our embedding\nspace, induced under a novel conditional framework, not only enables the\naccurate prediction of face similarity, but also provides a human-interpretable\ndecomposition of the dimensions used in the human-decision making process, and\nthe importance distinct annotators place on each dimension. We additionally\nshow the practicality of the dimensions for collecting continuous attributes,\nperforming classification, and comparing dataset attribute disparities.\n","authors":["Jerone T. A. Andrews","Przemyslaw Joniak","Alice Xiang"],"pdf_url":"https://arxiv.org/pdf/2303.17176v1.pdf","comment":"Accepted to ICLR 2023. Code and data may be found at\n  https://github.com/SonyAI/a_view_from_somewhere"},{"id":"http://arxiv.org/abs/2210.15511v4","updated":"2023-03-30T06:12:26Z","published":"2022-10-27T14:47:19Z","title":"ProContEXT: Exploring Progressive Context Transformer for Tracking","summary":"  Existing Visual Object Tracking (VOT) only takes the target area in the first\nframe as a template. This causes tracking to inevitably fail in fast-changing\nand crowded scenes, as it cannot account for changes in object appearance\nbetween frames. To this end, we revamped the tracking framework with\nProgressive Context Encoding Transformer Tracker (ProContEXT), which coherently\nexploits spatial and temporal contexts to predict object motion trajectories.\nSpecifically, ProContEXT leverages a context-aware self-attention module to\nencode the spatial and temporal context, refining and updating the multi-scale\nstatic and dynamic templates to progressively perform accurately tracking. It\nexplores the complementary between spatial and temporal context, raising a new\npathway to multi-context modeling for transformer-based trackers. In addition,\nProContEXT revised the token pruning technique to reduce computational\ncomplexity. Extensive experiments on popular benchmark datasets such as GOT-10k\nand TrackingNet demonstrate that the proposed ProContEXT achieves\nstate-of-the-art performance.\n","authors":["Jin-Peng Lan","Zhi-Qi Cheng","Jun-Yan He","Chenyang Li","Bin Luo","Xu Bao","Wangmeng Xiang","Yifeng Geng","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2210.15511v4.pdf","comment":"Accepted at ICASSP 2023, source code is at\n  https://github.com/zhiqic/ProContEXT"},{"id":"http://arxiv.org/abs/2303.17169v1","updated":"2023-03-30T06:02:40Z","published":"2023-03-30T06:02:40Z","title":"Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models","summary":"  Prompt learning has become one of the most efficient paradigms for adapting\nlarge pre-trained vision-language models to downstream tasks. Current\nstate-of-the-art methods, like CoOp and ProDA, tend to adopt soft prompts to\nlearn an appropriate prompt for each specific task. Recent CoCoOp further\nboosts the base-to-new generalization performance via an image-conditional\nprompt. However, it directly fuses identical image semantics to prompts of\ndifferent labels and significantly weakens the discrimination among different\nclasses as shown in our experiments. Motivated by this observation, we first\npropose a class-aware text prompt (CTP) to enrich generated prompts with\nlabel-related image information. Unlike CoCoOp, CTP can effectively involve\nimage semantics and avoid introducing extra ambiguities into different prompts.\nOn the other hand, instead of reserving the complete image representations, we\npropose text-guided feature tuning (TFT) to make the image branch attend to\nclass-related representation. A contrastive loss is employed to align such\naugmented text and image representations on downstream tasks. In this way, the\nimage-to-text CTP and text-to-image TFT can be mutually promoted to enhance the\nadaptation of VLMs for downstream tasks. Extensive experiments demonstrate that\nour method outperforms the existing methods by a significant margin.\nEspecially, compared to CoCoOp, we achieve an average improvement of 4.03% on\nnew classes and 3.19% on harmonic-mean over eleven classification benchmarks.\n","authors":["Sifan Long","Zhen Zhao","Junkun Yuan","Zichang Tan","Jiangjiang Liu","Luping Zhou","Shengsheng Wang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2303.17169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17167v1","updated":"2023-03-30T05:59:43Z","published":"2023-03-30T05:59:43Z","title":"Rethinking the Approximation Error in 3D Surface Fitting for Point Cloud\n  Normal Estimation","summary":"  Most existing approaches for point cloud normal estimation aim to locally fit\na geometric surface and calculate the normal from the fitted surface. Recently,\nlearning-based methods have adopted a routine of predicting point-wise weights\nto solve the weighted least-squares surface fitting problem. Despite achieving\nremarkable progress, these methods overlook the approximation error of the\nfitting problem, resulting in a less accurate fitted surface. In this paper, we\nfirst carry out in-depth analysis of the approximation error in the surface\nfitting problem. Then, in order to bridge the gap between estimated and precise\nsurface normals, we present two basic design principles: 1) applies the\n$Z$-direction Transform to rotate local patches for a better surface fitting\nwith a lower approximation error; 2) models the error of the normal estimation\nas a learnable term. We implement these two principles using deep neural\nnetworks, and integrate them with the state-of-the-art (SOTA) normal estimation\nmethods in a plug-and-play manner. Extensive experiments verify our approaches\nbring benefits to point cloud normal estimation and push the frontier of\nstate-of-the-art performance on both synthetic and real-world datasets.\n","authors":["Hang Du","Xuejun Yan","Jingjing Wang","Di Xie","Shiliang Pu"],"pdf_url":"https://arxiv.org/pdf/2303.17167v1.pdf","comment":"The first two authors contributed equally to this work. The source\n  code are available at https://github.com/hikvision-research/3DVision.\n  Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17166v1","updated":"2023-03-30T05:57:59Z","published":"2023-03-30T05:57:59Z","title":"Deep Single Image Camera Calibration by Heatmap Regression to Recover\n  Fisheye Images Under ManhattanWorld AssumptionWithout Ambiguity","summary":"  In orthogonal world coordinates, a Manhattan world lying along cuboid\nbuildings is widely useful for various computer vision tasks. However, the\nManhattan world has much room for improvement because the origin of pan angles\nfrom an image is arbitrary, that is, four-fold rotational symmetric ambiguity\nof pan angles. To address this problem, we propose a definition for the\npan-angle origin based on the directions of the roads with respect to a camera\nand the direction of travel. We propose a learning-based calibration method\nthat uses heatmap regression to remove the ambiguity by each direction of\nlabeled image coordinates, similar to pose estimation keypoints.\nSimultaneously, our two-branched network recovers the rotation and removes\nfisheye distortion from a general scene image. To alleviate the lack of\nvanishing points in images, we introduce auxiliary diagonal points that have\nthe optimal 3D arrangement of spatial uniformity. Extensive experiments\ndemonstrated that our method outperforms conventional methods on large-scale\ndatasets and with off-the-shelf cameras.\n","authors":["Nobuhiko Wakai","Satoshi Sato","Yasunori Ishii","Takayoshi Yamashita"],"pdf_url":"https://arxiv.org/pdf/2303.17166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09334v2","updated":"2023-03-30T05:56:40Z","published":"2023-03-16T14:15:32Z","title":"Depth-Aware Image Compositing Model for Parallax Camera Motion Blur","summary":"  Camera motion introduces spatially varying blur due to the depth changes in\nthe 3D world. This work investigates scene configurations where such blur is\nproduced under parallax camera motion. We present a simple, yet accurate, Image\nCompositing Blur (ICB) model for depth-dependent spatially varying blur. The\n(forward) model produces realistic motion blur from a single image, depth map,\nand camera trajectory. Furthermore, we utilize the ICB model, combined with a\ncoordinate-based MLP, to learn a sharp neural representation from the blurred\ninput. Experimental results are reported for synthetic and real examples. The\nresults verify that the ICB forward model is computationally efficient and\nproduces realistic blur, despite the lack of occlusion information.\nAdditionally, our method for restoring a sharp representation proves to be a\ncompetitive approach for the deblurring task.\n","authors":["German F. Torres","Joni-Kristian Kämäräinen"],"pdf_url":"https://arxiv.org/pdf/2303.09334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16839v2","updated":"2023-03-30T05:44:47Z","published":"2023-03-29T16:42:30Z","title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks","summary":"  The development of language models have moved from encoder-decoder to\ndecoder-only designs. In addition, the common knowledge has it that the two\nmost popular multimodal tasks, the generative and contrastive tasks, tend to\nconflict with one another, are hard to accommodate in one architecture, and\nfurther need complex adaptations for downstream tasks. We propose a novel\nparadigm of training with a decoder-only model for multimodal tasks, which is\nsurprisingly effective in jointly learning of these disparate vision-language\ntasks. This is done with a simple model, called MaMMUT. It consists of a single\nvision encoder and a text decoder, and is able to accommodate contrastive and\ngenerative learning by a novel two-pass approach on the text decoder. We\ndemonstrate that joint learning of these diverse objectives is simple,\neffective, and maximizes the weight-sharing of the model across these tasks.\nFurthermore, the same architecture enables straightforward extensions to\nopen-vocabulary object detection and video-language tasks. The model tackles a\ndiverse range of tasks, while being modest in capacity. Our model achieves the\nstate of the art on image-text and text-image retrieval, video question\nanswering and open-vocabulary detection tasks, outperforming much larger and\nmore extensively trained foundational models. It shows very competitive results\non VQA and Video Captioning, especially considering its capacity. Ablations\nconfirm the flexibility and advantages of our approach.\n","authors":["Weicheng Kuo","AJ Piergiovanni","Dahun Kim","Xiyang Luo","Ben Caine","Wei Li","Abhijit Ogale","Luowei Zhou","Andrew Dai","Zhifeng Chen","Claire Cui","Anelia Angelova"],"pdf_url":"https://arxiv.org/pdf/2303.16839v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17158v1","updated":"2023-03-30T05:36:06Z","published":"2023-03-30T05:36:06Z","title":"KD-DLGAN: Data Limited Image Generation via Knowledge Distillation","summary":"  Generative Adversarial Networks (GANs) rely heavily on large-scale training\ndata for training high-quality image generation models. With limited training\ndata, the GAN discriminator often suffers from severe overfitting which\ndirectly leads to degraded generation especially in generation diversity.\nInspired by the recent advances in knowledge distillation (KD), we propose\nKD-DLGAN, a knowledge-distillation based generation framework that introduces\npre-trained vision-language models for training effective data-limited\ngeneration models. KD-DLGAN consists of two innovative designs. The first is\naggregated generative KD that mitigates the discriminator overfitting by\nchallenging the discriminator with harder learning tasks and distilling more\ngeneralizable knowledge from the pre-trained models. The second is correlated\ngenerative KD that improves the generation diversity by distilling and\npreserving the diverse image-text correlation within the pre-trained models.\nExtensive experiments over multiple benchmarks show that KD-DLGAN achieves\nsuperior image generation with limited training data. In addition, KD-DLGAN\ncomplements the state-of-the-art with consistent and substantial performance\ngains.\n","authors":["Kaiwen Cui","Yingchen Yu","Fangneng Zhan","Shengcai Liao","Shijian Lu1","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2303.17158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17155v1","updated":"2023-03-30T05:25:20Z","published":"2023-03-30T05:25:20Z","title":"Discriminative Class Tokens for Text-to-Image Diffusion Models","summary":"  Recent advances in text-to-image diffusion models have enabled the generation\nof diverse and high-quality images. However, generated images often fall short\nof depicting subtle details and are susceptible to errors due to ambiguity in\nthe input text. One way of alleviating these issues is to train diffusion\nmodels on class-labeled datasets. This comes with a downside, doing so limits\ntheir expressive power: (i) supervised datasets are generally small compared to\nlarge-scale scraped text-image datasets on which text-to-image models are\ntrained, and so the quality and diversity of generated images are severely\naffected, or (ii) the input is a hard-coded label, as opposed to free-form\ntext, which limits the control over the generated images.\n  In this work, we propose a non-invasive fine-tuning technique that\ncapitalizes on the expressive potential of free-form text while achieving high\naccuracy through discriminative signals from a pretrained classifier, which\nguides the generation. This is done by iteratively modifying the embedding of a\nsingle input token of a text-to-image diffusion model, using the classifier, by\nsteering generated images toward a given target class. Our method is fast\ncompared to prior fine-tuning methods and does not require a collection of\nin-class images or retraining of a noise-tolerant classifier. We evaluate our\nmethod extensively, showing that the generated images are: (i) more accurate\nand of higher quality than standard diffusion models, (ii) can be used to\naugment training data in a low-resource setting, and (iii) reveal information\nabout the data used to train the guiding classifier. The code is available at\n\\url{https://github.com/idansc/discriminative_class_tokens}\n","authors":["Idan Schwartz","Vésteinn Snæbjarnarson","Sagie Benaim","Hila Chefer","Ryan Cotterell","Lior Wolf","Serge Belongie"],"pdf_url":"https://arxiv.org/pdf/2303.17155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17152v1","updated":"2023-03-30T05:19:43Z","published":"2023-03-30T05:19:43Z","title":"Mixed Autoencoder for Self-supervised Visual Representation Learning","summary":"  Masked Autoencoder (MAE) has demonstrated superior performance on various\nvision tasks via randomly masking image patches and reconstruction. However,\neffective data augmentation strategies for MAE still remain open questions,\ndifferent from those in contrastive learning that serve as the most important\npart. This paper studies the prevailing mixing augmentation for MAE. We first\ndemonstrate that naive mixing will in contrast degenerate model performance due\nto the increase of mutual information (MI). To address, we propose homologous\nrecognition, an auxiliary pretext task, not only to alleviate the MI\nincreasement by explicitly requiring each patch to recognize homologous\npatches, but also to perform object-aware self-supervised pre-training for\nbetter downstream dense perception performance. With extensive experiments, we\ndemonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the\nstate-of-the-art transfer results among masked image modeling (MIM)\naugmentations on different downstream tasks with significant efficiency.\nSpecifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9\nAP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base.\nMoreover, MixedAE surpasses iBOT, a strong MIM method combined with instance\ndiscrimination, while accelerating training by 2x. To our best knowledge, this\nis the very first work to consider mixing for MIM from the perspective of\npretext task design. Code will be made available.\n","authors":["Kai Chen","Zhili Liu","Lanqing Hong","Hang Xu","Zhenguo Li","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2303.17152v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2211.12634v3","updated":"2023-03-30T05:17:19Z","published":"2022-11-22T23:45:27Z","title":"PNI : Industrial Anomaly Detection using Position and Neighborhood\n  Information","summary":"  Because anomalous samples cannot be used for training, many anomaly detection\nand localization methods use pre-trained networks and non-parametric modeling\nto estimate encoded feature distribution. However, these methods neglect the\nimpact of position and neighborhood information on the distribution of normal\nfeatures. To overcome this, we propose a new algorithm, \\textbf{PNI}, which\nestimates the normal distribution using conditional probability given\nneighborhood features, modeled with a multi-layer perceptron network. Moreover,\nposition information is utilized by creating a histogram of representative\nfeatures at each position. Instead of simply resizing the anomaly map, the\nproposed method employs an additional refine network trained on synthetic\nanomaly images to better interpolate and account for the shape and edge of the\ninput image. We conducted experiments on the MVTec AD benchmark dataset and\nachieved state-of-the-art performance, with \\textbf{99.56\\%} and\n\\textbf{98.98\\%} AUROC scores in anomaly detection and localization,\nrespectively.\n","authors":["Jaehyeok Bae","Jae-Han Lee","Seyun Kim"],"pdf_url":"https://arxiv.org/pdf/2211.12634v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16557v2","updated":"2023-03-30T05:14:12Z","published":"2023-03-29T09:26:54Z","title":"Self-accumulative Vision Transformer for Bone Age Assessment Using the\n  Sauvegrain Method","summary":"  This study presents a novel approach to bone age assessment (BAA) using a\nmulti-view, multi-task classification model based on the Sauvegrain method. A\nstraightforward solution to automating the Sauvegrain method, which assesses a\nmaturity score for each landmark in the elbow and predicts the bone age, is to\ntrain classifiers independently to score each region of interest (RoI), but\nthis approach limits the accessible information to local morphologies and\nincreases computational costs. As a result, this work proposes a\nself-accumulative vision transformer (SAT) that mitigates anisotropic behavior,\nwhich usually occurs in multi-view, multi-task problems and limits the\neffectiveness of a vision transformer, by applying token replay and regional\nattention bias. A number of experiments show that SAT successfully exploits the\nrelationships between landmarks and learns global morphological features,\nresulting in a mean absolute error of BAA that is 0.11 lower than that of the\nprevious work. Additionally, the proposed SAT has four times reduced parameters\nthan an ensemble of individual classifiers of the previous work. Lastly, this\nwork also provides informative implications for clinical practice, improving\nthe accuracy and efficiency of BAA in diagnosing abnormal growth in\nadolescents.\n","authors":["Hong-Jun Choi","Dongbin Na","Kyungjin Cho","Byunguk Bae","Seo Taek Kong","Hyunjoon An"],"pdf_url":"https://arxiv.org/pdf/2303.16557v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2303.15999v2","updated":"2023-03-30T05:13:59Z","published":"2023-03-28T14:15:13Z","title":"Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised\n  Regression Deep Learning Models","summary":"  In this work, the authors develop regression approaches based on deep\nlearning to perform thread density estimation for plain weave canvas analysis.\nPrevious approaches were based on Fourier analysis, which is quite robust for\nsome scenarios but fails in some others, in machine learning tools, that\ninvolve pre-labeling of the painting at hand, or the segmentation of thread\ncrossing points, that provides good estimations in all scenarios with no need\nof pre-labeling. The segmentation approach is time-consuming as the estimation\nof the densities is performed after locating the crossing points. In this novel\nproposal, we avoid this step by computing the density of threads directly from\nthe image with a regression deep learning model. We also incorporate some\nimprovements in the initial preprocessing of the input image with an impact on\nthe final error. Several models are proposed and analyzed to retain the best\none. Furthermore, we further reduce the density estimation error by introducing\na semi-supervised approach. The performance of our novel algorithm is analyzed\nwith works by Ribera, Vel\\'azquez, and Poussin where we compare our results to\nthe ones of previous approaches. Finally, the method is put into practice to\nsupport the change of authorship or a masterpiece at the Museo del Prado.\n","authors":["A. D. Bejarano","Juan J. Murillo-Fuentes","Laura Alba-Carcelen"],"pdf_url":"https://arxiv.org/pdf/2303.15999v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2110.10921v2","updated":"2023-03-30T05:05:23Z","published":"2021-10-21T06:26:31Z","title":"CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization","summary":"  Deep convolutional neural networks are shown to be overkill with high\nparametric and computational redundancy in many application scenarios, and an\nincreasing number of works have explored model pruning to obtain lightweight\nand efficient networks. However, most existing pruning approaches are driven by\nempirical heuristic and rarely consider the joint impact of channels, leading\nto unguaranteed and suboptimal performance. In this paper, we propose a novel\nchannel pruning method via Class-Aware Trace Ratio Optimization (CATRO) to\nreduce the computational burden and accelerate the model inference. Utilizing\nclass information from a few samples, CATRO measures the joint impact of\nmultiple channels by feature space discriminations and consolidates the\nlayer-wise impact of preserved channels. By formulating channel pruning as a\nsubmodular set function maximization problem, CATRO solves it efficiently via a\ntwo-stage greedy iterative optimization procedure. More importantly, we present\ntheoretical justifications on convergence of CATRO and performance of pruned\nnetworks. Experimental results demonstrate that CATRO achieves higher accuracy\nwith similar computation cost or lower computation cost with similar accuracy\nthan other state-of-the-art channel pruning algorithms. In addition, because of\nits class-aware property, CATRO is suitable to prune efficient networks\nadaptively for various classification subtasks, enhancing handy deployment and\nusage of deep networks in real-world applications.\n","authors":["Wenzheng Hu","Zhengping Che","Ning Liu","Mingyang Li","Jian Tang","Changshui Zhang","Jianqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2110.10921v2.pdf","comment":"Paper accepted by IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS)"},{"id":"http://arxiv.org/abs/2212.00794v2","updated":"2023-03-30T05:04:28Z","published":"2022-12-01T18:59:57Z","title":"Scaling Language-Image Pre-training via Masking","summary":"  We present Fast Language-Image Pre-training (FLIP), a simple and more\nefficient method for training CLIP. Our method randomly masks out and removes a\nlarge portion of image patches during training. Masking allows us to learn from\nmore image-text pairs given the same wall-clock time and contrast more samples\nper iteration with similar memory footprint. It leads to a favorable trade-off\nbetween accuracy and training time. In our experiments on 400 million\nimage-text pairs, FLIP improves both accuracy and speed over the no-masking\nbaseline. On a large diversity of downstream tasks, FLIP dominantly outperforms\nthe CLIP counterparts trained on the same data. Facilitated by the speedup, we\nexplore the scaling behavior of increasing the model size, data size, or\ntraining length, and report encouraging results and comparisons. We hope that\nour work will foster future research on scaling vision-language learning.\n","authors":["Yanghao Li","Haoqi Fan","Ronghang Hu","Christoph Feichtenhofer","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2212.00794v2.pdf","comment":"Tech report; arXiv v2: update scaling results and add code repo"},{"id":"http://arxiv.org/abs/2212.04673v2","updated":"2023-03-30T05:04:25Z","published":"2022-12-09T05:38:07Z","title":"MSI: Maximize Support-Set Information for Few-Shot Segmentation","summary":"  FSS(Few-shot segmentation) aims to segment a target class using a small\nnumber of labeled images (support set). To extract the information relevant to\ntarget class, a dominant approach in best performing FSS methods removes\nbackground features using a support mask. We observe that this feature excision\nthrough a limiting support mask introduces an information bottleneck in several\nchallenging FSS cases, e.g., for small targets and/or inaccurate target\nboundaries. To this end, we present a novel method (MSI), which maximizes the\nsupport-set information by exploiting two complementary sources of features to\ngenerate super correlation maps. We validate the effectiveness of our approach\nby instantiating it into three recent and strong FSS methods. Experimental\nresults on several publicly available FSS benchmarks show that our proposed\nmethod consistently improves the performance by visible margins and leads to\nfaster convergence. Our code and models will be publicly released.\n","authors":["Seonghyeon Moon","Samuel S. Sohn","Honglu Zhou","Sejong Yoon","Vladimir Pavlovic","Muhammad Haris Khan","Mubbasir Kapadia"],"pdf_url":"https://arxiv.org/pdf/2212.04673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17147v1","updated":"2023-03-30T04:59:48Z","published":"2023-03-30T04:59:48Z","title":"NeILF++: Inter-Reflectable Light Fields for Geometry and Material\n  Estimation","summary":"  We present a novel differentiable rendering framework for joint geometry,\nmaterial, and lighting estimation from multi-view images. In contrast to\nprevious methods which assume a simplified environment map or co-located\nflashlights, in this work, we formulate the lighting of a static scene as one\nneural incident light field (NeILF) and one outgoing neural radiance field\n(NeRF). The key insight of the proposed method is the union of the incident and\noutgoing light fields through physically-based rendering and inter-reflections\nbetween surfaces, making it possible to disentangle the scene geometry,\nmaterial, and lighting from image observations in a physically-based manner.\nThe proposed incident light and inter-reflection framework can be easily\napplied to other NeRF systems. We show that our method can not only decompose\nthe outgoing radiance into incident lights and surface materials, but also\nserve as a surface refinement module that further improves the reconstruction\ndetail of the neural surface. We demonstrate on several datasets that the\nproposed method is able to achieve state-of-the-art results in terms of\ngeometry reconstruction quality, material estimation accuracy, and the fidelity\nof novel view rendering.\n","authors":["Jingyang Zhang","Yao Yao","Shiwei Li","Jingbo Liu","Tian Fang","David McKinnon","Yanghai Tsin","Long Quan"],"pdf_url":"https://arxiv.org/pdf/2303.17147v1.pdf","comment":"Project page: \\url{https://yoyo000.github.io/NeILF_pp}"},{"id":"http://arxiv.org/abs/2211.09480v2","updated":"2023-03-30T04:53:35Z","published":"2022-11-17T11:57:01Z","title":"ArcAid: Analysis of Archaeological Artifacts using Drawings","summary":"  Archaeology is an intriguing domain for computer vision. It suffers not only\nfrom shortage in (labeled) data, but also from highly-challenging data, which\nis often extremely abraded and damaged. This paper proposes a novel\nsemi-supervised model for classification and retrieval of images of\narchaeological artifacts. This model utilizes unique data that exists in the\ndomain -- manual drawings made by special artists. These are used during\ntraining to implicitly transfer the domain knowledge from the drawings to their\ncorresponding images, improving their classification results. We show that\nwhile learning how to classify, our model also learns how to generate drawings\nof the artifacts, an important documentation task, which is currently performed\nmanually. Last but not least, we collected a new dataset of stamp-seals of the\nSouthern Levant. The dataset and the code will be released upon acceptance.\n","authors":["Offry Hayon","Stefan Münger","Ilan Shimshoni","Ayellet Tal"],"pdf_url":"https://arxiv.org/pdf/2211.09480v2.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2212.04088v3","updated":"2023-03-30T04:50:44Z","published":"2022-12-08T05:46:32Z","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models","summary":"  This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n","authors":["Chan Hee Song","Jiaman Wu","Clayton Washington","Brian M. Sadler","Wei-Lun Chao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2212.04088v3.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17144v1","updated":"2023-03-30T04:34:31Z","published":"2023-03-30T04:34:31Z","title":"DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving","summary":"  Real-time perception, or streaming perception, is a crucial aspect of\nautonomous driving that has yet to be thoroughly explored in existing research.\nTo address this gap, we present DAMO-StreamNet, an optimized framework that\ncombines recent advances from the YOLO series with a comprehensive analysis of\nspatial and temporal perception mechanisms, delivering a cutting-edge solution.\nThe key innovations of DAMO-StreamNet are: (1) A robust neck structure\nincorporating deformable convolution, enhancing the receptive field and feature\nalignment capabilities. (2) A dual-branch structure that integrates short-path\nsemantic features and long-path temporal features, improving motion state\nprediction accuracy. (3) Logits-level distillation for efficient optimization,\naligning the logits of teacher and student networks in semantic space. (4) A\nreal-time forecasting mechanism that updates support frame features with the\ncurrent frame, ensuring seamless streaming perception during inference. Our\nexperiments demonstrate that DAMO-StreamNet surpasses existing state-of-the-art\nmethods, achieving 37.8% (normal size (600, 960)) and 43.3% (large size (1200,\n1920)) sAP without using extra data. This work not only sets a new benchmark\nfor real-time perception but also provides valuable insights for future\nresearch. Additionally, DAMO-StreamNet can be applied to various autonomous\nsystems, such as drones and robots, paving the way for real-time perception.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Wangmeng Xiang","Binghui Chen","Bin Luo","Yifeng Geng","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2303.17144v1.pdf","comment":"he source code is at https://shorturl.at/BJPZ6"},{"id":"http://arxiv.org/abs/2303.07337v2","updated":"2023-03-30T04:34:04Z","published":"2023-03-13T17:58:54Z","title":"PoseExaminer: Automated Testing of Out-of-Distribution Robustness in\n  Human Pose and Shape Estimation","summary":"  Human pose and shape (HPS) estimation methods achieve remarkable results.\nHowever, current HPS benchmarks are mostly designed to test models in scenarios\nthat are similar to the training data. This can lead to critical situations in\nreal-world applications when the observed data differs significantly from the\ntraining data and hence is out-of-distribution (OOD). It is therefore important\nto test and improve the OOD robustness of HPS methods. To address this\nfundamental problem, we develop a simulator that can be controlled in a\nfine-grained manner using interpretable parameters to explore the manifold of\nimages of human pose, e.g. by varying poses, shapes, and clothes. We introduce\na learning-based testing method, termed PoseExaminer, that automatically\ndiagnoses HPS algorithms by searching over the parameter space of human pose\nimages to find the failure modes. Our strategy for exploring this\nhigh-dimensional parameter space is a multi-agent reinforcement learning\nsystem, in which the agents collaborate to explore different parts of the\nparameter space. We show that our PoseExaminer discovers a variety of\nlimitations in current state-of-the-art models that are relevant in real-world\nscenarios but are missed by current benchmarks. For example, it finds large\nregions of realistic human poses that are not predicted correctly, as well as\nreduced performance for humans with skinny and corpulent body shapes. In\naddition, we show that fine-tuning HPS methods by exploiting the failure modes\nfound by PoseExaminer improve their robustness and even their performance on\nstandard benchmarks by a significant margin. The code are available for\nresearch purposes.\n","authors":["Qihao Liu","Adam Kortylewski","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2303.07337v2.pdf","comment":"Accepted to CVPR 2023; Code: https://github.com/qihao067/PoseExaminer"},{"id":"http://arxiv.org/abs/2303.08132v2","updated":"2023-03-30T04:23:32Z","published":"2023-03-14T17:58:44Z","title":"InstMove: Instance Motion for Object-centric Video Segmentation","summary":"  Despite significant efforts, cutting-edge video segmentation methods still\nremain sensitive to occlusion and rapid movement, due to their reliance on the\nappearance of objects in the form of object embeddings, which are vulnerable to\nthese disturbances. A common solution is to use optical flow to provide motion\ninformation, but essentially it only considers pixel-level motion, which still\nrelies on appearance similarity and hence is often inaccurate under occlusion\nand fast movement. In this work, we study the instance-level motion and present\nInstMove, which stands for Instance Motion for Object-centric Video\nSegmentation. In comparison to pixel-wise motion, InstMove mainly relies on\ninstance-level motion information that is free from image feature embeddings,\nand features physical interpretations, making it more accurate and robust\ntoward occlusion and fast-moving objects. To better fit in with the video\nsegmentation tasks, InstMove uses instance masks to model the physical presence\nof an object and learns the dynamic model through a memory network to predict\nits position and shape in the next frame. With only a few lines of code,\nInstMove can be integrated into current SOTA methods for three different video\nsegmentation tasks and boost their performance. Specifically, we improve the\nprevious arts by 1.5 AP on OVIS dataset, which features heavy occlusions, and\n4.9 AP on YouTubeVIS-Long dataset, which mainly contains fast-moving objects.\nThese results suggest that instance-level motion is robust and accurate, and\nhence serving as a powerful solution in complex scenarios for object-centric\nvideo segmentation.\n","authors":["Qihao Liu","Junfeng Wu","Yi Jiang","Xiang Bai","Alan Yuille","Song Bai"],"pdf_url":"https://arxiv.org/pdf/2303.08132v2.pdf","comment":"Accepted to CVPR 2023; Code: https://github.com/wjf5203/VNext"},{"id":"http://arxiv.org/abs/2303.17142v1","updated":"2023-03-30T04:22:07Z","published":"2023-03-30T04:22:07Z","title":"Soft Neighbors are Positive Supporters in Contrastive Visual\n  Representation Learning","summary":"  Contrastive learning methods train visual encoders by comparing views from\none instance to others. Typically, the views created from one instance are set\nas positive, while views from other instances are negative. This binary\ninstance discrimination is studied extensively to improve feature\nrepresentations in self-supervised learning. In this paper, we rethink the\ninstance discrimination framework and find the binary instance labeling\ninsufficient to measure correlations between different samples. For an\nintuitive example, given a random image instance, there may exist other images\nin a mini-batch whose content meanings are the same (i.e., belonging to the\nsame category) or partially related (i.e., belonging to a similar category).\nHow to treat the images that correlate similarly to the current image instance\nleaves an unexplored problem. We thus propose to support the current image by\nexploring other correlated instances (i.e., soft neighbors). We first carefully\ncultivate a candidate neighbor set, which will be further utilized to explore\nthe highly-correlated instances. A cross-attention module is then introduced to\npredict the correlation score (denoted as positiveness) of other correlated\ninstances with respect to the current one. The positiveness score\nquantitatively measures the positive support from each correlated instance, and\nis encoded into the objective for pretext training. To this end, our proposed\nmethod benefits in discriminating uncorrelated instances while absorbing\ncorrelated instances for SSL. We evaluate our soft neighbor contrastive\nlearning method (SNCLR) on standard visual recognition benchmarks, including\nimage classification, object detection, and instance segmentation. The\nstate-of-the-art recognition performance shows that SNCLR is effective in\nimproving feature representations from both ViT and CNN encoders.\n","authors":["Chongjian Ge","Jiangliu Wang","Zhan Tong","Shoufa Chen","Yibing Song","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2303.17142v1.pdf","comment":"Accepted by ICLR23"},{"id":"http://arxiv.org/abs/2210.15518v4","updated":"2023-03-30T04:02:18Z","published":"2022-10-27T14:57:14Z","title":"LongShortNet: Exploring Temporal and Semantic Features Fusion in\n  Streaming Perception","summary":"  Streaming perception is a critical task in autonomous driving that requires\nbalancing the latency and accuracy of the autopilot system. However, current\nmethods for streaming perception are limited as they only rely on the current\nand adjacent two frames to learn movement patterns. This restricts their\nability to model complex scenes, often resulting in poor detection results. To\naddress this limitation, we propose LongShortNet, a novel dual-path network\nthat captures long-term temporal motion and integrates it with short-term\nspatial semantics for real-time perception. LongShortNet is notable as it is\nthe first work to extend long-term temporal modeling to streaming perception,\nenabling spatiotemporal feature fusion. We evaluate LongShortNet on the\nchallenging Argoverse-HD dataset and demonstrate that it outperforms existing\nstate-of-the-art methods with almost no additional computational cost.\n","authors":["Chenyang Li","Zhi-Qi Cheng","Jun-Yan He","Pengyu Li","Bin Luo","Hanyuan Chen","Yifeng Geng","Jin-Peng Lan","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2210.15518v4.pdf","comment":"Accepted at ICASSP 2023, source code is at\n  https://github.com/zhiqic/LongShortNet"},{"id":"http://arxiv.org/abs/2303.17137v1","updated":"2023-03-30T04:01:48Z","published":"2023-03-30T04:01:48Z","title":"Online Camera-to-ground Calibration for Autonomous Driving","summary":"  Online camera-to-ground calibration is to generate a non-rigid body\ntransformation between the camera and the road surface in a real-time manner.\nExisting solutions utilize static calibration, suffering from environmental\nvariations such as tire pressure changes, vehicle loading volume variations,\nand road surface diversity. Other online solutions exploit the usage of road\nelements or photometric consistency between overlapping views across images,\nwhich require continuous detection of specific targets on the road or\nassistance with multiple cameras to facilitate calibration. In our work, we\npropose an online monocular camera-to-ground calibration solution that does not\nutilize any specific targets while driving. We perform a coarse-to-fine\napproach for ground feature extraction through wheel odometry and estimate the\ncamera-to-ground calibration parameters through a sliding-window-based factor\ngraph optimization. Considering the non-rigid transformation of\ncamera-to-ground while driving, we provide metrics to quantify calibration\nperformance and stopping criteria to report/broadcast our satisfying\ncalibration results. Extensive experiments using real-world data demonstrate\nthat our algorithm is effective and outperforms state-of-the-art techniques.\n","authors":["Binbin Li","Xinyu Du","Yao Hu","Hao Yu","Wende Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.17137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10049v3","updated":"2023-03-30T03:44:31Z","published":"2023-03-17T15:23:15Z","title":"Uncertainty-informed Mutual Learning for Joint Medical Image\n  Classification and Segmentation","summary":"  Classification and segmentation are crucial in medical image analysis as they\nenable accurate diagnosis and disease monitoring. However, current methods\noften prioritize the mutual learning features and shared model parameters,\nwhile neglecting the reliability of features and performances. In this paper,\nwe propose a novel Uncertainty-informed Mutual Learning (UML) framework for\nreliable and interpretable medical image analysis. Our UML introduces\nreliability to joint classification and segmentation tasks, leveraging mutual\nlearning with uncertainty to improve performance. To achieve this, we first use\nevidential deep learning to provide image-level and pixel-wise confidences.\nThen, an Uncertainty Navigator Decoder is constructed for better using mutual\nfeatures and generating segmentation results. Besides, an Uncertainty\nInstructor is proposed to screen reliable masks for classification. Overall,\nUML could produce confidence estimation in features and performance for each\nlink (classification and segmentation). The experiments on the public datasets\ndemonstrate that our UML outperforms existing methods in terms of both accuracy\nand robustness. Our UML has the potential to explore the development of more\nreliable and explainable medical image analysis models. We will release the\ncodes for reproduction after acceptance.\n","authors":["Kai Ren","Ke Zou","Xianjie Liu","Yidi Chen","Xuedong Yuan","Xiaojing Shen","Meng Wang","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2303.10049v3.pdf","comment":"10 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2303.17132v1","updated":"2023-03-30T03:42:54Z","published":"2023-03-30T03:42:54Z","title":"C-SFDA: A Curriculum Learning Aided Self-Training Framework for\n  Efficient Source Free Domain Adaptation","summary":"  Unsupervised domain adaptation (UDA) approaches focus on adapting models\ntrained on a labeled source domain to an unlabeled target domain. UDA methods\nhave a strong assumption that the source data is accessible during adaptation,\nwhich may not be feasible in many real-world scenarios due to privacy concerns\nand resource constraints of devices. In this regard, source-free domain\nadaptation (SFDA) excels as access to source data is no longer required during\nadaptation. Recent state-of-the-art (SOTA) methods on SFDA mostly focus on\npseudo-label refinement based self-training which generally suffers from two\nissues: i) inevitable occurrence of noisy pseudo-labels that could lead to\nearly training time memorization, ii) refinement process requires maintaining a\nmemory bank which creates a significant burden in resource constraint\nscenarios. To address these concerns, we propose C-SFDA, a curriculum learning\naided self-training framework for SFDA that adapts efficiently and reliably to\nchanges across domains based on selective pseudo-labeling. Specifically, we\nemploy a curriculum learning scheme to promote learning from a restricted\namount of pseudo labels selected based on their reliabilities. This simple yet\neffective step successfully prevents label noise propagation during different\nstages of adaptation and eliminates the need for costly memory-bank based label\nrefinement. Our extensive experimental evaluations on both image recognition\nand semantic segmentation tasks confirm the effectiveness of our method. C-SFDA\nis readily applicable to online test-time domain adaptation and also\noutperforms previous SOTA methods in this task.\n","authors":["Nazmul Karim","Niluthpol Chowdhury Mithun","Abhinav Rajvanshi","Han-pang Chiu","Supun Samarasekera","Nazanin Rahnavard"],"pdf_url":"https://arxiv.org/pdf/2303.17132v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.02110v4","updated":"2023-03-30T03:23:16Z","published":"2023-03-03T17:51:08Z","title":"Need for Objective Task-based Evaluation of Deep Learning-Based\n  Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT","summary":"  Artificial intelligence-based methods have generated substantial interest in\nnuclear medicine. An area of significant interest has been using deep-learning\n(DL)-based approaches for denoising images acquired with lower doses, shorter\nacquisition times, or both. Objective evaluation of these approaches is\nessential for clinical application. DL-based approaches for denoising\nnuclear-medicine images have typically been evaluated using fidelity-based\nfigures of merit (FoMs) such as RMSE and SSIM. However, these images are\nacquired for clinical tasks and thus should be evaluated based on their\nperformance in these tasks. Our objectives were to (1) investigate whether\nevaluation with these FoMs is consistent with objective clinical-task-based\nevaluation; (2) provide a theoretical analysis for determining the impact of\ndenoising on signal-detection tasks; (3) demonstrate the utility of virtual\nclinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a\nDL-based method for denoising myocardial perfusion SPECT (MPS) images was\nconducted. The impact of DL-based denoising was evaluated using fidelity-based\nFoMs and AUC, which quantified performance on detecting perfusion defects in\nMPS images as obtained using a model observer with anthropomorphic channels.\nBased on fidelity-based FoMs, denoising using the considered DL-based method\nled to significantly superior performance. However, based on ROC analysis,\ndenoising did not improve, and in fact, often degraded detection-task\nperformance. The results motivate the need for objective task-based evaluation\nof DL-based denoising approaches. Further, this study shows how VCTs provide a\nmechanism to conduct such evaluations using VCTs. Finally, our theoretical\ntreatment reveals insights into the reasons for the limited performance of the\ndenoising approach.\n","authors":["Zitong Yu","Md Ashequr Rahman","Richard Laforest","Thomas H. Schindler","Robert J. Gropler","Richard L. Wahl","Barry A. Siegel","Abhinav K. Jha"],"pdf_url":"https://arxiv.org/pdf/2303.02110v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17127v1","updated":"2023-03-30T03:22:52Z","published":"2023-03-30T03:22:52Z","title":"Adaptive Cross Batch Normalization for Metric Learning","summary":"  Metric learning is a fundamental problem in computer vision whereby a model\nis trained to learn a semantically useful embedding space via ranking losses.\nTraditionally, the effectiveness of a ranking loss depends on the minibatch\nsize, and is, therefore, inherently limited by the memory constraints of the\nunderlying hardware. While simply accumulating the embeddings across\nminibatches has proved useful (Wang et al. [2020]), we show that it is equally\nimportant to ensure that the accumulated embeddings are up to date. In\nparticular, it is necessary to circumvent the representational drift between\nthe accumulated embeddings and the feature embeddings at the current training\niteration as the learnable parameters are being updated. In this paper, we\nmodel representational drift as distribution misalignment and tackle it using\nmoment matching. The result is a simple method for updating the stored\nembeddings to match the first and second moments of the current embeddings at\neach training iteration. Experiments on three popular image retrieval datasets,\nnamely, SOP, In-Shop, and DeepFashion2, demonstrate that our approach\nsignificantly improves the performance in all scenarios.\n","authors":["Thalaiyasingam Ajanthan","Matt Ma","Anton van den Hengel","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.17127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17123v1","updated":"2023-03-30T03:21:14Z","published":"2023-03-30T03:21:14Z","title":"Masked and Adaptive Transformer for Exemplar Based Image Translation","summary":"  We present a novel framework for exemplar based image translation. Recent\nadvanced methods for this task mainly focus on establishing cross-domain\nsemantic correspondence, which sequentially dominates image generation in the\nmanner of local style control. Unfortunately, cross-domain semantic matching is\nchallenging; and matching errors ultimately degrade the quality of generated\nimages. To overcome this challenge, we improve the accuracy of matching on the\none hand, and diminish the role of matching in image generation on the other\nhand. To achieve the former, we propose a masked and adaptive transformer (MAT)\nfor learning accurate cross-domain correspondence, and executing context-aware\nfeature augmentation. To achieve the latter, we use source features of the\ninput and global style codes of the exemplar, as supplementary information, for\ndecoding an image. Besides, we devise a novel contrastive style learning\nmethod, for acquire quality-discriminative style representations, which in turn\nbenefit high-quality image generation. Experimental results show that our\nmethod, dubbed MATEBIT, performs considerably better than state-of-the-art\nmethods, in diverse image translation tasks. The codes are available at\n\\url{https://github.com/AiArt-HDU/MATEBIT}.\n","authors":["Chang Jiang","Fei Gao","Biao Ma","Yuhao Lin","Nannan Wang","Gang Xu"],"pdf_url":"https://arxiv.org/pdf/2303.17123v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2210.05944v3","updated":"2023-03-30T03:12:05Z","published":"2022-10-12T06:16:34Z","title":"ACSeg: Adaptive Conceptualization for Unsupervised Semantic Segmentation","summary":"  Recently, self-supervised large-scale visual pre-training models have shown\ngreat promise in representing pixel-level semantic relationships, significantly\npromoting the development of unsupervised dense prediction tasks, e.g.,\nunsupervised semantic segmentation (USS). The extracted relationship among\npixel-level representations typically contains rich class-aware information\nthat semantically identical pixel embeddings in the representation space gather\ntogether to form sophisticated concepts. However, leveraging the learned models\nto ascertain semantically consistent pixel groups or regions in the image is\nnon-trivial since over/ under-clustering overwhelms the conceptualization\nprocedure under various semantic distributions of different images. In this\nwork, we investigate the pixel-level semantic aggregation in self-supervised\nViT pre-trained models as image Segmentation and propose the Adaptive\nConceptualization approach for USS, termed ACSeg. Concretely, we explicitly\nencode concepts into learnable prototypes and design the Adaptive Concept\nGenerator (ACG), which adaptively maps these prototypes to informative concepts\nfor each image. Meanwhile, considering the scene complexity of different\nimages, we propose the modularity loss to optimize ACG independent of the\nconcept number based on estimating the intensity of pixel pairs belonging to\nthe same concept. Finally, we turn the USS task into classifying the discovered\nconcepts in an unsupervised manner. Extensive experiments with state-of-the-art\nresults demonstrate the effectiveness of the proposed ACSeg.\n","authors":["Kehan Li","Zhennan Wang","Zesen Cheng","Runyi Yu","Yian Zhao","Guoli Song","Chang Liu","Li Yuan","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2210.05944v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17117v1","updated":"2023-03-30T03:09:25Z","published":"2023-03-30T03:09:25Z","title":"Learning Reliable Representations for Incomplete Multi-View Partial\n  Multi-Label Classification","summary":"  As a cross-topic of multi-view learning and multi-label classification,\nmulti-view multi-label classification has gradually gained traction in recent\nyears. The application of multi-view contrastive learning has further\nfacilitated this process, however, the existing multi-view contrastive learning\nmethods crudely separate the so-called negative pair, which largely results in\nthe separation of samples belonging to the same category or similar ones.\nBesides, plenty of multi-view multi-label learning methods ignore the possible\nabsence of views and labels. To address these issues, in this paper, we propose\nan incomplete multi-view partial multi-label classification network named RANK.\nIn this network, a label-driven multi-view contrastive learning strategy is\nproposed to leverage supervised information to preserve the structure within\nview and perform consistent alignment across views. Furthermore, we break\nthrough the view-level weights inherent in existing methods and propose a\nquality-aware sub-network to dynamically assign quality scores to each view of\neach sample. The label correlation information is fully utilized in the final\nmulti-label cross-entropy classification loss, effectively improving the\ndiscriminative power. Last but not least, our model is not only able to handle\ncomplete multi-view multi-label datasets, but also works on datasets with\nmissing instances and labels. Extensive experiments confirm that our RANK\noutperforms existing state-of-the-art methods.\n","authors":["Chengliang Liu","Jie Wen","Yong Xu","Liqiang Nie","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.17117v1.pdf","comment":"Please contact me if you have any questions: liucl1996@163.com"},{"id":"http://arxiv.org/abs/2303.17111v1","updated":"2023-03-30T02:51:52Z","published":"2023-03-30T02:51:52Z","title":"Hierarchical Fine-Grained Image Forgery Detection and Localization","summary":"  Differences in forgery attributes of images generated in CNN-synthesized and\nimage-editing domains are large, and such differences make a unified image\nforgery detection and localization (IFDL) challenging. To this end, we present\na hierarchical fine-grained formulation for IFDL representation learning.\nSpecifically, we first represent forgery attributes of a manipulated image with\nmultiple labels at different levels. Then we perform fine-grained\nclassification at these levels using the hierarchical dependency between them.\nAs a result, the algorithm is encouraged to learn both comprehensive features\nand inherent hierarchical nature of different forgery attributes, thereby\nimproving the IFDL representation. Our proposed IFDL framework contains three\ncomponents: multi-branch feature extractor, localization and classification\nmodules. Each branch of the feature extractor learns to classify forgery\nattributes at one level, while localization and classification modules segment\nthe pixel-level forgery region and detect image-level forgery, respectively.\nLastly, we construct a hierarchical fine-grained dataset to facilitate our\nstudy. We demonstrate the effectiveness of our method on $7$ different\nbenchmarks, for both tasks of IFDL and forgery attribute classification. Our\nsource code and dataset can be found:\n\\href{https://github.com/CHELSEA234/HiFi_IFDL}{github.com/CHELSEA234/HiFi-IFDL}.\n","authors":["Xiao Guo","Xiaohong Liu","Zhiyuan Ren","Steven Grosz","Iacopo Masi","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2303.17111v1.pdf","comment":"To appear at CVPR2023; 17 pages, 15 figures and 10 tables"},{"id":"http://arxiv.org/abs/2303.17099v1","updated":"2023-03-30T02:18:07Z","published":"2023-03-30T02:18:07Z","title":"BEVFusion4D: Learning LiDAR-Camera Fusion Under Bird's-Eye-View via\n  Cross-Modality Guidance and Temporal Aggregation","summary":"  Integrating LiDAR and Camera information into Bird's-Eye-View (BEV) has\nbecome an essential topic for 3D object detection in autonomous driving.\nExisting methods mostly adopt an independent dual-branch framework to generate\nLiDAR and camera BEV, then perform an adaptive modality fusion. Since point\nclouds provide more accurate localization and geometry information, they could\nserve as a reliable spatial prior to acquiring relevant semantic information\nfrom the images. Therefore, we design a LiDAR-Guided View Transformer (LGVT) to\neffectively obtain the camera representation in BEV space and thus benefit the\nwhole dual-branch fusion system. LGVT takes camera BEV as the primitive\nsemantic query, repeatedly leveraging the spatial cue of LiDAR BEV for\nextracting image features across multiple camera views. Moreover, we extend our\nframework into the temporal domain with our proposed Temporal Deformable\nAlignment (TDA) module, which aims to aggregate BEV features from multiple\nhistorical frames. Including these two modules, our framework dubbed\nBEVFusion4D achieves state-of-the-art results in 3D object detection, with\n72.0% mAP and 73.5% NDS on the nuScenes validation set, and 73.3% mAP and 74.7%\nNDS on nuScenes test set, respectively.\n","authors":["Hongxiang Cai","Zeyuan Zhang","Zhenyu Zhou","Ziyin Li","Wenbo Ding","Jiuhua Zhao"],"pdf_url":"https://arxiv.org/pdf/2303.17099v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.17096v1","updated":"2023-03-30T02:02:32Z","published":"2023-03-30T02:02:32Z","title":"ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing","summary":"  Recent studies have shown that higher accuracy on ImageNet usually leads to\nbetter robustness against different corruptions. Therefore, in this paper,\ninstead of following the traditional research paradigm that investigates new\nout-of-distribution corruptions or perturbations deep models may encounter, we\nconduct model debugging in in-distribution data to explore which object\nattributes a model may be sensitive to. To achieve this goal, we create a\ntoolkit for object editing with controls of backgrounds, sizes, positions, and\ndirections, and create a rigorous benchmark named ImageNet-E(diting) for\nevaluating the image classifier robustness in terms of object attributes. With\nour ImageNet-E, we evaluate the performance of current deep learning models,\nincluding both convolutional neural networks and vision transformers. We find\nthat most models are quite sensitive to attribute changes. A small change in\nthe background can lead to an average of 9.23\\% drop on top-1 accuracy. We also\nevaluate some robust models including both adversarially trained models and\nother robust trained models and find that some models show worse robustness\nagainst attribute changes than vanilla models. Based on these findings, we\ndiscover ways to enhance attribute robustness with preprocessing, architecture\ndesigns, and training strategies. We hope this work can provide some insights\nto the community and open up a new avenue for research in robust computer\nvision. The code and dataset are available at\nhttps://github.com/alibaba/easyrobust.\n","authors":["Xiaodan Li","Yuefeng Chen","Yao Zhu","Shuhui Wang","Rong Zhang","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2303.17096v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.17094v1","updated":"2023-03-30T01:53:14Z","published":"2023-03-30T01:53:14Z","title":"Enhanced Stable View Synthesis","summary":"  We introduce an approach to enhance the novel view synthesis from images\ntaken from a freely moving camera. The introduced approach focuses on outdoor\nscenes where recovering accurate geometric scaffold and camera pose is\nchallenging, leading to inferior results using the state-of-the-art stable view\nsynthesis (SVS) method. SVS and related methods fail for outdoor scenes\nprimarily due to (i) over-relying on the multiview stereo (MVS) for geometric\nscaffold recovery and (ii) assuming COLMAP computed camera poses as the best\npossible estimates, despite it being well-studied that MVS 3D reconstruction\naccuracy is limited to scene disparity and camera-pose accuracy is sensitive to\nkey-point correspondence selection. This work proposes a principled way to\nenhance novel view synthesis solutions drawing inspiration from the basics of\nmultiple view geometry. By leveraging the complementary behavior of MVS and\nmonocular depth, we arrive at a better scene depth per view for nearby and far\npoints, respectively. Moreover, our approach jointly refines camera poses with\nimage-based rendering via multiple rotation averaging graph optimization. The\nrecovered scene depth and the camera-pose help better view-dependent on-surface\nfeature aggregation of the entire scene. Extensive evaluation of our approach\non the popular benchmark dataset, such as Tanks and Temples, shows substantial\nimprovement in view synthesis results compared to the prior art. For instance,\nour method shows 1.5 dB of PSNR improvement on the Tank and Temples. Similar\nstatistics are observed when tested on other benchmark datasets such as FVS,\nMip-NeRF 360, and DTU.\n","authors":["Nishant Jain","Suryansh Kumar","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.17094v1.pdf","comment":"Accepted to IEEE/CVF CVPR 2023. Draft info: 13 pages, 6 Figures, 7\n  Tables"},{"id":"http://arxiv.org/abs/2212.13738v2","updated":"2023-03-30T01:42:53Z","published":"2022-12-28T08:10:31Z","title":"TempCLR: Temporal Alignment Representation with Contrastive Learning","summary":"  Video representation learning has been successful in video-text pre-training\nfor zero-shot transfer, where each sentence is trained to be close to the\npaired video clips in a common feature space. For long videos, given a\nparagraph of description where the sentences describe different segments of the\nvideo, by matching all sentence-clip pairs, the paragraph and the full video\nare aligned implicitly. However, such unit-level comparison may ignore global\ntemporal context, which inevitably limits the generalization ability. In this\npaper, we propose a contrastive learning framework TempCLR to compare the full\nvideo and the paragraph explicitly. As the video/paragraph is formulated as a\nsequence of clips/sentences, under the constraint of their temporal order, we\nuse dynamic time warping to compute the minimum cumulative cost over\nsentence-clip pairs as the sequence-level distance. To explore the temporal\ndynamics, we break the consistency of temporal succession by shuffling video\nclips w.r.t. temporal granularity. Then, we obtain the representations for\nclips/sentences, which perceive the temporal information and thus facilitate\nthe sequence alignment. In addition to pre-training on the video and paragraph,\nour approach can also generalize on the matching between video instances. We\nevaluate our approach on video retrieval, action step localization, and\nfew-shot action recognition, and achieve consistent performance gain over all\nthree tasks. Detailed ablation studies are provided to justify the approach\ndesign.\n","authors":["Yuncong Yang","Jiawei Ma","Shiyuan Huang","Long Chen","Xudong Lin","Guangxing Han","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2212.13738v2.pdf","comment":"ICLR 2023 Camera Ready. Code Link:\n  https://github.com/yyuncong/TempCLR"},{"id":"http://arxiv.org/abs/2303.17088v1","updated":"2023-03-30T01:19:27Z","published":"2023-03-30T01:19:27Z","title":"Depth-NeuS: Neural Implicit Surfaces Learning for Multi-view\n  Reconstruction Based on Depth Information Optimization","summary":"  Recently, methods for neural surface representation and rendering, for\nexample NeuS, have shown that learning neural implicit surfaces through volume\nrendering is becoming increasingly popular and making good progress. However,\nthese methods still face some challenges. Existing methods lack a direct\nrepresentation of depth information, which makes object reconstruction\nunrestricted by geometric features, resulting in poor reconstruction of objects\nwith texture and color features. This is because existing methods only use\nsurface normals to represent implicit surfaces without using depth information.\nTherefore, these methods cannot model the detailed surface features of objects\nwell. To address this problem, we propose a neural implicit surface learning\nmethod called Depth-NeuS based on depth information optimization for multi-view\nreconstruction. In this paper, we introduce depth loss to explicitly constrain\nSDF regression and introduce geometric consistency loss to optimize for\nlow-texture areas. Specific experiments show that Depth-NeuS outperforms\nexisting technologies in multiple scenarios and achieves high-quality surface\nreconstruction in multiple scenarios.\n","authors":["Hanqi Jiang","Cheng Zeng","Runnan Chen","Shuai Liang","Yinhe Han","Yichao Gao","Conglin Wang"],"pdf_url":"https://arxiv.org/pdf/2303.17088v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2202.10650v3","updated":"2023-03-30T00:51:47Z","published":"2022-02-22T03:31:33Z","title":"Movies2Scenes: Using Movie Metadata to Learn Scene Representation","summary":"  Understanding scenes in movies is crucial for a variety of applications such\nas video moderation, search, and recommendation. However, labeling individual\nscenes is a time-consuming process. In contrast, movie level metadata (e.g.,\ngenre, synopsis, etc.) regularly gets produced as part of the film production\nprocess, and is therefore significantly more commonly available. In this work,\nwe propose a novel contrastive learning approach that uses movie metadata to\nlearn a general-purpose scene representation. Specifically, we use movie\nmetadata to define a measure of movie similarity, and use it during contrastive\nlearning to limit our search for positive scene-pairs to only the movies that\nare considered similar to each other. Our learned scene representation\nconsistently outperforms existing state-of-the-art methods on a diverse set of\ntasks evaluated using multiple benchmark datasets. Notably, our learned\nrepresentation offers an average improvement of 7.9% on the seven\nclassification tasks and 9.7% improvement on the two regression tasks in LVU\ndataset. Furthermore, using a newly collected movie dataset, we present\ncomparative results of our scene representation on a set of video moderation\ntasks to demonstrate its generalizability on previously less explored tasks.\n","authors":["Shixing Chen","Chun-Hao Liu","Xiang Hao","Xiaohan Nie","Maxim Arap","Raffay Hamid"],"pdf_url":"https://arxiv.org/pdf/2202.10650v3.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17076v1","updated":"2023-03-30T00:51:12Z","published":"2023-03-30T00:51:12Z","title":"DiffCollage: Parallel Generation of Large Content with Diffusion Models","summary":"  We present DiffCollage, a compositional diffusion model that can generate\nlarge content by leveraging diffusion models trained on generating pieces of\nthe large content. Our approach is based on a factor graph representation where\neach factor node represents a portion of the content and a variable node\nrepresents their overlap. This representation allows us to aggregate\nintermediate outputs from diffusion models defined on individual nodes to\ngenerate content of arbitrary size and shape in parallel without resorting to\nan autoregressive generation procedure. We apply DiffCollage to various tasks,\nincluding infinite image generation, panorama image generation, and\nlong-duration text-guided motion generation. Extensive experimental results\nwith a comparison to strong autoregressive baselines verify the effectiveness\nof our approach.\n","authors":["Qinsheng Zhang","Jiaming Song","Xun Huang","Yongxin Chen","Ming-Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2303.17076v1.pdf","comment":"CVPR 2023 project page\n  https://research.nvidia.com/labs/dir/diffcollage"},{"id":"http://arxiv.org/abs/2303.17743v1","updated":"2023-03-30T23:30:42Z","published":"2023-03-30T23:30:42Z","title":"FairGen: Towards Fair Graph Generation","summary":"  There have been tremendous efforts over the past decades dedicated to the\ngeneration of realistic graphs in a variety of domains, ranging from social\nnetworks to computer networks, from gene regulatory networks to online\ntransaction networks. Despite the remarkable success, the vast majority of\nthese works are unsupervised in nature and are typically trained to minimize\nthe expected graph reconstruction loss, which would result in the\nrepresentation disparity issue in the generated graphs, i.e., the protected\ngroups (often minorities) contribute less to the objective and thus suffer from\nsystematically higher errors. In this paper, we aim to tailor graph generation\nto downstream mining tasks by leveraging label information and user-preferred\nparity constraint. In particular, we start from the investigation of\nrepresentation disparity in the context of graph generative models. To mitigate\nthe disparity, we propose a fairness-aware graph generative model named\nFairGen. Our model jointly trains a label-informed graph generation module and\na fair representation learning module by progressively learning the behaviors\nof the protected and unprotected groups, from the `easy' concepts to the `hard'\nones. In addition, we propose a generic context sampling strategy for graph\ngenerative models, which is proven to be capable of fairly capturing the\ncontextual information of each group with a high probability. Experimental\nresults on seven real-world data sets, including web-based graphs, demonstrate\nthat FairGen (1) obtains performance on par with state-of-the-art graph\ngenerative models across six network properties, (2) mitigates the\nrepresentation disparity issues in the generated graphs, and (3) substantially\nboosts the model performance by up to 17% in downstream tasks via data\naugmentation.\n","authors":["Lecheng Zheng","Dawei Zhou","Hanghang Tong","Jiejun Xu","Yada Zhu","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2303.17743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16630v2","updated":"2023-03-30T22:14:20Z","published":"2022-11-29T23:22:44Z","title":"DINER: Depth-aware Image-based NEural Radiance fields","summary":"  We present Depth-aware Image-based NEural Radiance fields (DINER). Given a\nsparse set of RGB input views, we predict depth and feature maps to guide the\nreconstruction of a volumetric scene representation that allows us to render 3D\nobjects under novel views. Specifically, we propose novel techniques to\nincorporate depth information into feature fusion and efficient scene sampling.\nIn comparison to the previous state of the art, DINER achieves higher synthesis\nquality and can process input views with greater disparity. This allows us to\ncapture scenes more completely without changing capturing hardware requirements\nand ultimately enables larger viewpoint changes during novel view synthesis. We\nevaluate our method by synthesizing novel views, both for human heads and for\ngeneral objects, and observe significantly improved qualitative results and\nincreased perceptual metrics compared to the previous state of the art. The\ncode is publicly available for research purposes.\n","authors":["Malte Prinzler","Otmar Hilliges","Justus Thies"],"pdf_url":"https://arxiv.org/pdf/2211.16630v2.pdf","comment":"Website: https://malteprinzler.github.io/projects/diner/diner.html ;\n  Video: https://www.youtube.com/watch?v=iI_fpjY5k8Y&t=1s"},{"id":"http://arxiv.org/abs/2211.11417v2","updated":"2023-03-30T21:56:33Z","published":"2022-11-21T13:01:52Z","title":"DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular\n  Automata","summary":"  Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic\nvideos. However, they require a slow iterative optimization process to\nsynthesize a single fixed-size short video, and they do not offer any\npost-training control over the synthesis process. We propose Dynamic Neural\nCellular Automata (DyNCA), a framework for real-time and controllable dynamic\ntexture synthesis. Our method is built upon the recently introduced NCA models\nand can synthesize infinitely long and arbitrary-sized realistic video textures\nin real time. We quantitatively and qualitatively evaluate our model and show\nthat our synthesized videos appear more realistic than the existing results. We\nimprove the SOTA DyTS performance by $2\\sim 4$ orders of magnitude. Moreover,\nour model offers several real-time video controls including motion speed,\nmotion direction, and an editing brush tool. We exhibit our trained models in\nan online interactive demo that runs on local hardware and is accessible on\npersonal computers and smartphones.\n","authors":["Ehsan Pajouheshgar","Yitao Xu","Tong Zhang","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2211.11417v2.pdf","comment":"Link to the demo: https://dynca.github.io/"},{"id":"http://arxiv.org/abs/2303.17720v1","updated":"2023-03-30T21:42:50Z","published":"2023-03-30T21:42:50Z","title":"Generating Adversarial Samples in Mini-Batches May Be Detrimental To\n  Adversarial Robustness","summary":"  Neural networks have been proven to be both highly effective within computer\nvision, and highly vulnerable to adversarial attacks. Consequently, as the use\nof neural networks increases due to their unrivaled performance, so too does\nthe threat posed by adversarial attacks. In this work, we build towards\naddressing the challenge of adversarial robustness by exploring the\nrelationship between the mini-batch size used during adversarial sample\ngeneration and the strength of the adversarial samples produced. We demonstrate\nthat an increase in mini-batch size results in a decrease in the efficacy of\nthe samples produced, and we draw connections between these observations and\nthe phenomenon of vanishing gradients. Next, we formulate loss functions such\nthat adversarial sample strength is not degraded by mini-batch size. Our\nfindings highlight a potential risk for underestimating the true (practical)\nstrength of adversarial attacks, and a risk of overestimating a model's\nrobustness. We share our codes to let others replicate our experiments and to\nfacilitate further exploration of the connections between batch size and\nadversarial sample strength.\n","authors":["Timothy Redgrave","Colton Crum"],"pdf_url":"https://arxiv.org/pdf/2303.17720v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.17719v1","updated":"2023-03-30T21:41:42Z","published":"2023-03-30T21:41:42Z","title":"Why is the winner the best?","summary":"  International benchmarking competitions have become fundamental for the\ncomparative performance assessment of image analysis methods. However, little\nattention has been given to investigating what can be learnt from these\ncompetitions. Do they really generate scientific progress? What are common and\nsuccessful participation strategies? What makes a solution superior to a\ncompeting method? To address this gap in the literature, we performed a\nmulti-center study with all 80 competitions that were conducted in the scope of\nIEEE ISBI 2021 and MICCAI 2021. Statistical analyses performed based on\ncomprehensive descriptions of the submitted algorithms linked to their rank as\nwell as the underlying participation strategies revealed common characteristics\nof winning solutions. These typically include the use of multi-task learning\n(63%) and/or multi-stage pipelines (61%), and a focus on augmentation (100%),\nimage preprocessing (97%), data curation (79%), and postprocessing (66%). The\n\"typical\" lead of a winning team is a computer scientist with a doctoral\ndegree, five years of experience in biomedical image analysis, and four years\nof experience in deep learning. Two core general development strategies stood\nout for highly-ranked teams: the reflection of the metrics in the method design\nand the focus on analyzing and handling failure cases. According to the\norganizers, 43% of the winning algorithms exceeded the state of the art but\nonly 11% completely solved the respective domain problem. The insights of our\nstudy could help researchers (1) improve algorithm development strategies when\napproaching new problems, and (2) focus on open research questions revealed by\nthis work.\n","authors":["Matthias Eisenmann","Annika Reinke","Vivienn Weru","Minu Dietlinde Tizabi","Fabian Isensee","Tim J. Adler","Sharib Ali","Vincent Andrearczyk","Marc Aubreville","Ujjwal Baid","Spyridon Bakas","Niranjan Balu","Sophia Bano","Jorge Bernal","Sebastian Bodenstedt","Alessandro Casella","Veronika Cheplygina","Marie Daum","Marleen de Bruijne","Adrien Depeursinge","Reuben Dorent","Jan Egger","David G. Ellis","Sandy Engelhardt","Melanie Ganz","Noha Ghatwary","Gabriel Girard","Patrick Godau","Anubha Gupta","Lasse Hansen","Kanako Harada","Mattias Heinrich","Nicholas Heller","Alessa Hering","Arnaud Huaulmé","Pierre Jannin","Ali Emre Kavur","Oldřich Kodym","Michal Kozubek","Jianning Li","Hongwei Li","Jun Ma","Carlos Martín-Isla","Bjoern Menze","Alison Noble","Valentin Oreiller","Nicolas Padoy","Sarthak Pati","Kelly Payette","Tim Rädsch","Jonathan Rafael-Patiño","Vivek Singh Bawa","Stefanie Speidel","Carole H. Sudre","Kimberlin van Wijnen","Martin Wagner","Donglai Wei","Amine Yamlahi","Moi Hoon Yap","Chun Yuan","Maximilian Zenk","Aneeq Zia","David Zimmerer","Dogu Baran Aydogan","Binod Bhattarai","Louise Bloch","Raphael Brüngel","Jihoon Cho","Chanyeol Choi","Qi Dou","Ivan Ezhov","Christoph M. Friedrich","Clifton Fuller","Rebati Raman Gaire","Adrian Galdran","Álvaro García Faura","Maria Grammatikopoulou","SeulGi Hong","Mostafa Jahanifar","Ikbeom Jang","Abdolrahim Kadkhodamohammadi","Inha Kang","Florian Kofler","Satoshi Kondo","Hugo Kuijf","Mingxing Li","Minh Huan Luu","Tomaž Martinčič","Pedro Morais","Mohamed A. Naser","Bruno Oliveira","David Owen","Subeen Pang","Jinah Park","Sung-Hong Park","Szymon Płotka","Elodie Puybareau","Nasir Rajpoot","Kanghyun Ryu","Numan Saeed","Adam Shephard","Pengcheng Shi","Dejan Štepec","Ronast Subedi","Guillaume Tochon","Helena R. Torres","Helene Urien","João L. Vilaça","Kareem Abdul Wahid","Haojie Wang","Jiacheng Wang","Liansheng Wang","Xiyue Wang","Benedikt Wiestler","Marek Wodzinski","Fangfang Xia","Juanying Xie","Zhiwei Xiong","Sen Yang","Yanwu Yang","Zixuan Zhao","Klaus Maier-Hein","Paul F. Jäger","Annette Kopp-Schneider","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.17719v1.pdf","comment":"accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2302.04476v2","updated":"2023-03-30T21:26:37Z","published":"2023-02-09T07:39:02Z","title":"GFM: Building Geospatial Foundation Models via Continual Pretraining","summary":"  Geospatial technologies are becoming increasingly essential in our world for\na wide range of applications, including agriculture, urban planning, and\ndisaster response. To help improve the applicability and performance of deep\nlearning models on these geospatial tasks, various works have begun\ninvestigating foundation models for this domain. Researchers have explored two\nprominent approaches for introducing such models in geospatial applications,\nbut both have drawbacks in terms of limited performance benefit or prohibitive\ntraining cost. Therefore, in this work, we propose a novel paradigm for\nbuilding highly effective geospatial foundation models with minimal resource\ncost and carbon impact. We first construct a compact yet diverse dataset from\nmultiple sources to promote feature diversity, which we term GeoPile. Then, we\ninvestigate the potential of continual pretraining from large-scale\nImageNet-22k models and propose a multi-objective continual pretraining\nparadigm, which leverages the strong representations of ImageNet while\nsimultaneously providing the freedom to learn valuable in-domain features. Our\napproach outperforms previous state-of-the-art geospatial pretraining methods\nin an extensive evaluation on seven downstream datasets covering various tasks\nsuch as change detection, classification, multi-label classification, semantic\nsegmentation, and super-resolution.\n","authors":["Matias Mendieta","Boran Han","Xingjian Shi","Yi Zhu","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2302.04476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12456v2","updated":"2023-03-30T21:20:35Z","published":"2023-01-29T14:44:27Z","title":"Towards Verifying the Geometric Robustness of Large-scale Neural\n  Networks","summary":"  Deep neural networks (DNNs) are known to be vulnerable to adversarial\ngeometric transformation. This paper aims to verify the robustness of\nlarge-scale DNNs against the combination of multiple geometric transformations\nwith a provable guarantee. Given a set of transformations (e.g., rotation,\nscaling, etc.), we develop GeoRobust, a black-box robustness analyser built\nupon a novel global optimisation strategy, for locating the worst-case\ncombination of transformations that affect and even alter a network's output.\nGeoRobust can provide provable guarantees on finding the worst-case combination\nbased on recent advances in Lipschitzian theory. Due to its black-box nature,\nGeoRobust can be deployed on large-scale DNNs regardless of their\narchitectures, activation functions, and the number of neurons. In practice,\nGeoRobust can locate the worst-case geometric transformation with high\nprecision for the ResNet50 model on ImageNet in a few seconds on average. We\nexamined 18 ImageNet classifiers, including the ResNet family and vision\ntransformers, and found a positive correlation between the geometric robustness\nof the networks and the parameter numbers. We also observe that increasing the\ndepth of DNN is more beneficial than increasing its width in terms of improving\nits geometric robustness. Our tool GeoRobust is available at\nhttps://github.com/TrustAI/GeoRobust.\n","authors":["Fu Wang","Peipei Xu","Wenjie Ruan","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2301.12456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17712v1","updated":"2023-03-30T21:10:58Z","published":"2023-03-30T21:10:58Z","title":"S-VolSDF: Sparse Multi-View Stereo Regularization of Neural Implicit\n  Surfaces","summary":"  Neural rendering of implicit surfaces performs well in 3D vision\napplications. However, it requires dense input views as supervision. When only\nsparse input images are available, output quality drops significantly due to\nthe shape-radiance ambiguity problem. We note that this ambiguity can be\nconstrained when a 3D point is visible in multiple views, as is the case in\nmulti-view stereo (MVS). We thus propose to regularize neural rendering\noptimization with an MVS solution. The use of an MVS probability volume and a\ngeneralized cross entropy loss leads to a noise-tolerant optimization process.\nIn addition, neural rendering provides global consistency constraints that\nguide the MVS depth hypothesis sampling and thus improves MVS performance.\nGiven only three sparse input views, experiments show that our method not only\noutperforms generic neural rendering models by a large margin but also\nsignificantly increases the reconstruction quality of MVS models. Project\nwebpage: https://hao-yu-wu.github.io/s-volsdf/.\n","authors":["Haoyu Wu","Alexandros Graikos","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2303.17712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17703v1","updated":"2023-03-30T20:52:08Z","published":"2023-03-30T20:52:08Z","title":"If At First You Don't Succeed: Test Time Re-ranking for Zero-shot,\n  Cross-domain Retrieval","summary":"  In this paper we propose a novel method for zero-shot, cross-domain image\nretrieval in which we make two key contributions. The first is a test-time\nre-ranking procedure that enables query-gallery pairs, without meaningful\nshared visual features, to be matched by incorporating gallery-gallery ranks\ninto an iterative re-ranking process. The second is the use of cross-attention\nat training time and knowledge distillation to encourage cross-attention-like\nfeatures to be extracted at test time from a single image. When combined with\nthe Vision Transformer architecture and zero-shot retrieval losses, our\napproach yields state-of-the-art results on the Sketchy and TU-Berlin\nsketch-based image retrieval benchmarks. However, unlike many previous methods,\nnone of the components in our approach are engineered specifically towards the\nsketch-based image retrieval task - it can be generally applied to any\ncross-domain, zero-shot retrieval task. We therefore also show results on\nzero-shot cartoon-to-photo retrieval using the Office-Home dataset.\n","authors":["Finlay G. C. Hudson","William A. P. Smith"],"pdf_url":"https://arxiv.org/pdf/2303.17703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17696v1","updated":"2023-03-30T20:24:57Z","published":"2023-03-30T20:24:57Z","title":"Dual Cross-Attention for Medical Image Segmentation","summary":"  We propose Dual Cross-Attention (DCA), a simple yet effective attention\nmodule that is able to enhance skip-connections in U-Net-based architectures\nfor medical image segmentation. DCA addresses the semantic gap between encoder\nand decoder features by sequentially capturing channel and spatial dependencies\nacross multi-scale encoder features. First, the Channel Cross-Attention (CCA)\nextracts global channel-wise dependencies by utilizing cross-attention across\nchannel tokens of multi-scale encoder features. Then, the Spatial\nCross-Attention (SCA) module performs cross-attention to capture spatial\ndependencies across spatial tokens. Finally, these fine-grained encoder\nfeatures are up-sampled and connected to their corresponding decoder parts to\nform the skip-connection scheme. Our proposed DCA module can be integrated into\nany encoder-decoder architecture with skip-connections such as U-Net and its\nvariants. We test our DCA module by integrating it into six U-Net-based\narchitectures such as U-Net, V-Net, R2Unet, ResUnet++, DoubleUnet and\nMultiResUnet. Our DCA module shows Dice Score improvements up to 2.05% on GlaS,\n2.74% on MoNuSeg, 1.37% on CVC-ClinicDB, 1.12% on Kvasir-Seg and 1.44% on\nSynapse datasets. Our codes are available at:\nhttps://github.com/gorkemcanates/Dual-Cross-Attention\n","authors":["Gorkem Can Ates","Prasoon Mohan","Emrah Celik"],"pdf_url":"https://arxiv.org/pdf/2303.17696v1.pdf","comment":"Code: https://github.com/gorkemcanates/Dual-Cross-Attention"},{"id":"http://arxiv.org/abs/2303.14152v3","updated":"2023-03-30T20:16:26Z","published":"2023-03-24T17:03:40Z","title":"Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken\n  Objects and Their Complete Counterparts","summary":"  Automated shape repair approaches currently lack access to datasets that\ndescribe real-world damaged geometry. We present Fantastic Breaks (and Where to\nFind Them:\nhttps://terascale-all-sensing-research-studio.github.io/FantasticBreaks), a\ndataset containing scanned, waterproofed, and cleaned 3D meshes for 150 broken\nobjects, paired and geometrically aligned with complete counterparts. Fantastic\nBreaks contains class and material labels, proxy repair parts that join to\nbroken meshes to generate complete meshes, and manually annotated fracture\nboundaries. Through a detailed analysis of fracture geometry, we reveal\ndifferences between Fantastic Breaks and synthetic fracture datasets generated\nusing geometric and physics-based methods. We show experimental shape repair\nevaluation with Fantastic Breaks using multiple learning-based approaches\npre-trained with synthetic datasets and re-trained with subset of Fantastic\nBreaks.\n","authors":["Nikolas Lamb","Cameron Palmer","Benjamin Molloy","Sean Banerjee","Natasha Kholgade Banerjee"],"pdf_url":"https://arxiv.org/pdf/2303.14152v3.pdf","comment":"To be published at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17688v1","updated":"2023-03-30T20:02:29Z","published":"2023-03-30T20:02:29Z","title":"Learning Garment DensePose for Robust Warping in Virtual Try-On","summary":"  Virtual try-on, i.e making people virtually try new garments, is an active\nresearch area in computer vision with great commercial applications. Current\nvirtual try-on methods usually work in a two-stage pipeline. First, the garment\nimage is warped on the person's pose using a flow estimation network. Then in\nthe second stage, the warped garment is fused with the person image to render a\nnew try-on image. Unfortunately, such methods are heavily dependent on the\nquality of the garment warping which often fails when dealing with hard poses\n(e.g., a person lifting or crossing arms). In this work, we propose a robust\nwarping method for virtual try-on based on a learned garment DensePose which\nhas a direct correspondence with the person's DensePose. Due to the lack of\nannotated data, we show how to leverage an off-the-shelf person DensePose model\nand a pretrained flow model to learn the garment DensePose in a weakly\nsupervised manner. The garment DensePose allows a robust warping to any\nperson's pose without any additional computation. Our method achieves the\nstate-of-the-art equivalent on virtual try-on benchmarks and shows warping\nrobustness on in-the-wild person images with hard poses, making it more suited\nfor real-world virtual try-on applications.\n","authors":["Aiyu Cui","Sen He","Tao Xiang","Antoine Toisoul"],"pdf_url":"https://arxiv.org/pdf/2303.17688v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2205.13452v2","updated":"2023-03-30T19:44:21Z","published":"2022-05-26T15:56:08Z","title":"Continual evaluation for lifelong learning: Identifying the stability\n  gap","summary":"  Time-dependent data-generating distributions have proven to be difficult for\ngradient-based training of neural networks, as the greedy updates result in\ncatastrophic forgetting of previously learned knowledge. Despite the progress\nin the field of continual learning to overcome this forgetting, we show that a\nset of common state-of-the-art methods still suffers from substantial\nforgetting upon starting to learn new tasks, except that this forgetting is\ntemporary and followed by a phase of performance recovery. We refer to this\nintriguing but potentially problematic phenomenon as the stability gap. The\nstability gap had likely remained under the radar due to standard practice in\nthe field of evaluating continual learning models only after each task.\nInstead, we establish a framework for continual evaluation that uses\nper-iteration evaluation and we define a new set of metrics to quantify\nworst-case performance. Empirically we show that experience replay,\nconstraint-based replay, knowledge-distillation, and parameter regularization\nmethods are all prone to the stability gap; and that the stability gap can be\nobserved in class-, task-, and domain-incremental learning benchmarks.\nAdditionally, a controlled experiment shows that the stability gap increases\nwhen tasks are more dissimilar. Finally, by disentangling gradients into\nplasticity and stability components, we propose a conceptual explanation for\nthe stability gap.\n","authors":["Matthias De Lange","Gido van de Ven","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2205.13452v2.pdf","comment":"Published as spotlight paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2212.12645v2","updated":"2023-03-30T19:38:05Z","published":"2022-12-24T03:37:02Z","title":"HandsOff: Labeled Dataset Generation With No Additional Human\n  Annotations","summary":"  Recent work leverages the expressive power of generative adversarial networks\n(GANs) to generate labeled synthetic datasets. These dataset generation methods\noften require new annotations of synthetic images, which forces practitioners\nto seek out annotators, curate a set of synthetic images, and ensure the\nquality of generated labels. We introduce the HandsOff framework, a technique\ncapable of producing an unlimited number of synthetic images and corresponding\nlabels after being trained on less than 50 pre-existing labeled images. Our\nframework avoids the practical drawbacks of prior work by unifying the field of\nGAN inversion with dataset generation. We generate datasets with rich\npixel-wise labels in multiple challenging domains such as faces, cars,\nfull-body human poses, and urban driving scenes. Our method achieves\nstate-of-the-art performance in semantic segmentation, keypoint detection, and\ndepth estimation compared to prior dataset generation approaches and transfer\nlearning baselines. We additionally showcase its ability to address broad\nchallenges in model development which stem from fixed, hand-annotated datasets,\nsuch as the long-tail problem in semantic segmentation. Project page:\naustinxu87.github.io/handsoff.\n","authors":["Austin Xu","Mariya I. Vasileva","Achal Dave","Arjun Seshadri"],"pdf_url":"https://arxiv.org/pdf/2212.12645v2.pdf","comment":"22 pages, 20 figures. CVPR 2023"},{"id":"http://arxiv.org/abs/2211.07021v2","updated":"2023-03-30T19:14:54Z","published":"2022-11-13T21:47:31Z","title":"Using Hand Pose Estimation To Automate Open Surgery Training Feedback","summary":"  Purpose: This research aims to facilitate the use of state-of-the-art\ncomputer vision algorithms for the automated training of surgeons and the\nanalysis of surgical footage. By estimating 2D hand poses, we model the\nmovement of the practitioner's hands, and their interaction with surgical\ninstruments, to study their potential benefit for surgical training.\n  Methods: We leverage pre-trained models on a publicly-available hands dataset\nto create our own in-house dataset of 100 open surgery simulation videos with\n2D hand poses. We also assess the ability of pose estimations to segment\nsurgical videos into gestures and tool-usage segments and compare them to\nkinematic sensors and I3D features. Furthermore, we introduce 6 novel surgical\ndexterity proxies stemming from domain experts' training advice, all of which\nour framework can automatically detect given raw video footage.\n  Results: State-of-the-art gesture segmentation accuracy of 88.35\\% on the\nOpen Surgery Simulation dataset is achieved with the fusion of 2D poses and I3D\nfeatures from multiple angles. The introduced surgical skill proxies presented\nsignificant differences for novices compared to experts and produced actionable\nfeedback for improvement.\n  Conclusion: This research demonstrates the benefit of pose estimations for\nopen surgery by analyzing their effectiveness in gesture segmentation and skill\nassessment. Gesture segmentation using pose estimations achieved comparable\nresults to physical sensors while being remote and markerless. Surgical\ndexterity proxies that rely on pose estimation proved they can be used to work\ntowards automated training feedback. We hope our findings encourage additional\ncollaboration on novel skill proxies to make surgical training more efficient.\n","authors":["Eddie Bkheet","Anne-Lise D'Angelo","Adam Goldbraikh","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2211.07021v2.pdf","comment":"Accepted to IPCAI 2023, 12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17658v1","updated":"2023-03-30T18:44:14Z","published":"2023-03-30T18:44:14Z","title":"Establishing baselines and introducing TernaryMixOE for fine-grained\n  out-of-distribution detection","summary":"  Machine learning models deployed in the open world may encounter observations\nthat they were not trained to recognize, and they risk misclassifying such\nobservations with high confidence. Therefore, it is essential that these models\nare able to ascertain what is in-distribution (ID) and out-of-distribution\n(OOD), to avoid this misclassification. In recent years, huge strides have been\nmade in creating models that are robust to this distinction. As a result, the\ncurrent state-of-the-art has reached near perfect performance on relatively\ncoarse-grained OOD detection tasks, such as distinguishing horses from trucks,\nwhile struggling with finer-grained classification, like differentiating models\nof commercial aircraft. In this paper, we describe a new theoretical framework\nfor understanding fine- and coarse-grained OOD detection, we re-conceptualize\nfine grained classification into a three part problem, and we propose a new\nbaseline task for OOD models on two fine-grained hierarchical data sets, two\nnew evaluation methods to differentiate fine- and coarse-grained OOD\nperformance, along with a new loss function for models in this task.\n","authors":["Noah Fleischmann","Walter Bennette","Nathan Inkawhich"],"pdf_url":"https://arxiv.org/pdf/2303.17658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12976v2","updated":"2023-03-30T18:36:33Z","published":"2023-03-23T00:55:48Z","title":"NVAutoNet: Fast and Accurate 360$^{\\circ}$ 3D Visual Perception For Self\n  Driving","summary":"  Robust real-time perception of 3D world is essential to the autonomous\nvehicle. We introduce an end-to-end surround camera perception system for\nself-driving. Our perception system is a novel multi-task, multi-camera network\nwhich takes a variable set of time-synced camera images as input and produces a\nrich collection of 3D signals such as sizes, orientations, locations of\nobstacles, parking spaces and free-spaces, etc. Our perception network is\nmodular and end-to-end: 1) the outputs can be consumed directly by downstream\nmodules without any post-processing such as clustering and fusion -- improving\nspeed of model deployment and in-car testing 2) the whole network training is\ndone in one single stage -- improving speed of model improvement and\niterations. The network is well designed to have high accuracy while running at\n53 fps on NVIDIA Orin SoC (system-on-a-chip). The network is robust to sensor\nmounting variations (within some tolerances) and can be quickly customized for\ndifferent vehicle types via efficient model fine-tuning thanks of its\ncapability of taking calibration parameters as additional inputs during\ntraining and testing. Most importantly, our network has been successfully\ndeployed and being tested on real roads.\n","authors":["Trung Pham","Mehran Maghoumi","Wanli Jiang","Bala Siva Sashank Jujjavarapu","Mehdi Sajjadi Xin Liu","Hsuan-Chu Lin","Bor-Jeng Chen","Giang Truong","Chao Fang","Junghyun Kwon","Minwoo Park"],"pdf_url":"https://arxiv.org/pdf/2303.12976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.12113v2","updated":"2023-03-30T18:36:27Z","published":"2022-10-21T17:13:14Z","title":"Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological\n  Report","summary":"  Despite the ever-increasing interest in applying deep learning (DL) models to\nmedical imaging, the typical scarcity and imbalance of medical datasets can\nseverely impact the performance of DL models. The generation of synthetic data\nthat might be freely shared without compromising patient privacy is a\nwell-known technique for addressing these difficulties. Inpainting algorithms\nare a subset of DL generative models that can alter one or more regions of an\ninput image while matching its surrounding context and, in certain cases,\nnon-imaging input conditions. Although the majority of inpainting techniques\nfor medical imaging data use generative adversarial networks (GANs), the\nperformance of these algorithms is frequently suboptimal due to their limited\noutput variety, a problem that is already well-known for GANs. Denoising\ndiffusion probabilistic models (DDPMs) are a recently introduced family of\ngenerative networks that can generate results of comparable quality to GANs,\nbut with diverse outputs. In this paper, we describe a DDPM to execute multiple\ninpainting tasks on 2D axial slices of brain MRI with various sequences, and\npresent proof-of-concept examples of its performance in a variety of evaluation\nscenarios. Our model and a public online interface to try our tool are\navailable at: https://github.com/Mayo-Radiology-Informatics-Lab/MBTI\n","authors":["Pouria Rouzrokh","Bardia Khosravi","Shahriar Faghani","Mana Moassefi","Sanaz Vahdati","Bradley J. Erickson"],"pdf_url":"https://arxiv.org/pdf/2210.12113v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2301.08243v2","updated":"2023-03-30T18:28:46Z","published":"2023-01-19T18:59:01Z","title":"Self-Supervised Learning from Images with a Joint-Embedding Predictive\n  Architecture","summary":"  This paper demonstrates an approach for learning highly semantic image\nrepresentations without relying on hand-crafted data-augmentations. We\nintroduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a\nnon-generative approach for self-supervised learning from images. The idea\nbehind I-JEPA is simple: from a single context block, predict the\nrepresentations of various target blocks in the same image. A core design\nchoice to guide I-JEPA towards producing semantic representations is the\nmasking strategy; specifically, it is crucial to (a) sample target blocks with\nsufficiently large scale (semantic), and to (b) use a sufficiently informative\n(spatially distributed) context block. Empirically, when combined with Vision\nTransformers, we find I-JEPA to be highly scalable. For instance, we train a\nViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong\ndownstream performance across a wide range of tasks, from linear classification\nto object counting and depth prediction.\n","authors":["Mahmoud Assran","Quentin Duval","Ishan Misra","Piotr Bojanowski","Pascal Vincent","Michael Rabbat","Yann LeCun","Nicolas Ballas"],"pdf_url":"https://arxiv.org/pdf/2301.08243v2.pdf","comment":"2023 IEEE/CVF International Conference on Computer Vision"},{"id":"http://arxiv.org/abs/2303.17647v1","updated":"2023-03-30T18:24:06Z","published":"2023-03-30T18:24:06Z","title":"Detecting and Grounding Important Characters in Visual Stories","summary":"  Characters are essential to the plot of any story. Establishing the\ncharacters before writing a story can improve the clarity of the plot and the\noverall flow of the narrative. However, previous work on visual storytelling\ntends to focus on detecting objects in images and discovering relationships\nbetween them. In this approach, characters are not distinguished from other\nobjects when they are fed into the generation pipeline. The result is a\ncoherent sequence of events rather than a character-centric story. In order to\naddress this limitation, we introduce the VIST-Character dataset, which\nprovides rich character-centric annotations, including visual and textual\nco-reference chains and importance ratings for characters. Based on this\ndataset, we propose two new tasks: important character detection and character\ngrounding in visual stories. For both tasks, we develop simple, unsupervised\nmodels based on distributional similarity and pre-trained vision-and-language\nmodels. Our new dataset, together with these models, can serve as the\nfoundation for subsequent work on analysing and generating stories from a\ncharacter-centric perspective.\n","authors":["Danyang Liu","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2303.17647v1.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.17646v1","updated":"2023-03-30T18:23:20Z","published":"2023-03-30T18:23:20Z","title":"XPert: Peripheral Circuit & Neural Architecture Co-search for Area and\n  Energy-efficient Xbar-based Computing","summary":"  The hardware-efficiency and accuracy of Deep Neural Networks (DNNs)\nimplemented on In-memory Computing (IMC) architectures primarily depend on the\nDNN architecture and the peripheral circuit parameters. It is therefore\nessential to holistically co-search the network and peripheral parameters to\nachieve optimal performance. To this end, we propose XPert, which co-searches\nnetwork architecture in tandem with peripheral parameters such as the type and\nprecision of analog-to-digital converters, crossbar column sharing and the\nlayer-specific input precision using an optimization-based design space\nexploration. Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lower\nEDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%)\naccuracy for CIFAR10 (TinyImagenet) datasets. The code for this paper is\navailable at https://github.com/Intelligent-Computing-Lab-Yale/XPert.\n","authors":["Abhishek Moitra","Abhiroop Bhattacharjee","Youngeun Kim","Priyadarshini Panda"],"pdf_url":"https://arxiv.org/pdf/2303.17646v1.pdf","comment":"Accepted to Design and Automation Conference (DAC)"},{"id":"http://arxiv.org/abs/2303.17644v1","updated":"2023-03-30T18:20:00Z","published":"2023-03-30T18:20:00Z","title":"Vision-Language Modelling For Radiological Imaging and Reports In The\n  Low Data Regime","summary":"  This paper explores training medical vision-language models (VLMs) -- where\nthe visual and language inputs are embedded into a common space -- with a\nparticular focus on scenarios where training data is limited, as is often the\ncase in clinical datasets. We explore several candidate methods to improve\nlow-data performance, including: (i) adapting generic pre-trained models to\nnovel image and text domains (i.e. medical imaging and reports) via unimodal\nself-supervision; (ii) using local (e.g. GLoRIA) & global (e.g. InfoNCE)\ncontrastive loss functions as well as a combination of the two; (iii) extra\nsupervision during VLM training, via: (a) image- and text-only\nself-supervision, and (b) creating additional positive image-text pairs for\ntraining through augmentation and nearest-neighbour search.\n  Using text-to-image retrieval as a benchmark, we evaluate the performance of\nthese methods with variable sized training datasets of paired chest X-rays and\nradiological reports. Combined, they significantly improve retrieval compared\nto fine-tuning CLIP, roughly equivalent to training with the data. A similar\npattern is found in the downstream task classification of CXR-related\nconditions with our method outperforming CLIP and also BioVIL, a strong CXR VLM\nbenchmark, in the zero-shot and linear probing settings. We conclude with a set\nof recommendations for researchers aiming to train vision-language models on\nother medical imaging modalities when training data is scarce. To facilitate\nfurther research, we will make our code and models publicly available.\n","authors":["Rhydian Windsor","Amir Jamaludin","Timor Kadir","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2303.17644v1.pdf","comment":"Accepted to MIDL 2023"},{"id":"http://arxiv.org/abs/2303.17636v1","updated":"2023-03-30T18:01:26Z","published":"2023-03-30T18:01:26Z","title":"Whether and When does Endoscopy Domain Pretraining Make Sense?","summary":"  Automated endoscopy video analysis is a challenging task in medical computer\nvision, with the primary objective of assisting surgeons during procedures. The\ndifficulty arises from the complexity of surgical scenes and the lack of a\nsufficient amount of annotated data. In recent years, large-scale pretraining\nhas shown great success in natural language processing and computer vision\ncommunities. These approaches reduce the need for annotated data, which is\nalways a concern in the medical domain. However, most works on endoscopic video\nunderstanding use models pretrained on natural images, creating a domain gap\nbetween pretraining and finetuning. In this work, we investigate the need for\nendoscopy domain-specific pretraining based on downstream objectives. To this\nend, we first collect Endo700k, the largest publicly available corpus of\nendoscopic images, extracted from nine public Minimally Invasive Surgery (MIS)\ndatasets. Endo700k comprises more than 700,000 unannotated raw images. Next, we\nintroduce EndoViT, an endoscopy pretrained Vision Transformer (ViT). Through\nablations, we demonstrate that domain-specific pretraining is particularly\nbeneficial for more complex downstream tasks, such as Action Triplet Detection,\nand less effective and even unnecessary for simpler tasks, such as Surgical\nPhase Recognition. We will release both our code and pretrained models upon\nacceptance to facilitate further research in this direction.\n","authors":["Dominik Batić","Felix Holm","Ege Özsoy","Tobias Czempiel","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2303.17636v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2303.17431v1","updated":"2023-03-30T14:55:43Z","published":"2023-03-30T14:55:43Z","title":"An evaluation framework for comparing epidemic intelligence systems","summary":"  In the context of Epidemic Intelligence, many Event-Based Surveillance (EBS)\nsystems have been proposed in the literature to promote the early\nidentification and characterization of potential health threats from online\nsources of any nature. Each EBS system has its own surveillance definitions and\npriorities, therefore this makes the task of selecting the most appropriate EBS\nsystem for a given situation a challenge for end-users. In this work, we\npropose a new evaluation framework to address this issue. It first transforms\nthe raw input epidemiological event data into a set of normalized events with\nmulti-granularity, then conducts a descriptive retrospective analysis based on\nfour evaluation objectives: spatial, temporal, thematic and source analysis. We\nillustrate its relevance by applying it to an Avian Influenza dataset collected\nby a selection of EBS systems, and show how our framework allows identifying\ntheir strengths and drawbacks in terms of epidemic surveillance.\n","authors":["Nejat Arinik","Roberto Interdonato","Mathieu Roche","Maguelonne Teisseire"],"pdf_url":"https://arxiv.org/pdf/2303.17431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17371v1","updated":"2023-03-30T13:35:46Z","published":"2023-03-30T13:35:46Z","title":"Methods and advancement of content-based fashion image retrieval: A\n  Review","summary":"  Content-based fashion image retrieval (CBFIR) has been widely used in our\ndaily life for searching fashion images or items from online platforms. In\ne-commerce purchasing, the CBFIR system can retrieve fashion items or products\nwith the same or comparable features when a consumer uploads a reference image,\nimage with text, sketch or visual stream from their daily life. This lowers the\nCBFIR system reliance on text and allows for a more accurate and direct\nsearching of the desired fashion product. Considering recent developments,\nCBFIR still has limits when it comes to visual searching in the real world due\nto the simultaneous availability of multiple fashion items, occlusion of\nfashion products, and shape deformation. This paper focuses on CBFIR methods\nwith the guidance of images, images with text, sketches, and videos.\nAccordingly, we categorized CBFIR methods into four main categories, i.e.,\nimage-guided CBFIR (with the addition of attributes and styles), image and\ntext-guided, sketch-guided, and video-guided CBFIR methods. The baseline\nmethodologies have been thoroughly analyzed, and the most recent developments\nin CBFIR over the past six years (2017 to 2022) have been thoroughly examined.\nFinally, key issues are highlighted for CBFIR with promising directions for\nfuture research.\n","authors":["Amin Muhammad Shoib","Jabeen Summaira","Changbo Wang","Abdul Jabbar"],"pdf_url":"https://arxiv.org/pdf/2303.17371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17322v1","updated":"2023-03-30T12:23:39Z","published":"2023-03-30T12:23:39Z","title":"Yes but.. Can ChatGPT Identify Entities in Historical Documents?","summary":"  Large language models (LLMs) have been leveraged for several years now,\nobtaining state-of-the-art performance in recognizing entities from modern\ndocuments. For the last few months, the conversational agent ChatGPT has\n\"prompted\" a lot of interest in the scientific community and public due to its\ncapacity of generating plausible-sounding answers. In this paper, we explore\nthis ability by probing it in the named entity recognition and classification\n(NERC) task in primary sources (e.g., historical newspapers and classical\ncommentaries) in a zero-shot manner and by comparing it with state-of-the-art\nLM-based systems. Our findings indicate several shortcomings in identifying\nentities in historical text that range from the consistency of entity\nannotation guidelines, entity complexity, and code-switching, to the\nspecificity of prompting. Moreover, as expected, the inaccessibility of\nhistorical archives to the public (and thus on the Internet) also impacts its\nperformance.\n","authors":["Carlos-Emiliano González-Gallardo","Emanuela Boros","Nancy Girdhar","Ahmed Hamdi","Jose G. Moreno","Antoine Doucet"],"pdf_url":"https://arxiv.org/pdf/2303.17322v1.pdf","comment":"5 pages, accepted to JCDL2023"},{"id":"http://arxiv.org/abs/2112.13747v6","updated":"2023-03-30T10:50:25Z","published":"2021-12-27T15:49:25Z","title":"MOEF: Modeling Occasion Evolution in Frequency Domain for\n  Promotion-Aware Click-Through Rate Prediction","summary":"  Promotions are becoming more important and prevalent in e-commerce to attract\ncustomers and boost sales, leading to frequent changes of occasions, which\ndrives users to behave differently. In such situations, most existing\nClick-Through Rate (CTR) models can't generalize well to online serving due to\ndistribution uncertainty of the upcoming occasion. In this paper, we propose a\nnovel CTR model named MOEF for recommendations under frequent changes of\noccasions. Firstly, we design a time series that consists of occasion signals\ngenerated from the online business scenario. Since occasion signals are more\ndiscriminative in the frequency domain, we apply Fourier Transformation to\nsliding time windows upon the time series, obtaining a sequence of frequency\nspectrum which is then processed by Occasion Evolution Layer (OEL). In this\nway, a high-order occasion representation can be learned to handle the online\ndistribution uncertainty. Moreover, we adopt multiple experts to learn feature\nrepresentations from multiple aspects, which are guided by the occasion\nrepresentation via an attention mechanism. Accordingly, a mixture of feature\nrepresentations is obtained adaptively for different occasions to predict the\nfinal CTR. Experimental results on real-world datasets validate the superiority\nof MOEF and online A/B tests also show MOEF outperforms representative CTR\nmodels significantly.\n","authors":["Xiaofeng Pan","Yibin Shen","Jing Zhang","Xu He","Yang Huang","Hong Wen","Chengjun Mao","Bo Cao"],"pdf_url":"https://arxiv.org/pdf/2112.13747v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12700v3","updated":"2023-03-30T09:18:52Z","published":"2023-01-30T07:12:38Z","title":"CSDR-BERT: a pre-trained scientific dataset match model for Chinese\n  Scientific Dataset Retrieval","summary":"  As the number of open and shared scientific datasets on the Internet\nincreases under the open science movement, efficiently retrieving these\ndatasets is a crucial task in information retrieval (IR) research. In recent\nyears, the development of large models, particularly the pre-training and\nfine-tuning paradigm, which involves pre-training on large models and\nfine-tuning on downstream tasks, has provided new solutions for IR match tasks.\nIn this study, we use the original BERT token in the embedding layer, improve\nthe Sentence-BERT model structure in the model layer by introducing the SimCSE\nand K-Nearest Neighbors method, and use the cosent loss function in the\noptimization phase to optimize the target output. Our experimental results show\nthat our model outperforms other competing models on both public and self-built\ndatasets through comparative experiments and ablation implementations. This\nstudy explores and validates the feasibility and efficiency of pre-training\ntechniques for semantic retrieval of Chinese scientific datasets.\n","authors":["Xintao Chu","Jianping Liu","Jian Wang","Xiaofeng Wang","Yingfei Wang","Meng Wang","Xunxun Gu"],"pdf_url":"https://arxiv.org/pdf/2301.12700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17695v1","updated":"2023-03-30T20:23:49Z","published":"2023-03-30T20:23:49Z","title":"Task Oriented Conversational Modelling With Subjective Knowledge","summary":"  Existing conversational models are handled by a database(DB) and API based\nsystems. However, very often users' questions require information that cannot\nbe handled by such systems. Nonetheless, answers to these questions are\navailable in the form of customer reviews and FAQs. DSTC-11 proposes a three\nstage pipeline consisting of knowledge seeking turn detection, knowledge\nselection and response generation to create a conversational model grounded on\nthis subjective knowledge. In this paper, we focus on improving the knowledge\nselection module to enhance the overall system performance. In particular, we\npropose entity retrieval methods which result in an accurate and faster\nknowledge search. Our proposed Named Entity Recognition (NER) based entity\nretrieval method results in 7X faster search compared to the baseline model.\nAdditionally, we also explore a potential keyword extraction method which can\nimprove the accuracy of knowledge selection. Preliminary results show a 4 \\%\nimprovement in exact match score on knowledge selection task. The code is\navailable https://github.com/raja-kumar/knowledge-grounded-TODS\n","authors":["Raja Kumar"],"pdf_url":"https://arxiv.org/pdf/2303.17695v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2211.09790v2","updated":"2023-03-30T17:59:16Z","published":"2022-11-17T18:57:03Z","title":"ConStruct-VL: Data-Free Continual Structured VL Concepts Learning","summary":"  Recently, large-scale pre-trained Vision-and-Language (VL) foundation models\nhave demonstrated remarkable capabilities in many zero-shot downstream tasks,\nachieving competitive results for recognizing objects defined by as little as\nshort text prompts. However, it has also been shown that VL models are still\nbrittle in Structured VL Concept (SVLC) reasoning, such as the ability to\nrecognize object attributes, states, and inter-object relations. This leads to\nreasoning mistakes, which need to be corrected as they occur by teaching VL\nmodels the missing SVLC skills; often this must be done using private data\nwhere the issue was found, which naturally leads to a data-free continual (no\ntask-id) VL learning setting. In this work, we introduce the first Continual\nData-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it\nis challenging for many existing data-free CL strategies. We, therefore,\npropose a data-free method comprised of a new approach of Adversarial\nPseudo-Replay (APR) which generates adversarial reminders of past tasks from\npast task models. To use this method efficiently, we also propose a continual\nparameter-efficient Layered-LoRA (LaLo) neural architecture allowing\nno-memory-cost access to all past models at train time. We show this approach\noutperforms all data-free methods by as much as ~7% while even matching some\nlevels of experience-replay (prohibitive for applications where data-privacy\nmust be preserved). Our code is publicly available at\nhttps://github.com/jamessealesmith/ConStruct-VL\n","authors":["James Seale Smith","Paola Cascante-Bonilla","Assaf Arbelle","Donghyun Kim","Rameswar Panda","David Cox","Diyi Yang","Zsolt Kira","Rogerio Feris","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2211.09790v2.pdf","comment":"Accepted by the 2023 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR 2023)"},{"id":"http://arxiv.org/abs/2303.17595v1","updated":"2023-03-30T17:59:02Z","published":"2023-03-30T17:59:02Z","title":"Neglected Free Lunch -- Learning Image Classifiers Using Annotation\n  Byproducts","summary":"  Supervised learning of image classifiers distills human knowledge into a\nparametric model through pairs of images and corresponding labels (X,Y). We\nargue that this simple and widely used representation of human knowledge\nneglects rich auxiliary information from the annotation procedure, such as the\ntime-series of mouse traces and clicks left after image selection. Our insight\nis that such annotation byproducts Z provide approximate human attention that\nweakly guides the model to focus on the foreground cues, reducing spurious\ncorrelations and discouraging shortcut learning. To verify this, we create\nImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with\nsample-wise annotation byproducts, collected by replicating the respective\noriginal annotation tasks. We refer to the new paradigm of training models with\nannotation byproducts as learning using annotation byproducts (LUAB). We show\nthat a simple multitask loss for regressing Z together with Y already improves\nthe generalisability and robustness of the learned models. Compared to the\noriginal supervised learning, LUAB does not require extra annotation costs.\nImageNet-AB and COCO-AB are at https://github.com/naver-ai/NeglectedFreeLunch.\n","authors":["Dongyoon Han","Junsuk Choe","Seonghyeok Chun","John Joon Young Chung","Minsuk Chang","Sangdoo Yun","Jean Y. Song","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2303.17595v1.pdf","comment":"Code at https://github.com/naver-ai/NeglectedFreeLunch"},{"id":"http://arxiv.org/abs/2211.13218v2","updated":"2023-03-30T17:58:59Z","published":"2022-11-23T18:57:11Z","title":"CODA-Prompt: COntinual Decomposed Attention-based Prompting for\n  Rehearsal-Free Continual Learning","summary":"  Computer vision models suffer from a phenomenon known as catastrophic\nforgetting when learning novel concepts from continuously shifting training\ndata. Typical solutions for this continual learning problem require extensive\nrehearsal of previously seen data, which increases memory costs and may violate\ndata privacy. Recently, the emergence of large-scale pre-trained vision\ntransformer models has enabled prompting approaches as an alternative to\ndata-rehearsal. These approaches rely on a key-query mechanism to generate\nprompts and have been found to be highly resistant to catastrophic forgetting\nin the well-established rehearsal-free continual learning setting. However, the\nkey mechanism of these methods is not trained end-to-end with the task\nsequence. Our experiments show that this leads to a reduction in their\nplasticity, hence sacrificing new task accuracy, and inability to benefit from\nexpanded parameter capacity. We instead propose to learn a set of prompt\ncomponents which are assembled with input-conditioned weights to produce\ninput-conditioned prompts, resulting in a novel attention-based end-to-end\nkey-query scheme. Our experiments show that we outperform the current SOTA\nmethod DualPrompt on established benchmarks by as much as 4.5% in average final\naccuracy. We also outperform the state of art by as much as 4.4% accuracy on a\ncontinual learning benchmark which contains both class-incremental and\ndomain-incremental task shifts, corresponding to many practical settings. Our\ncode is available at https://github.com/GT-RIPL/CODA-Prompt\n","authors":["James Seale Smith","Leonid Karlinsky","Vyshnavi Gutta","Paola Cascante-Bonilla","Donghyun Kim","Assaf Arbelle","Rameswar Panda","Rogerio Feris","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2211.13218v2.pdf","comment":"Accepted by the 2023 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR 2023)"},{"id":"http://arxiv.org/abs/2303.17592v1","updated":"2023-03-30T17:58:36Z","published":"2023-03-30T17:58:36Z","title":"Learning Human-to-Robot Handovers from Point Clouds","summary":"  We propose the first framework to learn control policies for vision-based\nhuman-to-robot handovers, a critical task for human-robot interaction. While\nresearch in Embodied AI has made significant progress in training robot agents\nin simulated environments, interacting with humans remains challenging due to\nthe difficulties of simulating humans. Fortunately, recent research has\ndeveloped realistic simulated environments for human-to-robot handovers.\nLeveraging this result, we introduce a method that is trained with a\nhuman-in-the-loop via a two-stage teacher-student framework that uses motion\nand grasp planning, reinforcement learning, and self-supervision. We show\nsignificant performance gains over baselines on a simulation benchmark,\nsim-to-sim transfer and sim-to-real transfer.\n","authors":["Sammy Christen","Wei Yang","Claudia Pérez-D'Arpino","Otmar Hilliges","Dieter Fox","Yu-Wei Chao"],"pdf_url":"https://arxiv.org/pdf/2303.17592v1.pdf","comment":"Accepted at CVPR 2023 as highlight. Project page at\n  https://handover-sim2real.github.io"},{"id":"http://arxiv.org/abs/2303.17591v1","updated":"2023-03-30T17:58:11Z","published":"2023-03-30T17:58:11Z","title":"Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models","summary":"  The unlearning problem of deep learning models, once primarily an academic\nconcern, has become a prevalent issue in the industry. The significant advances\nin text-to-image generation techniques have prompted global discussions on\nprivacy, copyright, and safety, as numerous unauthorized personal IDs, content,\nartistic creations, and potentially harmful materials have been learned by\nthese models and later utilized to generate and distribute uncontrolled\ncontent. To address this challenge, we propose \\textbf{Forget-Me-Not}, an\nefficient and low-cost solution designed to safely remove specified IDs,\nobjects, or styles from a well-configured text-to-image model in as little as\n30 seconds, without impairing its ability to generate other content. Alongside\nour method, we introduce the \\textbf{Memorization Score (M-Score)} and\n\\textbf{ConceptBench} to measure the models' capacity to generate general\nconcepts, grouped into three primary categories: ID, object, and style. Using\nM-Score and ConceptBench, we demonstrate that Forget-Me-Not can effectively\neliminate targeted concepts while maintaining the model's performance on other\nconcepts. Furthermore, Forget-Me-Not offers two practical extensions: a)\nremoval of potentially harmful or NSFW content, and b) enhancement of model\naccuracy, inclusion and diversity through \\textbf{concept correction and\ndisentanglement}. It can also be adapted as a lightweight model patch for\nStable Diffusion, allowing for concept manipulation and convenient\ndistribution. To encourage future research in this critical area and promote\nthe development of safe and inclusive generative models, we will open-source\nour code and ConceptBench at\n\\href{https://github.com/SHI-Labs/Forget-Me-Not}{https://github.com/SHI-Labs/Forget-Me-Not}.\n","authors":["Eric Zhang","Kai Wang","Xingqian Xu","Zhangyang Wang","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2303.17591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.09184v2","updated":"2023-03-30T17:50:19Z","published":"2022-10-17T15:37:04Z","title":"Packed-Ensembles for Efficient Uncertainty Estimation","summary":"  Deep Ensembles (DE) are a prominent approach for achieving excellent\nperformance on key metrics such as accuracy, calibration, uncertainty\nestimation, and out-of-distribution detection. However, hardware limitations of\nreal-world systems constrain to smaller ensembles and lower-capacity networks,\nsignificantly deteriorating their performance and properties. We introduce\nPacked-Ensembles (PE), a strategy to design and train lightweight structured\nensembles by carefully modulating the dimension of their encoding space. We\nleverage grouped convolutions to parallelize the ensemble into a single shared\nbackbone and forward pass to improve training and inference speeds. PE is\ndesigned to operate within the memory limits of a standard neural network. Our\nextensive research indicates that PE accurately preserves the properties of DE,\nsuch as diversity, and performs equally well in terms of accuracy, calibration,\nout-of-distribution detection, and robustness to distribution shift. We make\nour code available at https://github.com/ENSTA-U2IS/torch-uncertainty.\n","authors":["Olivier Laurent","Adrien Lafage","Enzo Tartaglione","Geoffrey Daniel","Jean-Marc Martinez","Andrei Bursuc","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2210.09184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17580v1","updated":"2023-03-30T17:48:28Z","published":"2023-03-30T17:48:28Z","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace","summary":"  Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence (AGI). While there are abundant AI\nmodels available for different domains and modalities, they cannot handle\ncomplicated AI tasks. Considering large language models (LLMs) have exhibited\nexceptional ability in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks and language could be a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, a\nsystem that leverages LLMs (e.g., ChatGPT) to connect various AI models in\nmachine learning communities (e.g., HuggingFace) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHuggingFace, execute each subtask with the selected AI model, and summarize the\nresponse according to the execution results. By leveraging the strong language\ncapability of ChatGPT and abundant AI models in HuggingFace, HuggingGPT is able\nto cover numerous sophisticated AI tasks in different modalities and domains\nand achieve impressive results in language, vision, speech, and other\nchallenging tasks, which paves a new way towards AGI.\n","authors":["Yongliang Shen","Kaitao Song","Xu Tan","Dongsheng Li","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2303.17580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17578v1","updated":"2023-03-30T17:46:50Z","published":"2023-03-30T17:46:50Z","title":"Online Learning and Disambiguations of Partial Concept Classes","summary":"  In a recent article, Alon, Hanneke, Holzman, and Moran (FOCS '21) introduced\na unifying framework to study the learnability of classes of partial concepts.\nOne of the central questions studied in their work is whether the learnability\nof a partial concept class is always inherited from the learnability of some\n``extension'' of it to a total concept class.\n  They showed this is not the case for PAC learning but left the problem open\nfor the stronger notion of online learnability.\n  We resolve this problem by constructing a class of partial concepts that is\nonline learnable, but no extension of it to a class of total concepts is online\nlearnable (or even PAC learnable).\n","authors":["Tsun-Ming Cheung","Hamed Hatami","Pooya Hatami","Kaave Hosseini"],"pdf_url":"https://arxiv.org/pdf/2303.17578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17574v1","updated":"2023-03-30T17:40:30Z","published":"2023-03-30T17:40:30Z","title":"Elastic Weight Removal for Faithful and Abstractive Dialogue Generation","summary":"  Ideally, dialogue systems should generate responses that are faithful to the\nknowledge contained in relevant documents. However, many models generate\nhallucinated responses instead that contradict it or contain unverifiable\ninformation. To mitigate such undesirable behaviour, it has been proposed to\nfine-tune a `negative expert' on negative examples and subtract its parameters\nfrom those of a pre-trained model. However, intuitively, this does not take\ninto account that some parameters are more responsible than others in causing\nhallucinations. Thus, we propose to weigh their individual importance via (an\napproximation of) the Fisher Information matrix, which measures the uncertainty\nof their estimate. We call this method Elastic Weight Removal (EWR). We\nevaluate our method -- using different variants of Flan-T5 as a backbone\nlanguage model -- on multiple datasets for information-seeking dialogue\ngeneration and compare our method with state-of-the-art techniques for\nfaithfulness, such as CTRL, Quark, DExperts, and Noisy Channel reranking.\nExtensive automatic and human evaluation shows that EWR systematically\nincreases faithfulness at minor costs in terms of other metrics. However, we\nnotice that only discouraging hallucinations may increase extractiveness, i.e.\nshallow copy-pasting of document spans, which can be undesirable. Hence, as a\nsecond main contribution, we show that our method can be extended to\nsimultaneously discourage hallucinations and extractive responses. We publicly\nrelease the code for reproducing EWR and all baselines.\n","authors":["Nico Daheim","Nouha Dziri","Mrinmaya Sachan","Iryna Gurevych","Edoardo M. Ponti"],"pdf_url":"https://arxiv.org/pdf/2303.17574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17573v1","updated":"2023-03-30T17:40:14Z","published":"2023-03-30T17:40:14Z","title":"Using AI to Measure Parkinson's Disease Severity at Home","summary":"  We present an artificial intelligence system to remotely assess the motor\nperformance of individuals with Parkinson's disease (PD). Participants\nperformed a motor task (i.e., tapping fingers) in front of a webcam, and data\nfrom 250 global participants were rated by three expert neurologists following\nthe Movement Disorder Society Unified Parkinson's Disease Rating Scale\n(MDS-UPDRS). The neurologists' ratings were highly reliable, with an\nintra-class correlation coefficient (ICC) of 0.88. We developed computer\nalgorithms to obtain objective measurements that align with the MDS-UPDRS\nguideline and are strongly correlated with the neurologists' ratings. Our\nmachine learning model trained on these measures outperformed an MDS-UPDRS\ncertified rater, with a mean absolute error (MAE) of 0.59 compared to the\nrater's MAE of 0.79. However, the model performed slightly worse than the\nexpert neurologists (0.53 MAE). The methodology can be replicated for similar\nmotor tasks, providing the possibility of evaluating individuals with PD and\nother movement disorders remotely, objectively, and in areas with limited\naccess to neurological care.\n","authors":["Md Saiful Islam","Wasifur Rahman","Abdelrahman Abdelkader","Phillip T. Yang","Sangwu Lee","Jamie L. Adams","Ruth B. Schneider","E. Ray Dorsey","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2303.17573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17568v1","updated":"2023-03-30T17:34:01Z","published":"2023-03-30T17:34:01Z","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual\n  Evaluations on HumanEval-X","summary":"  Large pre-trained code generation models, such as OpenAI Codex, can generate\nsyntax- and function-correct code, making the coding of programmers more\nproductive and our pursuit of artificial general intelligence closer. In this\npaper, we introduce CodeGeeX, a multilingual model with 13 billion parameters\nfor code generation. CodeGeeX is pre-trained on 850 billion tokens of 23\nprogramming languages as of June 2022. Our extensive experiments suggest that\nCodeGeeX outperforms multilingual code models of similar scale for both the\ntasks of code generation and translation on HumanEval-X. Building upon\nHumanEval (Python only), we develop the HumanEval-X benchmark for evaluating\nmultilingual models by hand-writing the solutions in C++, Java, JavaScript, and\nGo. In addition, we build CodeGeeX-based extensions on Visual Studio Code,\nJetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of\nthousands of active users per week. Our user study demonstrates that CodeGeeX\ncan help to increase coding efficiency for 83.4% of its users. Finally,\nCodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code,\nmodel weights (the version of 850B tokens), API, extensions, and HumanEval-X at\nhttps://github.com/THUDM/CodeGeeX.\n","authors":["Qinkai Zheng","Xiao Xia","Xu Zou","Yuxiao Dong","Shan Wang","Yufei Xue","Zihan Wang","Lei Shen","Andi Wang","Yang Li","Teng Su","Zhilin Yang","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2303.17568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17566v1","updated":"2023-03-30T17:30:42Z","published":"2023-03-30T17:30:42Z","title":"Non-Invasive Fairness in Learning through the Lens of Data Drift","summary":"  Machine Learning (ML) models are widely employed to drive many modern data\nsystems. While they are undeniably powerful tools, ML models often demonstrate\nimbalanced performance and unfair behaviors. The root of this problem often\nlies in the fact that different subpopulations commonly display divergent\ntrends: as a learning algorithm tries to identify trends in the data, it\nnaturally favors the trends of the majority groups, leading to a model that\nperforms poorly and unfairly for minority populations. Our goal is to improve\nthe fairness and trustworthiness of ML models by applying only non-invasive\ninterventions, i.e., without altering the data or the learning algorithm. We\nuse a simple but key insight: the divergence of trends between different\npopulations, and, consecutively, between a learned model and minority\npopulations, is analogous to data drift, which indicates the poor conformance\nbetween parts of the data and the trained model. We explore two strategies\n(model-splitting and reweighing) to resolve this drift, aiming to improve the\noverall conformance of models to the underlying data. Both our methods\nintroduce novel ways to employ the recently-proposed data profiling primitive\nof Conformance Constraints. Our experimental evaluation over 7 real-world\ndatasets shows that both DifFair and ConFair improve the fairness of ML models.\n","authors":["Ke Yang","Alexandra Meliou"],"pdf_url":"https://arxiv.org/pdf/2303.17566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17564v1","updated":"2023-03-30T17:30:36Z","published":"2023-03-30T17:30:36Z","title":"BloombergGPT: A Large Language Model for Finance","summary":"  The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. As a next step, we plan to release\ntraining logs (Chronicles) detailing our experience in training BloombergGPT.\n","authors":["Shijie Wu","Ozan Irsoy","Steven Lu","Vadim Dabravolski","Mark Dredze","Sebastian Gehrmann","Prabhanjan Kambadur","David Rosenberg","Gideon Mann"],"pdf_url":"https://arxiv.org/pdf/2303.17564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17557v1","updated":"2023-03-30T17:26:16Z","published":"2023-03-30T17:26:16Z","title":"Recognition, recall, and retention of few-shot memories in large\n  language models","summary":"  The training of modern large language models (LLMs) takes place in a regime\nwhere most training examples are seen only a few times by the model during the\ncourse of training. What does a model remember about such examples seen only a\nfew times during training and how long does that memory persist in the face of\ncontinuous training with new examples? Here, we investigate these questions\nthrough simple recognition, recall, and retention experiments with LLMs. In\nrecognition experiments, we ask if the model can distinguish the seen example\nfrom a novel example; in recall experiments, we ask if the model can correctly\nrecall the seen example when cued by a part of it; and in retention\nexperiments, we periodically probe the model's memory for the original examples\nas the model is trained continuously with new examples. We find that a single\nexposure is generally sufficient for a model to achieve near perfect accuracy\neven in very challenging recognition experiments. We estimate that the\nrecognition performance of even small language models easily exceeds human\nrecognition performance reported in similar experiments with humans (Shepard,\n1967). Achieving near perfect recall takes more exposures, but most models can\ndo it in just 3 exposures. The flip side of this remarkable capacity for fast\nlearning is that precise memories are quickly overwritten: recall performance\nfor the original examples drops steeply over the first 10 training updates with\nnew examples, followed by a more gradual decline. Even after 100K updates,\nhowever, some of the original examples are still recalled near perfectly. A\nqualitatively similar retention pattern has been observed in human long-term\nmemory retention studies before (Bahrick, 1984). Finally, recognition is much\nmore robust to interference than recall and memory for natural language\nsentences is generally superior to memory for stimuli without structure.\n","authors":["A. Emin Orhan"],"pdf_url":"https://arxiv.org/pdf/2303.17557v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.17548v1","updated":"2023-03-30T17:17:08Z","published":"2023-03-30T17:17:08Z","title":"Whose Opinions Do Language Models Reflect?","summary":"  Language models (LMs) are increasingly being used in open-ended contexts,\nwhere the opinions reflected by LMs in response to subjective queries can have\na profound impact, both on user satisfaction, as well as shaping the views of\nsociety at large. In this work, we put forth a quantitative framework to\ninvestigate the opinions reflected by LMs -- by leveraging high-quality public\nopinion polls and their associated human responses. Using this framework, we\ncreate OpinionsQA, a new dataset for evaluating the alignment of LM opinions\nwith those of 60 US demographic groups over topics ranging from abortion to\nautomation. Across topics, we find substantial misalignment between the views\nreflected by current LMs and those of US demographic groups: on par with the\nDemocrat-Republican divide on climate change. Notably, this misalignment\npersists even after explicitly steering the LMs towards particular demographic\ngroups. Our analysis not only confirms prior observations about the\nleft-leaning tendencies of some human feedback-tuned LMs, but also surfaces\ngroups whose opinions are poorly reflected by current LMs (e.g., 65+ and\nwidowed individuals). Our code and data are available at\nhttps://github.com/tatsu-lab/opinions_qa.\n","authors":["Shibani Santurkar","Esin Durmus","Faisal Ladhak","Cinoo Lee","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2303.17548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12186v2","updated":"2023-03-30T17:15:42Z","published":"2022-05-24T16:41:30Z","title":"Enhancing Continual Learning with Global Prototypes: Counteracting\n  Negative Representation Drift","summary":"  Continual learning (CL) aims to learn a sequence of tasks over time, with\ndata distributions shifting from one task to another. When training on new task\ndata, data representations from old tasks may drift. Some negative\nrepresentation drift can result in catastrophic forgetting, by causing the\nlocally learned class prototypes and data representations to correlate poorly\nacross tasks. To mitigate such representation drift, we propose a method that\nfinds global prototypes to guide the learning, and learns data representations\nwith the regularization of the self-supervised information. Specifically, for\nNLP tasks, we formulate each task in a masked language modeling style, and\nlearn the task via a neighbor attention mechanism over a pre-trained language\nmodel. Experimental results show that our proposed method can learn fairly\nconsistent representations with less representation drift, and significantly\nreduce catastrophic forgetting in CL without resampling data from past tasks.\n","authors":["Xueying Bai","Jinghuan Shang","Yifan Sun","Niranjan Balasubramanian"],"pdf_url":"https://arxiv.org/pdf/2205.12186v2.pdf","comment":"version 2"},{"id":"http://arxiv.org/abs/2303.17546v1","updated":"2023-03-30T17:13:56Z","published":"2023-03-30T17:13:56Z","title":"PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance\n  Paired Diffusion Models","summary":"  Image editing using diffusion models has witnessed extremely fast-paced\ngrowth recently. There are various ways in which previous works enable\ncontrolling and editing images. Some works use high-level conditioning such as\ntext, while others use low-level conditioning. Nevertheless, most of them lack\nfine-grained control over the properties of the different objects present in\nthe image, i.e. object-level image editing. In this work, we consider an image\nas a composition of multiple objects, each defined by various properties. Out\nof these properties, we identify structure and appearance as the most intuitive\nto understand and useful for editing purposes. We propose\nStructure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is\ntrained using structure and appearance information explicitly extracted from\nthe images. The proposed model enables users to inject a reference image's\nappearance into the input image at both the object and global levels.\nAdditionally, PAIR-Diffusion allows editing the structure while maintaining the\nstyle of individual components of the image unchanged. We extensively evaluate\nour method on LSUN datasets and the CelebA-HQ face dataset, and we demonstrate\nfine-grained control over both structure and appearance at the object level. We\nalso applied the method to Stable Diffusion to edit any real image at the\nobject level.\n","authors":["Vidit Goel","Elia Peruzzo","Yifan Jiang","Dejia Xu","Nicu Sebe","Trevor Darrell","Zhangyang Wang","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2303.17546v1.pdf","comment":"20 pages and 16 figures"},{"id":"http://arxiv.org/abs/2301.05609v3","updated":"2023-03-30T17:13:48Z","published":"2023-01-13T15:24:40Z","title":"Co-manipulation of soft-materials estimating deformation from depth\n  images","summary":"  Human-robot co-manipulation of soft materials, such as fabrics, composites,\nand sheets of paper/cardboard, is a challenging operation that presents several\nrelevant industrial applications. Estimating the deformation state of the\nco-manipulated material is one of the main challenges. Viable methods provide\nthe indirect measure by calculating the human-robot relative distance. In this\npaper, we develop a data-driven model to estimate the deformation state of the\nmaterial from a depth image through a Convolutional Neural Network (CNN).\nFirst, we define the deformation state of the material as the relative\nroto-translation from the current robot pose and a human grasping position. The\nmodel estimates the current deformation state through a Convolutional Neural\nNetwork, specifically a DenseNet-121 pretrained on ImageNet.The delta between\nthe current and the desired deformation state is fed to the robot controller\nthat outputs twist commands. The paper describes the developed approach to\nacquire, preprocess the dataset and train the model. The model is compared with\nthe current state-of-the-art method based on a skeletal tracker from cameras.\nResults show that our approach achieves better performances and avoids the\nvarious drawbacks caused by using a skeletal tracker.Finally, we also studied\nthe model performance according to different architectures and dataset\ndimensions to minimize the time required for dataset acquisition\n","authors":["Giorgio Nicola","Enrico Villagrossi","Nicola Pedrocchi"],"pdf_url":"https://arxiv.org/pdf/2301.05609v3.pdf","comment":"Pre-print, submitted to Robotics and Computer Integrated\n  Manufacturing"},{"id":"http://arxiv.org/abs/2303.15202v2","updated":"2023-03-30T16:44:45Z","published":"2023-03-24T14:34:09Z","title":"Towards Outcome-Driven Patient Subgroups: A Machine Learning Analysis\n  Across Six Depression Treatment Studies","summary":"  Major depressive disorder (MDD) is a heterogeneous condition; multiple\nunderlying neurobiological substrates could be associated with treatment\nresponse variability. Understanding the sources of this variability and\npredicting outcomes has been elusive. Machine learning has shown promise in\npredicting treatment response in MDD, but one limitation has been the lack of\nclinical interpretability of machine learning models. We analyzed data from six\nclinical trials of pharmacological treatment for depression (total n = 5438)\nusing the Differential Prototypes Neural Network (DPNN), a neural network model\nthat derives patient prototypes which can be used to derive treatment-relevant\npatient clusters while learning to generate probabilities for differential\ntreatment response. A model classifying remission and outputting individual\nremission probabilities for five first-line monotherapies and three combination\ntreatments was trained using clinical and demographic data. Model validity and\nclinical utility were measured based on area under the curve (AUC) and expected\nimprovement in sample remission rate with model-guided treatment, respectively.\nPost-hoc analyses yielded clusters (subgroups) based on patient prototypes\nlearned during training. Prototypes were evaluated for interpretability by\nassessing differences in feature distributions and treatment-specific outcomes.\nA 3-prototype model achieved an AUC of 0.66 and an expected absolute\nimprovement in population remission rate compared to the sample remission rate.\nWe identified three treatment-relevant patient clusters which were clinically\ninterpretable. It is possible to produce novel treatment-relevant patient\nprofiles using machine learning models; doing so may improve precision medicine\nfor depression. Note: This model is not currently the subject of any active\nclinical trials and is not intended for clinical use.\n","authors":["David Benrimoh","Akiva Kleinerman","Toshi A. Furukawa","Charles F. Reynolds III","Eric Lenze","Jordan Karp","Benoit Mulsant","Caitrin Armstrong","Joseph Mehltretter","Robert Fratila","Kelly Perlman","Sonia Israel","Myriam Tanguay-Sela","Christina Popescu","Grace Golden","Sabrina Qassim","Alexandra Anacleto","Adam Kapelner","Ariel Rosenfeld","Gustavo Turecki"],"pdf_url":"https://arxiv.org/pdf/2303.15202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17523v1","updated":"2023-03-30T16:44:12Z","published":"2023-03-30T16:44:12Z","title":"Quantum Circuit Fidelity Improvement with Long Short-Term Memory\n  Networks","summary":"  Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ)\nera. Currently, the quantum processors we have are sensitive to environmental\nvariables like radiation and temperature, thus producing noisy outputs.\nAlthough many proposed algorithms and applications exist for NISQ processors,\nwe still face uncertainties when interpreting their noisy results.\nSpecifically, how much confidence do we have in the quantum states we are\npicking as the output? This confidence is important since a NISQ computer will\noutput a probability distribution of its qubit measurements, and it is\nsometimes hard to distinguish whether the distribution represents meaningful\ncomputation or just random noise. This paper presents a novel approach to\nattack this problem by framing quantum circuit fidelity prediction as a Time\nSeries Forecasting problem, therefore making it possible to utilize the power\nof Long Short-Term Memory (LSTM) neural networks. A complete workflow to build\nthe training circuit dataset and LSTM architecture is introduced, including an\nintuitive method of calculating the quantum circuit fidelity. The trained LSTM\nsystem, Q-fid, can predict the output fidelity of a quantum circuit running on\na specific processor, without the need for any separate input of hardware\ncalibration data or gate error rates. Evaluated on the QASMbench NISQ benchmark\nsuite, Q-fid's prediction achieves an average RMSE of 0.0515, up to 24.7x more\naccurate than the default Qiskit transpile tool mapomatic. When used to find\nthe high-fidelity circuit layouts from the available circuit transpilations,\nQ-fid predicts the fidelity for the top 10% layouts with an average RMSE of\n0.0252, up to 32.8x more accurate than mapomatic.\n","authors":["Yikai Mao","Shaswot Shresthamali","Masaaki Kondo"],"pdf_url":"https://arxiv.org/pdf/2303.17523v1.pdf","comment":"17 pages, 17 figures, 3 tables"},{"id":"http://arxiv.org/abs/2302.08646v3","updated":"2023-03-30T16:10:25Z","published":"2023-02-17T01:31:53Z","title":"AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust\n  Autonomous Driving","summary":"  Object detection with on-board sensors (e.g., lidar, radar, and camera) play\na crucial role in autonomous driving (AD), and these sensors complement each\nother in modalities. While crowdsensing may potentially exploit these sensors\n(of huge quantity) to derive more comprehensive knowledge, \\textit{federated\nlearning} (FL) appears to be the necessary tool to reach this potential: it\nenables autonomous vehicles (AVs) to train machine learning models without\nexplicitly sharing raw sensory data. However, the multimodal sensors introduce\nvarious data heterogeneity across distributed AVs (e.g., label quantity skews\nand varied modalities), posing critical challenges to effective FL. To this\nend, we present AutoFed as a heterogeneity-aware FL framework to fully exploit\nmultimodal sensory data on AVs and thus enable robust AD. Specifically, we\nfirst propose a novel model leveraging pseudo-labeling to avoid mistakenly\ntreating unlabeled objects as the background. We also propose an\nautoencoder-based data imputation method to fill missing data modality (of\ncertain AVs) with the available ones. To further reconcile the heterogeneity,\nwe finally present a client selection mechanism exploiting the similarities\namong client models to improve both training stability and convergence rate.\nOur experiments on benchmark dataset confirm that AutoFed substantially\nimproves over status quo approaches in both precision and recall, while\ndemonstrating strong robustness to adverse weather conditions.\n","authors":["Tianyue Zheng","Ang Li","Zhe Chen","Hongbo Wang","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2302.08646v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17491v1","updated":"2023-03-30T16:01:52Z","published":"2023-03-30T16:01:52Z","title":"Language Models can Solve Computer Tasks","summary":"  Agents capable of carrying out general tasks on a computer can improve\nefficiency and productivity by automating repetitive tasks and assisting in\ncomplex problem-solving. Ideally, such agents should be able to solve new\ncomputer tasks presented to them through natural language commands. However,\nprevious approaches to this problem require large amounts of expert\ndemonstrations and task-specific reward functions, both of which are\nimpractical for new tasks. In this work, we show that a pre-trained large\nlanguage model (LLM) agent can execute computer tasks guided by natural\nlanguage using a simple prompting scheme where the agent recursively criticizes\nand improves its output (RCI). The RCI approach significantly outperforms\nexisting LLM methods for automating computer tasks and surpasses supervised\nlearning (SL) and reinforcement learning (RL) approaches on the MiniWoB++\nbenchmark. RCI is competitive with the state-of-the-art SL+RL method, using\nonly a handful of demonstrations per task rather than tens of thousands, and\nwithout a task-specific reward function. Furthermore, we demonstrate RCI\nprompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of\nnatural language reasoning tasks, outperforming chain of thought (CoT)\nprompting. We find that RCI combined with CoT performs better than either\nseparately.\n","authors":["Geunwoo Kim","Pierre Baldi","Stephen McAleer"],"pdf_url":"https://arxiv.org/pdf/2303.17491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17477v1","updated":"2023-03-30T15:49:10Z","published":"2023-03-30T15:49:10Z","title":"On the Analysis of Computational Delays in Reinforcement Learning-based\n  Rate Adaptation Algorithms","summary":"  Several research works have applied Reinforcement Learning (RL) algorithms to\nsolve the Rate Adaptation (RA) problem in Wi-Fi networks. The dynamic nature of\nthe radio link requires the algorithms to be responsive to changes in link\nquality. Delays in the execution of the algorithm may be detrimental to its\nperformance, which in turn may decrease network performance. This aspect has\nbeen overlooked in the state of the art. In this paper, we present an analysis\nof common computational delays in RL-based RA algorithms, and propose a\nmethodology that may be applied to reduce these computational delays and\nincrease the efficiency of this type of algorithms. We apply the proposed\nmethodology to an existing RL-based RA algorithm. The obtained experimental\nresults indicate a reduction of one order of magnitude in the execution time of\nthe algorithm, improving its responsiveness to link quality changes.\n","authors":["Ricardo Trancoso","Ruben Queiros","Helder Fontes","Rui Campos"],"pdf_url":"https://arxiv.org/pdf/2303.17477v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.17475v1","updated":"2023-03-30T15:48:26Z","published":"2023-03-30T15:48:26Z","title":"Efficient distributed representations beyond negative sampling","summary":"  This article describes an efficient method to learn distributed\nrepresentations, also known as embeddings. This is accomplished minimizing an\nobjective function similar to the one introduced in the Word2Vec algorithm and\nlater adopted in several works. The optimization computational bottleneck is\nthe calculation of the softmax normalization constants for which a number of\noperations scaling quadratically with the sample size is required. This\ncomplexity is unsuited for large datasets and negative sampling is a popular\nworkaround, allowing one to obtain distributed representations in linear time\nwith respect to the sample size. Negative sampling consists, however, in a\nchange of the loss function and hence solves a different optimization problem\nfrom the one originally proposed. Our contribution is to show that the sotfmax\nnormalization constants can be estimated in linear time, allowing us to design\nan efficient optimization strategy to learn distributed representations. We\ntest our approximation on two popular applications related to word and node\nembeddings. The results evidence competing performance in terms of accuracy\nwith respect to negative sampling with a remarkably lower computational time.\n","authors":["Lorenzo Dall'Amico","Enrico Maria Belliardo"],"pdf_url":"https://arxiv.org/pdf/2303.17475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17468v1","updated":"2023-03-30T15:44:30Z","published":"2023-03-30T15:44:30Z","title":"Surrogate Neural Networks for Efficient Simulation-based Trajectory\n  Planning Optimization","summary":"  This paper presents a novel methodology that uses surrogate models in the\nform of neural networks to reduce the computation time of simulation-based\noptimization of a reference trajectory. Simulation-based optimization is\nnecessary when there is no analytical form of the system accessible, only\ninput-output data that can be used to create a surrogate model of the\nsimulation. Like many high-fidelity simulations, this trajectory planning\nsimulation is very nonlinear and computationally expensive, making it\nchallenging to optimize iteratively. Through gradient descent optimization, our\napproach finds the optimal reference trajectory for landing a hypersonic\nvehicle. In contrast to the large datasets used to create the surrogate models\nin prior literature, our methodology is specifically designed to minimize the\nnumber of simulation executions required by the gradient descent optimizer. We\ndemonstrated this methodology to be more efficient than the standard practice\nof hand-tuning the inputs through trial-and-error or randomly sampling the\ninput parameter space. Due to the intelligently selected input values to the\nsimulation, our approach yields better simulation outcomes that are achieved\nmore rapidly and to a higher degree of accuracy. Optimizing the hypersonic\nvehicle's reference trajectory is very challenging due to the simulation's\nextreme nonlinearity, but even so, this novel approach found a 74%\nbetter-performing reference trajectory compared to nominal, and the numerical\nresults clearly show a substantial reduction in computation time for designing\nfuture trajectories.\n","authors":["Evelyn Ruff","Rebecca Russell","Matthew Stoeckle","Piero Miotto","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2303.17468v1.pdf","comment":"8 pages, 11 figures, submitted to the IEEE Conference of Decision and\n  Control 2023"},{"id":"http://arxiv.org/abs/2303.09483v2","updated":"2023-03-30T15:41:42Z","published":"2023-03-16T17:00:42Z","title":"Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks\n  in Continual Learning","summary":"  In contrast to the natural capabilities of humans to learn new tasks in a\nsequential fashion, neural networks are known to suffer from catastrophic\nforgetting, where the model's performances on old tasks drop dramatically after\nbeing optimized for a new task. Since then, the continual learning (CL)\ncommunity has proposed several solutions aiming to equip the neural network\nwith the ability to learn the current task (plasticity) while still achieving\nhigh accuracy on the previous tasks (stability). Despite remarkable\nimprovements, the plasticity-stability trade-off is still far from being solved\nand its underlying mechanism is poorly understood. In this work, we propose\nAuxiliary Network Continual Learning (ANCL), a novel method that applies an\nadditional auxiliary network which promotes plasticity to the continually\nlearned model which mainly focuses on stability. More concretely, the proposed\nframework materializes in a regularizer that naturally interpolates between\nplasticity and stability, surpassing strong baselines on task incremental and\nclass incremental scenarios. Through extensive analyses on ANCL solutions, we\nidentify some essential principles beneath the stability-plasticity trade-off.\n","authors":["Sanghwan Kim","Lorenzo Noci","Antonio Orvieto","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2303.09483v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17463v1","updated":"2023-03-30T15:40:26Z","published":"2023-03-30T15:40:26Z","title":"Can I Trust My Simulation Model? Measuring the Quality of Business\n  Process Simulation Models","summary":"  Business Process Simulation (BPS) is an approach to analyze the performance\nof business processes under different scenarios. For example, BPS allows us to\nestimate what would be the cycle time of a process if one or more resources\nbecame unavailable. The starting point of BPS is a process model annotated with\nsimulation parameters (a BPS model). BPS models may be manually designed, based\non information collected from stakeholders and empirical observations, or\nautomatically discovered from execution data. Regardless of its origin, a key\nquestion when using a BPS model is how to assess its quality. In this paper, we\npropose a collection of measures to evaluate the quality of a BPS model w.r.t.\nits ability to replicate the observed behavior of the process. We advocate an\napproach whereby different measures tackle different process perspectives. We\nevaluate the ability of the proposed measures to discern the impact of\nmodifications to a BPS model, and their ability to uncover the relative\nstrengths and weaknesses of two approaches for automated discovery of BPS\nmodels. The evaluation shows that the measures not only capture how close a BPS\nmodel is to the observed behavior, but they also help us to identify sources of\ndiscrepancies.\n","authors":["David Chapela-Campa","Ismail Benchekroun","Opher Baron","Marlon Dumas","Dmitry Krass","Arik Senderovich"],"pdf_url":"https://arxiv.org/pdf/2303.17463v1.pdf","comment":"Preprint submitted to the 21st International Conference on Business\n  Process Management (BPM 2023)"},{"id":"http://arxiv.org/abs/2303.17448v1","updated":"2023-03-30T15:20:21Z","published":"2023-03-30T15:20:21Z","title":"NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change\n  Detection in Heterogeneous Remote Sensing Images","summary":"  Change detection (CD) in heterogeneous remote sensing images is a practical\nand challenging issue for real-life emergencies. In the past decade, the\nheterogeneous CD problem has significantly benefited from the development of\ndeep neural networks (DNN). However, the data-driven DNNs always perform like a\nblack box where the lack of interpretability limits the trustworthiness and\ncontrollability of DNNs in most practical CD applications. As a strong\nknowledge-driven tool to measure correlation between random variables, Copula\ntheory has been introduced into CD, yet it suffers from non-robust CD\nperformance without manual prior selection for Copula functions. To address the\nabove issues, we propose a knowledge-data-driven heterogeneous CD method\n(NN-Copula-CD) based on the Copula-guided interpretable neural network. In our\nNN-Copula-CD, the mathematical characteristics of Copula are designed as the\nlosses to supervise a simple fully connected neural network to learn the\ncorrelation between bi-temporal image patches, and then the changed regions are\nidentified via binary classification for the correlation coefficients of all\nimage patch pairs of the bi-temporal images. We conduct in-depth experiments on\nthree datasets with multimodal images (e.g., Optical, SAR, and NIR), where the\nquantitative results and visualized analysis demonstrate both the effectiveness\nand interpretability of the proposed NN-Copula-CD.\n","authors":["Weiming Li","Xueqian Wang","Gang Li"],"pdf_url":"https://arxiv.org/pdf/2303.17448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00388v2","updated":"2023-03-30T15:12:17Z","published":"2023-02-01T11:48:10Z","title":"Short-term Prediction and Filtering of Solar Power Using State-Space\n  Gaussian Processes","summary":"  Short-term forecasting of solar photovoltaic energy (PV) production is\nimportant for powerplant management. Ideally these forecasts are equipped with\nerror bars, so that downstream decisions can account for uncertainty. To\nproduce predictions with error bars in this setting, we consider Gaussian\nprocesses (GPs) for modelling and predicting solar photovoltaic energy\nproduction in the UK. A standard application of GP regression on the PV\ntimeseries data is infeasible due to the large data size and non-Gaussianity of\nPV readings. However, this is made possible by leveraging recent advances in\nscalable GP inference, in particular, by using the state-space form of GPs,\ncombined with modern variational inference techniques. The resulting model is\nnot only scalable to large datasets but can also handle continuous data streams\nvia Kalman filtering.\n","authors":["Sean Nassimiha","Peter Dudfield","Jack Kelly","Marc Peter Deisenroth","So Takao"],"pdf_url":"https://arxiv.org/pdf/2302.00388v2.pdf","comment":"Workshop paper submitted to \"Tackling Climate Change with Machine\n  Learning: workshop at NeurIPS 2022\""},{"id":"http://arxiv.org/abs/2211.07717v3","updated":"2023-03-30T15:03:17Z","published":"2022-10-28T18:31:52Z","title":"Deep Temporal Modelling of Clinical Depression through Social Media Text","summary":"  We describe the development of a model to detect user-level clinical\ndepression based on a user's temporal social media posts. Our model uses a\nDepression Symptoms Detection (DSD) classifier, which is trained on the largest\nexisting samples of clinician annotated tweets for clinical depression\nsymptoms. We subsequently use our DSD model to extract clinically relevant\nfeatures, e.g., depression scores and their consequent temporal patterns, as\nwell as user posting activity patterns, e.g., quantifying their ``no activity''\nor ``silence.'' Furthermore, to evaluate the efficacy of these extracted\nfeatures, we create three kinds of datasets including a test dataset, from two\nexisting well-known benchmark datasets for user-level depression detection. We\nthen provide accuracy measures based on single features, baseline features and\nfeature ablation tests, at several different levels of temporal granularity.\nThe relevant data distributions and clinical depression detection related\nsettings can be exploited to draw a complete picture of the impact of different\nfeatures across our created datasets. Finally, we show that, in general, only\nsemantic oriented representation models perform well. However, clinical\nfeatures may enhance overall performance provided that the training and testing\ndistribution is similar, and there is more data in a user's timeline. The\nconsequence is that the predictive capability of depression scores increase\nsignificantly while used in a more sensitive clinical depression detection\nsettings.\n","authors":["Nawshad Farruque","Randy Goebel","Sudhakar Sivapalan","Osmar R. Zaïane"],"pdf_url":"https://arxiv.org/pdf/2211.07717v3.pdf","comment":"Tables are properly oriented and some more typos were fixed"},{"id":"http://arxiv.org/abs/2302.11510v4","updated":"2023-03-30T15:00:52Z","published":"2023-02-22T17:27:03Z","title":"Selective experience replay compression using coresets for lifelong deep\n  reinforcement learning in medical imaging","summary":"  Selective experience replay is a popular strategy for integrating lifelong\nlearning with deep reinforcement learning. Selective experience replay aims to\nrecount selected experiences from previous tasks to avoid catastrophic\nforgetting. Furthermore, selective experience replay based techniques are model\nagnostic and allow experiences to be shared across different models. However,\nstoring experiences from all previous tasks make lifelong learning using\nselective experience replay computationally very expensive and impractical as\nthe number of tasks increase. To that end, we propose a reward\ndistribution-preserving coreset compression technique for compressing\nexperience replay buffers stored for selective experience replay.\n  We evaluated the coreset compression technique on the brain tumor\nsegmentation (BRATS) dataset for the task of ventricle localization and on the\nwhole-body MRI for localization of left knee cap, left kidney, right\ntrochanter, left lung, and spleen. The coreset lifelong learning models trained\non a sequence of 10 different brain MR imaging environments demonstrated\nexcellent performance localizing the ventricle with a mean pixel error distance\nof 12.93 for the compression ratio of 10x. In comparison, the conventional\nlifelong learning model localized the ventricle with a mean pixel distance of\n10.87. Similarly, the coreset lifelong learning models trained on whole-body\nMRI demonstrated no significant difference (p=0.28) between the 10x compressed\ncoreset lifelong learning models and conventional lifelong learning models for\nall the landmarks. The mean pixel distance for the 10x compressed models across\nall the landmarks was 25.30, compared to 19.24 for the conventional lifelong\nlearning models. Our results demonstrate that the potential of the\ncoreset-based ERB compression method for compressing experiences without a\nsignificant drop in performance.\n","authors":["Guangyao Zheng","Samson Zhou","Vladimir Braverman","Michael A. Jacobs","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2302.11510v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09419v2","updated":"2023-03-30T14:44:09Z","published":"2023-02-18T20:51:09Z","title":"A Comprehensive Survey on Pretrained Foundation Models: A History from\n  BERT to ChatGPT","summary":"  Pretrained Foundation Models (PFMs) are regarded as the foundation for\nvarious downstream tasks with different data modalities. A PFM (e.g., BERT,\nChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable\nparameter initialization for a wide range of downstream applications. BERT\nlearns bidirectional encoder representations from Transformers, which are\ntrained on large datasets as contextual language models. Similarly, the\ngenerative pretrained transformer (GPT) method employs Transformers as the\nfeature extractor and is trained using an autoregressive paradigm on large\ndatasets. Recently, ChatGPT shows promising success on large language models,\nwhich applies an autoregressive language model with zero shot or few shot\nprompting. The remarkable achievements of PFM have brought significant\nbreakthroughs to various fields of AI. Numerous studies have proposed different\nmethods, raising the demand for an updated survey. This study provides a\ncomprehensive review of recent research advancements, challenges, and\nopportunities for PFMs in text, image, graph, as well as other data modalities.\nThe review covers the basic components and existing pretraining methods used in\nnatural language processing, computer vision, and graph learning. Additionally,\nit explores advanced PFMs used for different data modalities and unified PFMs\nthat consider data quality and quantity. The review also discusses research\nrelated to the fundamentals of PFMs, such as model efficiency and compression,\nsecurity, and privacy. Finally, the study provides key implications, future\nresearch directions, challenges, and open problems in the field of PFMs.\nOverall, this survey aims to shed light on the research of the PFMs on\nscalability, security, logical reasoning ability, cross-domain learning\nability, and the user-friendly interactive ability for artificial general\nintelligence.\n","authors":["Ce Zhou","Qian Li","Chen Li","Jun Yu","Yixin Liu","Guangjing Wang","Kai Zhang","Cheng Ji","Qiben Yan","Lifang He","Hao Peng","Jianxin Li","Jia Wu","Ziwei Liu","Pengtao Xie","Caiming Xiong","Jian Pei","Philip S. Yu","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2302.09419v2.pdf","comment":"99 pages, 16 figures"},{"id":"http://arxiv.org/abs/2206.07796v4","updated":"2023-03-30T14:30:46Z","published":"2022-06-15T20:18:43Z","title":"FixEval: Execution-based Evaluation of Program Fixes for Programming\n  Problems","summary":"  The complexity of modern software has led to a drastic increase in the time\nand cost associated with detecting and rectifying software bugs. In response,\nresearchers have explored various methods to automatically generate fixes for\nbuggy code. However, due to the large combinatorial space of possible fixes for\nany given bug, few tools and datasets are available to evaluate model-generated\nfixes effectively. To address this issue, we introduce FixEval, a benchmark\ncomprising of buggy code submissions to competitive programming problems and\ntheir corresponding fixes. FixEval offers an extensive collection of unit tests\nto evaluate the correctness of model-generated program fixes and assess further\ninformation regarding time, memory constraints, and acceptance based on a\nverdict. We consider two Transformer language models pretrained on programming\nlanguages as our baseline and compare them using match-based and\nexecution-based evaluation metrics. Our experiments show that match-based\nmetrics do not reflect model-generated program fixes accurately. At the same\ntime, execution-based methods evaluate programs through all cases and scenarios\ndesigned explicitly for that solution. Therefore, we believe FixEval provides a\nstep towards real-world automatic bug fixing and model-generated code\nevaluation. The dataset and models are open-sourced at\nhttps://github.com/mahimanzum/FixEval.\n","authors":["Md Mahim Anjum Haque","Wasi Uddin Ahmad","Ismini Lourentzou","Chris Brown"],"pdf_url":"https://arxiv.org/pdf/2206.07796v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17410v1","updated":"2023-03-30T14:27:42Z","published":"2023-03-30T14:27:42Z","title":"Removing supervision in semantic segmentation with local-global matching\n  and area balancing","summary":"  Removing supervision in semantic segmentation is still tricky. Current\napproaches can deal with common categorical patterns yet resort to multi-stage\narchitectures. We design a novel end-to-end model leveraging local-global patch\nmatching to predict categories, good localization, area and shape of objects\nfor semantic segmentation. The local-global matching is, in turn, compelled by\noptimal transport plans fulfilling area constraints nearing a solution for\nexact shape prediction. Our model attains state-of-the-art in Weakly Supervised\nSemantic Segmentation, only image-level labels, with 75% mIoU on PascalVOC2012\nval set and 46% on MS-COCO2014 val set. Dropping the image-level labels and\nclustering self-supervised learned features to yield pseudo-multi-level labels,\nwe obtain an unsupervised model for semantic segmentation. We also attain\nstate-of-the-art on Unsupervised Semantic Segmentation with 43.6% mIoU on\nPascalVOC2012 val set and 19.4% on MS-COCO2014 val set. The code is available\nat https://github.com/deepplants/PC2M.\n","authors":["Simone Rossetti","Nico Samà","Fiora Pirri"],"pdf_url":"https://arxiv.org/pdf/2303.17410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17396v1","updated":"2023-03-30T14:08:31Z","published":"2023-03-30T14:08:31Z","title":"Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs\n  and Practical Solutions","summary":"  Offline reinforcement learning (RL) allows for the training of competent\nagents from offline datasets without any interaction with the environment.\nOnline finetuning of such offline models can further improve performance. But\nhow should we ideally finetune agents obtained from offline RL training? While\noffline RL algorithms can in principle be used for finetuning, in practice,\ntheir online performance improves slowly. In contrast, we show that it is\npossible to use standard online off-policy algorithms for faster improvement.\nHowever, we find this approach may suffer from policy collapse, where the\npolicy undergoes severe performance deterioration during initial online\nlearning. We investigate the issue of policy collapse and how it relates to\ndata diversity, algorithm choices and online replay distribution. Based on\nthese insights, we propose a conservative policy optimization procedure that\ncan achieve stable and sample-efficient online learning from offline\npretraining.\n","authors":["Yicheng Luo","Jackie Kay","Edward Grefenstette","Marc Peter Deisenroth"],"pdf_url":"https://arxiv.org/pdf/2303.17396v1.pdf","comment":"An abstract of this paper was accepted at RLDM 2022"},{"id":"http://arxiv.org/abs/2303.17387v1","updated":"2023-03-30T13:58:47Z","published":"2023-03-30T13:58:47Z","title":"Explainable Intrusion Detection Systems Using Competitive Learning\n  Techniques","summary":"  The current state of the art systems in Artificial Intelligence (AI) enabled\nintrusion detection use a variety of black box methods. These black box methods\nare generally trained using Error Based Learning (EBL) techniques with a focus\non creating accurate models. These models have high performative costs and are\nnot easily explainable. A white box Competitive Learning (CL) based eXplainable\nIntrusion Detection System (X-IDS) offers a potential solution to these\nproblem. CL models utilize an entirely different learning paradigm than EBL\napproaches. This different learning process makes the CL family of algorithms\ninnately explainable and less resource intensive. In this paper, we create an\nX-IDS architecture that is based on DARPA's recommendation for explainable\nsystems. In our architecture we leverage CL algorithms like, Self Organizing\nMaps (SOM), Growing Self Organizing Maps (GSOM), and Growing Hierarchical Self\nOrganizing Map (GHSOM). The resulting models can be data-mined to create\nstatistical and visual explanations. Our architecture is tested using NSL-KDD\nand CIC-IDS-2017 benchmark datasets, and produces accuracies that are 1% - 3%\nless than EBL models. However, CL models are much more explainable than EBL\nmodels. Additionally, we use a pruning process that is able to significantly\nreduce the size of these CL based models. By pruning our models, we are able to\nincrease prediction speeds. Lastly, we analyze the statistical and visual\nexplanations generated by our architecture, and we give a strategy that users\ncould use to help navigate the set of explanations. These explanations will\nhelp users build trust with an Intrusion Detection System (IDS), and allow\nusers to discover ways to increase the IDS's potency.\n","authors":["Jesse Ables","Thomas Kirby","Sudip Mittal","Ioana Banicescu","Shahram Rahimi","William Anderson","Maria Seale"],"pdf_url":"https://arxiv.org/pdf/2303.17387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00713v2","updated":"2023-03-30T13:56:35Z","published":"2022-11-01T19:23:45Z","title":"MAgNET: A Graph U-Net Architecture for Mesh-Based Simulations","summary":"  In many cutting-edge applications, high-fidelity computational models prove\ntoo slow to be practical and are thus replaced by much faster surrogate models.\nRecently, deep learning techniques have become increasingly important in\naccelerating such predictions. However, they tend to falter when faced with\nlarger and more complex problems. Therefore, this work introduces MAgNET:\nMulti-channel Aggregation Network, a novel geometric deep learning framework\ndesigned to operate on large-dimensional data of arbitrary structure (graph\ndata). MAgNET is built upon the MAg (Multichannel Aggregation) operation, which\ngeneralizes the concept of multi-channel local operations in convolutional\nneural networks to arbitrary non-grid inputs. The MAg layers are interleaved\nwith the proposed novel graph pooling/unpooling operations to form a graph\nU-Net architecture that is robust and can handle arbitrary complex meshes,\nefficiently performing supervised learning on large-dimensional\ngraph-structured data. We demonstrate the predictive capabilities of MAgNET for\nseveral non-linear finite element simulations and provide open-source datasets\nand codes to facilitate future research.\n","authors":["Saurabh Deshpande","Stéphane P. A. Bordas","Jakub Lengiewicz"],"pdf_url":"https://arxiv.org/pdf/2211.00713v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15689v2","updated":"2023-03-30T13:53:11Z","published":"2023-03-28T02:31:57Z","title":"Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and\n  Prototype Alignment","summary":"  The success of existing multi-view clustering relies on the assumption of\nsample integrity across multiple views. However, in real-world scenarios,\nsamples of multi-view are partially available due to data corruption or sensor\nfailure, which leads to incomplete multi-view clustering study (IMVC). Although\nseveral attempts have been proposed to address IMVC, they suffer from the\nfollowing drawbacks: i) Existing methods mainly adopt cross-view contrastive\nlearning forcing the representations of each sample across views to be exactly\nthe same, which might ignore view discrepancy and flexibility in\nrepresentations; ii) Due to the absence of non-observed samples across multiple\nviews, the obtained prototypes of clusters might be unaligned and biased,\nleading to incorrect fusion. To address the above issues, we propose a\nCross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep\nIncomplete Multi-view Clustering. Firstly, unlike existing contrastive-based\nmethods, we adopt pair-observed data alignment as 'proxy supervised signals' to\nguide instance-to-instance correspondence construction among views. Then,\nregarding of the shifted prototypes in IMVC, we further propose a prototype\nalignment module to achieve incomplete distribution calibration across views.\nExtensive experimental results showcase the effectiveness of our proposed\nmodules, attaining noteworthy performance improvements when compared to\nexisting IMVC competitors on benchmark datasets.\n","authors":["Jiaqi Jin","Siwei Wang","Zhibin Dong","Xinwang Liu","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.15689v2.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2023"},{"id":"http://arxiv.org/abs/2205.14737v3","updated":"2023-03-30T13:46:04Z","published":"2022-05-29T18:53:24Z","title":"Stochastic Zeroth Order Gradient and Hessian Estimators: Variance\n  Reduction and Refined Bias Bounds","summary":"  We study stochastic zeroth order gradient and Hessian estimators for\nreal-valued functions in $\\mathbb{R}^n$. We show that, via taking finite\ndifference along random orthogonal directions, the variance of the stochastic\nfinite difference estimators can be significantly reduced. In particular, we\ndesign estimators for smooth functions such that, if one uses $ \\Theta \\left( k\n\\right) $ random directions sampled from the Stiefel's manifold $ \\text{St}\n(n,k) $ and finite-difference granularity $\\delta$, the variance of the\ngradient estimator is bounded by $ \\mathcal{O} \\left( \\left( \\frac{n}{k} - 1\n\\right) + \\left( \\frac{n^2}{k} - n \\right) \\delta^2 + \\frac{ n^2 \\delta^4 }{ k\n} \\right) $, and the variance of the Hessian estimator is bounded by\n$\\mathcal{O} \\left( \\left( \\frac{n^2}{k^2} - 1 \\right) + \\left( \\frac{n^4}{k^2}\n- n^2 \\right) \\delta^2 + \\frac{n^4 \\delta^4 }{k^2} \\right) $. When $k = n$, the\nvariances become negligibly small. In addition, we provide improved bias bounds\nfor the estimators. The bias of both gradient and Hessian estimators for smooth\nfunction $f$ is of order $\\mathcal{O} \\left( \\delta^2 \\Gamma \\right)$, where\n$\\delta$ is the finite-difference granularity, and $ \\Gamma $ depends on high\norder derivatives of $f$. Our results are evidenced by empirical observations.\n","authors":["Yasong Feng","Tianyu Wang"],"pdf_url":"https://arxiv.org/pdf/2205.14737v3.pdf","comment":"Code available at: https://github.com/wangt1anyu/grad-hess-var-code"},{"id":"http://arxiv.org/abs/2211.06678v2","updated":"2023-03-30T13:44:26Z","published":"2022-11-12T14:36:13Z","title":"Learning dynamical systems: an example from open quantum system dynamics","summary":"  Machine learning algorithms designed to learn dynamical systems from data can\nbe used to forecast, control and interpret the observed dynamics. In this work\nwe exemplify the use of one of such algorithms, namely Koopman operator\nlearning, in the context of open quantum system dynamics. We will study the\ndynamics of a small spin chain coupled with dephasing gates and show how\nKoopman operator learning is an approach to efficiently learn not only the\nevolution of the density matrix, but also of every physical observable\nassociated to the system. Finally, leveraging the spectral decomposition of the\nlearned Koopman operator, we show how symmetries obeyed by the underlying\ndynamics can be inferred directly from data.\n","authors":["Pietro Novelli"],"pdf_url":"https://arxiv.org/pdf/2211.06678v2.pdf","comment":"A short note for the \"Machine Learning and the Physical Sciences\"\n  workshop, NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.17376v1","updated":"2023-03-30T13:42:58Z","published":"2023-03-30T13:42:58Z","title":"A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision","summary":"  There has been a recent explosion of computer vision models which perform\nmany tasks and are composed of an image encoder (usually a ViT) and an\nautoregressive decoder (usually a Transformer). However, most of this work\nsimply presents one system and its results, leaving many questions regarding\ndesign decisions and trade-offs of such systems unanswered. In this work, we\naim to provide such answers. We take a close look at autoregressive decoders\nfor multi-task learning in multimodal computer vision, including\nclassification, captioning, visual question answering, and optical character\nrecognition. Through extensive systematic experiments, we study the effects of\ntask and data mixture, training and regularization hyperparameters,\nconditioning type and specificity, modality combination, and more. Importantly,\nwe compare these to well-tuned single-task baselines to highlight the cost\nincurred by multi-tasking. A key finding is that a small decoder learned on top\nof a frozen pretrained encoder works surprisingly well. We call this setup\nlocked-image tuning with decoder (LiT-decoder). It can be seen as teaching a\ndecoder to interact with a pretrained vision model via natural language.\n","authors":["Lucas Beyer","Bo Wan","Gagan Madan","Filip Pavetic","Andreas Steiner","Alexander Kolesnikov","André Susano Pinto","Emanuele Bugliarello","Xiao Wang","Qihang Yu","Liang-Chieh Chen","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.17376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17361v1","updated":"2023-03-30T13:19:07Z","published":"2023-03-30T13:19:07Z","title":"Invertible Convolution with Symmetric Paddings","summary":"  We show that symmetrically padded convolution can be analytically inverted\nvia DFT. We comprehensively analyze several different symmetric and\nanti-symmetric padding modes and show that multiple cases exist where the\ninversion can be achieved. The implementation is available at\n\\url{https://github.com/prclibo/iconv_dft}.\n","authors":["Bo Li"],"pdf_url":"https://arxiv.org/pdf/2303.17361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17358v1","updated":"2023-03-30T13:14:54Z","published":"2023-03-30T13:14:54Z","title":"DPP-based Client Selection for Federated Learning with Non-IID Data","summary":"  This paper proposes a client selection (CS) method to tackle the\ncommunication bottleneck of federated learning (FL) while concurrently coping\nwith FL's data heterogeneity issue. Specifically, we first analyze the effect\nof CS in FL and show that FL training can be accelerated by adequately choosing\nparticipants to diversify the training dataset in each round of training. Based\non this, we leverage data profiling and determinantal point process (DPP)\nsampling techniques to develop an algorithm termed Federated Learning with\nDPP-based Participant Selection (FL-DP$^3$S). This algorithm effectively\ndiversifies the participants' datasets in each round of training while\npreserving their data privacy. We conduct extensive experiments to examine the\nefficacy of our proposed method. The results show that our scheme attains a\nfaster convergence rate, as well as a smaller communication overhead than\nseveral baselines.\n","authors":["Yuxuan Zhang","Chao Xu","Howard H. Yang","Xijun Wang","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2303.17358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17354v1","updated":"2023-03-30T13:11:26Z","published":"2023-03-30T13:11:26Z","title":"Incremental Self-Supervised Learning Based on Transformer for Anomaly\n  Detection and Localization","summary":"  In the machine learning domain, research on anomaly detection and\nlocalization within image data has garnered significant attention, particularly\nin practical applications such as industrial defect detection. While existing\napproaches predominantly rely on Convolutional Neural Networks (CNN) as their\nbackbone network, we propose an innovative method based on the Transformer\nbackbone network. Our approach employs a two-stage incremental learning\nstrategy. In the first stage, we train a Masked Autoencoder (MAE) model\nexclusively on normal images. Subsequently, in the second stage, we implement\npixel-level data augmentation techniques to generate corrupted normal images\nand their corresponding pixel labels. This process enables the model to learn\nhow to repair corrupted regions and classify the state of each pixel.\nUltimately, the model produces a pixel reconstruction error matrix and a pixel\nanomaly probability matrix, which are combined to create an anomaly scoring\nmatrix that effectively identifies abnormal regions. When compared to several\nstate-of-the-art CNN-based techniques, our method demonstrates superior\nperformance on the MVTec AD dataset, achieving an impressive 97.6% AUC.\n","authors":["Wenping Jin","Fei Guo","Li Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.17354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.06445v3","updated":"2023-03-30T13:00:41Z","published":"2022-04-13T15:06:12Z","title":"Random Manifold Sampling and Joint Sparse Regularization for Multi-label\n  Feature Selection","summary":"  Multi-label learning is usually used to mine the correlation between features\nand labels, and feature selection can retain as much information as possible\nthrough a small number of features. $\\ell_{2,1}$ regularization method can get\nsparse coefficient matrix, but it can not solve multicollinearity problem\neffectively. The model proposed in this paper can obtain the most relevant few\nfeatures by solving the joint constrained optimization problems of $\\ell_{2,1}$\nand $\\ell_{F}$ regularization.In manifold regularization, we implement random\nwalk strategy based on joint information matrix, and get a highly robust\nneighborhood graph.In addition, we given the algorithm for solving the model\nand proved its convergence.Comparative experiments on real-world data sets show\nthat the proposed method outperforms other methods.\n","authors":["Haibao Li","Hongzhi Zhai"],"pdf_url":"https://arxiv.org/pdf/2204.06445v3.pdf","comment":"17pages, 8figures, 6tables"},{"id":"http://arxiv.org/abs/2303.16372v2","updated":"2023-03-30T12:42:19Z","published":"2023-03-29T00:49:38Z","title":"Non-Asymptotic Lower Bounds For Training Data Reconstruction","summary":"  We investigate semantic guarantees of private learning algorithms for their\nresilience to training Data Reconstruction Attacks (DRAs) by informed\nadversaries. To this end, we derive non-asymptotic minimax lower bounds on the\nadversary's reconstruction error against learners that satisfy differential\nprivacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate\nthat our lower bound analysis for the latter also covers the high dimensional\nregime, wherein, the input data dimensionality may be larger than the\nadversary's query budget. Motivated by the theoretical improvements conferred\nby metric DP, we extend the privacy analysis of popular deep learning\nalgorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion\nof metric differential privacy.\n","authors":["Prateeti Mukherjee","Satya Lokam"],"pdf_url":"https://arxiv.org/pdf/2303.16372v2.pdf","comment":"Corrected minor typos and restructured appendix"},{"id":"http://arxiv.org/abs/2208.06073v6","updated":"2023-03-30T12:30:46Z","published":"2022-08-12T01:00:59Z","title":"Conditional Antibody Design as 3D Equivariant Graph Translation","summary":"  Antibody design is valuable for therapeutic usage and biological research.\nExisting deep-learning-based methods encounter several key issues: 1)\nincomplete context for Complementarity-Determining Regions (CDRs) generation;\n2) incapability of capturing the entire 3D geometry of the input structure; 3)\ninefficient prediction of the CDR sequences in an autoregressive manner. In\nthis paper, we propose Multi-channel Equivariant Attention Network (MEAN) to\nco-design 1D sequences and 3D structures of CDRs. To be specific, MEAN\nformulates antibody design as a conditional graph translation problem by\nimporting extra components including the target antigen and the light chain of\nthe antibody. Then, MEAN resorts to E(3)-equivariant message passing along with\na proposed attention mechanism to better capture the geometrical correlation\nbetween different components. Finally, it outputs both the 1D sequences and 3D\nstructure via a multi-round progressive full-shot scheme, which enjoys more\nefficiency and precision against previous autoregressive approaches. Our method\nsignificantly surpasses state-of-the-art models in sequence and structure\nmodeling, antigen-binding CDR design, and binding affinity optimization.\nSpecifically, the relative improvement to baselines is about 23% in\nantigen-binding CDR design and 34% for affinity optimization.\n","authors":["Xiangzhe Kong","Wenbing Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2208.06073v6.pdf","comment":"Accepted to ICLR 2023 as oral presentation. Outstanding paper\n  honorable mentions"},{"id":"http://arxiv.org/abs/2202.06545v3","updated":"2023-03-30T12:25:04Z","published":"2022-02-14T08:34:51Z","title":"Provably Efficient Causal Model-Based Reinforcement Learning for\n  Systematic Generalization","summary":"  In the sequential decision making setting, an agent aims to achieve\nsystematic generalization over a large, possibly infinite, set of environments.\nSuch environments are modeled as discrete Markov decision processes with both\nstates and actions represented through a feature vector. The underlying\nstructure of the environments allows the transition dynamics to be factored\ninto two components: one that is environment-specific and another that is\nshared. Consider a set of environments that share the laws of motion as an\nexample. In this setting, the agent can take a finite amount of reward-free\ninteractions from a subset of these environments. The agent then must be able\nto approximately solve any planning task defined over any environment in the\noriginal set, relying on the above interactions only. Can we design a provably\nefficient algorithm that achieves this ambitious goal of systematic\ngeneralization? In this paper, we give a partially positive answer to this\nquestion. First, we provide a tractable formulation of systematic\ngeneralization by employing a causal viewpoint. Then, under specific structural\nassumptions, we provide a simple learning algorithm that guarantees any desired\nplanning error up to an unavoidable sub-optimality term, while showcasing a\npolynomial sample complexity.\n","authors":["Mirco Mutti","Riccardo De Santi","Emanuele Rossi","Juan Felipe Calderon","Michael Bronstein","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2202.06545v3.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2303.08685v2","updated":"2023-03-30T11:56:29Z","published":"2023-03-15T15:12:36Z","title":"Making Vision Transformers Efficient from A Token Sparsification View","summary":"  The quadratic computational complexity to the number of tokens limits the\npractical applications of Vision Transformers (ViTs). Several works propose to\nprune redundant tokens to achieve efficient ViTs. However, these methods\ngenerally suffer from (i) dramatic accuracy drops, (ii) application difficulty\nin the local vision transformer, and (iii) non-general-purpose networks for\ndownstream tasks. In this work, we propose a novel Semantic Token ViT (STViT),\nfor efficient global and local vision transformers, which can also be revised\nto serve as backbone for downstream tasks. The semantic tokens represent\ncluster centers, and they are initialized by pooling image tokens in space and\nrecovered by attention, which can adaptively represent global or local semantic\ninformation. Due to the cluster properties, a few semantic tokens can attain\nthe same effect as vast image tokens, for both global and local vision\ntransformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base)\ncan achieve the same accuracy with more than 100% inference speed improvement\nand nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16\nsemantic tokens in each window to further speed it up by around 20% with slight\naccuracy increase. Besides great success in image classification, we also\nextend our method to video recognition. In addition, we design a\nSTViT-R(ecover) network to restore the detailed spatial information based on\nthe STViT, making it work for downstream tasks, which is powerless for previous\ntoken sparsification methods. Experiments demonstrate that our method can\nachieve competitive results compared to the original networks in object\ndetection and instance segmentation, with over 30% FLOPs reduction for\nbackbone. Code is available at http://github.com/changsn/STViT-R\n","authors":["Shuning Chang","Pichao Wang","Ming Lin","Fan Wang","David Junhao Zhang","Rong Jin","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2303.08685v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2210.10179v2","updated":"2023-03-30T11:52:39Z","published":"2022-10-18T21:58:58Z","title":"Inference in conditioned dynamics through causality restoration","summary":"  Computing observables from conditioned dynamics is typically computationally\nhard, because, although obtaining independent samples efficiently from the\nunconditioned dynamics is usually feasible, generally most of the samples must\nbe discarded (in a form of importance sampling) because they do not satisfy the\nimposed conditions. Sampling directly from the conditioned distribution is\nnon-trivial, as conditioning breaks the causal properties of the dynamics which\nultimately renders the sampling procedure efficient. One standard way of\nachieving it is through a Metropolis Monte-Carlo procedure, but this procedure\nis normally slow and a very large number of Monte-Carlo steps is needed to\nobtain a small number of statistically independent samples. In this work, we\npropose an alternative method to produce independent samples from a conditioned\ndistribution. The method learns the parameters of a generalized dynamical model\nthat optimally describe the conditioned distribution in a variational sense.\nThe outcome is an effective, unconditioned, dynamical model, from which one can\ntrivially obtain independent samples, effectively restoring causality of the\nconditioned distribution. The consequences are twofold: on the one hand, it\nallows us to efficiently compute observables from the conditioned dynamics by\nsimply averaging over independent samples. On the other hand, the method gives\nan effective unconditioned distribution which is easier to interpret. The\nmethod is flexible and can be applied virtually to any dynamics. We discuss an\nimportant application of the method, namely the problem of epidemic risk\nassessment from (imperfect) clinical tests, for a large family of\ntime-continuous epidemic models endowed with a Gillespie-like sampler. We show\nthat the method compares favorably against the state of the art, including the\nsoft-margin approach and mean-field methods.\n","authors":["Alfredo Braunstein","Giovanni Catania","Luca Dall'Asta","Matteo Mariani","Anna Paola Muntoni"],"pdf_url":"https://arxiv.org/pdf/2210.10179v2.pdf","comment":"22 pages, 7 figures"},{"id":"http://arxiv.org/abs/2212.13824v2","updated":"2023-03-30T11:26:56Z","published":"2022-12-28T13:56:54Z","title":"Multi-Realism Image Compression with a Conditional Generator","summary":"  By optimizing the rate-distortion-realism trade-off, generative compression\napproaches produce detailed, realistic images, even at low bit rates, instead\nof the blurry reconstructions produced by rate-distortion optimized models.\nHowever, previous methods do not explicitly control how much detail is\nsynthesized, which results in a common criticism of these methods: users might\nbe worried that a misleading reconstruction far from the input image is\ngenerated. In this work, we alleviate these concerns by training a decoder that\ncan bridge the two regimes and navigate the distortion-realism trade-off. From\na single compressed representation, the receiver can decide to either\nreconstruct a low mean squared error reconstruction that is close to the input,\na realistic reconstruction with high perceptual quality, or anything in\nbetween. With our method, we set a new state-of-the-art in distortion-realism,\npushing the frontier of achievable distortion-realism pairs, i.e., our method\nachieves better distortions at high realism and better realism at low\ndistortion than ever before.\n","authors":["Eirikur Agustsson","David Minnen","George Toderici","Fabian Mentzer"],"pdf_url":"https://arxiv.org/pdf/2212.13824v2.pdf","comment":"CVPR'23 Camera Ready"},{"id":"http://arxiv.org/abs/2303.17299v1","updated":"2023-03-30T11:24:56Z","published":"2023-03-30T11:24:56Z","title":"Sasaki Metric for Spline Models of Manifold-Valued Trajectories","summary":"  We propose a generic spatiotemporal framework to analyze manifold-valued\nmeasurements, which allows for employing an intrinsic and computationally\nefficient Riemannian hierarchical model. Particularly, utilizing regression, we\nrepresent discrete trajectories in a Riemannian manifold by composite B\\' ezier\nsplines, propose a natural metric induced by the Sasaki metric to compare the\ntrajectories, and estimate average trajectories as group-wise trends. We\nevaluate our framework in comparison to state-of-the-art methods within\nqualitative and quantitative experiments on hurricane tracks. Notably, our\nresults demonstrate the superiority of spline-based approaches for an intensity\nclassification of the tracks.\n","authors":["Esfandiar Nava-Yazdani","Felix Ambellan","Martin Hanik","Christoph von Tycowicz"],"pdf_url":"https://arxiv.org/pdf/2303.17299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16001v2","updated":"2023-03-30T11:20:13Z","published":"2023-03-28T14:16:08Z","title":"Adaptive Voronoi NeRFs","summary":"  Neural Radiance Fields (NeRFs) learn to represent a 3D scene from just a set\nof registered images. Increasing sizes of a scene demands more complex\nfunctions, typically represented by neural networks, to capture all details.\nTraining and inference then involves querying the neural network millions of\ntimes per image, which becomes impractically slow. Since such complex functions\ncan be replaced by multiple simpler functions to improve speed, we show that a\nhierarchy of Voronoi diagrams is a suitable choice to partition the scene. By\nequipping each Voronoi cell with its own NeRF, our approach is able to quickly\nlearn a scene representation. We propose an intuitive partitioning of the space\nthat increases quality gains during training by distributing information evenly\namong the networks and avoids artifacts through a top-down adaptive refinement.\nOur framework is agnostic to the underlying NeRF method and easy to implement,\nwhich allows it to be applied to various NeRF variants for improved learning\nand rendering speeds.\n","authors":["Tim Elsner","Victor Czech","Julia Berger","Zain Selman","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2303.16001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15954v2","updated":"2023-03-30T10:57:25Z","published":"2023-03-28T13:12:17Z","title":"TraffNet: Learning Causality of Traffic Generation for Road Network\n  Digital Twins","summary":"  Road network digital twins (RNDTs) play a critical role in the development of\nnext-generation intelligent transportation systems, enabling more precise\ntraffic planning and control. To support just-in-time (JIT) decision making,\nRNDTs require a model that dynamically learns the traffic patterns from online\nsensor data and generates high-fidelity simulation results. Although current\ntraffic prediction techniques based on graph neural networks have achieved\nstate-of-the-art performance, these techniques only predict future traffic by\nmining correlations in historical traffic data, disregarding the causes of\ntraffic generation, such as traffic demands and route selection. Therefore,\ntheir performance is unreliable for JIT decision making. To fill this gap, we\nintroduce a novel deep learning framework called TraffNet that learns the\ncausality of traffic volume from vehicle trajectory data. First, we use a\nheterogeneous graph to represent the road network, allowing the model to\nincorporate causal features of traffic volumes. Next, motivated by the traffic\ndomain knowledge, we propose a traffic causality learning method to learn an\nembedding vector that encodes travel demands and path-level dependencies for\neach road segment. Then, we model temporal dependencies to match the underlying\nprocess of traffic generation. Finally, the experiments verify the utility of\nTraffNet. The code of TraffNet is available at\nhttps://github.com/mayunyi-1999/TraffNet_code.git.\n","authors":["Ming Xu","Yunyi Ma","Ruimin Li","Geqi Qi","Xiangfu Meng","Haibo Jin"],"pdf_url":"https://arxiv.org/pdf/2303.15954v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00875v2","updated":"2023-03-30T10:52:55Z","published":"2022-09-27T12:56:56Z","title":"Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset\n  Copyright Protection","summary":"  Deep neural networks (DNNs) have demonstrated their superiority in practice.\nArguably, the rapid development of DNNs is largely benefited from high-quality\n(open-sourced) datasets, based on which researchers and developers can easily\nevaluate and improve their learning methods. Since the data collection is\nusually time-consuming or even expensive, how to protect their copyrights is of\ngreat significance and worth further exploration. In this paper, we revisit\ndataset ownership verification. We find that existing verification methods\nintroduced new security risks in DNNs trained on the protected dataset, due to\nthe targeted nature of poison-only backdoor watermarks. To alleviate this\nproblem, in this work, we explore the untargeted backdoor watermarking scheme,\nwhere the abnormal model behaviors are not deterministic. Specifically, we\nintroduce two dispersibilities and prove their correlation, based on which we\ndesign the untargeted backdoor watermark under both poisoned-label and\nclean-label settings. We also discuss how to use the proposed untargeted\nbackdoor watermark for dataset ownership verification. Experiments on benchmark\ndatasets verify the effectiveness of our methods and their resistance to\nexisting backdoor defenses. Our codes are available at\n\\url{https://github.com/THUYimingLi/Untargeted_Backdoor_Watermark}.\n","authors":["Yiming Li","Yang Bai","Yong Jiang","Yong Yang","Shu-Tao Xia","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2210.00875v2.pdf","comment":"This work is accepted by the NeurIPS 2022 (selected as Oral paper,\n  TOP 2%). The first two authors contributed equally to this work. 25 pages. We\n  have fixed some typos in the previous version"},{"id":"http://arxiv.org/abs/2112.13747v6","updated":"2023-03-30T10:50:25Z","published":"2021-12-27T15:49:25Z","title":"MOEF: Modeling Occasion Evolution in Frequency Domain for\n  Promotion-Aware Click-Through Rate Prediction","summary":"  Promotions are becoming more important and prevalent in e-commerce to attract\ncustomers and boost sales, leading to frequent changes of occasions, which\ndrives users to behave differently. In such situations, most existing\nClick-Through Rate (CTR) models can't generalize well to online serving due to\ndistribution uncertainty of the upcoming occasion. In this paper, we propose a\nnovel CTR model named MOEF for recommendations under frequent changes of\noccasions. Firstly, we design a time series that consists of occasion signals\ngenerated from the online business scenario. Since occasion signals are more\ndiscriminative in the frequency domain, we apply Fourier Transformation to\nsliding time windows upon the time series, obtaining a sequence of frequency\nspectrum which is then processed by Occasion Evolution Layer (OEL). In this\nway, a high-order occasion representation can be learned to handle the online\ndistribution uncertainty. Moreover, we adopt multiple experts to learn feature\nrepresentations from multiple aspects, which are guided by the occasion\nrepresentation via an attention mechanism. Accordingly, a mixture of feature\nrepresentations is obtained adaptively for different occasions to predict the\nfinal CTR. Experimental results on real-world datasets validate the superiority\nof MOEF and online A/B tests also show MOEF outperforms representative CTR\nmodels significantly.\n","authors":["Xiaofeng Pan","Yibin Shen","Jing Zhang","Xu He","Yang Huang","Hong Wen","Chengjun Mao","Bo Cao"],"pdf_url":"https://arxiv.org/pdf/2112.13747v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13024v2","updated":"2023-03-30T10:50:11Z","published":"2023-03-23T04:16:00Z","title":"Identifying TBI Physiological States by Clustering of Multivariate\n  Clinical Time-Series","summary":"  Determining clinically relevant physiological states from multivariate time\nseries data with missing values is essential for providing appropriate\ntreatment for acute conditions such as Traumatic Brain Injury (TBI),\nrespiratory failure, and heart failure. Utilizing non-temporal clustering or\ndata imputation and aggregation techniques may lead to loss of valuable\ninformation and biased analyses. In our study, we apply the SLAC-Time\nalgorithm, an innovative self-supervision-based approach that maintains data\nintegrity by avoiding imputation or aggregation, offering a more useful\nrepresentation of acute patient states. By using SLAC-Time to cluster data in a\nlarge research dataset, we identified three distinct TBI physiological states\nand their specific feature profiles. We employed various clustering evaluation\nmetrics and incorporated input from a clinical domain expert to validate and\ninterpret the identified physiological states. Further, we discovered how\nspecific clinical events and interventions can influence patient states and\nstate transitions.\n","authors":["Hamid Ghaderi","Brandon Foreman","Amin Nayebi","Sindhu Tipirneni","Chandan K. Reddy","Vignesh Subbian"],"pdf_url":"https://arxiv.org/pdf/2303.13024v2.pdf","comment":"10 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2003.06777v5","updated":"2023-03-30T10:48:54Z","published":"2020-03-15T08:13:16Z","title":"DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning","summary":"  In this work, we develop methods for few-shot image classification from a new\nperspective of optimal matching between image regions. We employ the Earth\nMover's Distance (EMD) as a metric to compute a structural distance between\ndense image representations to determine image relevance. The EMD generates the\noptimal matching flows between structural elements that have the minimum\nmatching cost, which is used to calculate the image distance for\nclassification. To generate the important weights of elements in the EMD\nformulation, we design a cross-reference mechanism, which can effectively\nalleviate the adverse impact caused by the cluttered background and large\nintra-class appearance variations. To implement k-shot classification, we\npropose to learn a structured fully connected layer that can directly classify\ndense image representations with the EMD. Based on the implicit function\ntheorem, the EMD can be inserted as a layer into the network for end-to-end\ntraining. Our extensive experiments validate the effectiveness of our algorithm\nwhich outperforms state-of-the-art methods by a significant margin on five\nwidely used few-shot classification benchmarks, namely, miniImageNet,\ntieredImageNet, Fewshot-CIFAR100 (FC100), Caltech-UCSD Birds-200-2011 (CUB),\nand CIFAR-FewShot (CIFAR-FS). We also demonstrate the effectiveness of our\nmethod on the image retrieval task in our experiments.\n","authors":["Chi Zhang","Yujun Cai","Guosheng Lin","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2003.06777v5.pdf","comment":"DeepEMD V2, accepted by TPAMI"},{"id":"http://arxiv.org/abs/2303.00515v5","updated":"2023-03-30T10:48:33Z","published":"2023-02-28T04:37:26Z","title":"Interpretable Water Level Forecaster with Spatiotemporal Causal\n  Attention Mechanisms","summary":"  Forecasting the water level of the Han river is important to control traffic\nand avoid natural disasters. There are many variables related to the Han river\nand they are intricately connected. In this work, we propose a novel\ntransformer that exploits the causal relationship based on the prior knowledge\namong the variables and forecasts the water level at the Jamsu bridge in the\nHan river. Our proposed model considers both spatial and temporal causation by\nformalizing the causal structure as a multilayer network and using masking\nmethods. Due to this approach, we can have interpretability that consistent\nwith prior knowledge. In real data analysis, we use the Han river dataset from\n2016 to 2021 and compare the proposed model with deep learning models.\n","authors":["Sunghcul Hong","Yunjin Choi","Jong-June Jeon"],"pdf_url":"https://arxiv.org/pdf/2303.00515v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17276v1","updated":"2023-03-30T10:32:18Z","published":"2023-03-30T10:32:18Z","title":"Humans in Humans Out: On GPT Converging Toward Common Sense in both\n  Success and Failure","summary":"  Increase in computational scale and fine-tuning has seen a dramatic\nimprovement in the quality of outputs of large language models (LLMs) like GPT.\nGiven that both GPT-3 and GPT-4 were trained on large quantities of\nhuman-generated text, we might ask to what extent their outputs reflect\npatterns of human thinking, both for correct and incorrect cases. The Erotetic\nTheory of Reason (ETR) provides a symbolic generative model of both human\nsuccess and failure in thinking, across propositional, quantified, and\nprobabilistic reasoning, as well as decision-making. We presented GPT-3,\nGPT-3.5, and GPT-4 with 61 central inference and judgment problems from a\nrecent book-length presentation of ETR, consisting of experimentally verified\ndata-points on human judgment and extrapolated data-points predicted by ETR,\nwith correct inference patterns as well as fallacies and framing effects (the\nETR61 benchmark). ETR61 includes classics like Wason's card task, illusory\ninferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3\nshowed evidence of ETR-predicted outputs for 59% of these examples, rising to\n77% in GPT-3.5 and 75% in GPT-4. Remarkably, the production of human-like\nfallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in\nGPT-4. This suggests that larger and more advanced LLMs may develop a tendency\ntoward more human-like mistakes, as relevant thought patterns are inherent in\nhuman-produced training data. According to ETR, the same fundamental patterns\nare involved both in successful and unsuccessful ordinary reasoning, so that\nthe \"bad\" cases could paradoxically be learned from the \"good\" cases. We\nfurther present preliminary evidence that ETR-inspired prompt engineering could\nreduce instances of these mistakes.\n","authors":["Philipp Koralus","Vincent Wang-Maścianica"],"pdf_url":"https://arxiv.org/pdf/2303.17276v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2303.17264v1","updated":"2023-03-30T10:01:58Z","published":"2023-03-30T10:01:58Z","title":"Multifactor Sequential Disentanglement via Structured Koopman\n  Autoencoders","summary":"  Disentangling complex data to its latent factors of variation is a\nfundamental task in representation learning. Existing work on sequential\ndisentanglement mostly provides two factor representations, i.e., it separates\nthe data to time-varying and time-invariant factors. In contrast, we consider\nmultifactor disentanglement in which multiple (more than two) semantic\ndisentangled components are generated. Key to our approach is a strong\ninductive bias where we assume that the underlying dynamics can be represented\nlinearly in the latent space. Under this assumption, it becomes natural to\nexploit the recently introduced Koopman autoencoder models. However,\ndisentangled representations are not guaranteed in Koopman approaches, and thus\nwe propose a novel spectral loss term which leads to structured Koopman\nmatrices and disentanglement. Overall, we propose a simple and easy to code new\ndeep model that is fully unsupervised and it supports multifactor\ndisentanglement. We showcase new disentangling abilities such as swapping of\nindividual static factors between characters, and an incremental swap of\ndisentangled factors from the source to the target. Moreover, we evaluate our\nmethod extensively on two factor standard benchmark tasks where we\nsignificantly improve over competing unsupervised approaches, and we perform\ncompetitively in comparison to weakly- and self-supervised state-of-the-art\napproaches. The code is available at https://github.com/azencot-group/SKD.\n","authors":["Nimrod Berman","Ilan Naiman","Omri Azencot"],"pdf_url":"https://arxiv.org/pdf/2303.17264v1.pdf","comment":"Accepted to ICLR 2023, Notable-top 25% (Spotlight); The first two\n  authors contributed equally"},{"id":"http://arxiv.org/abs/2303.17251v1","updated":"2023-03-30T09:29:53Z","published":"2023-03-30T09:29:53Z","title":"Demystifying Misconceptions in Social Bots Research","summary":"  The science of social bots seeks knowledge and solutions to one of the most\ndebated forms of online misinformation. Yet, social bots research is plagued by\nwidespread biases, hyped results, and misconceptions that set the stage for\nambiguities, unrealistic expectations, and seemingly irreconcilable findings.\nOvercoming such issues is instrumental towards ensuring reliable solutions and\nreaffirming the validity of the scientific method. In this contribution we\nrevise some recent results in social bots research, highlighting and correcting\nfactual errors as well as methodological and conceptual issues. More\nimportantly, we demystify common misconceptions, addressing fundamental points\non how social bots research is discussed. Our analysis surfaces the need to\ndiscuss misinformation research in a rigorous, unbiased, and responsible way.\nThis article bolsters such effort by identifying and refuting common fallacious\narguments used by both proponents and opponents of social bots research as well\nas providing indications on the correct methodologies and sound directions for\nfuture research in the field.\n","authors":["Stefano Cresci","Roberto Di Pietro","Angelo Spognardi","Maurizio Tesconi","Marinella Petrocchi"],"pdf_url":"https://arxiv.org/pdf/2303.17251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17245v1","updated":"2023-03-30T09:22:17Z","published":"2023-03-30T09:22:17Z","title":"Investigating and Mitigating the Side Effects of Noisy Views in\n  Multi-view Clustering in Practical Scenarios","summary":"  Multi-view clustering (MvC) aims at exploring the category structure among\nmulti-view data without label supervision. Multiple views provide more\ninformation than single views and thus existing MvC methods can achieve\nsatisfactory performance. However, their performance might seriously degenerate\nwhen the views are noisy in practical scenarios. In this paper, we first\nformally investigate the drawback of noisy views and then propose a\ntheoretically grounded deep MvC method (namely MvCAN) to address this issue.\nSpecifically, we propose a novel MvC objective that enables un-shared\nparameters and inconsistent clustering predictions across multiple views to\nreduce the side effects of noisy views. Furthermore, a non-parametric iterative\nprocess is designed to generate a robust learning target for mining multiple\nviews' useful information. Theoretical analysis reveals that MvCAN works by\nachieving the multi-view consistency, complementarity, and noise robustness.\nFinally, experiments on public datasets demonstrate that MvCAN outperforms\nstate-of-the-art methods and is robust against the existence of noisy views.\n","authors":["Jie Xu","Gang Niu","Xiaolong Wang","Yazhou Ren","Lei Feng","Xiaoshuang Shi","Heng Tao Shen","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.17245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17243v1","updated":"2023-03-30T09:19:24Z","published":"2023-03-30T09:19:24Z","title":"Shapley Chains: Extending Shapley Values to Classifier Chains","summary":"  In spite of increased attention on explainable machine learning models,\nexplaining multi-output predictions has not yet been extensively addressed.\nMethods that use Shapley values to attribute feature contributions to the\ndecision making are one of the most popular approaches to explain local\nindividual and global predictions. By considering each output separately in\nmulti-output tasks, these methods fail to provide complete feature\nexplanations. We propose Shapley Chains to overcome this issue by including\nlabel interdependencies in the explanation design process. Shapley Chains\nassign Shapley values as feature importance scores in multi-output\nclassification using classifier chains, by separating the direct and indirect\ninfluence of these feature scores. Compared to existing methods, this approach\nallows to attribute a more complete feature contribution to the predictions of\nmulti-output classification tasks. We provide a mechanism to distribute the\nhidden contributions of the outputs with respect to a given chaining order of\nthese outputs. Moreover, we show how our approach can reveal indirect feature\ncontributions missed by existing approaches. Shapley Chains help to emphasize\nthe real learning factors in multi-output applications and allows a better\nunderstanding of the flow of information through output interdependencies in\nsynthetic and real-world datasets.\n","authors":["Célia Wafa Ayad","Thomas Bonnier","Benjamin Bosch","Jesse Read"],"pdf_url":"https://arxiv.org/pdf/2303.17243v1.pdf","comment":"DS22"},{"id":"http://arxiv.org/abs/2303.17235v1","updated":"2023-03-30T09:08:57Z","published":"2023-03-30T09:08:57Z","title":"Practical self-supervised continual learning with continual fine-tuning","summary":"  Self-supervised learning (SSL) has shown remarkable performance in computer\nvision tasks when trained offline. However, in a Continual Learning (CL)\nscenario where new data is introduced progressively, models still suffer from\ncatastrophic forgetting. Retraining a model from scratch to adapt to newly\ngenerated data is time-consuming and inefficient. Previous approaches suggested\nre-purposing self-supervised objectives with knowledge distillation to mitigate\nforgetting across tasks, assuming that labels from all tasks are available\nduring fine-tuning. In this paper, we generalize self-supervised continual\nlearning in a practical setting where available labels can be leveraged in any\nstep of the SSL process. With an increasing number of continual tasks, this\noffers more flexibility in the pre-training and fine-tuning phases. With\nKaizen, we introduce a training architecture that is able to mitigate\ncatastrophic forgetting for both the feature extractor and classifier with a\ncarefully designed loss function. By using a set of comprehensive evaluation\nmetrics reflecting different aspects of continual learning, we demonstrated\nthat Kaizen significantly outperforms previous SSL models in competitive vision\nbenchmarks, with up to 16.5% accuracy improvement on split CIFAR-100. Kaizen is\nable to balance the trade-off between knowledge retention and learning from new\ndata with an end-to-end model, paving the way for practical deployment of\ncontinual learning systems.\n","authors":["Chi Ian Tang","Lorena Qendro","Dimitris Spathis","Fahim Kawsar","Cecilia Mascolo","Akhil Mathur"],"pdf_url":"https://arxiv.org/pdf/2303.17235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17229v1","updated":"2023-03-30T08:56:28Z","published":"2023-03-30T08:56:28Z","title":"The Graphical Nadaraya-Watson Estimator on Latent Position Models","summary":"  Given a graph with a subset of labeled nodes, we are interested in the\nquality of the averaging estimator which for an unlabeled node predicts the\naverage of the observations of its labeled neighbours. We rigorously study\nconcentration properties, variance bounds and risk bounds in this context.\nWhile the estimator itself is very simple and the data generating process is\ntoo idealistic for practical applications, we believe that our small steps will\ncontribute towards the theoretical understanding of more sophisticated methods\nsuch as Graph Neural Networks.\n","authors":["M. Gjorgjevski","N. Keriven","S. Barthelmé","Y. De Castro"],"pdf_url":"https://arxiv.org/pdf/2303.17229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17218v1","updated":"2023-03-30T08:25:27Z","published":"2023-03-30T08:25:27Z","title":"HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on\n  FPGA Devices","summary":"  For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks\nhave proven to be highly effective, achieving state-of-the-art results. This\nstudy introduces a novel streaming architecture based toolflow for mapping such\nmodels onto FPGAs considering the model's inherent characteristics and the\nfeatures of the targeted FPGA device. The HARFLOW3D toolflow takes as input a\n3D CNN in ONNX format and a description of the FPGA characteristics, generating\na design that minimizes the latency of the computation. The toolflow is\ncomprised of a number of parts, including i) a 3D CNN parser, ii) a performance\nand resource model, iii) a scheduling algorithm for executing 3D models on the\ngenerated hardware, iv) a resource-aware optimization engine tailored for 3D\nmodels, v) an automated mapping to synthesizable code for FPGAs. The ability of\nthe toolflow to support a broad range of models and devices is shown through a\nnumber of experiments on various 3D CNN and FPGA system pairs. Furthermore, the\ntoolflow has produced high-performing results for 3D CNN models that have not\nbeen mapped to FPGAs before, demonstrating the potential of FPGA-based systems\nin this space. Overall, HARFLOW3D has demonstrated its ability to deliver\ncompetitive latency compared to a range of state-of-the-art hand-tuned\napproaches being able to achieve up to 5$\\times$ better performance compared to\nsome of the existing works.\n","authors":["Petros Toupas","Alexander Montgomerie-Corcoran","Christos-Savvas Bouganis","Dimitrios Tzovaras"],"pdf_url":"https://arxiv.org/pdf/2303.17218v1.pdf","comment":"11 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2205.00459v2","updated":"2023-03-30T07:12:36Z","published":"2022-05-01T12:44:49Z","title":"Training High-Performance Low-Latency Spiking Neural Networks by\n  Differentiation on Spike Representation","summary":"  Spiking Neural Network (SNN) is a promising energy-efficient AI model when\nimplemented on neuromorphic hardware. However, it is a challenge to efficiently\ntrain SNNs due to their non-differentiability. Most existing methods either\nsuffer from high latency (i.e., long simulation time steps), or cannot achieve\nas high performance as Artificial Neural Networks (ANNs). In this paper, we\npropose the Differentiation on Spike Representation (DSR) method, which could\nachieve high performance that is competitive to ANNs yet with low latency.\nFirst, we encode the spike trains into spike representation using (weighted)\nfiring rate coding. Based on the spike representation, we systematically derive\nthat the spiking dynamics with common neural models can be represented as some\nsub-differentiable mapping. With this viewpoint, our proposed DSR method trains\nSNNs through gradients of the mapping and avoids the common\nnon-differentiability problem in SNN training. Then we analyze the error when\nrepresenting the specific mapping with the forward computation of the SNN. To\nreduce such error, we propose to train the spike threshold in each layer, and\nto introduce a new hyperparameter for the neural models. With these components,\nthe DSR method can achieve state-of-the-art SNN performance with low latency on\nboth static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet,\nand DVS-CIFAR10.\n","authors":["Qingyan Meng","Mingqing Xiao","Shen Yan","Yisen Wang","Zhouchen Lin","Zhi-Quan Luo"],"pdf_url":"https://arxiv.org/pdf/2205.00459v2.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2212.01141v4","updated":"2023-03-30T07:11:56Z","published":"2022-12-02T12:42:53Z","title":"MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for\n  Multivariate Time Series","summary":"  Learning semantic-rich representations from raw unlabeled time series data is\ncritical for downstream tasks such as classification and forecasting.\nContrastive learning has recently shown its promising representation learning\ncapability in the absence of expert annotations. However, existing contrastive\napproaches generally treat each instance independently, which leads to false\nnegative pairs that share the same semantics. To tackle this problem, we\npropose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model,\nwhich exploits semantic information obtained from the hierarchical structure\nconsisting of multiple latent partitions for multivariate time series.\nMotivated by the observation that fine-grained clustering preserves higher\npurity while coarse-grained one reflects higher-level semantics, we propose a\nnovel downward masking strategy to filter out fake negatives and supplement\npositives by incorporating the multi-granularity information from the\nclustering hierarchy. In addition, a novel upward masking strategy is designed\nin MHCCL to remove outliers of clusters at each partition to refine prototypes,\nwhich helps speed up the hierarchical clustering process and improves the\nclustering quality. We conduct experimental evaluations on seven widely-used\nmultivariate time series datasets. The results demonstrate the superiority of\nMHCCL over the state-of-the-art approaches for unsupervised time series\nrepresentation learning.\n","authors":["Qianwen Meng","Hangwei Qian","Yong Liu","Lizhen Cui","Yonghui Xu","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2212.01141v4.pdf","comment":"accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2302.11883v3","updated":"2023-03-30T07:09:35Z","published":"2023-02-23T09:42:21Z","title":"PIFON-EPT: MR-Based Electrical Property Tomography Using\n  Physics-Informed Fourier Networks","summary":"  \\textit{Objective:} In this paper, we introduce Physics-Informed Fourier\nNetworks for Electrical Properties Tomography (PIFON-EPT), a novel deep\nlearning-based method that solves an inverse scattering problem based on noisy\nand/or incomplete magnetic resonance (MR) measurements. \\textit{Methods:} We\nused two separate fully-connected neural networks, namely $B_1^{+}$ Net and EP\nNet, to solve the Helmholtz equation in order to learn a de-noised version of\nthe input $B_1^{+}$ maps and estimate the object's EP. A random Fourier\nfeatures mapping was embedded into $B_1^{+}$ Net, to learn the high-frequency\ndetails of $B_1^{+}$ more efficiently. The two neural networks were trained\njointly by minimizing the combination of a physics-informed loss and a data\nmismatch loss via gradient descent. \\textit{Results:} We performed several\nnumerical experiments, showing that PIFON-EPT could provide physically\nconsistent reconstructions of the EP and transmit field. Even when only $50\\%$\nof the noisy MR measurements were used as inputs, our method could still\nreconstruct the EP and transmit field with average error $2.49\\%$, $4.09\\%$ and\n$0.32\\%$ for the relative permittivity, conductivity and $B_{1}^{+}$,\nrespectively, over the entire volume of the phantom. The generalized version of\nPIFON-EPT that accounts for gradients of EP yielded accurate results at the\ninterface between regions of different EP values without requiring any boundary\nconditions. \\textit{Conclusion:} This work demonstrated the feasibility of\nPIFON-EPT, suggesting it could be an accurate and effective method for EP\nestimation. \\textit{Significance:} PIFON-EPT can efficiently de-noise $B_1^{+}$\nmaps, which has the potential to improve other MR-based EPT techniques.\nFurthermore, PIFON-EPT is the first technique that can reconstruct EP and\n$B_{1}^{+}$ simultaneously from incomplete noisy MR measurements.\n","authors":["Xinling Yu","José E. C. Serrallés","Ilias I. Giannakopoulos","Ziyue Liu","Luca Daniel","Riccardo Lattanzi","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.11883v3.pdf","comment":"10 pages, submitted to IEEE TBME"},{"id":"http://arxiv.org/abs/2302.07685v2","updated":"2023-03-30T07:08:21Z","published":"2023-02-15T14:22:34Z","title":"Video Probabilistic Diffusion Models in Projected Latent Space","summary":"  Despite the remarkable progress in deep generative models, synthesizing\nhigh-resolution and temporally coherent videos still remains a challenge due to\ntheir high-dimensionality and complex temporal dynamics along with large\nspatial variations. Recent works on diffusion models have shown their potential\nto solve this challenge, yet they suffer from severe computation- and\nmemory-inefficiency that limit the scalability. To handle this issue, we\npropose a novel generative model for videos, coined projected latent video\ndiffusion models (PVDM), a probabilistic diffusion model which learns a video\ndistribution in a low-dimensional latent space and thus can be efficiently\ntrained with high-resolution videos under limited resources. Specifically, PVDM\nis composed of two components: (a) an autoencoder that projects a given video\nas 2D-shaped latent vectors that factorize the complex cubic structure of video\npixels and (b) a diffusion model architecture specialized for our new\nfactorized latent space and the training/sampling procedure to synthesize\nvideos of arbitrary length with a single model. Experiments on popular video\ngeneration datasets demonstrate the superiority of PVDM compared with previous\nvideo synthesis methods; e.g., PVDM obtains the FVD score of 639.7 on the\nUCF-101 long video (128 frames) generation benchmark, which improves 1773.4 of\nthe prior state-of-the-art.\n","authors":["Sihyun Yu","Kihyuk Sohn","Subin Kim","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2302.07685v2.pdf","comment":"CVPR 2023. Project page: https://sihyun.me/PVDM"},{"id":"http://arxiv.org/abs/2303.15991v2","updated":"2023-03-30T07:04:39Z","published":"2023-03-26T16:09:48Z","title":"Efficient Parallel Split Learning over Resource-constrained Wireless\n  Edge Networks","summary":"  The increasingly deeper neural networks hinder the democratization of\nprivacy-enhancing distributed learning, such as federated learning (FL), to\nresource-constrained devices. To overcome this challenge, in this paper, we\nadvocate the integration of edge computing paradigm and parallel split learning\n(PSL), allowing multiple client devices to offload substantial training\nworkloads to an edge server via layer-wise model split. By observing that\nexisting PSL schemes incur excessive training latency and large volume of data\ntransmissions, we propose an innovative PSL framework, namely, efficient\nparallel split learning (EPSL), to accelerate model training. To be specific,\nEPSL parallelizes client-side model training and reduces the dimension of local\ngradients for back propagation (BP) via last-layer gradient aggregation,\nleading to a significant reduction in server-side training and communication\nlatency. Moreover, by considering the heterogeneous channel conditions and\ncomputing capabilities at client devices, we jointly optimize subchannel\nallocation, power control, and cut layer selection to minimize the per-round\nlatency. Simulation results show that the proposed EPSL framework significantly\ndecreases the training latency needed to achieve a target accuracy compared\nwith the state-of-the-art benchmarks, and the tailored resource management and\nlayer split strategy can considerably reduce latency than the counterpart\nwithout optimization.\n","authors":["Zheng Lin","Guangyu Zhu","Yiqin Deng","Xianhao Chen","Yue Gao","Kaibin Huang","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2303.15991v2.pdf","comment":"15 pages, 13 figures"},{"id":"http://arxiv.org/abs/2303.07580v2","updated":"2023-03-30T06:54:01Z","published":"2023-03-14T01:56:15Z","title":"Sensitive Region-based Metamorphic Testing Framework using Explainable\n  AI","summary":"  Deep Learning (DL) is one of the most popular research topics in machine\nlearning and DL-driven image recognition systems have developed rapidly. Recent\nresearch has employed metamorphic testing (MT) to detect misclassified images.\nMost of them discuss metamorphic relations (MR), with limited attention given\nto which regions should be transformed. We focus on the fact that there are\nsensitive regions where even small transformations can easily change the\nprediction results and propose an MT framework that efficiently tests for\nregions prone to misclassification by transforming these sensitive regions. Our\nevaluation demonstrated that the sensitive regions can be specified by\nExplainable AI (XAI) and our framework effectively detects faults.\n","authors":["Yuma Torikoshi","Yasuharu Nishi","Juichi Takahashi"],"pdf_url":"https://arxiv.org/pdf/2303.07580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13916v2","updated":"2023-03-30T06:35:24Z","published":"2022-11-25T06:10:57Z","title":"Towards Good Practices for Missing Modality Robust Action Recognition","summary":"  Standard multi-modal models assume the use of the same modalities in training\nand inference stages. However, in practice, the environment in which\nmulti-modal models operate may not satisfy such assumption. As such, their\nperformances degrade drastically if any modality is missing in the inference\nstage. We ask: how can we train a model that is robust to missing modalities?\nThis paper seeks a set of good practices for multi-modal action recognition,\nwith a particular interest in circumstances where some modalities are not\navailable at an inference time. First, we study how to effectively regularize\nthe model during training (e.g., data augmentation). Second, we investigate on\nfusion methods for robustness to missing modalities: we find that\ntransformer-based fusion shows better robustness for missing modality than\nsummation or concatenation. Third, we propose a simple modular network,\nActionMAE, which learns missing modality predictive coding by randomly dropping\nmodality features and tries to reconstruct them with the remaining modality\nfeatures. Coupling these good practices, we build a model that is not only\neffective in multi-modal action recognition but also robust to modality\nmissing. Our model achieves the state-of-the-arts on multiple benchmarks and\nmaintains competitive performances even in missing modality scenarios. Codes\nare available at https://github.com/sangminwoo/ActionMAE.\n","authors":["Sangmin Woo","Sumin Lee","Yeonju Park","Muhammad Adi Nugroho","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2211.13916v2.pdf","comment":"AAAI 2023 (Oral); Code: https://github.com/sangminwoo/ActionMAE"},{"id":"http://arxiv.org/abs/2107.13163v3","updated":"2023-03-30T06:31:06Z","published":"2021-07-28T04:28:55Z","title":"Statistically Meaningful Approximation: a Case Study on Approximating\n  Turing Machines with Transformers","summary":"  A common lens to theoretically study neural net architectures is to analyze\nthe functions they can approximate. However, constructions from approximation\ntheory may be unrealistic and therefore less meaningful. For example, a common\nunrealistic trick is to encode target function values using infinite precision.\nTo address these issues, this work proposes a formal definition of\nstatistically meaningful (SM) approximation which requires the approximating\nnetwork to exhibit good statistical learnability. We study SM approximation for\ntwo function classes: boolean circuits and Turing machines. We show that\noverparameterized feedforward neural nets can SM approximate boolean circuits\nwith sample complexity depending only polynomially on the circuit size, not the\nsize of the network. In addition, we show that transformers can SM approximate\nTuring machines with computation time bounded by $T$ with sample complexity\npolynomial in the alphabet size, state space size, and $\\log (T)$. We also\nintroduce new tools for analyzing generalization which provide much tighter\nsample complexities than the typical VC-dimension or norm-based bounds, which\nmay be of independent interest.\n","authors":["Colin Wei","Yining Chen","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2107.13163v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11718v2","updated":"2023-03-30T06:13:59Z","published":"2022-05-24T01:42:46Z","title":"Semi-Parametric Inducing Point Networks and Neural Processes","summary":"  We introduce semi-parametric inducing point networks (SPIN), a\ngeneral-purpose architecture that can query the training set at inference time\nin a compute-efficient manner. Semi-parametric architectures are typically more\ncompact than parametric models, but their computational complexity is often\nquadratic. In contrast, SPIN attains linear complexity via a cross-attention\nmechanism between datapoints inspired by inducing point methods. Querying large\ntraining sets can be particularly useful in meta-learning, as it unlocks\nadditional training signal, but often exceeds the scaling limits of existing\nmodels. We use SPIN as the basis of the Inducing Point Neural Process, a\nprobabilistic model which supports large contexts in meta-learning and achieves\nhigh accuracy where existing models fail. In our experiments, SPIN reduces\nmemory requirements, improves accuracy across a range of meta-learning tasks,\nand improves state-of-the-art performance on an important practical problem,\ngenotype imputation.\n","authors":["Richa Rastogi","Yair Schiff","Alon Hacohen","Zhaozhi Li","Ian Lee","Yuntian Deng","Mert R. Sabuncu","Volodymyr Kuleshov"],"pdf_url":"https://arxiv.org/pdf/2205.11718v2.pdf","comment":"ICLR 2023 conference paper"},{"id":"http://arxiv.org/abs/1909.05402v2","updated":"2023-03-30T06:09:10Z","published":"2019-09-11T23:43:41Z","title":"Relaxed Actor-Critic with Convergence Guarantees for Continuous-Time\n  Optimal Control of Nonlinear Systems","summary":"  This paper presents the Relaxed Continuous-Time Actor-critic (RCTAC)\nalgorithm, a method for finding the nearly optimal policy for nonlinear\ncontinuous-time (CT) systems with known dynamics and infinite horizon, such as\nthe path-tracking control of vehicles. RCTAC has several advantages over\nexisting adaptive dynamic programming algorithms for CT systems. It does not\nrequire the ``admissibility\" of the initialized policy or the input-affine\nnature of controlled systems for convergence. Instead, given any initial\npolicy, RCTAC can converge to an admissible, and subsequently nearly optimal\npolicy for a general nonlinear system with a saturated controller. RCTAC\nconsists of two phases: a warm-up phase and a generalized policy iteration\nphase. The warm-up phase minimizes the square of the Hamiltonian to achieve\nadmissibility, while the generalized policy iteration phase relaxes the update\ntermination conditions for faster convergence. The convergence and optimality\nof the algorithm are proven through Lyapunov analysis, and its effectiveness is\ndemonstrated through simulations and real-world path-tracking tasks.\n","authors":["Jingliang Duan","Jie Li","Qiang Ge","Shengbo Eben Li","Monimoy Bujarbaruah","Fei Ma","Dezhao Zhang"],"pdf_url":"https://arxiv.org/pdf/1909.05402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.15200v5","updated":"2023-03-30T06:01:28Z","published":"2022-09-30T03:09:53Z","title":"An efficient encoder-decoder architecture with top-down attention for\n  speech separation","summary":"  Deep neural networks have shown excellent prospects in speech separation\ntasks. However, obtaining good results while keeping a low model complexity\nremains challenging in real-world applications. In this paper, we provide a\nbio-inspired efficient encoder-decoder architecture by mimicking the brain's\ntop-down attention, called TDANet, with decreased model complexity without\nsacrificing performance. The top-down attention in TDANet is extracted by the\nglobal attention (GA) module and the cascaded local attention (LA) layers. The\nGA module takes multi-scale acoustic features as input to extract global\nattention signal, which then modulates features of different scales by direct\ntop-down connections. The LA layers use features of adjacent layers as input to\nextract the local attention signal, which is used to modulate the lateral input\nin a top-down manner. On three benchmark datasets, TDANet consistently achieved\ncompetitive separation performance to previous state-of-the-art (SOTA) methods\nwith higher efficiency. Specifically, TDANet's multiply-accumulate operations\n(MACs) are only 5\\% of Sepformer, one of the previous SOTA models, and CPU\ninference time is only 10\\% of Sepformer. In addition, a large-size version of\nTDANet obtained SOTA results on three datasets, with MACs still only 10\\% of\nSepformer and the CPU inference time only 24\\% of Sepformer.\n","authors":["Kai Li","Runxuan Yang","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2209.15200v5.pdf","comment":"Accepted by ICLR 2023; Code & Demos:\n  https://cslikai.cn/project/TDANet/"},{"id":"http://arxiv.org/abs/2212.01130v5","updated":"2023-03-30T05:49:01Z","published":"2022-12-02T12:19:12Z","title":"Improving Pareto Front Learning via Multi-Sample Hypernetworks","summary":"  Pareto Front Learning (PFL) was recently introduced as an effective approach\nto obtain a mapping function from a given trade-off vector to a solution on the\nPareto front, which solves the multi-objective optimization (MOO) problem. Due\nto the inherent trade-off between conflicting objectives, PFL offers a flexible\napproach in many scenarios in which the decision makers can not specify the\npreference of one Pareto solution over another, and must switch between them\ndepending on the situation. However, existing PFL methods ignore the\nrelationship between the solutions during the optimization process, which\nhinders the quality of the obtained front. To overcome this issue, we propose a\nnovel PFL framework namely PHN-HVI, which employs a hypernetwork to generate\nmultiple solutions from a set of diverse trade-off preferences and enhance the\nquality of the Pareto front by maximizing the Hypervolume indicator defined by\nthese solutions. The experimental results on several MOO machine learning tasks\nshow that the proposed framework significantly outperforms the baselines in\nproducing the trade-off Pareto front.\n","authors":["Long P. Hoang","Dung D. Le","Tran Anh Tuan","Tran Ngoc Thang"],"pdf_url":"https://arxiv.org/pdf/2212.01130v5.pdf","comment":"Accepted to AAAI-23"},{"id":"http://arxiv.org/abs/2303.16839v2","updated":"2023-03-30T05:44:47Z","published":"2023-03-29T16:42:30Z","title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks","summary":"  The development of language models have moved from encoder-decoder to\ndecoder-only designs. In addition, the common knowledge has it that the two\nmost popular multimodal tasks, the generative and contrastive tasks, tend to\nconflict with one another, are hard to accommodate in one architecture, and\nfurther need complex adaptations for downstream tasks. We propose a novel\nparadigm of training with a decoder-only model for multimodal tasks, which is\nsurprisingly effective in jointly learning of these disparate vision-language\ntasks. This is done with a simple model, called MaMMUT. It consists of a single\nvision encoder and a text decoder, and is able to accommodate contrastive and\ngenerative learning by a novel two-pass approach on the text decoder. We\ndemonstrate that joint learning of these diverse objectives is simple,\neffective, and maximizes the weight-sharing of the model across these tasks.\nFurthermore, the same architecture enables straightforward extensions to\nopen-vocabulary object detection and video-language tasks. The model tackles a\ndiverse range of tasks, while being modest in capacity. Our model achieves the\nstate of the art on image-text and text-image retrieval, video question\nanswering and open-vocabulary detection tasks, outperforming much larger and\nmore extensively trained foundational models. It shows very competitive results\non VQA and Video Captioning, especially considering its capacity. Ablations\nconfirm the flexibility and advantages of our approach.\n","authors":["Weicheng Kuo","AJ Piergiovanni","Dahun Kim","Xiyang Luo","Ben Caine","Wei Li","Abhijit Ogale","Luowei Zhou","Andrew Dai","Zhifeng Chen","Claire Cui","Anelia Angelova"],"pdf_url":"https://arxiv.org/pdf/2303.16839v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17156v1","updated":"2023-03-30T05:27:46Z","published":"2023-03-30T05:27:46Z","title":"MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning\n  from Observations","summary":"  We study a new paradigm for sequential decision making, called offline Policy\nLearning from Observation (PLfO). Offline PLfO aims to learn policies using\ndatasets with substandard qualities: 1) only a subset of trajectories is\nlabeled with rewards, 2) labeled trajectories may not contain actions, 3)\nlabeled trajectories may not be of high quality, and 4) the overall data may\nnot have full coverage. Such imperfection is common in real-world learning\nscenarios, so offline PLfO encompasses many existing offline learning setups,\nincluding offline imitation learning (IL), ILfO, and reinforcement learning\n(RL). In this work, we present a generic approach, called Modality-agnostic\nAdversarial Hypothesis Adaptation for Learning from Observations (MAHALO), for\noffline PLfO. Built upon the pessimism concept in offline RL, MAHALO optimizes\nthe policy using a performance lower bound that accounts for uncertainty due to\nthe dataset's insufficient converge. We implement this idea by adversarially\ntraining data-consistent critic and reward functions in policy optimization,\nwhich forces the learned policy to be robust to the data deficiency. We show\nthat MAHALO consistently outperforms or matches specialized algorithms across a\nvariety of offline PLfO tasks in theory and experiments.\n","authors":["Anqi Li","Byron Boots","Ching-An Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.17156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17152v1","updated":"2023-03-30T05:19:43Z","published":"2023-03-30T05:19:43Z","title":"Mixed Autoencoder for Self-supervised Visual Representation Learning","summary":"  Masked Autoencoder (MAE) has demonstrated superior performance on various\nvision tasks via randomly masking image patches and reconstruction. However,\neffective data augmentation strategies for MAE still remain open questions,\ndifferent from those in contrastive learning that serve as the most important\npart. This paper studies the prevailing mixing augmentation for MAE. We first\ndemonstrate that naive mixing will in contrast degenerate model performance due\nto the increase of mutual information (MI). To address, we propose homologous\nrecognition, an auxiliary pretext task, not only to alleviate the MI\nincreasement by explicitly requiring each patch to recognize homologous\npatches, but also to perform object-aware self-supervised pre-training for\nbetter downstream dense perception performance. With extensive experiments, we\ndemonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the\nstate-of-the-art transfer results among masked image modeling (MIM)\naugmentations on different downstream tasks with significant efficiency.\nSpecifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9\nAP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base.\nMoreover, MixedAE surpasses iBOT, a strong MIM method combined with instance\ndiscrimination, while accelerating training by 2x. To our best knowledge, this\nis the very first work to consider mixing for MIM from the perspective of\npretext task design. Code will be made available.\n","authors":["Kai Chen","Zhili Liu","Lanqing Hong","Hang Xu","Zhenguo Li","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2303.17152v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15999v2","updated":"2023-03-30T05:13:59Z","published":"2023-03-28T14:15:13Z","title":"Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised\n  Regression Deep Learning Models","summary":"  In this work, the authors develop regression approaches based on deep\nlearning to perform thread density estimation for plain weave canvas analysis.\nPrevious approaches were based on Fourier analysis, which is quite robust for\nsome scenarios but fails in some others, in machine learning tools, that\ninvolve pre-labeling of the painting at hand, or the segmentation of thread\ncrossing points, that provides good estimations in all scenarios with no need\nof pre-labeling. The segmentation approach is time-consuming as the estimation\nof the densities is performed after locating the crossing points. In this novel\nproposal, we avoid this step by computing the density of threads directly from\nthe image with a regression deep learning model. We also incorporate some\nimprovements in the initial preprocessing of the input image with an impact on\nthe final error. Several models are proposed and analyzed to retain the best\none. Furthermore, we further reduce the density estimation error by introducing\na semi-supervised approach. The performance of our novel algorithm is analyzed\nwith works by Ribera, Vel\\'azquez, and Poussin where we compare our results to\nthe ones of previous approaches. Finally, the method is put into practice to\nsupport the change of authorship or a masterpiece at the Museo del Prado.\n","authors":["A. D. Bejarano","Juan J. Murillo-Fuentes","Laura Alba-Carcelen"],"pdf_url":"https://arxiv.org/pdf/2303.15999v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2110.10921v2","updated":"2023-03-30T05:05:23Z","published":"2021-10-21T06:26:31Z","title":"CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization","summary":"  Deep convolutional neural networks are shown to be overkill with high\nparametric and computational redundancy in many application scenarios, and an\nincreasing number of works have explored model pruning to obtain lightweight\nand efficient networks. However, most existing pruning approaches are driven by\nempirical heuristic and rarely consider the joint impact of channels, leading\nto unguaranteed and suboptimal performance. In this paper, we propose a novel\nchannel pruning method via Class-Aware Trace Ratio Optimization (CATRO) to\nreduce the computational burden and accelerate the model inference. Utilizing\nclass information from a few samples, CATRO measures the joint impact of\nmultiple channels by feature space discriminations and consolidates the\nlayer-wise impact of preserved channels. By formulating channel pruning as a\nsubmodular set function maximization problem, CATRO solves it efficiently via a\ntwo-stage greedy iterative optimization procedure. More importantly, we present\ntheoretical justifications on convergence of CATRO and performance of pruned\nnetworks. Experimental results demonstrate that CATRO achieves higher accuracy\nwith similar computation cost or lower computation cost with similar accuracy\nthan other state-of-the-art channel pruning algorithms. In addition, because of\nits class-aware property, CATRO is suitable to prune efficient networks\nadaptively for various classification subtasks, enhancing handy deployment and\nusage of deep networks in real-world applications.\n","authors":["Wenzheng Hu","Zhengping Che","Ning Liu","Mingyang Li","Jian Tang","Changshui Zhang","Jianqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2110.10921v2.pdf","comment":"Paper accepted by IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS)"},{"id":"http://arxiv.org/abs/2212.04088v3","updated":"2023-03-30T04:50:44Z","published":"2022-12-08T05:46:32Z","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models","summary":"  This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n","authors":["Chan Hee Song","Jiaman Wu","Clayton Washington","Brian M. Sadler","Wei-Lun Chao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2212.04088v3.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2204.12723v3","updated":"2023-03-30T04:17:27Z","published":"2022-04-27T06:33:37Z","title":"Information-theoretic limitations of data-based price discrimination","summary":"  This paper studies third-degree price discrimination (3PD) based on a random\nsample of valuation and covariate data, where the covariate is continuous, and\nthe distribution of the data is unknown to the seller. The main results of this\npaper are twofold. The first set of results is pricing strategy independent and\nreveals the fundamental information-theoretic limitation of any data-based\npricing strategy in revenue generation for two cases: 3PD and uniform pricing.\nThe second set of results proposes the $K$-markets empirical revenue\nmaximization (ERM) strategy and shows that the $K$-markets ERM and the uniform\nERM strategies achieve the optimal rate of convergence in revenue to that\ngenerated by their respective true-distribution 3PD and uniform pricing optima.\nOur theoretical and numerical results suggest that the uniform (i.e.,\n$1$-market) ERM strategy generates a larger revenue than the $K$-markets ERM\nstrategy when the sample size is small enough, and vice versa.\n","authors":["Haitian Xie","Ying Zhu","Denis Shishkin"],"pdf_url":"https://arxiv.org/pdf/2204.12723v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.10543v2","updated":"2023-03-30T03:45:40Z","published":"2020-02-24T21:01:47Z","title":"Variational Wasserstein Barycenters for Geometric Clustering","summary":"  We propose to compute Wasserstein barycenters (WBs) by solving for Monge maps\nwith variational principle. We discuss the metric properties of WBs and explore\ntheir connections, especially the connections of Monge WBs, to K-means\nclustering and co-clustering. We also discuss the feasibility of Monge WBs on\nunbalanced measures and spherical domains. We propose two new problems --\nregularized K-means and Wasserstein barycenter compression. We demonstrate the\nuse of VWBs in solving these clustering-related problems.\n","authors":["Liang Mi"],"pdf_url":"https://arxiv.org/pdf/2002.10543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01883v2","updated":"2023-03-30T03:39:28Z","published":"2022-11-03T15:17:04Z","title":"Faster Adaptive Momentum-Based Federated Methods for Distributed\n  Composition Optimization","summary":"  Federated Learning is a popular distributed learning paradigm in machine\nlearning. Meanwhile, composition optimization is an effective hierarchical\nlearning model, which appears in many machine learning applications such as\nmeta learning and robust learning. More recently, although a few federated\ncomposition optimization algorithms have been proposed, they still suffer from\nhigh sample and communication complexities. In the paper, thus, we propose a\nclass of faster federated compositional optimization algorithms (i.e., MFCGD\nand AdaMFCGD) to solve the nonconvex distributed composition problems, which\nbuilds on the momentum-based variance reduced and local-SGD techniques. In\nparticular, our adaptive algorithm (i.e., AdaMFCGD) uses a unified adaptive\nmatrix to flexibly incorporate various adaptive learning rates. Moreover, we\nprovide a solid theoretical analysis for our algorithms under non-i.i.d.\nsetting, and prove our algorithms obtain a lower sample and communication\ncomplexities simultaneously than the existing federated compositional\nalgorithms. Specifically, our algorithms obtain lower sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$ with lower communication complexity of\n$\\tilde{O}(\\epsilon^{-2})$ in finding an $\\epsilon$-stationary solution. We\nconduct the numerical experiments on robust federated learning and distributed\nmeta learning tasks to demonstrate the efficiency of our algorithms.\n","authors":["Feihu Huang"],"pdf_url":"https://arxiv.org/pdf/2211.01883v2.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2201.09418v3","updated":"2023-03-30T03:26:24Z","published":"2022-01-24T02:19:05Z","title":"Approximation bounds for norm constrained neural networks with\n  applications to regression and GANs","summary":"  This paper studies the approximation capacity of ReLU neural networks with\nnorm constraint on the weights. We prove upper and lower bounds on the\napproximation error of these networks for smooth function classes. The lower\nbound is derived through the Rademacher complexity of neural networks, which\nmay be of independent interest. We apply these approximation bounds to analyze\nthe convergences of regression using norm constrained neural networks and\ndistribution estimation by GANs. In particular, we obtain convergence rates for\nover-parameterized neural networks. It is also shown that GANs can achieve\noptimal rate of learning probability distributions, when the discriminator is a\nproperly chosen norm constrained neural network.\n","authors":["Yuling Jiao","Yang Wang","Yunfei Yang"],"pdf_url":"https://arxiv.org/pdf/2201.09418v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02110v4","updated":"2023-03-30T03:23:16Z","published":"2023-03-03T17:51:08Z","title":"Need for Objective Task-based Evaluation of Deep Learning-Based\n  Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT","summary":"  Artificial intelligence-based methods have generated substantial interest in\nnuclear medicine. An area of significant interest has been using deep-learning\n(DL)-based approaches for denoising images acquired with lower doses, shorter\nacquisition times, or both. Objective evaluation of these approaches is\nessential for clinical application. DL-based approaches for denoising\nnuclear-medicine images have typically been evaluated using fidelity-based\nfigures of merit (FoMs) such as RMSE and SSIM. However, these images are\nacquired for clinical tasks and thus should be evaluated based on their\nperformance in these tasks. Our objectives were to (1) investigate whether\nevaluation with these FoMs is consistent with objective clinical-task-based\nevaluation; (2) provide a theoretical analysis for determining the impact of\ndenoising on signal-detection tasks; (3) demonstrate the utility of virtual\nclinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a\nDL-based method for denoising myocardial perfusion SPECT (MPS) images was\nconducted. The impact of DL-based denoising was evaluated using fidelity-based\nFoMs and AUC, which quantified performance on detecting perfusion defects in\nMPS images as obtained using a model observer with anthropomorphic channels.\nBased on fidelity-based FoMs, denoising using the considered DL-based method\nled to significantly superior performance. However, based on ROC analysis,\ndenoising did not improve, and in fact, often degraded detection-task\nperformance. The results motivate the need for objective task-based evaluation\nof DL-based denoising approaches. Further, this study shows how VCTs provide a\nmechanism to conduct such evaluations using VCTs. Finally, our theoretical\ntreatment reveals insights into the reasons for the limited performance of the\ndenoising approach.\n","authors":["Zitong Yu","Md Ashequr Rahman","Richard Laforest","Thomas H. Schindler","Robert J. Gropler","Richard L. Wahl","Barry A. Siegel","Abhinav K. Jha"],"pdf_url":"https://arxiv.org/pdf/2303.02110v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17114v1","updated":"2023-03-30T02:59:51Z","published":"2023-03-30T02:59:51Z","title":"Deep Generative Model and Its Applications in Efficient Wireless Network\n  Management: A Tutorial and Case Study","summary":"  With the phenomenal success of diffusion models and ChatGPT, deep generation\nmodels (DGMs) have been experiencing explosive growth from 2022. Not limited to\ncontent generation, DGMs are also widely adopted in Internet of Things,\nMetaverse, and digital twin, due to their outstanding ability to represent\ncomplex patterns and generate plausible samples. In this article, we explore\nthe applications of DGMs in a crucial task, i.e., improving the efficiency of\nwireless network management. Specifically, we firstly overview the generative\nAI, as well as three representative DGMs. Then, a DGM-empowered framework for\nwireless network management is proposed, in which we elaborate the issues of\nthe conventional network management approaches, why DGMs can address them\nefficiently, and the step-by-step workflow for applying DGMs in managing\nwireless networks. Moreover, we conduct a case study on network economics,\nusing the state-of-the-art DGM model, i.e., diffusion model, to generate\neffective contracts for incentivizing the mobile AI-Generated Content (AIGC)\nservices. Last but not least, we discuss important open directions for the\nfurther research.\n","authors":["Yinqiu Liu","Hongyang Du","Dusit Niyato","Jiawen Kang","Zehui Xiong","Dong In Kim","Abbas Jamalipour"],"pdf_url":"https://arxiv.org/pdf/2303.17114v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17110v1","updated":"2023-03-30T02:51:00Z","published":"2023-03-30T02:51:00Z","title":"Contextual Combinatorial Bandits with Probabilistically Triggered Arms","summary":"  We study contextual combinatorial bandits with probabilistically triggered\narms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide\nrange of applications, such as contextual cascading bandits and contextual\ninfluence maximization bandits. Under the triggering probability modulated\n(TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel\nanalysis that achieves an $\\tilde{O}(d\\sqrt{KT})$ regret bound, removing a\npotentially exponentially large factor $O(1/p_{\\min})$, where $d$ is the\ndimension of contexts, $p_{\\min}$ is the minimum positive probability that any\narm can be triggered, and batch-size $K$ is the maximum number of arms that can\nbe triggered per round. Under the variance modulated (VM) or triggering\nprobability and variance modulated (TPVM) conditions, we propose a new\nvariance-adaptive algorithm VAC$^2$-UCB and derive a regret bound\n$\\tilde{O}(d\\sqrt{T})$, which is independent of the batch-size $K$. As a\nvaluable by-product, we find our analysis technique and variance-adaptive\nalgorithm can be applied to the CMAB-T and C$^2$MAB~setting, improving existing\nresults there as well. We also include experiments that demonstrate the\nimproved performance of our algorithms compared with benchmark algorithms on\nsynthetic and real-world datasets.\n","authors":["Xutong Liu","Jinhang Zuo","Siwei Wang","John C. S. Lui","Mohammad Hajiesmaili","Adam Wierman","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2303.17110v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2208.14837"},{"id":"http://arxiv.org/abs/2303.17109v1","updated":"2023-03-30T02:50:49Z","published":"2023-03-30T02:50:49Z","title":"Efficient Sampling of Stochastic Differential Equations with Positive\n  Semi-Definite Models","summary":"  This paper deals with the problem of efficient sampling from a stochastic\ndifferential equation, given the drift function and the diffusion matrix. The\nproposed approach leverages a recent model for probabilities\n\\citep{rudi2021psd} (the positive semi-definite -- PSD model) from which it is\npossible to obtain independent and identically distributed (i.i.d.) samples at\nprecision $\\varepsilon$ with a cost that is $m^2 d \\log(1/\\varepsilon)$ where\n$m$ is the dimension of the model, $d$ the dimension of the space. The proposed\napproach consists in: first, computing the PSD model that satisfies the\nFokker-Planck equation (or its fractional variant) associated with the SDE, up\nto error $\\varepsilon$, and then sampling from the resulting PSD model.\nAssuming some regularity of the Fokker-Planck solution (i.e. $\\beta$-times\ndifferentiability plus some geometric condition on its zeros) We obtain an\nalgorithm that: (a) in the preparatory phase obtains a PSD model with L2\ndistance $\\varepsilon$ from the solution of the equation, with a model of\ndimension $m = \\varepsilon^{-(d+1)/(\\beta-2s)} (\\log(1/\\varepsilon))^{d+1}$\nwhere $0<s\\leq1$ is the fractional power to the Laplacian, and total\ncomputational complexity of $O(m^{3.5} \\log(1/\\varepsilon))$ and then (b) for\nFokker-Planck equation, it is able to produce i.i.d.\\ samples with error\n$\\varepsilon$ in Wasserstein-1 distance, with a cost that is $O(d\n\\varepsilon^{-2(d+1)/\\beta-2} \\log(1/\\varepsilon)^{2d+3})$ per sample. This\nmeans that, if the probability associated with the SDE is somewhat regular,\ni.e. $\\beta \\geq 4d+2$, then the algorithm requires $O(\\varepsilon^{-0.88}\n\\log(1/\\varepsilon)^{4.5d})$ in the preparatory phase, and\n$O(\\varepsilon^{-1/2}\\log(1/\\varepsilon)^{2d+2})$ for each sample. Our results\nsuggest that as the true solution gets smoother, we can circumvent the curse of\ndimensionality without requiring any sort of convexity.\n","authors":["Anant Raj","Umut Şimşekli","Alessandro Rudi"],"pdf_url":"https://arxiv.org/pdf/2303.17109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13750v2","updated":"2023-03-30T02:25:54Z","published":"2023-03-24T02:07:46Z","title":"LON-GNN: Spectral GNNs with Learnable Orthonormal Basis","summary":"  In recent years, a plethora of spectral graph neural networks (GNN) methods\nhave utilized polynomial basis with learnable coefficients to achieve top-tier\nperformances on many node-level tasks. Although various kinds of polynomial\nbases have been explored, each such method adopts a fixed polynomial basis\nwhich might not be the optimal choice for the given graph. Besides, we identify\nthe so-called over-passing issue of these methods and show that it is somewhat\nrooted in their less-principled regularization strategy and unnormalized basis.\nIn this paper, we make the first attempts to address these two issues.\nLeveraging Jacobi polynomials, we design a novel spectral GNN, LON-GNN, with\nLearnable OrthoNormal bases and prove that regularizing coefficients becomes\nequivalent to regularizing the norm of learned filter function now. We conduct\nextensive experiments on diverse graph datasets to evaluate the fitting and\ngeneralization capability of LON-GNN, where the results imply its superiority.\n","authors":["Qian Tao","Zhen Wang","Wenyuan Yu","Yaliang Li","Zhewei Wei"],"pdf_url":"https://arxiv.org/pdf/2303.13750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17093v1","updated":"2023-03-30T01:47:23Z","published":"2023-03-30T01:47:23Z","title":"OpenMix: Exploring Outlier Samples for Misclassification Detection","summary":"  Reliable confidence estimation for deep neural classifiers is a challenging\nyet fundamental requirement in high-stakes applications. Unfortunately, modern\ndeep neural networks are often overconfident for their erroneous predictions.\nIn this work, we exploit the easily available outlier samples, i.e., unlabeled\nsamples coming from non-target classes, for helping detect misclassification\nerrors. Particularly, we find that the well-known Outlier Exposure, which is\npowerful in detecting out-of-distribution (OOD) samples from unknown classes,\ndoes not provide any gain in identifying misclassification errors. Based on\nthese observations, we propose a novel method called OpenMix, which\nincorporates open-world knowledge by learning to reject uncertain\npseudo-samples generated via outlier transformation. OpenMix significantly\nimproves confidence reliability under various scenarios, establishing a strong\nand unified framework for detecting both misclassified samples from known\nclasses and OOD samples from unknown classes. The code is publicly available at\nhttps://github.com/Impression2805/OpenMix.\n","authors":["Fei Zhu","Zhen Cheng","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2303.17093v1.pdf","comment":"Accepted by CVPR 2023 (Highlight)"},{"id":"http://arxiv.org/abs/2301.00497v2","updated":"2023-03-30T01:29:39Z","published":"2023-01-02T01:12:29Z","title":"Efficient Online Learning with Memory via Frank-Wolfe Optimization:\n  Algorithms with Bounded Dynamic Regret and Applications to Control","summary":"  Projection operations are a typical computation bottleneck in online\nlearning. In this paper, we enable projection-free online learning within the\nframework of Online Convex Optimization with Memory (OCO-M) -- OCO-M captures\nhow the history of decisions affects the current outcome by allowing the online\nlearning loss functions to depend on both current and past decisions.\nParticularly, we introduce the first projection-free meta-base learning\nalgorithm with memory that minimizes dynamic regret, i.e., that minimizes the\nsuboptimality against any sequence of time-varying decisions. We are motivated\nby artificial intelligence applications where autonomous agents need to adapt\nto time-varying environments in real-time, accounting for how past decisions\naffect the present. Examples of such applications are: online control of\ndynamical systems; statistical arbitrage; and time series prediction. The\nalgorithm builds on the Online Frank-Wolfe (OFW) and Hedge algorithms. We\ndemonstrate how our algorithm can be applied to the online control of linear\ntime-varying systems in the presence of unpredictable process noise. To this\nend, we develop the first controller with memory and bounded dynamic regret\nagainst any optimal time-varying linear feedback control policy. We validate\nour algorithm in simulated scenarios of online control of linear time-invariant\nsystems.\n","authors":["Hongyu Zhou","Zirui Xu","Vasileios Tzoumas"],"pdf_url":"https://arxiv.org/pdf/2301.00497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.15430v3","updated":"2023-03-30T01:08:17Z","published":"2022-10-27T14:08:25Z","title":"Student-centric Model of Learning Management System Activity and\n  Academic Performance: from Correlation to Causation","summary":"  In recent years, there is a lot of interest in modeling students' digital\ntraces in Learning Management System (LMS) to understand students' learning\nbehavior patterns including aspects of meta-cognition and self-regulation, with\nthe ultimate goal to turn those insights into actionable information to support\nstudents to improve their learning outcomes. In achieving this goal, however,\nthere are two main issues that need to be addressed given the existing\nliterature. Firstly, most of the current work is course-centered (i.e. models\nare built from data for a specific course) rather than student-centered;\nsecondly, a vast majority of the models are correlational rather than causal.\nThose issues make it challenging to identify the most promising actionable\nfactors for intervention at the student level where most of the campus-wide\nacademic support is designed for. In this paper, we explored a student-centric\nanalytical framework for LMS activity data that can provide not only\ncorrelational but causal insights mined from observational data. We\ndemonstrated this approach using a dataset of 1651 computing major students at\na public university in the US during one semester in the Fall of 2019. This\ndataset includes students' fine-grained LMS interaction logs and administrative\ndata, e.g. demographics and academic performance. In addition, we expand the\nrepository of LMS behavior indicators to include those that can characterize\nthe time-of-the-day of login (e.g. chronotype). Our analysis showed that\nstudent login volume, compared with other login behavior indicators, is both\nstrongly correlated and causally linked to student academic performance,\nespecially among students with low academic performance. We envision that those\ninsights will provide convincing evidence for college student support groups to\nlaunch student-centered and targeted interventions that are effective and\nscalable.\n","authors":["Varun Mandalapu","Lujie Karen Chen","Sushruta Shetty","Zhiyuan Chen","Jiaqi Gong"],"pdf_url":"https://arxiv.org/pdf/2210.15430v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16317v2","updated":"2023-03-30T01:08:03Z","published":"2023-03-28T21:27:36Z","title":"Operator learning with PCA-Net: upper and lower complexity bounds","summary":"  PCA-Net is a recently proposed neural operator architecture which combines\nprincipal component analysis (PCA) with neural networks to approximate\noperators between infinite-dimensional function spaces. The present work\ndevelops approximation theory for this approach, improving and significantly\nextending previous work in this direction: First, a novel universal\napproximation result is derived, under minimal assumptions on the underlying\noperator and the data-generating distribution. Then, two potential obstacles to\nefficient operator learning with PCA-Net are identified, and made precise\nthrough lower complexity bounds; the first relates to the complexity of the\noutput distribution, measured by a slow decay of the PCA eigenvalues. The other\nobstacle relates to the inherent complexity of the space of operators between\ninfinite-dimensional input and output spaces, resulting in a rigorous and\nquantifiable statement of the curse of dimensionality. In addition to these\nlower bounds, upper complexity bounds are derived. A suitable smoothness\ncriterion is shown to ensure an algebraic decay of the PCA eigenvalues.\nFurthermore, it is shown that PCA-Net can overcome the general curse of\ndimensionality for specific operators of interest, arising from the Darcy flow\nand the Navier-Stokes equations.\n","authors":["Samuel Lanthaler"],"pdf_url":"https://arxiv.org/pdf/2303.16317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17080v1","updated":"2023-03-30T00:59:37Z","published":"2023-03-30T00:59:37Z","title":"Mole Recruitment: Poisoning of Image Classifiers via Selective Batch\n  Sampling","summary":"  In this work, we present a data poisoning attack that confounds machine\nlearning models without any manipulation of the image or label. This is\nachieved by simply leveraging the most confounding natural samples found within\nthe training data itself, in a new form of a targeted attack coined \"Mole\nRecruitment.\" We define moles as the training samples of a class that appear\nmost similar to samples of another class, and show that simply restructuring\ntraining batches with an optimal number of moles can lead to significant\ndegradation in the performance of the targeted class. We show the efficacy of\nthis novel attack in an offline setting across several standard image\nclassification datasets, and demonstrate the real-world viability of this\nattack in a continual learning (CL) setting. Our analysis reveals that\nstate-of-the-art models are susceptible to Mole Recruitment, thereby exposing a\npreviously undetected vulnerability of image classifiers.\n","authors":["Ethan Wisdom","Tejas Gokhale","Chaowei Xiao","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2303.17080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17078v1","updated":"2023-03-30T00:57:59Z","published":"2023-03-30T00:57:59Z","title":"Machine Learning for Partial Differential Equations","summary":"  Partial differential equations (PDEs) are among the most universal and\nparsimonious descriptions of natural physical laws, capturing a rich variety of\nphenomenology and multi-scale physics in a compact and symbolic representation.\nThis review will examine several promising avenues of PDE research that are\nbeing advanced by machine learning, including: 1) the discovery of new\ngoverning PDEs and coarse-grained approximations for complex natural and\nengineered systems, 2) learning effective coordinate systems and reduced-order\nmodels to make PDEs more amenable to analysis, and 3) representing solution\noperators and improving traditional numerical algorithms. In each of these\nfields, we summarize key advances, ongoing challenges, and opportunities for\nfurther development.\n","authors":["Steven L. Brunton","J. Nathan Kutz"],"pdf_url":"https://arxiv.org/pdf/2303.17078v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.17076v1","updated":"2023-03-30T00:51:12Z","published":"2023-03-30T00:51:12Z","title":"DiffCollage: Parallel Generation of Large Content with Diffusion Models","summary":"  We present DiffCollage, a compositional diffusion model that can generate\nlarge content by leveraging diffusion models trained on generating pieces of\nthe large content. Our approach is based on a factor graph representation where\neach factor node represents a portion of the content and a variable node\nrepresents their overlap. This representation allows us to aggregate\nintermediate outputs from diffusion models defined on individual nodes to\ngenerate content of arbitrary size and shape in parallel without resorting to\nan autoregressive generation procedure. We apply DiffCollage to various tasks,\nincluding infinite image generation, panorama image generation, and\nlong-duration text-guided motion generation. Extensive experimental results\nwith a comparison to strong autoregressive baselines verify the effectiveness\nof our approach.\n","authors":["Qinsheng Zhang","Jiaming Song","Xun Huang","Yongxin Chen","Ming-Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2303.17076v1.pdf","comment":"CVPR 2023 project page\n  https://research.nvidia.com/labs/dir/diffcollage"},{"id":"http://arxiv.org/abs/2303.16208v2","updated":"2023-03-30T00:50:26Z","published":"2023-03-27T23:55:25Z","title":"Lifting uniform learners via distributional decomposition","summary":"  We show how any PAC learning algorithm that works under the uniform\ndistribution can be transformed, in a blackbox fashion, into one that works\nunder an arbitrary and unknown distribution $\\mathcal{D}$. The efficiency of\nour transformation scales with the inherent complexity of $\\mathcal{D}$,\nrunning in $\\mathrm{poly}(n, (md)^d)$ time for distributions over $\\{\\pm 1\\}^n$\nwhose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample\ncomplexity of the original algorithm. For monotone distributions our\ntransformation uses only samples from $\\mathcal{D}$, and for general ones it\nuses subcube conditioning samples.\n  A key technical ingredient is an algorithm which, given the aforementioned\naccess to $\\mathcal{D}$, produces an optimal decision tree decomposition of\n$\\mathcal{D}$: an approximation of $\\mathcal{D}$ as a mixture of uniform\ndistributions over disjoint subcubes. With this decomposition in hand, we run\nthe uniform-distribution learner on each subcube and combine the hypotheses\nusing the decision tree. This algorithmic decomposition lemma also yields new\nalgorithms for learning decision tree distributions with runtimes that\nexponentially improve on the prior state of the art -- results of independent\ninterest in distribution learning.\n","authors":["Guy Blanc","Jane Lange","Ali Malik","Li-Yang Tan"],"pdf_url":"https://arxiv.org/pdf/2303.16208v2.pdf","comment":"To appear in STOC 2023"},{"id":"http://arxiv.org/abs/2303.16270v2","updated":"2023-03-30T00:42:31Z","published":"2023-03-28T19:30:23Z","title":"Communication-Efficient Vertical Federated Learning with Limited\n  Overlapping Samples","summary":"  Federated learning is a popular collaborative learning approach that enables\nclients to train a global model without sharing their local data. Vertical\nfederated learning (VFL) deals with scenarios in which the data on clients have\ndifferent feature spaces but share some overlapping samples. Existing VFL\napproaches suffer from high communication costs and cannot deal efficiently\nwith limited overlapping samples commonly seen in the real world. We propose a\npractical vertical federated learning (VFL) framework called \\textbf{one-shot\nVFL} that can solve the communication bottleneck and the problem of limited\noverlapping samples simultaneously based on semi-supervised learning. We also\npropose \\textbf{few-shot VFL} to improve the accuracy further with just one\nmore communication round between the server and the clients. In our proposed\nframework, the clients only need to communicate with the server once or only a\nfew times. We evaluate the proposed VFL framework on both image and tabular\ndatasets. Our methods can improve the accuracy by more than 46.5\\% and reduce\nthe communication cost by more than 330$\\times$ compared with state-of-the-art\nVFL methods when evaluated on CIFAR-10. Our code will be made publicly\navailable at \\url{https://nvidia.github.io/NVFlare/research/one-shot-vfl}.\n","authors":["Jingwei Sun","Ziyue Xu","Dong Yang","Vishwesh Nath","Wenqi Li","Can Zhao","Daguang Xu","Yiran Chen","Holger R. Roth"],"pdf_url":"https://arxiv.org/pdf/2303.16270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08049v6","updated":"2023-03-30T00:32:17Z","published":"2022-12-15T18:55:23Z","title":"Sliced Optimal Partial Transport","summary":"  Optimal transport (OT) has become exceedingly popular in machine learning,\ndata science, and computer vision. The core assumption in the OT problem is the\nequal total amount of mass in source and target measures, which limits its\napplication. Optimal Partial Transport (OPT) is a recently proposed solution to\nthis limitation. Similar to the OT problem, the computation of OPT relies on\nsolving a linear programming problem (often in high dimensions), which can\nbecome computationally prohibitive. In this paper, we propose an efficient\nalgorithm for calculating the OPT problem between two non-negative measures in\none dimension. Next, following the idea of sliced OT distances, we utilize\nslicing to define the sliced OPT distance. Finally, we demonstrate the\ncomputational and accuracy benefits of the sliced OPT-based method in various\nnumerical experiments. In particular, we show an application of our proposed\nSliced-OPT in noisy point cloud registration.\n","authors":["Yikun Bai","Bernard Schmitzer","Mathew Thorpe","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2212.08049v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17071v1","updated":"2023-03-30T00:30:19Z","published":"2023-03-30T00:30:19Z","title":"DERA: Enhancing Large Language Model Completions with Dialog-Enabled\n  Resolving Agents","summary":"  Large language models (LLMs) have emerged as valuable tools for many natural\nlanguage understanding tasks. In safety-critical applications such as\nhealthcare, the utility of these models is governed by their ability to\ngenerate outputs that are factually accurate and complete. In this work, we\npresent dialog-enabled resolving agents (DERA). DERA is a paradigm made\npossible by the increased conversational abilities of LLMs, namely GPT-4. It\nprovides a simple, interpretable forum for models to communicate feedback and\niteratively improve output. We frame our dialog as a discussion between two\nagent types - a Researcher, who processes information and identifies crucial\nproblem components, and a Decider, who has the autonomy to integrate the\nResearcher's information and makes judgments on the final output.\n  We test DERA against three clinically-focused tasks. For medical conversation\nsummarization and care plan generation, DERA shows significant improvement over\nthe base GPT-4 performance in both human expert preference evaluations and\nquantitative metrics. In a new finding, we also show that GPT-4's performance\n(70%) on an open-ended version of the MedQA question-answering (QA) dataset\n(Jin et al. 2021, USMLE) is well above the passing level (60%), with DERA\nshowing similar performance. We release the open-ended MEDQA dataset at\nhttps://github.com/curai/curai-research/tree/main/DERA.\n","authors":["Varun Nair","Elliot Schumacher","Geoffrey Tso","Anitha Kannan"],"pdf_url":"https://arxiv.org/pdf/2303.17071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17745v1","updated":"2023-03-30T23:43:22Z","published":"2023-03-30T23:43:22Z","title":"A Note On Nonlinear Regression Under L2 Loss","summary":"  We investigate the nonlinear regression problem under L2 loss (square loss)\nfunctions. Traditional nonlinear regression models often result in non-convex\noptimization problems with respect to the parameter set. We show that a convex\nnonlinear regression model exists for the traditional least squares problem,\nwhich can be a promising towards designing more complex systems with easier to\ntrain models.\n","authors":["Kaan Gokcesu","Hakan Gokcesu"],"pdf_url":"https://arxiv.org/pdf/2303.17745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17743v1","updated":"2023-03-30T23:30:42Z","published":"2023-03-30T23:30:42Z","title":"FairGen: Towards Fair Graph Generation","summary":"  There have been tremendous efforts over the past decades dedicated to the\ngeneration of realistic graphs in a variety of domains, ranging from social\nnetworks to computer networks, from gene regulatory networks to online\ntransaction networks. Despite the remarkable success, the vast majority of\nthese works are unsupervised in nature and are typically trained to minimize\nthe expected graph reconstruction loss, which would result in the\nrepresentation disparity issue in the generated graphs, i.e., the protected\ngroups (often minorities) contribute less to the objective and thus suffer from\nsystematically higher errors. In this paper, we aim to tailor graph generation\nto downstream mining tasks by leveraging label information and user-preferred\nparity constraint. In particular, we start from the investigation of\nrepresentation disparity in the context of graph generative models. To mitigate\nthe disparity, we propose a fairness-aware graph generative model named\nFairGen. Our model jointly trains a label-informed graph generation module and\na fair representation learning module by progressively learning the behaviors\nof the protected and unprotected groups, from the `easy' concepts to the `hard'\nones. In addition, we propose a generic context sampling strategy for graph\ngenerative models, which is proven to be capable of fairly capturing the\ncontextual information of each group with a high probability. Experimental\nresults on seven real-world data sets, including web-based graphs, demonstrate\nthat FairGen (1) obtains performance on par with state-of-the-art graph\ngenerative models across six network properties, (2) mitigates the\nrepresentation disparity issues in the generated graphs, and (3) substantially\nboosts the model performance by up to 17% in downstream tasks via data\naugmentation.\n","authors":["Lecheng Zheng","Dawei Zhou","Hanghang Tong","Jiejun Xu","Yada Zhu","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2303.17743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.01752v2","updated":"2023-03-30T23:15:07Z","published":"2022-02-03T18:18:28Z","title":"Near-Optimal Learning of Extensive-Form Games with Imperfect Information","summary":"  This paper resolves the open question of designing near-optimal algorithms\nfor learning imperfect-information extensive-form games from bandit feedback.\nWe present the first line of algorithms that require only\n$\\widetilde{\\mathcal{O}}((XA+YB)/\\varepsilon^2)$ episodes of play to find an\n$\\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where\n$X,Y$ are the number of information sets and $A,B$ are the number of actions\nfor the two players. This improves upon the best known sample complexity of\n$\\widetilde{\\mathcal{O}}((X^2A+Y^2B)/\\varepsilon^2)$ by a factor of\n$\\widetilde{\\mathcal{O}}(\\max\\{X, Y\\})$, and matches the information-theoretic\nlower bound up to logarithmic factors. We achieve this sample complexity by two\nnew algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual\nRegret Minimization. Both algorithms rely on novel approaches of integrating\n\\emph{balanced exploration policies} into their classical counterparts. We also\nextend our results to learning Coarse Correlated Equilibria in multi-player\ngeneral-sum games.\n","authors":["Yu Bai","Chi Jin","Song Mei","Tiancheng Yu"],"pdf_url":"https://arxiv.org/pdf/2202.01752v2.pdf","comment":"Updated V2 to be consistent with ICML 2022 camera-ready version, with\n  an additional analysis of CFR in full-feedback setting in Appendix F"},{"id":"http://arxiv.org/abs/1902.09037v2","updated":"2023-03-30T22:42:38Z","published":"2019-02-24T23:41:54Z","title":"Adaptive Estimators Show Information Compression in Deep Neural Networks","summary":"  To improve how neural networks function it is crucial to understand their\nlearning process. The information bottleneck theory of deep learning proposes\nthat neural networks achieve good generalization by compressing their\nrepresentations to disregard information that is not relevant to the task.\nHowever, empirical evidence for this theory is conflicting, as compression was\nonly observed when networks used saturating activation functions. In contrast,\nnetworks with non-saturating activation functions achieved comparable levels of\ntask performance but did not show compression. In this paper we developed more\nrobust mutual information estimation techniques, that adapt to hidden activity\nof neural networks and produce more sensitive measurements of activations from\nall functions, especially unbounded functions. Using these adaptive estimation\ntechniques, we explored compression in networks with a range of different\nactivation functions. With two improved methods of estimation, firstly, we show\nthat saturation of the activation function is not required for compression, and\nthe amount of compression varies between different activation functions. We\nalso find that there is a large amount of variation in compression between\ndifferent network initializations. Secondary, we see that L2 regularization\nleads to significantly increased compression, while preventing overfitting.\nFinally, we show that only compression of the last layer is positively\ncorrelated with generalization.\n","authors":["Ivan Chelombiev","Conor Houghton","Cian O'Donnell"],"pdf_url":"https://arxiv.org/pdf/1902.09037v2.pdf","comment":"Accepted as a poster presentation at ICLR 2019 and reviewed on\n  OpenReview (available at https://openreview.net/forum?id=SkeZisA5t7). Pages:\n  11. Figures: 9"},{"id":"http://arxiv.org/abs/2303.17732v1","updated":"2023-03-30T22:20:16Z","published":"2023-03-30T22:20:16Z","title":"Optimal Input Gain: All You Need to Supercharge a Feed-Forward Neural\n  Network","summary":"  Linear transformation of the inputs alters the training performance of\nfeed-forward networks that are otherwise equivalent. However, most linear\ntransforms are viewed as a pre-processing operation separate from the actual\ntraining. Starting from equivalent networks, it is shown that pre-processing\ninputs using linear transformation are equivalent to multiplying the negative\ngradient matrix with an autocorrelation matrix per training iteration. Second\norder method is proposed to find the autocorrelation matrix that maximizes\nlearning in a given iteration. When the autocorrelation matrix is diagonal, the\nmethod optimizes input gains. This optimal input gain (OIG) approach is used to\nimprove two first-order two-stage training algorithms, namely back-propagation\n(BP) and hidden weight optimization (HWO), which alternately update the input\nweights and solve linear equations for output weights. Results show that the\nproposed OIG approach greatly enhances the performance of the first-order\nalgorithms, often allowing them to rival the popular Levenberg-Marquardt\napproach with far less computation. It is shown that HWO is equivalent to BP\nwith Whitening transformation applied to the inputs. HWO effectively combines\nWhitening transformation with learning. Thus, OIG improved HWO could be a\nsignificant building block to more complex deep learning architectures.\n","authors":["Chinmay Rane","Kanishka Tyagi","Sanjeev Malalur","Yash Shinge","Michael Manry"],"pdf_url":"https://arxiv.org/pdf/2303.17732v1.pdf","comment":"under submission at Neurocomputing"},{"id":"http://arxiv.org/abs/2303.17731v1","updated":"2023-03-30T22:13:11Z","published":"2023-03-30T22:13:11Z","title":"$β^{4}$-IRT: A New $β^{3}$-IRT with Enhanced Discrimination\n  Estimation","summary":"  Item response theory aims to estimate respondent's latent skills from their\nresponses in tests composed of items with different levels of difficulty.\nSeveral models of item response theory have been proposed for different types\nof tasks, such as binary or probabilistic responses, response time, multiple\nresponses, among others. In this paper, we propose a new version of\n$\\beta^3$-IRT, called $\\beta^{4}$-IRT, which uses the gradient descent method\nto estimate the model parameters. In $\\beta^3$-IRT, abilities and difficulties\nare bounded, thus we employ link functions in order to turn $\\beta^{4}$-IRT\ninto an unconstrained gradient descent process. The original $\\beta^3$-IRT had\na symmetry problem, meaning that, if an item was initialised with a\ndiscrimination value with the wrong sign, e.g. negative when the actual\ndiscrimination should be positive, the fitting process could be unable to\nrecover the correct discrimination and difficulty values for the item. In order\nto tackle this limitation, we modelled the discrimination parameter as the\nproduct of two new parameters, one corresponding to the sign and the second\nassociated to the magnitude. We also proposed sensible priors for all\nparameters. We performed experiments to compare $\\beta^{4}$-IRT and\n$\\beta^3$-IRT regarding parameter recovery and our new version outperformed the\noriginal $\\beta^3$-IRT. Finally, we made $\\beta^{4}$-IRT publicly available as\na Python package, along with the implementation of $\\beta^3$-IRT used in our\nexperiments.\n","authors":["Manuel Ferreira-Junior","Jessica T. S. Reinaldo","Telmo M. Silva Filho","Eufrasio A. Lima Neto","Ricardo B. C. Prudencio"],"pdf_url":"https://arxiv.org/pdf/2303.17731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17727v1","updated":"2023-03-30T22:03:43Z","published":"2023-03-30T22:03:43Z","title":"BOLT: An Automated Deep Learning Framework for Training and Deploying\n  Large-Scale Neural Networks on Commodity CPU Hardware","summary":"  Efficient large-scale neural network training and inference on commodity CPU\nhardware is of immense practical significance in democratizing deep learning\n(DL) capabilities. Presently, the process of training massive models consisting\nof hundreds of millions to billions of parameters requires the extensive use of\nspecialized hardware accelerators, such as GPUs, which are only accessible to a\nlimited number of institutions with considerable financial resources. Moreover,\nthere is often an alarming carbon footprint associated with training and\ndeploying these models. In this paper, we address these challenges by\nintroducing BOLT, a sparse deep learning library for training massive neural\nnetwork models on standard CPU hardware. BOLT provides a flexible, high-level\nAPI for constructing models that will be familiar to users of existing popular\nDL frameworks. By automatically tuning specialized hyperparameters, BOLT also\nabstracts away the algorithmic details of sparse network training. We evaluate\nBOLT on a number of machine learning tasks drawn from recommendations, search,\nnatural language processing, and personalization. We find that our proposed\nsystem achieves competitive performance with state-of-the-art techniques at a\nfraction of the cost and energy consumption and an order-of-magnitude faster\ninference time. BOLT has also been successfully deployed by multiple businesses\nto address critical problems, and we highlight one customer deployment case\nstudy in the field of e-commerce.\n","authors":["Nicholas Meisburger","Vihan Lakshman","Benito Geordie","Joshua Engels","David Torres Ramos","Pratik Pranav","Benjamin Coleman","Benjamin Meisburger","Shubh Gupta","Yashwanth Adunukota","Tharun Medini","Anshumali Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2303.17727v1.pdf","comment":"11 pages, 8 tables, 5 figures"},{"id":"http://arxiv.org/abs/2211.11417v2","updated":"2023-03-30T21:56:33Z","published":"2022-11-21T13:01:52Z","title":"DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular\n  Automata","summary":"  Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic\nvideos. However, they require a slow iterative optimization process to\nsynthesize a single fixed-size short video, and they do not offer any\npost-training control over the synthesis process. We propose Dynamic Neural\nCellular Automata (DyNCA), a framework for real-time and controllable dynamic\ntexture synthesis. Our method is built upon the recently introduced NCA models\nand can synthesize infinitely long and arbitrary-sized realistic video textures\nin real time. We quantitatively and qualitatively evaluate our model and show\nthat our synthesized videos appear more realistic than the existing results. We\nimprove the SOTA DyTS performance by $2\\sim 4$ orders of magnitude. Moreover,\nour model offers several real-time video controls including motion speed,\nmotion direction, and an editing brush tool. We exhibit our trained models in\nan online interactive demo that runs on local hardware and is accessible on\npersonal computers and smartphones.\n","authors":["Ehsan Pajouheshgar","Yitao Xu","Tong Zhang","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2211.11417v2.pdf","comment":"Link to the demo: https://dynca.github.io/"},{"id":"http://arxiv.org/abs/2303.17720v1","updated":"2023-03-30T21:42:50Z","published":"2023-03-30T21:42:50Z","title":"Generating Adversarial Samples in Mini-Batches May Be Detrimental To\n  Adversarial Robustness","summary":"  Neural networks have been proven to be both highly effective within computer\nvision, and highly vulnerable to adversarial attacks. Consequently, as the use\nof neural networks increases due to their unrivaled performance, so too does\nthe threat posed by adversarial attacks. In this work, we build towards\naddressing the challenge of adversarial robustness by exploring the\nrelationship between the mini-batch size used during adversarial sample\ngeneration and the strength of the adversarial samples produced. We demonstrate\nthat an increase in mini-batch size results in a decrease in the efficacy of\nthe samples produced, and we draw connections between these observations and\nthe phenomenon of vanishing gradients. Next, we formulate loss functions such\nthat adversarial sample strength is not degraded by mini-batch size. Our\nfindings highlight a potential risk for underestimating the true (practical)\nstrength of adversarial attacks, and a risk of overestimating a model's\nrobustness. We share our codes to let others replicate our experiments and to\nfacilitate further exploration of the connections between batch size and\nadversarial sample strength.\n","authors":["Timothy Redgrave","Colton Crum"],"pdf_url":"https://arxiv.org/pdf/2303.17720v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.17719v1","updated":"2023-03-30T21:41:42Z","published":"2023-03-30T21:41:42Z","title":"Why is the winner the best?","summary":"  International benchmarking competitions have become fundamental for the\ncomparative performance assessment of image analysis methods. However, little\nattention has been given to investigating what can be learnt from these\ncompetitions. Do they really generate scientific progress? What are common and\nsuccessful participation strategies? What makes a solution superior to a\ncompeting method? To address this gap in the literature, we performed a\nmulti-center study with all 80 competitions that were conducted in the scope of\nIEEE ISBI 2021 and MICCAI 2021. Statistical analyses performed based on\ncomprehensive descriptions of the submitted algorithms linked to their rank as\nwell as the underlying participation strategies revealed common characteristics\nof winning solutions. These typically include the use of multi-task learning\n(63%) and/or multi-stage pipelines (61%), and a focus on augmentation (100%),\nimage preprocessing (97%), data curation (79%), and postprocessing (66%). The\n\"typical\" lead of a winning team is a computer scientist with a doctoral\ndegree, five years of experience in biomedical image analysis, and four years\nof experience in deep learning. Two core general development strategies stood\nout for highly-ranked teams: the reflection of the metrics in the method design\nand the focus on analyzing and handling failure cases. According to the\norganizers, 43% of the winning algorithms exceeded the state of the art but\nonly 11% completely solved the respective domain problem. The insights of our\nstudy could help researchers (1) improve algorithm development strategies when\napproaching new problems, and (2) focus on open research questions revealed by\nthis work.\n","authors":["Matthias Eisenmann","Annika Reinke","Vivienn Weru","Minu Dietlinde Tizabi","Fabian Isensee","Tim J. Adler","Sharib Ali","Vincent Andrearczyk","Marc Aubreville","Ujjwal Baid","Spyridon Bakas","Niranjan Balu","Sophia Bano","Jorge Bernal","Sebastian Bodenstedt","Alessandro Casella","Veronika Cheplygina","Marie Daum","Marleen de Bruijne","Adrien Depeursinge","Reuben Dorent","Jan Egger","David G. Ellis","Sandy Engelhardt","Melanie Ganz","Noha Ghatwary","Gabriel Girard","Patrick Godau","Anubha Gupta","Lasse Hansen","Kanako Harada","Mattias Heinrich","Nicholas Heller","Alessa Hering","Arnaud Huaulmé","Pierre Jannin","Ali Emre Kavur","Oldřich Kodym","Michal Kozubek","Jianning Li","Hongwei Li","Jun Ma","Carlos Martín-Isla","Bjoern Menze","Alison Noble","Valentin Oreiller","Nicolas Padoy","Sarthak Pati","Kelly Payette","Tim Rädsch","Jonathan Rafael-Patiño","Vivek Singh Bawa","Stefanie Speidel","Carole H. Sudre","Kimberlin van Wijnen","Martin Wagner","Donglai Wei","Amine Yamlahi","Moi Hoon Yap","Chun Yuan","Maximilian Zenk","Aneeq Zia","David Zimmerer","Dogu Baran Aydogan","Binod Bhattarai","Louise Bloch","Raphael Brüngel","Jihoon Cho","Chanyeol Choi","Qi Dou","Ivan Ezhov","Christoph M. Friedrich","Clifton Fuller","Rebati Raman Gaire","Adrian Galdran","Álvaro García Faura","Maria Grammatikopoulou","SeulGi Hong","Mostafa Jahanifar","Ikbeom Jang","Abdolrahim Kadkhodamohammadi","Inha Kang","Florian Kofler","Satoshi Kondo","Hugo Kuijf","Mingxing Li","Minh Huan Luu","Tomaž Martinčič","Pedro Morais","Mohamed A. Naser","Bruno Oliveira","David Owen","Subeen Pang","Jinah Park","Sung-Hong Park","Szymon Płotka","Elodie Puybareau","Nasir Rajpoot","Kanghyun Ryu","Numan Saeed","Adam Shephard","Pengcheng Shi","Dejan Štepec","Ronast Subedi","Guillaume Tochon","Helena R. Torres","Helene Urien","João L. Vilaça","Kareem Abdul Wahid","Haojie Wang","Jiacheng Wang","Liansheng Wang","Xiyue Wang","Benedikt Wiestler","Marek Wodzinski","Fangfang Xia","Juanying Xie","Zhiwei Xiong","Sen Yang","Yanwu Yang","Zixuan Zhao","Klaus Maier-Hein","Paul F. Jäger","Annette Kopp-Schneider","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2303.17719v1.pdf","comment":"accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17716v1","updated":"2023-03-30T21:35:48Z","published":"2023-03-30T21:35:48Z","title":"A Characterization of Online Multiclass Learnability","summary":"  We consider the problem of online multiclass learning when the number of\nlabels is unbounded. We show that the Multiclass Littlestone dimension, first\nintroduced in \\cite{DanielyERMprinciple}, continues to characterize online\nlearnability in this setting. Our result complements the recent work by\n\\cite{Brukhimetal2022} who give a characterization of batch multiclass\nlearnability when the label space is unbounded.\n","authors":["Vinod Raman","Unique Subedi","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2303.17716v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2301.12456v2","updated":"2023-03-30T21:20:35Z","published":"2023-01-29T14:44:27Z","title":"Towards Verifying the Geometric Robustness of Large-scale Neural\n  Networks","summary":"  Deep neural networks (DNNs) are known to be vulnerable to adversarial\ngeometric transformation. This paper aims to verify the robustness of\nlarge-scale DNNs against the combination of multiple geometric transformations\nwith a provable guarantee. Given a set of transformations (e.g., rotation,\nscaling, etc.), we develop GeoRobust, a black-box robustness analyser built\nupon a novel global optimisation strategy, for locating the worst-case\ncombination of transformations that affect and even alter a network's output.\nGeoRobust can provide provable guarantees on finding the worst-case combination\nbased on recent advances in Lipschitzian theory. Due to its black-box nature,\nGeoRobust can be deployed on large-scale DNNs regardless of their\narchitectures, activation functions, and the number of neurons. In practice,\nGeoRobust can locate the worst-case geometric transformation with high\nprecision for the ResNet50 model on ImageNet in a few seconds on average. We\nexamined 18 ImageNet classifiers, including the ResNet family and vision\ntransformers, and found a positive correlation between the geometric robustness\nof the networks and the parameter numbers. We also observe that increasing the\ndepth of DNN is more beneficial than increasing its width in terms of improving\nits geometric robustness. Our tool GeoRobust is available at\nhttps://github.com/TrustAI/GeoRobust.\n","authors":["Fu Wang","Peipei Xu","Wenjie Ruan","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2301.12456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17713v1","updated":"2023-03-30T21:16:44Z","published":"2023-03-30T21:16:44Z","title":"Mitigating Source Bias for Fairer Weak Supervision","summary":"  Weak supervision overcomes the label bottleneck, enabling efficient\ndevelopment of training sets. Millions of models trained on such datasets have\nbeen deployed in the real world and interact with users on a daily basis.\nHowever, the techniques that make weak supervision attractive -- such as\nintegrating any source of signal to estimate unknown labels -- also ensure that\nthe pseudolabels it produces are highly biased. Surprisingly, given everyday\nuse and the potential for increased bias, weak supervision has not been studied\nfrom the point of view of fairness. This work begins such a study. Our\ndeparture point is the observation that even when a fair model can be built\nfrom a dataset with access to ground-truth labels, the corresponding dataset\nlabeled via weak supervision can be arbitrarily unfair. Fortunately, not all is\nlost: we propose and empirically validate a model for source unfairness in weak\nsupervision, then introduce a simple counterfactual fairness-based technique\nthat can mitigate these biases. Theoretically, we show that it is possible for\nour approach to simultaneously improve both accuracy and fairness metrics -- in\ncontrast to standard fairness approaches that suffer from tradeoffs.\nEmpirically, we show that our technique improves accuracy on weak supervision\nbaselines by as much as 32% while reducing demographic parity gap by 82.5%.\n","authors":["Changho Shin","Sonia Cromp","Dyah Adila","Frederic Sala"],"pdf_url":"https://arxiv.org/pdf/2303.17713v1.pdf","comment":"46 pages"},{"id":"http://arxiv.org/abs/2303.17708v1","updated":"2023-03-30T21:00:38Z","published":"2023-03-30T21:00:38Z","title":"Analysis of Failures and Risks in Deep Learning Model Converters: A Case\n  Study in the ONNX Ecosystem","summary":"  Software engineers develop, fine-tune, and deploy deep learning (DL) models.\nThey use and re-use models in a variety of development frameworks and deploy\nthem on a range of runtime environments. In this diverse ecosystem, engineers\nuse DL model converters to move models from frameworks to runtime environments.\nHowever, errors in converters can compromise model quality and disrupt\ndeployment. The failure frequency and failure modes of DL model converters are\nunknown.\n  In this paper, we conduct the first failure analysis on DL model converters.\nSpecifically, we characterize failures in model converters associated with ONNX\n(Open Neural Network eXchange). We analyze past failures in the ONNX converters\nin two major DL frameworks, PyTorch and TensorFlow. The symptoms, causes, and\nlocations of failures (for N=200 issues), and trends over time are also\nreported. We also evaluate present-day failures by converting 8,797 models,\nboth real-world and synthetically generated instances. The consistent result\nfrom both parts of the study is that DL model converters commonly fail by\nproducing models that exhibit incorrect behavior: 33% of past failures and 8%\nof converted models fell into this category. Our results motivate future\nresearch on making DL software simpler to maintain, extend, and validate.\n","authors":["Purvish Jajal","Wenxin Jiang","Arav Tewari","Joseph Woo","Yung-Hsiang Lu","George K. Thiruvathukal","James C. Davis"],"pdf_url":"https://arxiv.org/pdf/2303.17708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13555v2","updated":"2023-03-30T20:55:26Z","published":"2023-03-22T23:05:28Z","title":"Efficient hybrid modeling and sorption model discovery for non-linear\n  advection-diffusion-sorption systems: A systematic scientific machine\n  learning approach","summary":"  This study presents a systematic machine learning approach for creating\nefficient hybrid models and discovering sorption uptake models in non-linear\nadvection-diffusion-sorption systems. It demonstrates an effective method to\ntrain these complex systems using gradient based optimizers, adjoint\nsensitivity analysis, and JIT-compiled vector Jacobian products, combined with\nspatial discretization and adaptive integrators. Sparse and symbolic regression\nwere employed to identify missing functions in the artificial neural network.\nThe robustness of the proposed method was tested on an in-silico data set of\nnoisy breakthrough curve observations of fixed-bed adsorption, resulting in a\nwell-fitted hybrid model. The study successfully reconstructed sorption uptake\nkinetics using sparse and symbolic regression, and accurately predicted\nbreakthrough curves using identified polynomials, highlighting the potential of\nthe proposed framework for discovering sorption kinetic law structures.\n","authors":["Vinicius V. Santana","Erbet Costa","Carine M. Rebello","Ana Mafalda Ribeiro","Chris Rackauckas","Idelfonso B. R. Nogueira"],"pdf_url":"https://arxiv.org/pdf/2303.13555v2.pdf","comment":"Preprint paper to be submitted soon in Elsevier Journal"},{"id":"http://arxiv.org/abs/2303.17696v1","updated":"2023-03-30T20:24:57Z","published":"2023-03-30T20:24:57Z","title":"Dual Cross-Attention for Medical Image Segmentation","summary":"  We propose Dual Cross-Attention (DCA), a simple yet effective attention\nmodule that is able to enhance skip-connections in U-Net-based architectures\nfor medical image segmentation. DCA addresses the semantic gap between encoder\nand decoder features by sequentially capturing channel and spatial dependencies\nacross multi-scale encoder features. First, the Channel Cross-Attention (CCA)\nextracts global channel-wise dependencies by utilizing cross-attention across\nchannel tokens of multi-scale encoder features. Then, the Spatial\nCross-Attention (SCA) module performs cross-attention to capture spatial\ndependencies across spatial tokens. Finally, these fine-grained encoder\nfeatures are up-sampled and connected to their corresponding decoder parts to\nform the skip-connection scheme. Our proposed DCA module can be integrated into\nany encoder-decoder architecture with skip-connections such as U-Net and its\nvariants. We test our DCA module by integrating it into six U-Net-based\narchitectures such as U-Net, V-Net, R2Unet, ResUnet++, DoubleUnet and\nMultiResUnet. Our DCA module shows Dice Score improvements up to 2.05% on GlaS,\n2.74% on MoNuSeg, 1.37% on CVC-ClinicDB, 1.12% on Kvasir-Seg and 1.44% on\nSynapse datasets. Our codes are available at:\nhttps://github.com/gorkemcanates/Dual-Cross-Attention\n","authors":["Gorkem Can Ates","Prasoon Mohan","Emrah Celik"],"pdf_url":"https://arxiv.org/pdf/2303.17696v1.pdf","comment":"Code: https://github.com/gorkemcanates/Dual-Cross-Attention"},{"id":"http://arxiv.org/abs/2303.17695v1","updated":"2023-03-30T20:23:49Z","published":"2023-03-30T20:23:49Z","title":"Task Oriented Conversational Modelling With Subjective Knowledge","summary":"  Existing conversational models are handled by a database(DB) and API based\nsystems. However, very often users' questions require information that cannot\nbe handled by such systems. Nonetheless, answers to these questions are\navailable in the form of customer reviews and FAQs. DSTC-11 proposes a three\nstage pipeline consisting of knowledge seeking turn detection, knowledge\nselection and response generation to create a conversational model grounded on\nthis subjective knowledge. In this paper, we focus on improving the knowledge\nselection module to enhance the overall system performance. In particular, we\npropose entity retrieval methods which result in an accurate and faster\nknowledge search. Our proposed Named Entity Recognition (NER) based entity\nretrieval method results in 7X faster search compared to the baseline model.\nAdditionally, we also explore a potential keyword extraction method which can\nimprove the accuracy of knowledge selection. Preliminary results show a 4 \\%\nimprovement in exact match score on knowledge selection task. The code is\navailable https://github.com/raja-kumar/knowledge-grounded-TODS\n","authors":["Raja Kumar"],"pdf_url":"https://arxiv.org/pdf/2303.17695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.13452v2","updated":"2023-03-30T19:44:21Z","published":"2022-05-26T15:56:08Z","title":"Continual evaluation for lifelong learning: Identifying the stability\n  gap","summary":"  Time-dependent data-generating distributions have proven to be difficult for\ngradient-based training of neural networks, as the greedy updates result in\ncatastrophic forgetting of previously learned knowledge. Despite the progress\nin the field of continual learning to overcome this forgetting, we show that a\nset of common state-of-the-art methods still suffers from substantial\nforgetting upon starting to learn new tasks, except that this forgetting is\ntemporary and followed by a phase of performance recovery. We refer to this\nintriguing but potentially problematic phenomenon as the stability gap. The\nstability gap had likely remained under the radar due to standard practice in\nthe field of evaluating continual learning models only after each task.\nInstead, we establish a framework for continual evaluation that uses\nper-iteration evaluation and we define a new set of metrics to quantify\nworst-case performance. Empirically we show that experience replay,\nconstraint-based replay, knowledge-distillation, and parameter regularization\nmethods are all prone to the stability gap; and that the stability gap can be\nobserved in class-, task-, and domain-incremental learning benchmarks.\nAdditionally, a controlled experiment shows that the stability gap increases\nwhen tasks are more dissimilar. Finally, by disentangling gradients into\nplasticity and stability components, we propose a conceptual explanation for\nthe stability gap.\n","authors":["Matthias De Lange","Gido van de Ven","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2205.13452v2.pdf","comment":"Published as spotlight paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2212.12645v2","updated":"2023-03-30T19:38:05Z","published":"2022-12-24T03:37:02Z","title":"HandsOff: Labeled Dataset Generation With No Additional Human\n  Annotations","summary":"  Recent work leverages the expressive power of generative adversarial networks\n(GANs) to generate labeled synthetic datasets. These dataset generation methods\noften require new annotations of synthetic images, which forces practitioners\nto seek out annotators, curate a set of synthetic images, and ensure the\nquality of generated labels. We introduce the HandsOff framework, a technique\ncapable of producing an unlimited number of synthetic images and corresponding\nlabels after being trained on less than 50 pre-existing labeled images. Our\nframework avoids the practical drawbacks of prior work by unifying the field of\nGAN inversion with dataset generation. We generate datasets with rich\npixel-wise labels in multiple challenging domains such as faces, cars,\nfull-body human poses, and urban driving scenes. Our method achieves\nstate-of-the-art performance in semantic segmentation, keypoint detection, and\ndepth estimation compared to prior dataset generation approaches and transfer\nlearning baselines. We additionally showcase its ability to address broad\nchallenges in model development which stem from fixed, hand-annotated datasets,\nsuch as the long-tail problem in semantic segmentation. Project page:\naustinxu87.github.io/handsoff.\n","authors":["Austin Xu","Mariya I. Vasileva","Achal Dave","Arjun Seshadri"],"pdf_url":"https://arxiv.org/pdf/2212.12645v2.pdf","comment":"22 pages, 20 figures. CVPR 2023"},{"id":"http://arxiv.org/abs/2301.00437v3","updated":"2023-03-30T19:36:16Z","published":"2023-01-01T16:29:56Z","title":"Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced\n  Data","summary":"  Modern deep neural networks have achieved impressive performance on tasks\nfrom image classification to natural language processing. Surprisingly, these\ncomplex systems with massive amounts of parameters exhibit the same structural\nproperties in their last-layer features and classifiers across canonical\ndatasets when training until convergence. In particular, it has been observed\nthat the last-layer features collapse to their class-means, and those\nclass-means are the vertices of a simplex Equiangular Tight Frame (ETF). This\nphenomenon is known as Neural Collapse ($\\mathcal{NC}$). Recent papers have\ntheoretically shown that $\\mathcal{NC}$ emerges in the global minimizers of\ntraining problems with the simplified ``unconstrained feature model''. In this\ncontext, we take a step further and prove the $\\mathcal{NC}$ occurrences in\ndeep linear networks for the popular mean squared error (MSE) and cross entropy\n(CE) losses, showing that global solutions exhibit $\\mathcal{NC}$ properties\nacross the linear layers. Furthermore, we extend our study to imbalanced data\nfor MSE loss and present the first geometric analysis of $\\mathcal{NC}$ under\nbias-free setting. Our results demonstrate the convergence of the last-layer\nfeatures and classifiers to a geometry consisting of orthogonal vectors, whose\nlengths depend on the amount of data in their corresponding classes. Finally,\nwe empirically validate our theoretical analyses on synthetic and practical\nnetwork architectures with both balanced and imbalanced scenarios.\n","authors":["Hien Dang","Tho Tran","Stanley Osher","Hung Tran-The","Nhat Ho","Tan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2301.00437v3.pdf","comment":"93 pages, 20 figures, 4 tables. Hien Dang and Tho Tran contributed\n  equally to this work"},{"id":"http://arxiv.org/abs/2303.17674v1","updated":"2023-03-30T19:31:41Z","published":"2023-03-30T19:31:41Z","title":"Exact Characterization of the Convex Hulls of Reachable Sets","summary":"  We study the convex hulls of reachable sets of nonlinear systems with bounded\ndisturbances. Reachable sets play a critical role in control, but remain\nnotoriously challenging to compute, and existing over-approximation tools tend\nto be conservative or computationally expensive. In this work, we exactly\ncharacterize the convex hulls of reachable sets as the convex hulls of\nsolutions of an ordinary differential equation from all possible initial values\nof the disturbances. This finite-dimensional characterization unlocks a tight\nestimation algorithm to over-approximate reachable sets that is significantly\nfaster and more accurate than existing methods. We present applications to\nneural feedback loop analysis and robust model predictive control.\n","authors":["Thomas Lew","Riccardo Bonalli","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2303.17674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17671v1","updated":"2023-03-30T19:20:16Z","published":"2023-03-30T19:20:16Z","title":"Neural signature kernels as infinite-width-depth-limits of controlled\n  ResNets","summary":"  Motivated by the paradigm of reservoir computing, we consider randomly\ninitialized controlled ResNets defined as Euler-discretizations of neural\ncontrolled differential equations (Neural CDEs). We show that in the\ninfinite-width-then-depth limit and under proper scaling, these architectures\nconverge weakly to Gaussian processes indexed on some spaces of continuous\npaths and with kernels satisfying certain partial differential equations (PDEs)\nvarying according to the choice of activation function. In the special case\nwhere the activation is the identity, we show that the equation reduces to a\nlinear PDE and the limiting kernel agrees with the signature kernel of Salvi et\nal. (2021). In this setting, we also show that the width-depth limits commute.\nWe name this new family of limiting kernels neural signature kernels. Finally,\nwe show that in the infinite-depth regime, finite-width controlled ResNets\nconverge in distribution to Neural CDEs with random vector fields which,\ndepending on whether the weights are shared across layers, are either\ntime-independent and Gaussian or behave like a matrix-valued Brownian motion.\n","authors":["Nicola Muca Cirone","Maud Lemercier","Cristopher Salvi"],"pdf_url":"https://arxiv.org/pdf/2303.17671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00848v3","updated":"2023-03-30T18:58:38Z","published":"2023-03-01T22:36:05Z","title":"Understanding the Diffusion Objective as a Weighted Integral of ELBOs","summary":"  Diffusion models in the literature are optimized with various objectives that\nare special cases of a weighted loss, where the weighting function specifies\nthe weight per noise level. Uniform weighting corresponds to maximizing the\nELBO, a principled approximation of maximum likelihood. In current practice\ndiffusion models are optimized with non-uniform weighting due to better results\nin terms of sample quality. In this work we expose a direct relationship\nbetween the weighted loss (with any weighting) and the ELBO objective.\n  We show that the weighted loss can be written as a weighted integral of\nELBOs, with one ELBO per noise level. If the weighting function is monotonic,\nthen the weighted loss is a likelihood-based objective: it maximizes the ELBO\nunder simple data augmentation, namely Gaussian noise perturbation. Our main\ncontribution is a deeper theoretical understanding of the diffusion objective,\nbut we also performed some experiments comparing monotonic with non-monotonic\nweightings, finding that monotonic weighting performs competitively with the\nbest published results.\n","authors":["Diederik P. Kingma","Ruiqi Gao"],"pdf_url":"https://arxiv.org/pdf/2303.00848v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17661v1","updated":"2023-03-30T18:56:42Z","published":"2023-03-30T18:56:42Z","title":"MetaEnhance: Metadata Quality Improvement for Electronic Theses and\n  Dissertations of University Libraries","summary":"  Metadata quality is crucial for digital objects to be discovered through\ndigital library interfaces. However, due to various reasons, the metadata of\ndigital objects often exhibits incomplete, inconsistent, and incorrect values.\nWe investigate methods to automatically detect, correct, and canonicalize\nscholarly metadata, using seven key fields of electronic theses and\ndissertations (ETDs) as a case study. We propose MetaEnhance, a framework that\nutilizes state-of-the-art artificial intelligence methods to improve the\nquality of these fields. To evaluate MetaEnhance, we compiled a metadata\nquality evaluation benchmark containing 500 ETDs, by combining subsets sampled\nusing multiple criteria. We tested MetaEnhance on this benchmark and found that\nthe proposed methods achieved nearly perfect F1-scores in detecting errors and\nF1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven\nfields.\n","authors":["Muntabir Hasan Choudhury","Lamia Salsabil","Himarsha R. Jayanetti","Jian Wu","William A. Ingram","Edward A. Fox"],"pdf_url":"https://arxiv.org/pdf/2303.17661v1.pdf","comment":"7 pages, 3 tables, and 1 figure. Accepted by 2023 ACM/IEEE Joint\n  Conference on Digital Libraries (JCDL '23) as a short paper"},{"id":"http://arxiv.org/abs/2212.06370v2","updated":"2023-03-30T18:55:02Z","published":"2022-12-13T05:03:16Z","title":"Dual Accuracy-Quality-Driven Neural Network for Prediction Interval\n  Generation","summary":"  Accurate uncertainty quantification is necessary to enhance the reliability\nof deep learning models in real-world applications. In the case of regression\ntasks, prediction intervals (PIs) should be provided along with the\ndeterministic predictions of deep learning models. Such PIs are useful or\n\"high-quality\" as long as they are sufficiently narrow and capture most of the\nprobability density. In this paper, we present a method to learn prediction\nintervals for regression-based neural networks automatically in addition to the\nconventional target predictions. In particular, we train two companion neural\nnetworks: one that uses one output, the target estimate, and another that uses\ntwo outputs, the upper and lower bounds of the corresponding PI. Our main\ncontribution is the design of a novel loss function for the PI-generation\nnetwork that takes into account the output of the target-estimation network and\nhas two optimization objectives: minimizing the mean prediction interval width\nand ensuring the PI integrity using constraints that maximize the prediction\ninterval probability coverage implicitly. Furthermore, we introduce a\nself-adaptive coefficient that balances both objectives within the loss\nfunction, which alleviates the task of fine-tuning. Experiments using a\nsynthetic dataset, eight benchmark datasets, and a real-world crop yield\nprediction dataset showed that our method was able to maintain a nominal\nprobability coverage and produce significantly narrower PIs without detriment\nto its target estimation accuracy when compared to those PIs generated by three\nstate-of-the-art neural-network-based methods. In other words, our method was\nshown to produce higher-quality PIs.\n","authors":["Giorgio Morales","John W. Sheppard"],"pdf_url":"https://arxiv.org/pdf/2212.06370v2.pdf","comment":"Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems"},{"id":"http://arxiv.org/abs/2303.17657v1","updated":"2023-03-30T18:41:28Z","published":"2023-03-30T18:41:28Z","title":"Progress towards an improved particle flow algorithm at CMS with machine\n  learning","summary":"  The particle-flow (PF) algorithm, which infers particles based on tracks and\ncalorimeter clusters, is of central importance to event reconstruction in the\nCMS experiment at the CERN LHC, and has been a focus of development in light of\nplanned Phase-2 running conditions with an increased pileup and detector\ngranularity. In recent years, the machine learned particle-flow (MLPF)\nalgorithm, a graph neural network that performs PF reconstruction, has been\nexplored in CMS, with the possible advantages of directly optimizing for the\nphysical quantities of interest, being highly reconfigurable to new conditions,\nand being a natural fit for deployment to heterogeneous accelerators. We\ndiscuss progress in CMS towards an improved implementation of the MLPF\nreconstruction, now optimized using generator/simulation-level particle\ninformation as the target for the first time. This paves the way to potentially\nimproving the detector response in terms of physical quantities of interest. We\ndescribe the simulation-based training target, progress and studies on\nevent-based loss terms, details on the model hyperparameter tuning, as well as\nphysics validation with respect to the current PF algorithm in terms of\nhigh-level physical quantities such as the jet and missing transverse momentum\nresolutions. We find that the MLPF algorithm, trained on a generator/simulator\nlevel particle information for the first time, results in broadly compatible\nparticle and jet reconstruction performance with the baseline PF, setting the\nstage for improving the physics performance by additional training statistics\nand model tuning.\n","authors":["Farouk Mokhtar","Joosep Pata","Javier Duarte","Eric Wulff","Maurizio Pierini","Jean-Roch Vlimant"],"pdf_url":"https://arxiv.org/pdf/2303.17657v1.pdf","comment":"7 pages, 4 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2210.12113v2","updated":"2023-03-30T18:36:27Z","published":"2022-10-21T17:13:14Z","title":"Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological\n  Report","summary":"  Despite the ever-increasing interest in applying deep learning (DL) models to\nmedical imaging, the typical scarcity and imbalance of medical datasets can\nseverely impact the performance of DL models. The generation of synthetic data\nthat might be freely shared without compromising patient privacy is a\nwell-known technique for addressing these difficulties. Inpainting algorithms\nare a subset of DL generative models that can alter one or more regions of an\ninput image while matching its surrounding context and, in certain cases,\nnon-imaging input conditions. Although the majority of inpainting techniques\nfor medical imaging data use generative adversarial networks (GANs), the\nperformance of these algorithms is frequently suboptimal due to their limited\noutput variety, a problem that is already well-known for GANs. Denoising\ndiffusion probabilistic models (DDPMs) are a recently introduced family of\ngenerative networks that can generate results of comparable quality to GANs,\nbut with diverse outputs. In this paper, we describe a DDPM to execute multiple\ninpainting tasks on 2D axial slices of brain MRI with various sequences, and\npresent proof-of-concept examples of its performance in a variety of evaluation\nscenarios. Our model and a public online interface to try our tool are\navailable at: https://github.com/Mayo-Radiology-Informatics-Lab/MBTI\n","authors":["Pouria Rouzrokh","Bardia Khosravi","Shahriar Faghani","Mana Moassefi","Sanaz Vahdati","Bradley J. Erickson"],"pdf_url":"https://arxiv.org/pdf/2210.12113v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.17651v1","updated":"2023-03-30T18:30:01Z","published":"2023-03-30T18:30:01Z","title":"Self-Refine: Iterative Refinement with Self-Feedback","summary":"  Like people, LLMs do not always generate the best text for a given generation\nproblem on their first try (e.g., summaries, answers, explanations). Just as\npeople then refine their text, we introduce SELF-REFINE, a framework for\nsimilarly improving initial outputs from LLMs through iterative feedback and\nrefinement. The main idea is to generate an output using an LLM, then allow the\nsame model to provide multi-aspect feedback for its own output; finally, the\nsame model refines its previously generated output given its own feedback.\nUnlike earlier work, our iterative refinement framework does not require\nsupervised training data or reinforcement learning, and works with a single\nLLM. We experiment with 7 diverse tasks, ranging from review rewriting to math\nreasoning, demonstrating that our approach outperforms direct generation. In\nall tasks, outputs generated with SELF-REFINE are preferred by humans and by\nautomated metrics over those generated directly with GPT-3.5 and GPT-4,\nimproving on average by absolute 20% across tasks.\n","authors":["Aman Madaan","Niket Tandon","Prakhar Gupta","Skyler Hallinan","Luyu Gao","Sarah Wiegreffe","Uri Alon","Nouha Dziri","Shrimai Prabhumoye","Yiming Yang","Sean Welleck","Bodhisattwa Prasad Majumder","Shashank Gupta","Amir Yazdanbakhsh","Peter Clark"],"pdf_url":"https://arxiv.org/pdf/2303.17651v1.pdf","comment":"Code, data, and demo at https://selfrefine.info/"},{"id":"http://arxiv.org/abs/2301.08243v2","updated":"2023-03-30T18:28:46Z","published":"2023-01-19T18:59:01Z","title":"Self-Supervised Learning from Images with a Joint-Embedding Predictive\n  Architecture","summary":"  This paper demonstrates an approach for learning highly semantic image\nrepresentations without relying on hand-crafted data-augmentations. We\nintroduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a\nnon-generative approach for self-supervised learning from images. The idea\nbehind I-JEPA is simple: from a single context block, predict the\nrepresentations of various target blocks in the same image. A core design\nchoice to guide I-JEPA towards producing semantic representations is the\nmasking strategy; specifically, it is crucial to (a) sample target blocks with\nsufficiently large scale (semantic), and to (b) use a sufficiently informative\n(spatially distributed) context block. Empirically, when combined with Vision\nTransformers, we find I-JEPA to be highly scalable. For instance, we train a\nViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong\ndownstream performance across a wide range of tasks, from linear classification\nto object counting and depth prediction.\n","authors":["Mahmoud Assran","Quentin Duval","Ishan Misra","Piotr Bojanowski","Pascal Vincent","Michael Rabbat","Yann LeCun","Nicolas Ballas"],"pdf_url":"https://arxiv.org/pdf/2301.08243v2.pdf","comment":"2023 IEEE/CVF International Conference on Computer Vision"},{"id":"http://arxiv.org/abs/2303.01684v2","updated":"2023-03-30T18:28:16Z","published":"2023-03-03T02:56:05Z","title":"BO-Muse: A human expert and AI teaming framework for accelerated\n  experimental design","summary":"  In this paper we introduce BO-Muse, a new approach to human-AI teaming for\nthe optimization of expensive black-box functions. Inspired by the intrinsic\ndifficulty of extracting expert knowledge and distilling it back into AI models\nand by observations of human behavior in real-world experimental design, our\nalgorithm lets the human expert take the lead in the experimental process. The\nhuman expert can use their domain expertise to its full potential, while the AI\nplays the role of a muse, injecting novelty and searching for areas of weakness\nto break the human out of over-exploitation induced by cognitive entrenchment.\nWith mild assumptions, we show that our algorithm converges sub-linearly, at a\nrate faster than the AI or human alone. We validate our algorithm using\nsynthetic data and with human experts performing real-world experiments.\n","authors":["Sunil Gupta","Alistair Shilton","Arun Kumar A V","Shannon Ryan","Majid Abdolshah","Hung Le","Santu Rana","Julian Berk","Mahad Rashid","Svetha Venkatesh"],"pdf_url":"https://arxiv.org/pdf/2303.01684v2.pdf","comment":"34 Pages, 7 Figures and 5 Tables"},{"id":"http://arxiv.org/abs/2303.06135v2","updated":"2023-03-30T18:28:05Z","published":"2023-03-10T18:53:52Z","title":"Rewarding Chatbots for Real-World Engagement with Millions of Users","summary":"  The emergence of pretrained large language models has led to the deployment\nof a range of social chatbots for chitchat. Although these chatbots demonstrate\nlanguage ability and fluency, they are not guaranteed to be engaging and can\nstruggle to retain users. This work investigates the development of social\nchatbots that prioritize user engagement to enhance retention, specifically\nexamining the use of human feedback to efficiently develop highly engaging\nchatbots. The proposed approach uses automatic pseudo-labels collected from\nuser interactions to train a reward model that can be used to reject\nlow-scoring sample responses generated by the chatbot model at inference time.\nIntuitive evaluation metrics, such as mean conversation length (MCL), are\nintroduced as proxies to measure the level of engagement of deployed chatbots.\nA/B testing on groups of 10,000 new daily chatbot users on the Chai Research\nplatform shows that this approach increases the MCL by up to 70%, which\ntranslates to a more than 30% increase in user retention for a GPT-J 6B model.\nFuture work aims to use the reward model to realise a data fly-wheel, where the\nlatest user conversations can be used to alternately fine-tune the language\nmodel and the reward model.\n","authors":["Robert Irvine","Douglas Boubert","Vyas Raina","Adian Liusie","Ziyi Zhu","Vineet Mudupalli","Aliaksei Korshuk","Zongyi Liu","Fritz Cremer","Valentin Assassi","Christie-Carol Beauchamp","Xiaoding Lu","Thomas Rialan","William Beauchamp"],"pdf_url":"https://arxiv.org/pdf/2303.06135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17649v1","updated":"2023-03-30T18:27:15Z","published":"2023-03-30T18:27:15Z","title":"Aligning a medium-size GPT model in English to a small closed domain in\n  Spanish using reinforcement learning","summary":"  In this paper, we propose a methodology to align a medium-sized GPT model,\noriginally trained in English for an open domain, to a small closed domain in\nSpanish. The application for which the model is finely tuned is the question\nanswering task. To achieve this we also needed to train and implement another\nneural network (which we called the reward model) that could score and\ndetermine whether an answer is appropriate for a given question. This component\nserved to improve the decoding and generation of the answers of the system.\nNumerical metrics such as BLEU and perplexity were used to evaluate the model,\nand human judgment was also used to compare the decoding technique with others.\nFinally, the results favored the proposed method, and it was determined that it\nis feasible to use a reward model to align the generation of responses.\n","authors":["Oscar R. Navarrete-Parra","Victor Uc-Cetina","Jorge Reyes-Magana"],"pdf_url":"https://arxiv.org/pdf/2303.17649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17648v1","updated":"2023-03-30T18:25:11Z","published":"2023-03-30T18:25:11Z","title":"Practical Policy Optimization with Personalized Experimentation","summary":"  Many organizations measure treatment effects via an experimentation platform\nto evaluate the casual effect of product variations prior to full-scale\ndeployment. However, standard experimentation platforms do not perform\noptimally for end user populations that exhibit heterogeneous treatment effects\n(HTEs). Here we present a personalized experimentation framework, Personalized\nExperiments (PEX), which optimizes treatment group assignment at the user level\nvia HTE modeling and sequential decision policy optimization to optimize\nmultiple short-term and long-term outcomes simultaneously. We describe an\nend-to-end workflow that has proven to be successful in practice and can be\nreadily implemented using open-source software.\n","authors":["Mia Garrard","Hanson Wang","Ben Letham","Shaun Singh","Abbas Kazerouni","Sarah Tan","Zehui Wang","Yin Huang","Yichun Hu","Chad Zhou","Norm Zhou","Eytan Bakshy"],"pdf_url":"https://arxiv.org/pdf/2303.17648v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2211.03329v3","updated":"2023-03-30T18:05:53Z","published":"2022-11-07T06:17:21Z","title":"Implicit Graphon Neural Representation","summary":"  Graphons are general and powerful models for generating graphs of varying\nsize. In this paper, we propose to directly model graphons using neural\nnetworks, obtaining Implicit Graphon Neural Representation (IGNR). Existing\nwork in modeling and reconstructing graphons often approximates a target\ngraphon by a fixed resolution piece-wise constant representation. Our IGNR has\nthe benefit that it can represent graphons up to arbitrary resolutions, and\nenables natural and efficient generation of arbitrary sized graphs with desired\nstructure once the model is learned. Furthermore, we allow the input graph data\nto be unaligned and have different sizes by leveraging the Gromov-Wasserstein\ndistance. We first demonstrate the effectiveness of our model by showing its\nsuperior performance on a graphon learning task. We then propose an extension\nof IGNR that can be incorporated into an auto-encoder framework, and\ndemonstrate its good performance under a more general setting of graphon\nlearning. We also show that our model is suitable for graph representation\nlearning and graph generation.\n","authors":["Xinyue Xia","Gal Mishne","Yusu Wang"],"pdf_url":"https://arxiv.org/pdf/2211.03329v3.pdf","comment":"3 figures"},{"id":"http://arxiv.org/abs/2210.08140v2","updated":"2023-03-30T18:04:22Z","published":"2022-10-14T22:33:28Z","title":"A Kernel Approach for PDE Discovery and Operator Learning","summary":"  This article presents a three-step framework for learning and solving partial\ndifferential equations (PDEs) using kernel methods. Given a training set\nconsisting of pairs of noisy PDE solutions and source/boundary terms on a mesh,\nkernel smoothing is utilized to denoise the data and approximate derivatives of\nthe solution. This information is then used in a kernel regression model to\nlearn the algebraic form of the PDE. The learned PDE is then used within a\nkernel based solver to approximate the solution of the PDE with a new\nsource/boundary term, thereby constituting an operator learning framework.\nNumerical experiments compare the method to state-of-the-art algorithms and\ndemonstrate its competitive performance.\n","authors":["Da Long","Nicole Mrvaljevic","Shandian Zhe","Bamdad Hosseini"],"pdf_url":"https://arxiv.org/pdf/2210.08140v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.14000v5","updated":"2023-03-30T18:02:33Z","published":"2021-11-27T22:44:54Z","title":"Factor-augmented tree ensembles","summary":"  This manuscript proposes to extend the information set of time-series\nregression trees with latent stationary factors extracted via state-space\nmethods. In doing so, this approach generalises time-series regression trees on\ntwo dimensions. First, it allows to handle predictors that exhibit measurement\nerror, non-stationary trends, seasonality and/or irregularities such as missing\nobservations. Second, it gives a transparent way for using domain-specific\ntheory to inform time-series regression trees. Empirically, ensembles of these\nfactor-augmented trees provide a reliable approach for macro-finance problems.\nThis article highlights it focussing on the lead-lag effect between equity\nvolatility and the business cycle in the United States.\n","authors":["Filippo Pellegrino"],"pdf_url":"https://arxiv.org/pdf/2111.14000v5.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.17550v1","updated":"2023-03-30T17:18:31Z","published":"2023-03-30T17:18:31Z","title":"DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with\n  Diffusion Autoencoder","summary":"  While recent research has made significant progress in speech-driven talking\nface generation, the quality of the generated video still lags behind that of\nreal recordings. One reason for this is the use of handcrafted intermediate\nrepresentations like facial landmarks and 3DMM coefficients, which are designed\nbased on human knowledge and are insufficient to precisely describe facial\nmovements. Additionally, these methods require an external pretrained model for\nextracting these representations, whose performance sets an upper bound on\ntalking face generation. To address these limitations, we propose a novel\nmethod called DAE-Talker that leverages data-driven latent representations\nobtained from a diffusion autoencoder (DAE). DAE contains an image encoder that\nencodes an image into a latent vector and a DDIM image decoder that\nreconstructs the image from it. We train our DAE on talking face video frames\nand then extract their latent representations as the training target for a\nConformer-based speech2latent model. This allows DAE-Talker to synthesize full\nvideo frames and produce natural head movements that align with the content of\nspeech, rather than relying on a predetermined head pose from a template video.\nWe also introduce pose modelling in speech2latent for pose controllability.\nAdditionally, we propose a novel method for generating continuous video frames\nwith the DDIM image decoder trained on individual frames, eliminating the need\nfor modelling the joint distribution of consecutive frames directly. Our\nexperiments show that DAE-Talker outperforms existing popular methods in\nlip-sync, video fidelity, and pose naturalness. We also conduct ablation\nstudies to analyze the effectiveness of the proposed techniques and demonstrate\nthe pose controllability of DAE-Talker.\n","authors":["Chenpng Du","Qi Chen","Tianyu He","Xu Tan","Xie Chen","Kai Yu","Sheng Zhao","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2303.17550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17490v1","updated":"2023-03-30T16:01:50Z","published":"2023-03-30T16:01:50Z","title":"Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment","summary":"  How does audio describe the world around us? In this paper, we propose a\nmethod for generating an image of a scene from sound. Our method addresses the\nchallenges of dealing with the large gaps that often exist between sight and\nsound. We design a model that works by scheduling the learning procedure of\neach model component to associate audio-visual modalities despite their\ninformation gaps. The key idea is to enrich the audio features with visual\ninformation by learning to align audio to visual latent space. We translate the\ninput audio to visual features, then use a pre-trained generator to produce an\nimage. To further improve the quality of our generated images, we use sound\nsource localization to select the audio-visual pairs that have strong\ncross-modal correlations. We obtain substantially better results on the VEGAS\nand VGGSound datasets than prior approaches. We also show that we can control\nour model's predictions by applying simple manipulations to the input waveform,\nor to the latent space.\n","authors":["Kim Sung-Bin","Arda Senocak","Hyunwoo Ha","Andrew Owens","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2303.17490v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17489v1","updated":"2023-03-30T16:01:28Z","published":"2023-03-30T16:01:28Z","title":"Prefix tuning for automated audio captioning","summary":"  Audio captioning aims to generate text descriptions from environmental\nsounds. One challenge of audio captioning is the difficulty of the\ngeneralization due to the lack of audio-text paired training data. In this\nwork, we propose a simple yet effective method of dealing with small-scaled\ndatasets by leveraging a pre-trained language model. We keep the language model\nfrozen to maintain the expressivity for text generation, and we only learn to\nextract global and temporal features from the input audio. To bridge a modality\ngap between the audio features and the language model, we employ mapping\nnetworks that translate audio features to the continuous vectors the language\nmodel can understand, called prefixes. We evaluate our proposed method on the\nClotho and AudioCaps dataset and show our method outperforms prior arts in\ndiverse experimental settings.\n","authors":["Minkyu Kim","Kim Sung-Bin2","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2303.17489v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2210.06756v2","updated":"2023-03-30T15:27:33Z","published":"2022-10-13T05:49:33Z","title":"Decoding Visual Neural Representations by Multimodal Learning of\n  Brain-Visual-Linguistic Features","summary":"  Decoding human visual neural representations is a challenging task with great\nscientific significance in revealing vision-processing mechanisms and\ndeveloping brain-like intelligent machines. Most existing methods are difficult\nto generalize to novel categories that have no corresponding neural data for\ntraining. The two main reasons are 1) the under-exploitation of the multimodal\nsemantic knowledge underlying the neural data and 2) the small number of paired\n(stimuli-responses) training data. To overcome these limitations, this paper\npresents a generic neural decoding method called BraVL that uses multimodal\nlearning of brain-visual-linguistic features. We focus on modeling the\nrelationships between brain, visual and linguistic features via multimodal deep\ngenerative models. Specifically, we leverage the mixture-of-product-of-experts\nformulation to infer a latent code that enables a coherent joint generation of\nall three modalities. To learn a more consistent joint representation and\nimprove the data efficiency in the case of limited brain activity data, we\nexploit both intra- and inter-modality mutual information maximization\nregularization terms. In particular, our BraVL model can be trained under\nvarious semi-supervised scenarios to incorporate the visual and textual\nfeatures obtained from the extra categories. Finally, we construct three\ntrimodal matching datasets, and the extensive experiments lead to some\ninteresting conclusions and cognitive insights: 1) decoding novel visual\ncategories from human brain activity is practically possible with good\naccuracy; 2) decoding models using the combination of visual and linguistic\nfeatures perform much better than those using either of them alone; 3) visual\nperception may be accompanied by linguistic influences to represent the\nsemantics of visual stimuli. Code and data: https://github.com/ChangdeDu/BraVL.\n","authors":["Changde Du","Kaicheng Fu","Jinpeng Li","Huiguang He"],"pdf_url":"https://arxiv.org/pdf/2210.06756v2.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI)"},{"id":"http://arxiv.org/abs/2303.17395v1","updated":"2023-03-30T14:07:47Z","published":"2023-03-30T14:07:47Z","title":"WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for\n  Audio-Language Multimodal Research","summary":"  The advancement of audio-language (AL) multimodal learning tasks has been\nsignificant in recent years. However, researchers face challenges due to the\ncostly and time-consuming collection process of existing audio-language\ndatasets, which are limited in size. To address this data scarcity issue, we\nintroduce WavCaps, the first large-scale weakly-labelled audio captioning\ndataset, comprising approximately 400k audio clips with paired captions. We\nsourced audio clips and their raw descriptions from web sources and a sound\nevent detection dataset. However, the online-harvested raw descriptions are\nhighly noisy and unsuitable for direct use in tasks such as automated audio\ncaptioning. To overcome this issue, we propose a three-stage processing\npipeline for filtering noisy data and generating high-quality captions, where\nChatGPT, a large language model, is leveraged to filter and transform raw\ndescriptions automatically. We conduct a comprehensive analysis of the\ncharacteristics of WavCaps dataset and evaluate it on multiple downstream\naudio-language multimodal learning tasks. The systems trained on WavCaps\noutperform previous state-of-the-art (SOTA) models by a significant margin. Our\naspiration is for the WavCaps dataset we have proposed to facilitate research\nin audio-language multimodal learning and demonstrate the potential of\nutilizing ChatGPT to enhance academic research. Our dataset and codes are\navailable at https://github.com/XinhaoMei/WavCaps.\n","authors":["Xinhao Mei","Chutong Meng","Haohe Liu","Qiuqiang Kong","Tom Ko","Chengqi Zhao","Mark D. Plumbley","Yuexian Zou","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2303.17395v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2210.15511v4","updated":"2023-03-30T06:12:26Z","published":"2022-10-27T14:47:19Z","title":"ProContEXT: Exploring Progressive Context Transformer for Tracking","summary":"  Existing Visual Object Tracking (VOT) only takes the target area in the first\nframe as a template. This causes tracking to inevitably fail in fast-changing\nand crowded scenes, as it cannot account for changes in object appearance\nbetween frames. To this end, we revamped the tracking framework with\nProgressive Context Encoding Transformer Tracker (ProContEXT), which coherently\nexploits spatial and temporal contexts to predict object motion trajectories.\nSpecifically, ProContEXT leverages a context-aware self-attention module to\nencode the spatial and temporal context, refining and updating the multi-scale\nstatic and dynamic templates to progressively perform accurately tracking. It\nexplores the complementary between spatial and temporal context, raising a new\npathway to multi-context modeling for transformer-based trackers. In addition,\nProContEXT revised the token pruning technique to reduce computational\ncomplexity. Extensive experiments on popular benchmark datasets such as GOT-10k\nand TrackingNet demonstrate that the proposed ProContEXT achieves\nstate-of-the-art performance.\n","authors":["Jin-Peng Lan","Zhi-Qi Cheng","Jun-Yan He","Chenyang Li","Bin Luo","Xu Bao","Wangmeng Xiang","Yifeng Geng","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2210.15511v4.pdf","comment":"Accepted at ICASSP 2023, source code is at\n  https://github.com/zhiqic/ProContEXT"},{"id":"http://arxiv.org/abs/2303.17144v1","updated":"2023-03-30T04:34:31Z","published":"2023-03-30T04:34:31Z","title":"DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving","summary":"  Real-time perception, or streaming perception, is a crucial aspect of\nautonomous driving that has yet to be thoroughly explored in existing research.\nTo address this gap, we present DAMO-StreamNet, an optimized framework that\ncombines recent advances from the YOLO series with a comprehensive analysis of\nspatial and temporal perception mechanisms, delivering a cutting-edge solution.\nThe key innovations of DAMO-StreamNet are: (1) A robust neck structure\nincorporating deformable convolution, enhancing the receptive field and feature\nalignment capabilities. (2) A dual-branch structure that integrates short-path\nsemantic features and long-path temporal features, improving motion state\nprediction accuracy. (3) Logits-level distillation for efficient optimization,\naligning the logits of teacher and student networks in semantic space. (4) A\nreal-time forecasting mechanism that updates support frame features with the\ncurrent frame, ensuring seamless streaming perception during inference. Our\nexperiments demonstrate that DAMO-StreamNet surpasses existing state-of-the-art\nmethods, achieving 37.8% (normal size (600, 960)) and 43.3% (large size (1200,\n1920)) sAP without using extra data. This work not only sets a new benchmark\nfor real-time perception but also provides valuable insights for future\nresearch. Additionally, DAMO-StreamNet can be applied to various autonomous\nsystems, such as drones and robots, paving the way for real-time perception.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Wangmeng Xiang","Binghui Chen","Bin Luo","Yifeng Geng","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2303.17144v1.pdf","comment":"he source code is at https://shorturl.at/BJPZ6"},{"id":"http://arxiv.org/abs/2210.15518v4","updated":"2023-03-30T04:02:18Z","published":"2022-10-27T14:57:14Z","title":"LongShortNet: Exploring Temporal and Semantic Features Fusion in\n  Streaming Perception","summary":"  Streaming perception is a critical task in autonomous driving that requires\nbalancing the latency and accuracy of the autopilot system. However, current\nmethods for streaming perception are limited as they only rely on the current\nand adjacent two frames to learn movement patterns. This restricts their\nability to model complex scenes, often resulting in poor detection results. To\naddress this limitation, we propose LongShortNet, a novel dual-path network\nthat captures long-term temporal motion and integrates it with short-term\nspatial semantics for real-time perception. LongShortNet is notable as it is\nthe first work to extend long-term temporal modeling to streaming perception,\nenabling spatiotemporal feature fusion. We evaluate LongShortNet on the\nchallenging Argoverse-HD dataset and demonstrate that it outperforms existing\nstate-of-the-art methods with almost no additional computational cost.\n","authors":["Chenyang Li","Zhi-Qi Cheng","Jun-Yan He","Pengyu Li","Bin Luo","Hanyuan Chen","Yifeng Geng","Jin-Peng Lan","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2210.15518v4.pdf","comment":"Accepted at ICASSP 2023, source code is at\n  https://github.com/zhiqic/LongShortNet"},{"id":"http://arxiv.org/abs/2304.00988v1","updated":"2023-03-30T11:13:59Z","published":"2023-03-30T11:13:59Z","title":"The Music Annotation Pattern","summary":"  The annotation of music content is a complex process to represent due to its\ninherent multifaceted, subjectivity, and interdisciplinary nature. Numerous\nsystems and conventions for annotating music have been developed as independent\nstandards over the past decades. Little has been done to make them\ninteroperable, which jeopardises cross-corpora studies as it requires users to\nfamiliarise with a multitude of conventions. Most of these systems lack the\nsemantic expressiveness needed to represent the complexity of the musical\nlanguage and cannot model multi-modal annotations originating from audio and\nsymbolic sources. In this article, we introduce the Music Annotation Pattern,\nan Ontology Design Pattern (ODP) to homogenise different annotation systems and\nto represent several types of musical objects (e.g. chords, patterns,\nstructures). This ODP preserves the semantics of the object's content at\ndifferent levels and temporal granularity. Moreover, our ODP accounts for\nmulti-modality upfront, to describe annotations derived from different sources,\nand it is the first to enable the integration of music datasets at a large\nscale.\n","authors":["Jacopo de Berardinis","Albert Meroño-Peñuela","Andrea Poltronieri","Valentina Presutti"],"pdf_url":"https://arxiv.org/pdf/2304.00988v1.pdf","comment":"12 pages, 3 figures. Proceedings of the 13th Workshop on Ontology\n  Design and Patterns, edited by V. Sv\\'atek et al., WOP, 2022"},{"id":"http://arxiv.org/abs/2304.00986v1","updated":"2023-03-30T10:51:10Z","published":"2023-03-30T10:51:10Z","title":"The Music Note Ontology","summary":"  In this paper we propose the Music Note Ontology, an ontology for modelling\nmusic notes and their realisation. The ontology addresses the relation between\na note represented in a symbolic representation system, and its realisation,\ni.e. a musical performance. This work therefore aims to solve the modelling and\nrepresentation issues that arise when analysing the relationships between\nabstract symbolic features and the corresponding physical features of an audio\nsignal. The ontology is composed of three different Ontology Design Patterns\n(ODP), which model the structure of the score (Score Part Pattern), the note in\nthe symbolic notation (Music Note Pattern) and its realisation (Musical Object\nPattern).\n","authors":["Andrea Poltronieri","Aldo Gangemi"],"pdf_url":"https://arxiv.org/pdf/2304.00986v1.pdf","comment":"12 pages, 1 figure, 1 table. Proceedings of the 12th Workshop on\n  Ontology Design and Patterns (WOP 2021), Online, edited by K. Hammar et al.,\n  2021"}]},"2023-03-31T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.13367v2","updated":"2023-03-31T17:56:28Z","published":"2023-03-21T14:35:07Z","title":"ChatGPT and a New Academic Reality: Artificial Intelligence-Written\n  Research Papers and the Ethics of the Large Language Models in Scholarly\n  Publishing","summary":"  This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.\n","authors":["Brady Lund","Ting Wang","Nishith Reddy Mannuru","Bing Nie","Somipam Shimray","Ziang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.13367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.07533v3","updated":"2023-03-31T17:54:25Z","published":"2021-11-15T04:44:57Z","title":"Automated scholarly paper review: Concepts, technologies, and challenges","summary":"  Peer review is a widely accepted mechanism for research evaluation, playing a\npivotal role in academic publishing. However, criticisms have long been leveled\non this mechanism, mostly because of its poor efficiency and low\nreproducibility. Recent years have seen the application of artificial\nintelligence (AI) in assisting the peer review process. Nonetheless, with the\ninvolvement of humans, such limitations remain inevitable. In this paper, we\npropose the concept and pipeline of automated scholarly paper review (ASPR) and\nreview the relevant literature and technologies of achieving a full-scale\ncomputerized review process. On the basis of the review and discussion, we\nconclude that there is already corresponding research and preliminary\nimplementation at each stage of ASPR. We further look into the challenges in\nASPR with the existing technologies. The major difficulties lie in imperfect\ndocument parsing and representation, inadequate data, defective human-computer\ninteraction, and flawed deep logical reasoning. Moreover, we discuss the\npossible moral and ethical issues and point out the future directions of ASPR.\nIn the foreseeable future, ASPR and peer review will coexist in a reinforcing\nmanner before ASPR is able to fully undertake the reviewing workload from\nhumans.\n","authors":["Jialiang Lin","Jiaxin Song","Zhangping Zhou","Yidong Chen","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2111.07533v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18223v1","updated":"2023-03-31T17:28:46Z","published":"2023-03-31T17:28:46Z","title":"A Survey of Large Language Models","summary":"  Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n","authors":["Wayne Xin Zhao","Kun Zhou","Junyi Li","Tianyi Tang","Xiaolei Wang","Yupeng Hou","Yingqian Min","Beichen Zhang","Junjie Zhang","Zican Dong","Yifan Du","Chen Yang","Yushuo Chen","Zhipeng Chen","Jinhao Jiang","Ruiyang Ren","Yifan Li","Xinyu Tang","Zikang Liu","Peiyu Liu","Jian-Yun Nie","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2303.18223v1.pdf","comment":"ongoing work; 51 pages"},{"id":"http://arxiv.org/abs/2303.18190v1","updated":"2023-03-31T16:45:42Z","published":"2023-03-31T16:45:42Z","title":"Assessing Language Model Deployment with Risk Cards","summary":"  This paper introduces RiskCards, a framework for structured assessment and\ndocumentation of risks associated with an application of language models. As\nwith all language, text generated by language models can be harmful, or used to\nbring about harm. Automating language generation adds both an element of scale\nand also more subtle or emergent undesirable tendencies to the generated text.\nPrior work establishes a wide variety of language model harms to many different\nactors: existing taxonomies identify categories of harms posed by language\nmodels; benchmarks establish automated tests of these harms; and documentation\nstandards for models, tasks and datasets encourage transparent reporting.\nHowever, there is no risk-centric framework for documenting the complexity of a\nlandscape in which some risks are shared across models and contexts, while\nothers are specific, and where certain conditions may be required for risks to\nmanifest as harms. RiskCards address this methodological gap by providing a\ngeneric framework for assessing the use of a given language model in a given\nscenario. Each RiskCard makes clear the routes for the risk to manifest harm,\ntheir placement in harm taxonomies, and example prompt-output pairs. While\nRiskCards are designed to be open-source, dynamic and participatory, we present\na \"starter set\" of RiskCards taken from a broad literature survey, each of\nwhich details a concrete risk presentation. Language model RiskCards initiate a\ncommunity knowledge base which permits the mapping of risks and harms to a\nspecific model or its application scenario, ultimately contributing to a\nbetter, safer and shared understanding of the risk landscape.\n","authors":["Leon Derczynski","Hannah Rose Kirk","Vidhisha Balachandran","Sachin Kumar","Yulia Tsvetkov","M. R. Leiser","Saif Mohammad"],"pdf_url":"https://arxiv.org/pdf/2303.18190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18162v1","updated":"2023-03-31T15:54:54Z","published":"2023-03-31T15:54:54Z","title":"A Multiple Choices Reading Comprehension Corpus for Vietnamese Language\n  Education","summary":"  Machine reading comprehension has been an interesting and challenging task in\nrecent years, with the purpose of extracting useful information from texts. To\nattain the computer ability to understand the reading text and answer relevant\ninformation, we introduce ViMMRC 2.0 - an extension of the previous ViMMRC for\nthe task of multiple-choice reading comprehension in Vietnamese Textbooks which\ncontain the reading articles for students from Grade 1 to Grade 12. This\ndataset has 699 reading passages which are prose and poems, and 5,273\nquestions. The questions in the new dataset are not fixed with four options as\nin the previous version. Moreover, the difficulty of questions is increased,\nwhich challenges the models to find the correct choice. The computer must\nunderstand the whole context of the reading passage, the question, and the\ncontent of each choice to extract the right answers. Hence, we propose the\nmulti-stage approach that combines the multi-step attention network (MAN) with\nthe natural language inference (NLI) task to enhance the performance of the\nreading comprehension model. Then, we compare the proposed methodology with the\nbaseline BERTology models on the new dataset and the ViMMRC 1.0. Our\nmulti-stage models achieved 58.81% by Accuracy on the test set, which is 5.34%\nbetter than the highest BERTology models. From the results of the error\nanalysis, we found the challenge of the reading comprehension models is\nunderstanding the implicit context in texts and linking them together in order\nto find the correct answers. Finally, we hope our new dataset will motivate\nfurther research in enhancing the language understanding ability of computers\nin the Vietnamese language.\n","authors":["Son T. Luu","Khoi Trong Hoang","Tuong Quang Pham","Kiet Van Nguyen","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.18162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18149v1","updated":"2023-03-31T15:37:17Z","published":"2023-03-31T15:37:17Z","title":"Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles\n  and Practice of Engineering (PE) Structural Exams?","summary":"  The engineering community has recently witnessed the emergence of chatbot\ntechnology with the release of OpenAI ChatGPT-4 and Google Bard. While these\nchatbots have been reported to perform well and even pass various standardized\ntests, including medical and law exams, this forum paper explores whether these\nchatbots can also pass the Fundamentals of Engineering (FE) and Principles and\nPractice of Engineering (PE) exams. A diverse range of civil and environmental\nengineering questions and scenarios are used to evaluate the chatbots'\nperformance, as commonly present in the FE and PE exams. The chatbots'\nresponses were analyzed based on their relevance, accuracy, and clarity and\nthen compared against the recommendations of the National Council of Examiners\nfor Engineering and Surveying (NCEES). Our report shows that ChatGPT-4 and\nBard, respectively scored 70.9% and 39.2% in the FE exam and 46.2% and 41% in\nthe PE exam. It is evident that the current version of ChatGPT-4 could\npotentially pass the FE exam. While future editions are much more likely to\npass both exams, this study also highlights the potential of using chatbots as\nteaching assistants and guiding engineers.\n","authors":["M. Z. Naser","Brandon Ross","Jennier Ogle","Venkatesh Kodur","Rami Hawileh","Jamal Abdalla","Huu-Tai Thai"],"pdf_url":"https://arxiv.org/pdf/2303.18149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04089v3","updated":"2023-03-31T15:27:01Z","published":"2022-12-08T05:50:53Z","title":"Editing Models with Task Arithmetic","summary":"  Changing how pre-trained models behave -- e.g., improving their performance\non a downstream task or mitigating biases learned during pre-training -- is a\ncommon practice when developing machine learning systems. In this work, we\npropose a new paradigm for steering the behavior of neural networks, centered\naround \\textit{task vectors}. A task vector specifies a direction in the weight\nspace of a pre-trained model, such that movement in that direction improves\nperformance on the task. We build task vectors by subtracting the weights of a\npre-trained model from the weights of the same model after fine-tuning on a\ntask. We show that these task vectors can be modified and combined together\nthrough arithmetic operations such as negation and addition, and the behavior\nof the resulting model is steered accordingly. Negating a task vector decreases\nperformance on the target task, with little change in model behavior on control\ntasks. Moreover, adding task vectors together can improve performance on\nmultiple tasks at once. Finally, when tasks are linked by an analogy\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\nthree of the tasks can improve performance on the fourth, even when no data\nfrom the fourth task is used for training. Overall, our experiments with\nseveral models, modalities and tasks show that task arithmetic is a simple,\nefficient and effective way of editing models.\n","authors":["Gabriel Ilharco","Marco Tulio Ribeiro","Mitchell Wortsman","Suchin Gururangan","Ludwig Schmidt","Hannaneh Hajishirzi","Ali Farhadi"],"pdf_url":"https://arxiv.org/pdf/2212.04089v3.pdf","comment":"In Proceedings of the 11th International Conference on Learning\n  Representations (ICLR 2023)"},{"id":"http://arxiv.org/abs/2303.18121v1","updated":"2023-03-31T15:07:40Z","published":"2023-03-31T15:07:40Z","title":"BERTino: an Italian DistilBERT model","summary":"  The recent introduction of Transformers language representation models\nallowed great improvements in many natural language processing (NLP) tasks.\nHowever, if on one hand the performances achieved by this kind of architectures\nare surprising, on the other their usability is limited by the high number of\nparameters which constitute their network, resulting in high computational and\nmemory demands. In this work we present BERTino, a DistilBERT model which\nproposes to be the first lightweight alternative to the BERT architecture\nspecific for the Italian language. We evaluated BERTino on the Italian ISDT,\nItalian ParTUT, Italian WikiNER and multiclass classification tasks, obtaining\nF1 scores comparable to those obtained by a BERTBASE with a remarkable\nimprovement in training and inference speed.\n","authors":["Matteo Muffo","Enrico Bertino"],"pdf_url":"https://arxiv.org/pdf/2303.18121v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2303.18120v1","updated":"2023-03-31T15:07:36Z","published":"2023-03-31T15:07:36Z","title":"UKP-SQuARE v3: A Platform for Multi-Agent QA Research","summary":"  The continuous development of Question Answering (QA) datasets has drawn the\nresearch community's attention toward multi-domain models. A popular approach\nis to use multi-dataset models, which are models trained on multiple datasets\nto learn their regularities and prevent overfitting to a single dataset.\nHowever, with the proliferation of QA models in online repositories such as\nGitHub or Hugging Face, an alternative is becoming viable. Recent works have\ndemonstrated that combining expert agents can yield large performance gains\nover multi-dataset models. To ease research in multi-agent models, we extend\nUKP-SQuARE, an online platform for QA research, to support three families of\nmulti-agent systems: i) agent selection, ii) early-fusion of agents, and iii)\nlate-fusion of agents. We conduct experiments to evaluate their inference speed\nand discuss the performance vs. speed trade-off compared to multi-dataset\nmodels. UKP-SQuARE is open-source and publicly available at\nhttp://square.ukp-lab.de.\n","authors":["Haritz Puerto","Tim Baumgärtner","Rachneet Sachdeva","Haishuo Fang","Hao Zhang","Sewin Tariverdian","Kexin Wang","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2303.18120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17466v2","updated":"2023-03-31T15:02:48Z","published":"2023-03-30T15:43:39Z","title":"Assessing Cross-Cultural Alignment between ChatGPT and Human Societies:\n  An Empirical Study","summary":"  The recent release of ChatGPT has garnered widespread recognition for its\nexceptional ability to generate human-like responses in dialogue. Given its\nusage by users from various nations and its training on a vast multilingual\ncorpus that incorporates diverse cultural and societal norms, it is crucial to\nevaluate its effectiveness in cultural adaptation. In this paper, we\ninvestigate the underlying cultural background of ChatGPT by analyzing its\nresponses to questions designed to quantify human cultural differences. Our\nfindings suggest that, when prompted with American context, ChatGPT exhibits a\nstrong alignment with American culture, but it adapts less effectively to other\ncultural contexts. Furthermore, by using different prompts to probe the model,\nwe show that English prompts reduce the variance in model responses, flattening\nout cultural differences and biasing them towards American culture. This study\nprovides valuable insights into the cultural implications of ChatGPT and\nhighlights the necessity of greater diversity and cultural awareness in\nlanguage technologies.\n","authors":["Yong Cao","Li Zhou","Seolhwa Lee","Laura Cabello","Min Chen","Daniel Hershcovich"],"pdf_url":"https://arxiv.org/pdf/2303.17466v2.pdf","comment":"C3NLP@EACL 2023"},{"id":"http://arxiv.org/abs/2303.18116v1","updated":"2023-03-31T15:02:48Z","published":"2023-03-31T15:02:48Z","title":"Pair Programming with Large Language Models for Sampling and Estimation\n  of Copulas","summary":"  Without writing a single line of code by a human, an example Monte Carlo\nsimulation based application for stochastic dependence modeling with copulas is\ndeveloped using a state-of-the-art large language model (LLM) fine-tuned for\nconversations. This includes interaction with ChatGPT in natural language and\nusing mathematical formalism, which, under careful supervision by a\nhuman-expert, led to producing a working code in MATLAB, Python and R for\nsampling from a given copula model, evaluation of the model's density,\nperforming maximum likelihood estimation, optimizing the code for parallel\ncomputing for CPUs as well as for GPUs, and visualization of the computed\nresults. In contrast to other emerging studies that assess the accuracy of LLMs\nlike ChatGPT on tasks from a selected area, this work rather investigates ways\nhow to achieve a successful solution of a standard statistical task in a\ncollaboration of a human-expert and artificial intelligence (AI). Particularly,\nthrough careful prompt engineering, we separate successful solutions generated\nby ChatGPT from unsuccessful ones, resulting in a comprehensive list of related\npros and cons. It is demonstrated that if the typical pitfalls are avoided, we\ncan substantially benefit from collaborating with an AI partner. For example,\nwe show that if ChatGPT is not able to provide a correct solution due to a lack\nof or incorrect knowledge, the human-expert can feed it with the correct\nknowledge, e.g., in the form of mathematical theorems and formulas, and make it\nto apply the gained knowledge in order to provide a solution that is correct.\nSuch ability presents an attractive opportunity to achieve a programmed\nsolution even for users with rather limited knowledge of programming\ntechniques.\n","authors":["Jan Górecki"],"pdf_url":"https://arxiv.org/pdf/2303.18116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18110v1","updated":"2023-03-31T14:56:54Z","published":"2023-03-31T14:56:54Z","title":"The Edinburgh International Accents of English Corpus: Towards the\n  Democratization of English ASR","summary":"  English is the most widely spoken language in the world, used daily by\nmillions of people as a first or second language in many different contexts. As\na result, there are many varieties of English. Although the great many advances\nin English automatic speech recognition (ASR) over the past decades, results\nare usually reported based on test datasets which fail to represent the\ndiversity of English as spoken today around the globe. We present the first\nrelease of The Edinburgh International Accents of English Corpus (EdAcc). This\ndataset attempts to better represent the wide diversity of English,\nencompassing almost 40 hours of dyadic video call conversations between\nfriends. Unlike other datasets, EdAcc includes a wide range of first and\nsecond-language varieties of English and a linguistic background profile of\neach speaker. Results on latest public, and commercial models show that EdAcc\nhighlights shortcomings of current English ASR models. The best performing\nmodel, trained on 680 thousand hours of transcribed data, obtains an average of\n19.7% word error rate (WER) -- in contrast to the 2.7% WER obtained when\nevaluated on US English clean read speech. Across all models, we observe a drop\nin performance on Indian, Jamaican, and Nigerian English speakers. Recordings,\nlinguistic backgrounds, data statement, and evaluation scripts are released on\nour website (https://groups.inf.ed.ac.uk/edacc/) under CC-BY-SA license.\n","authors":["Ramon Sanabria","Nikolay Bogoychev","Nina Markl","Andrea Carmantini","Ondrej Klejch","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2303.18110v1.pdf","comment":"Accepted to IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.18103v1","updated":"2023-03-31T14:49:05Z","published":"2023-03-31T14:49:05Z","title":"Dataset and Baseline System for Multi-lingual Extraction and\n  Normalization of Temporal and Numerical Expressions","summary":"  Temporal and numerical expression understanding is of great importance in\nmany downstream Natural Language Processing (NLP) and Information Retrieval\n(IR) tasks. However, much previous work covers only a few sub-types and focuses\nonly on entity extraction, which severely limits the usability of identified\nmentions. In order for such entities to be useful in downstream scenarios,\ncoverage and granularity of sub-types are important; and, even more so,\nproviding resolution into concrete values that can be manipulated. Furthermore,\nmost previous work addresses only a handful of languages. Here we describe a\nmulti-lingual evaluation dataset - NTX - covering diverse temporal and\nnumerical expressions across 14 languages and covering extraction,\nnormalization, and resolution. Along with the dataset we provide a robust\nrule-based system as a strong baseline for comparisons against other models to\nbe evaluated in this dataset. Data and code are available at\n\\url{https://aka.ms/NTX}.\n","authors":["Sanxing Chen","Yongqiang Chen","Börje F. Karlsson"],"pdf_url":"https://arxiv.org/pdf/2303.18103v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2303.16857v2","updated":"2023-03-31T13:34:52Z","published":"2023-03-29T17:07:26Z","title":"Did You Mean...? Confidence-based Trade-offs in Semantic Parsing","summary":"  We illustrate how a calibrated model can help balance common trade-offs in\ntask-oriented parsing. In a simulated annotator-in-the-loop experiment, we show\nthat well-calibrated confidence scores allow us to balance cost with annotator\nload, improving accuracy with a small number of interactions. We then examine\nhow confidence scores can help optimize the trade-off between usability and\nsafety. We show that confidence-based thresholding can substantially reduce the\nnumber of incorrect low-confidence programs executed; however, this comes at a\ncost to usability. We propose the DidYouMean system which better balances\nusability and safety.\n","authors":["Elias Stengel-Eskin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2303.16857v2.pdf","comment":"9 pages. previous version: aXiv:2211.07443. arXiv admin note:\n  substantial text overlap with arXiv:2211.07443"},{"id":"http://arxiv.org/abs/2303.18049v1","updated":"2023-03-31T13:33:53Z","published":"2023-03-31T13:33:53Z","title":"No Place to Hide: Dual Deep Interaction Channel Network for Fake News\n  Detection based on Data Augmentation","summary":"  Online Social Network (OSN) has become a hotbed of fake news due to the low\ncost of information dissemination. Although the existing methods have made many\nattempts in news content and propagation structure, the detection of fake news\nis still facing two challenges: one is how to mine the unique key features and\nevolution patterns, and the other is how to tackle the problem of small samples\nto build the high-performance model. Different from popular methods which take\nfull advantage of the propagation topology structure, in this paper, we propose\na novel framework for fake news detection from perspectives of semantic,\nemotion and data enhancement, which excavates the emotional evolution patterns\nof news participants during the propagation process, and a dual deep\ninteraction channel network of semantic and emotion is designed to obtain a\nmore comprehensive and fine-grained news representation with the consideration\nof comments. Meanwhile, the framework introduces a data enhancement module to\nobtain more labeled data with high quality based on confidence which further\nimproves the performance of the classification model. Experiments show that the\nproposed approach outperforms the state-of-the-art methods.\n","authors":["Biwei Cao","Lulu Hua","Jiuxin Cao","Jie Gui","Bo Liu","James Tin-Yau Kwok"],"pdf_url":"https://arxiv.org/pdf/2303.18049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07443v5","updated":"2023-03-31T13:30:08Z","published":"2022-11-14T15:17:55Z","title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing","summary":"  Sequence generation models are increasingly being used to translate language\ninto executable programs, i.e. to perform executable semantic parsing. The fact\nthat semantic parsing aims to execute actions in the real world motivates\ndeveloping safe systems, which in turn makes measuring calibration -- a central\ncomponent to safety -- particularly important. We investigate the calibration\nof common generation models across four popular semantic parsing datasets,\nfinding that it varies across models and datasets. We then analyze factors\nassociated with calibration error and release new confidence-based challenge\nsplits of two parsing datasets. To facilitate the inclusion of calibration in\nsemantic parsing evaluations, we release a library for computing calibration\nmetrics.\n","authors":["Elias Stengel-Eskin","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2211.07443v5.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2203.16799v4","updated":"2023-03-31T13:25:05Z","published":"2022-03-31T05:07:16Z","title":"M-MELD: A Multilingual Multi-Party Dataset for Emotion Recognition in\n  Conversations","summary":"  Expression of emotions is a crucial part of daily human communication.\nEmotion recognition in conversations (ERC) is an emerging field of study, where\nthe primary task is to identify the emotion behind each utterance in a\nconversation. Though a lot of work has been done on ERC in the past, these\nworks only focus on ERC in the English language, thereby ignoring any other\nlanguages. In this paper, we present Multilingual MELD (M-MELD), where we\nextend the Multimodal EmotionLines Dataset (MELD) \\cite{poria2018meld} to 4\nother languages beyond English, namely Greek, Polish, French, and Spanish.\nBeyond just establishing strong baselines for all of these 4 languages, we also\npropose a novel architecture, DiscLSTM, that uses both sequential and\nconversational discourse context in a conversational dialogue for ERC. Our\nproposed approach is computationally efficient, can transfer across languages\nusing just a cross-lingual encoder, and achieves better performance than most\nuni-modal text approaches in the literature on both MELD and M-MELD. We make\nour data and code publicly on GitHub.\n","authors":["Sreyan Ghosh","S Ramaneswaran","Utkarsh Tyagi","Harshvardhan Srivastava","Samden Lepcha","S Sakshi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2203.16799v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18027v1","updated":"2023-03-31T13:04:47Z","published":"2023-03-31T13:04:47Z","title":"Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations","summary":"  As large language models (LLMs) gain popularity among speakers of diverse\nlanguages, we believe that it is crucial to benchmark them to better understand\nmodel behaviors, failures, and limitations in languages beyond English. In this\nwork, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national\nmedical licensing examinations from the past five years. Our team comprises\nnative Japanese-speaking NLP researchers and a practicing cardiologist based in\nJapan. Our experiments show that GPT-4 outperforms ChatGPT and GPT-3 and passes\nall five years of the exams, highlighting LLMs' potential in a language that is\ntypologically distant from English. However, our evaluation also exposes\ncritical limitations of the current LLM APIs. First, LLMs sometimes select\nprohibited choices that should be strictly avoided in medical practice in\nJapan, such as suggesting euthanasia. Further, our analysis shows that the API\ncosts are generally higher and the maximum context size is smaller for Japanese\nbecause of the way non-Latin scripts are currently tokenized in the pipeline.\nWe release our benchmark as Igaku QA as well as all model outputs and exam\nmetadata. We hope that our results and benchmark will spur progress on more\ndiverse applications of LLMs. Our benchmark is available at\nhttps://github.com/jungokasai/IgakuQA.\n","authors":["Jungo Kasai","Yuhei Kasai","Keisuke Sakaguchi","Yutaro Yamada","Dragomir Radev"],"pdf_url":"https://arxiv.org/pdf/2303.18027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04535v2","updated":"2023-03-31T12:39:09Z","published":"2023-01-11T15:52:55Z","title":"Few-shot Learning for Cross-Target Stance Detection by Aggregating\n  Multimodal Embeddings","summary":"  Despite the increasing popularity of the stance detection task, existing\napproaches are predominantly limited to using the textual content of social\nmedia posts for the classification, overlooking the social nature of the task.\nThe stance detection task becomes particularly challenging in cross-target\nclassification scenarios, where even in few-shot training settings the model\nneeds to predict the stance towards new targets for which the model has only\nseen few relevant samples during training. To address the cross-target stance\ndetection in social media by leveraging the social nature of the task, we\nintroduce CT-TN, a novel model that aggregates multimodal embeddings derived\nfrom both textual and network features of the data. We conduct experiments in a\nfew-shot cross-target scenario on six different combinations of\nsource-destination target pairs. By comparing CT-TN with state-of-the-art\ncross-target stance detection models, we demonstrate the effectiveness of our\nmodel by achieving average performance improvements ranging from 11% to 21%\nacross different baseline models. Experiments with different numbers of shots\nshow that CT-TN can outperform other models after seeing 300 instances of the\ndestination target. Further, ablation experiments demonstrate the positive\ncontribution of each of the components of CT-TN towards the final performance.\nWe further analyse the network interactions between social media users, which\nreveal the potential of using social features for cross-target stance\ndetection.\n","authors":["Parisa Jamadi Khiabani","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2301.04535v2.pdf","comment":"To appear in IEEE Transactions on Computational Social Systems"},{"id":"http://arxiv.org/abs/2303.18011v1","updated":"2023-03-31T12:34:14Z","published":"2023-03-31T12:34:14Z","title":"Exploiting Multilingualism in Low-resource Neural Machine Translation\n  via Adversarial Learning","summary":"  Generative Adversarial Networks (GAN) offer a promising approach for Neural\nMachine Translation (NMT). However, feeding multiple morphologically languages\ninto a single model during training reduces the NMT's performance. In GAN,\nsimilar to bilingual models, multilingual NMT only considers one reference\ntranslation for each sentence during model training. This single reference\ntranslation limits the GAN model from learning sufficient information about the\nsource sentence representation. Thus, in this article, we propose Denoising\nAdversarial Auto-encoder-based Sentence Interpolation (DAASI) approach to\nperform sentence interpolation by learning the intermediate latent\nrepresentation of the source and target sentences of multilingual language\npairs. Apart from latent representation, we also use the Wasserstein-GAN\napproach for the multilingual NMT model by incorporating the model generated\nsentences of multiple languages for reward computation. This computed reward\noptimizes the performance of the GAN-based multilingual model in an effective\nmanner. We demonstrate the experiments on low-resource language pairs and find\nthat our approach outperforms the existing state-of-the-art approaches for\nmultilingual NMT with a performance gain of up to 4 BLEU points. Moreover, we\nuse our trained model on zero-shot language pairs under an unsupervised\nscenario and show the robustness of the proposed approach.\n","authors":["Amit Kumar","Ajay Pratap","Anil Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2303.18011v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.17972v1","updated":"2023-03-31T11:16:20Z","published":"2023-03-31T11:16:20Z","title":"$\\mathcal{E}$ KÚ [MASK]: Integrating Yorùbá cultural greetings\n  into machine translation","summary":"  This paper investigates the performance of massively multilingual neural\nmachine translation (NMT) systems in translating Yor\\`ub\\'a greetings\n($\\mathcal{E}$ k\\'u [MASK]), which are a big part of Yor\\`ub\\'a language and\nculture, into English. To evaluate these models, we present IkiniYor\\`ub\\'a, a\nYor\\`ub\\'a-English translation dataset containing some Yor\\`ub\\'a greetings,\nand sample use cases. We analysed the performance of different multilingual NMT\nsystems including Google and NLLB and show that these models struggle to\naccurately translate Yor\\`ub\\'a greetings into English. In addition, we trained\na Yor\\`ub\\'a-English model by finetuning an existing NMT model on the training\nsplit of IkiniYor\\`ub\\'a and this achieved better performance when compared to\nthe pre-trained multilingual NMT models, although they were trained on a large\nvolume of data.\n","authors":["Idris Akinade","Jesujoba Alabi","David Adelani","Clement Odoje","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2303.17972v1.pdf","comment":"C3NLP Workshop @ EACL2023"},{"id":"http://arxiv.org/abs/2303.17932v1","updated":"2023-03-31T09:55:48Z","published":"2023-03-31T09:55:48Z","title":"Trimming Phonetic Alignments Improves the Inference of Sound\n  Correspondence Patterns from Multilingual Wordlists","summary":"  Sound correspondence patterns form the basis of cognate detection and\nphonological reconstruction in historical language comparison. Methods for the\nautomatic inference of correspondence patterns from phonetically aligned\ncognate sets have been proposed, but their application to multilingual\nwordlists requires extremely well annotated datasets. Since annotation is\ntedious and time consuming, it would be desirable to find ways to improve\naligned cognate data automatically. Taking inspiration from trimming techniques\nin evolutionary biology, which improve alignments by excluding problematic\nsites, we propose a workflow that trims phonetic alignments in comparative\nlinguistics prior to the inference of correspondence patterns. Testing these\ntechniques on a large standardized collection of ten datasets with expert\nannotations from different language families, we find that the best trimming\ntechnique substantially improves the overall consistency of the alignments. The\nresults show a clear increase in the proportion of frequent correspondence\npatterns and words exhibiting regular cognate relations.\n","authors":["Frederic Blum","Johann-Mattis List"],"pdf_url":"https://arxiv.org/pdf/2303.17932v1.pdf","comment":"The paper was accepted at the SIGTYP workshop 2023 co-located with\n  EACL"},{"id":"http://arxiv.org/abs/2303.17930v1","updated":"2023-03-31T09:54:47Z","published":"2023-03-31T09:54:47Z","title":"JobHam-place with smart recommend job options and candidate filtering\n  options","summary":"  Due to the increasing number of graduates, many applicants experience the\nsituation about finding a job, and employers experience difficulty filtering\njob applicants, which might negatively impact their effectiveness. However,\nmost job-hunting websites lack job recommendation and CV filtering or ranking\nfunctionality, which are not integrated into the system. Thus, a smart job\nhunter combined with the above functionality will be conducted in this project,\nwhich contains job recommendations, CV ranking and even a job dashboard for\nskills and job applicant functionality. Job recommendation and CV ranking\nstarts from the automatic keyword extraction and end with the Job/CV ranking\nalgorithm. Automatic keyword extraction is implemented by Job2Skill and the\nCV2Skill model based on Bert. Job2Skill consists of two components, text\nencoder and Gru-based layers, while CV2Skill is mainly based on Bert and\nfine-tunes the pre-trained model by the Resume- Entity dataset. Besides, to\nmatch skills from CV and job description and rank lists of jobs and candidates,\njob/CV ranking algorithms have been provided to compute the occurrence ratio of\nskill words based on TFIDF score and match ratio of the total skill numbers.\nBesides, some advanced features have been integrated into the website to\nimprove user experiences, such as the calendar and sweetalert2 plugin. And some\nbasic features to go through job application processes, such as job application\ntracking and interview arrangement.\n","authors":["Shiyao Wu"],"pdf_url":"https://arxiv.org/pdf/2303.17930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17927v1","updated":"2023-03-31T09:50:07Z","published":"2023-03-31T09:50:07Z","title":"Cross-Cultural Transfer Learning for Chinese Offensive Language\n  Detection","summary":"  Detecting offensive language is a challenging task. Generalizing across\ndifferent cultures and languages becomes even more challenging: besides\nlexical, syntactic and semantic differences, pragmatic aspects such as cultural\nnorms and sensitivities, which are particularly relevant in this context, vary\ngreatly. In this paper, we target Chinese offensive language detection and aim\nto investigate the impact of transfer learning using offensive language\ndetection data from different cultural backgrounds, specifically Korean and\nEnglish. We find that culture-specific biases in what is considered offensive\nnegatively impact the transferability of language models (LMs) and that LMs\ntrained on diverse cultural data are sensitive to different features in Chinese\noffensive language detection. In a few-shot learning scenario, however, our\nstudy shows promising prospects for non-English offensive language detection\nwith limited resources. Our findings highlight the importance of cross-cultural\ntransfer learning in improving offensive language detection and promoting\ninclusive digital spaces.\n","authors":["Li Zhou","Laura Cabello","Yong Cao","Daniel Hershcovich"],"pdf_url":"https://arxiv.org/pdf/2303.17927v1.pdf","comment":"C3NLP@EACL"},{"id":"http://arxiv.org/abs/2303.17910v1","updated":"2023-03-31T09:16:13Z","published":"2023-03-31T09:16:13Z","title":"Selective Knowledge Distillation for Non-Autoregressive Neural Machine\n  Translation","summary":"  Benefiting from the sequence-level knowledge distillation, the\nNon-Autoregressive Transformer (NAT) achieves great success in neural machine\ntranslation tasks. However, existing knowledge distillation has side effects,\nsuch as propagating errors from the teacher to NAT students, which may limit\nfurther improvements of NAT models and are rarely discussed in existing\nresearch. In this paper, we introduce selective knowledge distillation by\nintroducing an NAT evaluator to select NAT-friendly targets that are of high\nquality and easy to learn. In addition, we introduce a simple yet effective\nprogressive distillation method to boost NAT performance. Experiment results on\nmultiple WMT language directions and several representative NAT models show\nthat our approach can realize a flexible trade-off between the quality and\ncomplexity of training data for NAT models, achieving strong performances.\nFurther analysis shows that distilling only 5% of the raw translations can help\nan NAT outperform its counterpart trained on raw data by about 2.4 BLEU.\n","authors":["Min Liu","Yu Bao","Chengqi Zhao","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2303.17910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17876v1","updated":"2023-03-31T08:18:30Z","published":"2023-03-31T08:18:30Z","title":"WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset","summary":"  We create WebQAmGaze, a multilingual low-cost eye-tracking-while-reading\ndataset, designed to support the development of fair and transparent NLP\nmodels. WebQAmGaze includes webcam eye-tracking data from 332 participants\nnaturally reading English, Spanish, and German texts. Each participant performs\ntwo reading tasks composed of five texts, a normal reading and an\ninformation-seeking task. After preprocessing the data, we find that fixations\non relevant spans seem to indicate correctness when answering the comprehension\nquestions. Additionally, we perform a comparative analysis of the data\ncollected to high-quality eye-tracking data. The results show a moderate\ncorrelation between the features obtained with the webcam-ET compared to those\nof a commercial ET device. We believe this data can advance webcam-based\nreading studies and open a way to cheaper and more accessible data collection.\nWebQAmGaze is useful to learn about the cognitive processes behind question\nanswering (QA) and to apply these insights to computational models of language\nunderstanding.\n","authors":["Tiago Ribeiro","Stephanie Brandl","Anders Søgaard","Nora Hollenstein"],"pdf_url":"https://arxiv.org/pdf/2303.17876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16166v3","updated":"2023-03-31T08:15:53Z","published":"2023-03-28T17:28:52Z","title":"Reproducibility is Nothing without Correctness: The Importance of\n  Testing Code in NLP","summary":"  Despite its pivotal role in research experiments, code correctness is often\npresumed only on the basis of the perceived quality of the results. This comes\nwith the risk of erroneous outcomes and potentially misleading findings. To\naddress this issue, we posit that the current focus on result reproducibility\nshould go hand in hand with the emphasis on coding best practices. We bolster\nour call to the NLP community by presenting a case study, in which we identify\n(and correct) three bugs in widely used open-source implementations of the\nstate-of-the-art Conformer architecture. Through comparative experiments on\nautomatic speech recognition and translation in various language settings, we\ndemonstrate that the existence of bugs does not prevent the achievement of good\nand reproducible results and can lead to incorrect conclusions that potentially\nmisguide future research. In response to this, this study is a call to action\ntoward the adoption of coding best practices aimed at fostering correctness and\nimproving the quality of the developed software.\n","authors":["Sara Papi","Marco Gaido","Andrea Pilzer","Matteo Negri"],"pdf_url":"https://arxiv.org/pdf/2303.16166v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16539v2","updated":"2023-03-31T08:02:01Z","published":"2022-10-29T09:18:41Z","title":"Exploiting prompt learning with pre-trained language models for\n  Alzheimer's Disease detection","summary":"  Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating\npreventive care and to delay further progression. Speech based automatic AD\nscreening systems provide a non-intrusive and more scalable alternative to\nother clinical screening techniques. Textual embedding features produced by\npre-trained language models (PLMs) such as BERT are widely used in such\nsystems. However, PLM domain fine-tuning is commonly based on the masked word\nor sentence prediction costs that are inconsistent with the back-end AD\ndetection task. To this end, this paper investigates the use of prompt-based\nfine-tuning of PLMs that consistently uses AD classification errors as the\ntraining objective function. Disfluency features based on hesitation or pause\nfiller token frequencies are further incorporated into prompt phrases during\nPLM fine-tuning. The decision voting based combination among systems using\ndifferent PLMs (BERT and RoBERTa) or systems with different fine-tuning\nparadigms (conventional masked-language modelling fine-tuning and prompt-based\nfine-tuning) is further applied. Mean, standard deviation and the maximum among\naccuracy scores over 15 experiment runs are adopted as performance measurements\nfor the AD detection system. Mean detection accuracy of 84.20% (with std 2.09%,\nbest 87.5%) and 82.64% (with std 4.0%, best 89.58%) were obtained using manual\nand ASR speech transcripts respectively on the ADReSS20 test set consisting of\n48 elderly speakers.\n","authors":["Yi Wang","Jiajun Deng","Tianzi Wang","Bo Zheng","Shoukang Hu","Xunying Liu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2210.16539v2.pdf","comment":"Accepted ICASSP 2023 (will update with IEEE vision later)"},{"id":"http://arxiv.org/abs/2303.17853v1","updated":"2023-03-31T07:29:47Z","published":"2023-03-31T07:29:47Z","title":"Can AI Put Gamma-Ray Astrophysicists Out of a Job?","summary":"  In what will likely be a litany of generative-model-themed arXiv submissions\ncelebrating April the 1st, we evaluate the capacity of state-of-the-art\ntransformer models to create a paper detailing the detection of a Pulsar Wind\nNebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT)\nArray. We do this to evaluate the ability of such models to interpret\nastronomical observations and sources based on language information alone, and\nto assess potential means by which fraudulently generated scientific papers\ncould be identified during peer review (given that reliable generative model\nwatermarking has yet to be deployed for these tools). We conclude that our jobs\nas astronomers are safe for the time being. From this point on, prompts given\nto ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT\nis shown in black, whereas analysis by the (human) authors is in blue.\n","authors":["Samuel Timothy Spencer","Vikas Joshi","Alison Mairi Wallace Mitchell"],"pdf_url":"https://arxiv.org/pdf/2303.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17839v1","updated":"2023-03-31T07:02:26Z","published":"2023-03-31T07:02:26Z","title":"Learning Procedure-aware Video Representation from Instructional Videos\n  and Their Narrations","summary":"  The abundance of instructional videos and their narrations over the Internet\noffers an exciting avenue for understanding procedural activities. In this\nwork, we propose to learn video representation that encodes both action steps\nand their temporal ordering, based on a large-scale dataset of web\ninstructional videos and their narrations, without using human annotations. Our\nmethod jointly learns a video representation to encode individual step\nconcepts, and a deep probabilistic model to capture both temporal dependencies\nand immense individual variations in the step ordering. We empirically\ndemonstrate that learning temporal ordering not only enables new capabilities\nfor procedure reasoning, but also reinforces the recognition of individual\nsteps. Our model significantly advances the state-of-the-art results on step\nclassification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting\n(+7.4% on COIN). Moreover, our model attains promising results in zero-shot\ninference for step classification and forecasting, as well as in predicting\ndiverse and plausible steps for incomplete procedures. Our code is available at\nhttps://github.com/facebookresearch/ProcedureVRL.\n","authors":["Yiwu Zhong","Licheng Yu","Yang Bai","Shangwen Li","Xueting Yan","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2303.17839v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13217v3","updated":"2023-03-31T06:11:06Z","published":"2023-03-23T12:28:25Z","title":"Fairness-guided Few-shot Prompting for Large Language Models","summary":"  Large language models have demonstrated surprising ability to perform\nin-context learning, i.e., these models can be directly applied to solve\nnumerous downstream tasks by conditioning on a prompt constructed by a few\ninput-output examples. However, prior research has shown that in-context\nlearning can suffer from high instability due to variations in training\nexamples, example order, and prompt formats. Therefore, the construction of an\nappropriate prompt is essential for improving the performance of in-context\nlearning. In this paper, we revisit this problem from the view of predictive\nbias. Specifically, we introduce a metric to evaluate the predictive bias of a\nfixed prompt against labels or a given attributes. Then we empirically show\nthat prompts with higher bias always lead to unsatisfactory predictive quality.\nBased on this observation, we propose a novel search strategy based on the\ngreedy search to identify the near-optimal prompt for improving the performance\nof in-context learning. We perform comprehensive experiments with\nstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.\nOur results indicate that our method can enhance the model's in-context\nlearning performance in an effective and interpretable manner.\n","authors":["Huan Ma","Changqing Zhang","Yatao Bian","Lemao Liu","Zhirui Zhang","Peilin Zhao","Shu Zhang","Huazhu Fu","Qinghua Hu","Bingzhe Wu"],"pdf_url":"https://arxiv.org/pdf/2303.13217v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17811v1","updated":"2023-03-31T06:00:50Z","published":"2023-03-31T06:00:50Z","title":"Zero-shot Referring Image Segmentation with Global-Local Context\n  Features","summary":"  Referring image segmentation (RIS) aims to find a segmentation mask given a\nreferring expression grounded to a region of the input image. Collecting\nlabelled datasets for this task, however, is notoriously costly and\nlabor-intensive. To overcome this issue, we propose a simple yet effective\nzero-shot referring image segmentation method by leveraging the pre-trained\ncross-modal knowledge from CLIP. In order to obtain segmentation masks grounded\nto the input text, we propose a mask-guided visual encoder that captures global\nand local contextual information of an input image. By utilizing instance masks\nobtained from off-the-shelf mask proposal techniques, our method is able to\nsegment fine-detailed Istance-level groundings. We also introduce a\nglobal-local text encoder where the global feature captures complex\nsentence-level semantics of the entire input expression while the local feature\nfocuses on the target noun phrase extracted by a dependency parser. In our\nexperiments, the proposed method outperforms several zero-shot baselines of the\ntask and even the weakly supervised referring expression segmentation method\nwith substantial margins. Our code is available at\nhttps://github.com/Seonghoon-Yu/Zero-shot-RIS.\n","authors":["Seonghoon Yu","Paul Hongsuch Seo","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2303.17811v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17807v1","updated":"2023-03-31T05:43:21Z","published":"2023-03-31T05:43:21Z","title":"Exploring the Potential of Large Language models in Traditional Korean\n  Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare","summary":"  Introduction: Traditional Korean medicine (TKM) emphasizes individualized\ndiagnosis and treatment, making AI modeling difficult due to limited data and\nimplicit processes. GPT-3.5 and GPT-4, large language models, have shown\nimpressive medical knowledge despite lacking medicine-specific training. This\nstudy aimed to assess the capabilities of GPT-3.5 and GPT-4 for TKM using the\nKorean National Licensing Examination for Korean Medicine Doctors. Methods:\nGPT-3.5 (February 2023) and GPT-4 (March 2023) models answered 340 questions\nfrom the 2022 examination across 12 subjects. Each question was independently\nevaluated five times in an initialized session. Results: GPT-3.5 and GPT-4\nachieved 42.06% and 57.29% accuracy, respectively, with GPT-4 nearing passing\nperformance. There were significant differences in accuracy by subjects, with\n83.75% accuracy for neuropsychiatry compared to 28.75% for internal medicine\n(2). Both models showed high accuracy in recall-based and diagnosis-based\nquestions but struggled with intervention-based ones. The accuracy for\nquestions that require TKM-specialized knowledge was relatively lower than the\naccuracy for questions that do not GPT-4 showed high accuracy for table-based\nquestions, and both models demonstrated consistent responses. A positive\ncorrelation between consistency and accuracy was observed. Conclusion: Models\nin this study showed near-passing performance in decision-making for TKM\nwithout domain-specific training. However, limits were also observed that were\nbelieved to be caused by culturally-biased learning. Our study suggests that\nfoundation models have potential in culturally-adapted medicine, specifically\nTKM, for clinical assistance, medical education, and medical research.\n","authors":["Dongyeop Jang","Chang-Eop Kim"],"pdf_url":"https://arxiv.org/pdf/2303.17807v1.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.17799v1","updated":"2023-03-31T05:13:44Z","published":"2023-03-31T05:13:44Z","title":"Dialog act guided contextual adapter for personalized speech recognition","summary":"  Personalization in multi-turn dialogs has been a long standing challenge for\nend-to-end automatic speech recognition (E2E ASR) models. Recent work on\ncontextual adapters has tackled rare word recognition using user catalogs. This\nadaptation, however, does not incorporate an important cue, the dialog act,\nwhich is available in a multi-turn dialog scenario. In this work, we propose a\ndialog act guided contextual adapter network. Specifically, it leverages dialog\nacts to select the most relevant user catalogs and creates queries based on\nboth -- the audio as well as the semantic relationship between the carrier\nphrase and user catalogs to better guide the contextual biasing. On industrial\nvoice assistant datasets, our model outperforms both the baselines - dialog act\nencoder-only model, and the contextual adaptation, leading to the most\nimprovement over the no-context model: 58% average relative word error rate\nreduction (WERR) in the multi-turn dialog scenario, in comparison to the\nprior-art contextual adapter, which has achieved 39% WERR over the no-context\nmodel.\n","authors":["Feng-Ju Chang","Thejaswi Muniyappa","Kanthashree Mysore Sathyendra","Kai Wei","Grant P. Strimel","Ross McGowan"],"pdf_url":"https://arxiv.org/pdf/2303.17799v1.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2212.01611v2","updated":"2023-03-31T03:39:01Z","published":"2022-12-03T13:05:24Z","title":"CoP: Factual Inconsistency Detection by Controlling the Preference","summary":"  Abstractive summarization is the process of generating a summary given a\ndocument as input. Although significant progress has been made, the factual\ninconsistency between the document and the generated summary still limits its\npractical applications. Previous work found that the probabilities assigned by\nthe generation model reflect its preferences for the generated summary,\nincluding the preference for factual consistency, and the preference for the\nlanguage or knowledge prior as well. To separate the preference for factual\nconsistency, we propose an unsupervised framework named CoP by controlling the\npreference of the generation model with the help of prompt. More specifically,\nthe framework performs an extra inference step in which a text prompt is\nintroduced as an additional input. In this way, another preference is described\nby the generation probability of this extra inference process. The difference\nbetween the above two preferences, i.e. the difference between the\nprobabilities, could be used as measurements for detecting factual\ninconsistencies. Interestingly, we found that with the properly designed\nprompt, our framework could evaluate specific preferences and serve as\nmeasurements for fine-grained categories of inconsistency, such as\nentity-related inconsistency, coreference-related inconsistency, etc. Moreover,\nour framework could also be extended to the supervised setting to learn better\nprompt from the labeled data as well. Experiments show that our framework\nachieves new SOTA results on three factual inconsistency detection tasks.\n","authors":["Shuaijie She","Xiang Geng","Shujian Huang","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2212.01611v2.pdf","comment":"Accepted to AAAI2023 regular paper"},{"id":"http://arxiv.org/abs/2303.17786v1","updated":"2023-03-31T03:17:23Z","published":"2023-03-31T03:17:23Z","title":"Attention is Not Always What You Need: Towards Efficient Classification\n  of Domain-Specific Text","summary":"  For large-scale IT corpora with hundreds of classes organized in a hierarchy,\nthe task of accurate classification of classes at the higher level in the\nhierarchies is crucial to avoid errors propagating to the lower levels. In the\nbusiness world, an efficient and explainable ML model is preferred over an\nexpensive black-box model, especially if the performance increase is marginal.\nA current trend in the Natural Language Processing (NLP) community is towards\nemploying huge pre-trained language models (PLMs) or what is known as\nself-attention models (e.g., BERT) for almost any kind of NLP task (e.g.,\nquestion-answering, sentiment analysis, text classification). Despite the\nwidespread use of PLMs and the impressive performance in a broad range of NLP\ntasks, there is a lack of a clear and well-justified need to as why these\nmodels are being employed for domain-specific text classification (TC) tasks,\ngiven the monosemic nature of specialized words (i.e., jargon) found in\ndomain-specific text which renders the purpose of contextualized embeddings\n(e.g., PLMs) futile. In this paper, we compare the accuracies of some\nstate-of-the-art (SOTA) models reported in the literature against a Linear SVM\nclassifier and TFIDF vectorization model on three TC datasets. Results show a\ncomparable performance for the LinearSVM. The findings of this study show that\nfor domain-specific TC tasks, a linear model can provide a comparable, cheap,\nreproducible, and interpretable alternative to attention-based models.\n","authors":["Yasmen Wahba","Nazim Madhavji","John Steinbacher"],"pdf_url":"https://arxiv.org/pdf/2303.17786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17760v1","updated":"2023-03-31T01:09:00Z","published":"2023-03-31T01:09:00Z","title":"CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale\n  Language Model Society","summary":"  The rapid advancement of conversational and chat-based language models has\nled to remarkable progress in complex task-solving. However, their success\nheavily relies on human input to guide the conversation, which can be\nchallenging and time-consuming. This paper explores the potential of building\nscalable techniques to facilitate autonomous cooperation among communicative\nagents and provide insight into their \"cognitive\" processes. To address the\nchallenges of achieving autonomous cooperation, we propose a novel\ncommunicative agent framework named role-playing. Our approach involves using\ninception prompting to guide chat agents toward task completion while\nmaintaining consistency with human intentions. We showcase how role-playing can\nbe used to generate conversational data for studying the behaviors and\ncapabilities of chat agents, providing a valuable resource for investigating\nconversational language models. Our contributions include introducing a novel\ncommunicative agent framework, offering a scalable approach for studying the\ncooperative behaviors and capabilities of multi-agent systems, and\nopen-sourcing our library to support research on communicative agents and\nbeyond. The GitHub repository of this project is made publicly available on:\nhttps://github.com/lightaime/camel.\n","authors":["Guohao Li","Hasan Abed Al Kader Hammoud","Hani Itani","Dmitrii Khizbullin","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2303.17760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17750v1","updated":"2023-03-31T00:21:28Z","published":"2023-03-31T00:21:28Z","title":"Design by Contract Framework for Quantum Software","summary":"  To realize reliable quantum software, techniques to automatically ensure the\nquantum software's correctness have recently been investigated. However, they\nprimarily focus on fixed quantum circuits rather than the procedure of building\nquantum circuits. Despite being a common approach, the correctness of building\ncircuits using different parameters following the same procedure is not\nguaranteed. To this end, we propose a design-by-contract framework for quantum\nsoftware. Our framework provides a python-embedded language to write assertions\non the input and output states of all quantum circuits built by certain\nprocedures. Additionally, it provides a method to write assertions about the\nstatistical processing of measurement results to ensure the procedure's\ncorrectness for obtaining the final result. These assertions are automatically\nchecked using a quantum computer simulator. For evaluation, we implemented our\nframework and wrote assertions for some widely used quantum algorithms.\nConsequently, we found that our framework has sufficient expressive power to\nverify the whole procedure of quantum software.\n","authors":["Masaomi Yamaguchi","Nobukazu Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2303.17750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00180v1","updated":"2023-03-31T23:58:28Z","published":"2023-03-31T23:58:28Z","title":"FCC: Fusing Conversation History and Candidate Provenance for Contextual\n  Response Ranking in Dialogue Systems","summary":"  Response ranking in dialogues plays a crucial role in retrieval-based\nconversational systems. In a multi-turn dialogue, to capture the gist of a\nconversation, contextual information serves as essential knowledge to achieve\nthis goal. In this paper, we present a flexible neural framework that can\nintegrate contextual information from multiple channels. Specifically for the\ncurrent task, our approach is to provide two information channels in parallel,\nFusing Conversation history and domain knowledge extracted from Candidate\nprovenance (FCC), where candidate responses are curated, as contextual\ninformation to improve the performance of multi-turn dialogue response ranking.\nThe proposed approach can be generalized as a module to incorporate\nmiscellaneous contextual features for other context-oriented tasks. We evaluate\nour model on the MSDialog dataset widely used for evaluating conversational\nresponse ranking tasks. Our experimental results show that our framework\nsignificantly outperforms the previous state-of-the-art models, improving\nRecall@1 by 7% and MAP by 4%. Furthermore, we conduct ablation studies to\nevaluate the contributions of each information channel, and of the framework\ncomponents, to the overall ranking performance, providing additional insights\nand directions for further improvements.\n","authors":["Zihao Wang","Eugene Agichtein","Jinho Choi"],"pdf_url":"https://arxiv.org/pdf/2304.00180v1.pdf","comment":"The 13th International Workshop on Spoken Dialogue Systems Technology"},{"id":"http://arxiv.org/abs/2304.00173v1","updated":"2023-03-31T23:33:21Z","published":"2023-03-31T23:33:21Z","title":"Lego-Features: Exporting modular encoder features for streaming and\n  deliberation ASR","summary":"  In end-to-end (E2E) speech recognition models, a representational\ntight-coupling inevitably emerges between the encoder and the decoder. We build\nupon recent work that has begun to explore building encoders with modular\nencoded representations, such that encoders and decoders from different models\ncan be stitched together in a zero-shot manner without further fine-tuning.\nWhile previous research only addresses full-context speech models, we explore\nthe problem in a streaming setting as well. Our framework builds on top of\nexisting encoded representations, converting them to modular features, dubbed\nas Lego-Features, without modifying the pre-trained model. The features remain\ninterchangeable when the model is retrained with distinct initializations.\nThough sparse, we show that the Lego-Features are powerful when tested with\nRNN-T or LAS decoders, maintaining high-quality downstream performance. They\nare also rich enough to represent the first-pass prediction during two-pass\ndeliberation. In this scenario, they outperform the N-best hypotheses, since\nthey do not need to be supplemented with acoustic features to deliver the best\nresults. Moreover, generating the Lego-Features does not require beam search or\nauto-regressive computation. Overall, they present a modular, powerful and\ncheap alternative to the standard encoder output, as well as the N-best\nhypotheses.\n","authors":["Rami Botros","Rohit Prabhavalkar","Johan Schalkwyk","Ciprian Chelba","Tara N. Sainath","Françoise Beaufays"],"pdf_url":"https://arxiv.org/pdf/2304.00173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00171v1","updated":"2023-03-31T23:30:48Z","published":"2023-03-31T23:30:48Z","title":"Practical Conformer: Optimizing size, speed and flops of Conformer for\n  on-Device and cloud ASR","summary":"  Conformer models maintain a large number of internal states, the vast\nmajority of which are associated with self-attention layers. With limited\nmemory bandwidth, reading these from memory at each inference step can slow\ndown inference. In this paper, we design an optimized conformer that is small\nenough to meet on-device restrictions and has fast inference on TPUs. We\nexplore various ideas to improve the execution speed, including replacing lower\nconformer blocks with convolution-only blocks, strategically downsizing the\narchitecture, and utilizing an RNNAttention-Performer. Our optimized conformer\ncan be readily incorporated into a cascaded-encoder setting, allowing a\nsecond-pass decoder to operate on its output and improve the accuracy whenever\nmore resources are available. Altogether, we find that these optimizations can\nreduce latency by a factor of 6.8x, and come at a reasonable trade-off in\nquality. With the cascaded second-pass, we show that the recognition accuracy\nis completely recoverable. Thus, our proposed encoder can double as a strong\nstandalone encoder in on device, and as the first part of a high-performance\nASR pipeline.\n","authors":["Rami Botros","Anmol Gulati","Tara N. Sainath","Krzysztof Choromanski","Ruoming Pang","Trevor Strohman","Weiran Wang","Jiahui Yu"],"pdf_url":"https://arxiv.org/pdf/2304.00171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08142v2","updated":"2023-03-31T23:00:54Z","published":"2022-10-08T22:33:39Z","title":"Semantic Representations of Mathematical Expressions in a Continuous\n  Vector Space","summary":"  Mathematical notation makes up a large portion of STEM literature, yet,\nfinding semantic representations for formulae remains a challenging problem.\nBecause mathematical notation is precise, and its meaning changes significantly\nwith small character shifts, the methods that work for natural text do not\nnecessarily work well for mathematical expressions. In this work, we describe\nan approach for representing mathematical expressions in a continuous vector\nspace. We use the encoder of a sequence-to-sequence architecture, trained on\nvisually different but mathematically equivalent expressions, to generate\nvector representations (or embeddings). We compare this approach with an\nautoencoder and show that the former is better at capturing mathematical\nsemantics. Finally, to expedite future research, we publish a corpus of\nequivalent transcendental and algebraic expression pairs.\n","authors":["Neeraj Gangwar","Nickvash Kani"],"pdf_url":"https://arxiv.org/pdf/2211.08142v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2212.13201v2","updated":"2023-03-31T22:13:18Z","published":"2022-12-26T16:13:57Z","title":"Highlighting Named Entities in Input for Auto-Formulation of\n  Optimization Problems","summary":"  Operations research deals with modeling and solving real-world problems as\nmathematical optimization problems. While solving mathematical systems is\naccomplished by analytical software, formulating a problem as a set of\nmathematical operations has been typically done manually by domain experts.\nRecent machine learning methods have shown promise in converting textual\nproblem descriptions to corresponding mathematical formulations. This paper\npresents an approach that converts linear programming word problems into\nmathematical formulations. We leverage the named entities in the input and\naugment the input to highlight these entities. Our approach achieves the\nhighest accuracy among all submissions to the NL4Opt Competition, securing\nfirst place in the generation track.\n","authors":["Neeraj Gangwar","Nickvash Kani"],"pdf_url":"https://arxiv.org/pdf/2212.13201v2.pdf","comment":"Code: https://github.com/mlpgroup/nl4opt-generation"},{"id":"http://arxiv.org/abs/2301.09767v2","updated":"2023-03-31T22:05:53Z","published":"2023-01-24T00:32:56Z","title":"Truveta Mapper: A Zero-shot Ontology Alignment Framework","summary":"  In this paper, a new perspective is suggested for unsupervised Ontology\nMatching (OM) or Ontology Alignment (OA) by treating it as a translation task.\nOntologies are represented as graphs, and the translation is performed from a\nnode in the source ontology graph to a path in the target ontology graph. The\nproposed framework, Truveta Mapper (TM), leverages a multi-task\nsequence-to-sequence transformer model to perform alignment across multiple\nontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables\nthe model to implicitly learn the relationship between different ontologies via\ntransfer-learning without requiring any explicit cross-ontology manually\nlabeled data. This also enables the formulated framework to outperform existing\nsolutions for both runtime latency and alignment quality. The model is\npre-trained and fine-tuned only on publicly available text corpus and\ninner-ontologies data. The proposed solution outperforms state-of-the-art\napproaches, Edit-Similarity, LogMap, AML, BERTMap, and the recently presented\nnew OM frameworks in Ontology Alignment Evaluation Initiative (OAEI22), offers\nlog-linear complexity in contrast to quadratic in the existing end-to-end\nmethods, and overall makes the OM task efficient and more straightforward\nwithout much post-processing involving mapping extension or mapping repair.\n","authors":["Mariyam Amir","Murchana Baruah","Mahsa Eslamialishah","Sina Ehsani","Alireza Bahramali","Sadra Naddaf-Sh","Saman Zarandioon"],"pdf_url":"https://arxiv.org/pdf/2301.09767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12452v2","updated":"2023-03-31T22:01:44Z","published":"2022-05-25T02:51:12Z","title":"Sparse*BERT: Sparse Models Generalize To New tasks and Domains","summary":"  Large Language Models have become the core architecture upon which most\nmodern natural language processing (NLP) systems build. These models can\nconsistently deliver impressive accuracy and robustness across tasks and\ndomains, but their high computational overhead can make inference difficult and\nexpensive. To make using these models less costly, recent work has explored\nleveraging structured and unstructured pruning, quantization, and distillation\nto improve inference speed and decrease size. This paper studies how models\npruned using Gradual Unstructured Magnitude Pruning can transfer between\ndomains and tasks. Our experimentation shows that models that are pruned during\npretraining using general domain masked language models can transfer to novel\ndomains and tasks without extensive hyperparameter exploration or specialized\napproaches. We demonstrate that our general sparse model Sparse*BERT can become\nSparseBioBERT simply by pretraining the compressed architecture on unstructured\nbiomedical text. Moreover, we show that SparseBioBERT can match the quality of\nBioBERT with only 10\\% of the parameters.\n","authors":["Daniel Campos","Alexandre Marques","Tuan Nguyen","Mark Kurtz","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2205.12452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08562v4","updated":"2023-03-31T21:56:20Z","published":"2022-07-18T12:44:59Z","title":"DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link\n  Prediction and Entity Typing","summary":"  In the field of representation learning on knowledge graphs (KGs), a\nhyper-relational fact consists of a main triple and several auxiliary\nattribute-value descriptions, which is considered more comprehensive and\nspecific than a triple-based fact. However, currently available\nhyper-relational KG embedding methods in a single view are limited in\napplication because they weaken the hierarchical structure that represents the\naffiliation between entities. To overcome this limitation, we propose a\ndual-view hyper-relational KG structure (DH-KG) that contains a\nhyper-relational instance view for entities and a hyper-relational ontology\nview for concepts that are abstracted hierarchically from the entities. This\npaper defines link prediction and entity typing tasks on DH-KG for the first\ntime and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and\nHTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding\nmodel based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms\nbaseline models on DH-KG, according to experimental results. Finally, we\nprovide an example of how this technology can be used to treat hypertension.\nOur model and new datasets are publicly available.\n","authors":["Haoran Luo","Haihong E","Ling Tan","Gengxian Zhou","Tianyu Yao","Kaiyang Wan"],"pdf_url":"https://arxiv.org/pdf/2207.08562v4.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2304.00121v1","updated":"2023-03-31T20:33:03Z","published":"2023-03-31T20:33:03Z","title":"Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts","summary":"  Scholarly writing presents a complex space that generally follows a\nmethodical procedure to plan and produce both rationally sound and creative\ncompositions. Recent works involving large language models (LLM) demonstrate\nconsiderable success in text generation and revision tasks; however, LLMs still\nstruggle to provide structural and creative feedback on the document level that\nis crucial to academic writing. In this paper, we introduce a novel taxonomy\nthat categorizes scholarly writing behaviors according to intention, writer\nactions, and the information types of the written data. We also provide\nManuScript, an original dataset annotated with a simplified version of our\ntaxonomy to show writer actions and the intentions behind them. Motivated by\ncognitive writing theory, our taxonomy for scientific papers includes three\nlevels of categorization in order to trace the general writing flow and\nidentify the distinct writer activities embedded within each higher-level\nprocess. ManuScript intends to provide a complete picture of the scholarly\nwriting process by capturing the linearity and non-linearity of writing\ntrajectory, such that writing assistants can provide stronger feedback and\nsuggestions on an end-to-end level. The collected writing trajectories are\nviewed at https://minnesotanlp.github.io/REWARD_demo/\n","authors":["Ryan Koo","Anna Martin","Linghe Wang","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2304.00121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00116v1","updated":"2023-03-31T20:24:14Z","published":"2023-03-31T20:24:14Z","title":"Enhancing Large Language Models with Climate Resources","summary":"  Large language models (LLMs) have significantly transformed the landscape of\nartificial intelligence by demonstrating their ability in generating human-like\ntext across diverse topics. However, despite their impressive capabilities,\nLLMs lack recent information and often employ imprecise language, which can be\ndetrimental in domains where accuracy is crucial, such as climate change. In\nthis study, we make use of recent ideas to harness the potential of LLMs by\nviewing them as agents that access multiple sources, including databases\ncontaining recent and precise information about organizations, institutions,\nand companies. We demonstrate the effectiveness of our method through a\nprototype agent that retrieves emission data from ClimateWatch\n(https://www.climatewatchdata.org/) and leverages general Google search. By\nintegrating these resources with LLMs, our approach overcomes the limitations\nassociated with imprecise language and delivers more reliable and accurate\ninformation in the critical domain of climate change. This work paves the way\nfor future advancements in LLMs and their application in domains where\nprecision is of paramount importance.\n","authors":["Mathias Kraus","Julia Anna Bingler","Markus Leippold","Tobias Schimanski","Chiara Colesanti Senni","Dominik Stammbach","Saeid Ashraf Vaghefi","Nicolas Webersinke"],"pdf_url":"https://arxiv.org/pdf/2304.00116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00115v1","updated":"2023-03-31T20:23:58Z","published":"2023-03-31T20:23:58Z","title":"Extracting Thyroid Nodules Characteristics from Ultrasound Reports Using\n  Transformer-based Natural Language Processing Methods","summary":"  The ultrasound characteristics of thyroid nodules guide the evaluation of\nthyroid cancer in patients with thyroid nodules. However, the characteristics\nof thyroid nodules are often documented in clinical narratives such as\nultrasound reports. Previous studies have examined natural language processing\n(NLP) methods in extracting a limited number of characteristics (<9) using\nrule-based NLP systems. In this study, a multidisciplinary team of NLP experts\nand thyroid specialists, identified thyroid nodule characteristics that are\nimportant for clinical care, composed annotation guidelines, developed a\ncorpus, and compared 5 state-of-the-art transformer-based NLP methods,\nincluding BERT, RoBERTa, LongFormer, DeBERTa, and GatorTron, for extraction of\nthyroid nodule characteristics from ultrasound reports. Our GatorTron model, a\ntransformer-based large language model trained using over 90 billion words of\ntext, achieved the best strict and lenient F1-score of 0.8851 and 0.9495 for\nthe extraction of a total number of 16 thyroid nodule characteristics, and\n0.9321 for linking characteristics to nodules, outperforming other clinical\ntransformer models. To the best of our knowledge, this is the first study to\nsystematically categorize and apply transformer-based NLP models to extract a\nlarge number of clinical relevant thyroid nodule characteristics from\nultrasound reports. This study lays ground for assessing the documentation\nquality of thyroid ultrasound reports and examining outcomes of patients with\nthyroid nodules using electronic health records.\n","authors":["Aman Pathak","Zehao Yu","Daniel Paredes","Elio Paul Monsour","Andrea Ortiz Rocha","Juan P. Brito","Naykky Singh Ospina","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2304.00115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00114v1","updated":"2023-03-31T20:21:32Z","published":"2023-03-31T20:21:32Z","title":"Dense Sparse Retrieval: Using Sparse Language Models for Inference\n  Efficient Dense Retrieval","summary":"  Vector-based retrieval systems have become a common staple for academic and\nindustrial search applications because they provide a simple and scalable way\nof extending the search to leverage contextual representations for documents\nand queries. As these vector-based systems rely on contextual language models,\ntheir usage commonly requires GPUs, which can be expensive and difficult to\nmanage. Given recent advances in introducing sparsity into language models for\nimproved inference efficiency, in this paper, we study how sparse language\nmodels can be used for dense retrieval to improve inference efficiency. Using\nthe popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQA\ndatasets, we find that sparse language models can be used as direct\nreplacements with little to no drop in accuracy and up to 4.3x improved\ninference speeds\n","authors":["Daniel Campos","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2304.00114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00111v1","updated":"2023-03-31T20:16:44Z","published":"2023-03-31T20:16:44Z","title":"Identifying Symptoms of Delirium from Clinical Narratives Using Natural\n  Language Processing","summary":"  Delirium is an acute decline or fluctuation in attention, awareness, or other\ncognitive function that can lead to serious adverse outcomes. Despite the\nsevere outcomes, delirium is frequently unrecognized and uncoded in patients'\nelectronic health records (EHRs) due to its transient and diverse nature.\nNatural language processing (NLP), a key technology that extracts medical\nconcepts from clinical narratives, has shown great potential in studies of\ndelirium outcomes and symptoms. To assist in the diagnosis and phenotyping of\ndelirium, we formed an expert panel to categorize diverse delirium symptoms,\ncomposed annotation guidelines, created a delirium corpus with diverse delirium\nsymptoms, and developed NLP methods to extract delirium symptoms from clinical\nnotes. We compared 5 state-of-the-art transformer models including 2 models\n(BERT and RoBERTa) from the general domain and 3 models (BERT_MIMIC,\nRoBERTa_MIMIC, and GatorTron) from the clinical domain. GatorTron achieved the\nbest strict and lenient F1 scores of 0.8055 and 0.8759, respectively. We\nconducted an error analysis to identify challenges in annotating delirium\nsymptoms and developing NLP systems. To the best of our knowledge, this is the\nfirst large language model-based delirium symptom extraction system. Our study\nlays the foundation for the future development of computable phenotypes and\ndiagnosis methods for delirium.\n","authors":["Aokun Chen","Daniel Paredes","Zehao Yu","Xiwei Lou","Roberta Brunson","Jamie N. Thomas","Kimberly A. Martinez","Robert J. Lucero","Tanja Magoc","Laurence M. Solberg","Urszula A. Snigurska","Sarah E. Ser","Mattia Prosperi","Jiang Bian","Ragnhildur I. Bjarnadottir","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2304.00111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00025v1","updated":"2023-03-31T16:41:15Z","published":"2023-03-31T16:41:15Z","title":"Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual\n  Assistance for Telehealth: The Mental Health Case","summary":"  After the pandemic, artificial intelligence (AI) powered support for mental\nhealth care has become increasingly important. The breadth and complexity of\nsignificant challenges required to provide adequate care involve: (a)\nPersonalized patient understanding, (b) Safety-constrained and medically\nvalidated chatbot patient interactions, and (c) Support for continued\nfeedback-based refinements in design using chatbot-patient interactions. We\npropose Alleviate, a chatbot designed to assist patients suffering from mental\nhealth challenges with personalized care and assist clinicians with\nunderstanding their patients better. Alleviate draws from an array of publicly\navailable clinically valid mental-health texts and databases, allowing\nAlleviate to make medically sound and informed decisions. In addition,\nAlleviate's modular design and explainable decision-making lends itself to\nrobust and continued feedback-based refinements to its design. In this paper,\nwe explain the different modules of Alleviate and submit a short video\ndemonstrating Alleviate's capabilities to help patients and clinicians\nunderstand each other better to facilitate optimal care strategies.\n","authors":["Kaushik Roy","Vedant Khandelwal","Raxit Goswami","Nathan Dolbir","Jinendra Malekar","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2304.00025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01008v1","updated":"2023-03-31T16:11:56Z","published":"2023-03-31T16:11:56Z","title":"Self-Supervised Multimodal Learning: A Survey","summary":"  Multimodal learning, which aims to understand and analyze information from\nmultiple modalities, has achieved substantial progress in the supervised regime\nin recent years. However, the heavy dependence on data paired with expensive\nhuman annotations impedes scaling up models. Meanwhile, given the availability\nof large-scale unannotated data in the wild, self-supervised learning has\nbecome an attractive strategy to alleviate the annotation bottleneck. Building\non these two directions, self-supervised multimodal learning (SSML) provides\nways to leverage supervision from raw multimodal data. In this survey, we\nprovide a comprehensive review of the state-of-the-art in SSML, which we\ncategorize along three orthogonal axes: objective functions, data alignment,\nand model architectures. These axes correspond to the inherent characteristics\nof self-supervised learning methods and multimodal data. Specifically, we\nclassify training objectives into instance discrimination, clustering, and\nmasked prediction categories. We also discuss multimodal input data pairing and\nalignment strategies during training. Finally, we review model architectures\nincluding the design of encoders, fusion modules, and decoders, which are\nessential components of SSML methods. We review downstream multimodal\napplication tasks, reporting the concrete performance of the state-of-the-art\nimage-text models and multimodal video models, and also review real-world\napplications of SSML algorithms in diverse fields such as healthcare, remote\nsensing, and machine translation. Finally, we discuss challenges and future\ndirections for SSML. A collection of related resources can be found at:\nhttps://github.com/ys-zong/awesome-self-supervised-multimodal-learning.\n","authors":["Yongshuo Zong","Oisin Mac Aodha","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2304.01008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01016v1","updated":"2023-03-31T15:44:13Z","published":"2023-03-31T15:44:13Z","title":"Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler\n  Alignment of Embeddings for Asymmetrical dual encoders","summary":"  In this paper, we consider the problem of improving the inference latency of\nlanguage model-based dense retrieval systems by introducing structural\ncompression and model size asymmetry between the context and query encoders.\nFirst, we investigate the impact of pre and post-training compression on the\nMSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that\nasymmetry in the dual encoders in dense retrieval can lead to improved\ninference efficiency. Knowing this, we introduce Kullback Leibler Alignment of\nEmbeddings (KALE), an efficient and accurate method for increasing the\ninference efficiency of dense retrieval methods by pruning and aligning the\nquery encoder after training. Specifically, KALE extends traditional Knowledge\nDistillation after bi-encoder training, allowing for effective query encoder\ncompression without full retraining or index generation. Using KALE and\nasymmetric training, we can generate models which exceed the performance of\nDistilBERT despite having 3x faster inference.\n","authors":["Daniel Campos","Alessandro Magnani","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2304.01016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00020v1","updated":"2023-03-31T11:22:03Z","published":"2023-03-31T11:22:03Z","title":"SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes\n  Analysis","summary":"  The prevalence of memes on social media has created the need to sentiment\nanalyze their underlying meanings for censoring harmful content. Meme censoring\nsystems by machine learning raise the need for a semi-supervised learning\nsolution to take advantage of the large number of unlabeled memes available on\nthe internet and make the annotation process less challenging. Moreover, the\napproach needs to utilize multimodal data as memes' meanings usually come from\nboth images and texts. This research proposes a multimodal semi-supervised\nlearning approach that outperforms other multimodal semi-supervised learning\nand supervised learning state-of-the-art models on two datasets, the Multimedia\nAutomatic Misogyny Identification and Hateful Memes dataset. Building on the\ninsights gained from Contrastive Language-Image Pre-training, which is an\neffective multimodal learning technique, this research introduces SemiMemes, a\nnovel training method that combines auto-encoder and classification task to\nmake use of the resourceful unlabeled data.\n","authors":["Pham Thai Hoang Tung","Nguyen Tan Viet","Ngo Tien Anh","Phan Duy Hung"],"pdf_url":"https://arxiv.org/pdf/2304.00020v1.pdf","comment":"preprint version"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.18248v1","updated":"2023-03-31T17:59:56Z","published":"2023-03-31T17:59:56Z","title":"Towards Flexible Multi-modal Document Models","summary":"  Creative workflows for generating graphical documents involve complex\ninter-related tasks, such as aligning elements, choosing appropriate fonts, or\nemploying aesthetically harmonious colors. In this work, we attempt at building\na holistic model that can jointly solve many different design tasks. Our model,\nwhich we denote by FlexDM, treats vector graphic documents as a set of\nmulti-modal elements, and learns to predict masked fields such as element type,\nposition, styling attributes, image, or text, using a unified architecture.\nThrough the use of explicit multi-task learning and in-domain pre-training, our\nmodel can better capture the multi-modal relationships among the different\ndocument fields. Experimental results corroborate that our single FlexDM is\nable to successfully solve a multitude of different design tasks, while\nachieving performance that is competitive with task-specific and costly\nbaselines.\n","authors":["Naoto Inoue","Kotaro Kikuchi","Edgar Simo-Serra","Mayu Otani","Kota Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2303.18248v1.pdf","comment":"To be published in CVPR2023 (highlight), project page:\n  https://cyberagentailab.github.io/flex-dm"},{"id":"http://arxiv.org/abs/2303.18247v1","updated":"2023-03-31T17:59:44Z","published":"2023-03-31T17:59:44Z","title":"Adaptive Sparse Pairwise Loss for Object Re-Identification","summary":"  Object re-identification (ReID) aims to find instances with the same identity\nas the given probe from a large gallery. Pairwise losses play an important role\nin training a strong ReID network. Existing pairwise losses densely exploit\neach instance as an anchor and sample its triplets in a mini-batch. This dense\nsampling mechanism inevitably introduces positive pairs that share few visual\nsimilarities, which can be harmful to the training. To address this problem, we\npropose a novel loss paradigm termed Sparse Pairwise (SP) loss that only\nleverages few appropriate pairs for each class in a mini-batch, and empirically\ndemonstrate that it is sufficient for the ReID tasks. Based on the proposed\nloss framework, we propose an adaptive positive mining strategy that can\ndynamically adapt to diverse intra-class variations. Extensive experiments show\nthat SP loss and its adaptive variant AdaSP loss outperform other pairwise\nlosses, and achieve state-of-the-art performance across several ReID\nbenchmarks. Code is available at https://github.com/Astaxanthin/AdaSP.\n","authors":["Xiao Zhou","Yujie Zhong","Zhen Cheng","Fan Liang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2303.18247v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2303.18246v1","updated":"2023-03-31T17:59:09Z","published":"2023-03-31T17:59:09Z","title":"3D Human Pose Estimation via Intuitive Physics","summary":"  Estimating 3D humans from images often produces implausible bodies that lean,\nfloat, or penetrate the floor. Such methods ignore the fact that bodies are\ntypically supported by the scene. A physics engine can be used to enforce\nphysical plausibility, but these are not differentiable, rely on unrealistic\nproxy bodies, and are difficult to integrate into existing optimization and\nlearning frameworks. In contrast, we exploit novel intuitive-physics (IP) terms\nthat can be inferred from a 3D SMPL body interacting with the scene. Inspired\nby biomechanics, we infer the pressure heatmap on the body, the Center of\nPressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). With\nthese, we develop IPMAN, to estimate a 3D body from a color image in a \"stable\"\nconfiguration by encouraging plausible floor contact and overlapping CoP and\nCoM. Our IP terms are intuitive, easy to implement, fast to compute,\ndifferentiable, and can be integrated into existing optimization and regression\nmethods. We evaluate IPMAN on standard datasets and MoYo, a new dataset with\nsynchronized multi-view images, ground-truth 3D bodies with complex poses,\nbody-floor contact, CoM and pressure. IPMAN produces more plausible results\nthan the state of the art, improving accuracy for static poses, while not\nhurting dynamic ones. Code and data are available for research at\nhttps://ipman.is.tue.mpg.de.\n","authors":["Shashank Tripathi","Lea Müller","Chun-Hao P. Huang","Omid Taheri","Michael J. Black","Dimitrios Tzionas"],"pdf_url":"https://arxiv.org/pdf/2303.18246v1.pdf","comment":"Accepted in CVPR'23. Project page: https://ipman.is.tue.mpg.de"},{"id":"http://arxiv.org/abs/2303.09483v3","updated":"2023-03-31T17:58:40Z","published":"2023-03-16T17:00:42Z","title":"Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks\n  in Continual Learning","summary":"  In contrast to the natural capabilities of humans to learn new tasks in a\nsequential fashion, neural networks are known to suffer from catastrophic\nforgetting, where the model's performances on old tasks drop dramatically after\nbeing optimized for a new task. Since then, the continual learning (CL)\ncommunity has proposed several solutions aiming to equip the neural network\nwith the ability to learn the current task (plasticity) while still achieving\nhigh accuracy on the previous tasks (stability). Despite remarkable\nimprovements, the plasticity-stability trade-off is still far from being solved\nand its underlying mechanism is poorly understood. In this work, we propose\nAuxiliary Network Continual Learning (ANCL), a novel method that applies an\nadditional auxiliary network which promotes plasticity to the continually\nlearned model which mainly focuses on stability. More concretely, the proposed\nframework materializes in a regularizer that naturally interpolates between\nplasticity and stability, surpassing strong baselines on task incremental and\nclass incremental scenarios. Through extensive analyses on ANCL solutions, we\nidentify some essential principles beneath the stability-plasticity trade-off.\n","authors":["Sanghwan Kim","Lorenzo Noci","Antonio Orvieto","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2303.09483v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.18242v1","updated":"2023-03-31T17:58:08Z","published":"2023-03-31T17:58:08Z","title":"$\\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified\n  States","summary":"  We introduce $\\infty$-Diff, a generative diffusion model which directly\noperates on infinite resolution data. By randomly sampling subsets of\ncoordinates during training and learning to denoise the content at those\ncoordinates, a continuous function is learned that allows sampling at arbitrary\nresolutions. In contrast to other recent infinite resolution generative models,\nour approach operates directly on the raw data, not requiring latent vector\ncompression for context, using hypernetworks, nor relying on discrete\ncomponents. As such, our approach achieves significantly higher sample quality,\nas evidenced by lower FID scores, as well as being able to effectively scale to\nhigher resolutions than the training data while retaining detail.\n","authors":["Sam Bond-Taylor","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2303.18242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18240v1","updated":"2023-03-31T17:56:33Z","published":"2023-03-31T17:56:33Z","title":"Where are we in the search for an Artificial Visual Cortex for Embodied\n  Intelligence?","summary":"  We present the largest and most comprehensive empirical study of pre-trained\nvisual representations (PVRs) or visual 'foundation models' for Embodied AI.\nFirst, we curate CortexBench, consisting of 17 different tasks spanning\nlocomotion, navigation, dexterous, and mobile manipulation. Next, we\nsystematically evaluate existing PVRs and find that none are universally\ndominant.\n  To study the effect of pre-training data scale and diversity, we combine over\n4,000 hours of egocentric videos from 7 different sources (over 5.6M images)\nand ImageNet to train different-sized vision transformers using Masked\nAuto-Encoding (MAE) on slices of this data. Contrary to inferences from prior\nwork, we find that scaling dataset size and diversity does not improve\nperformance universally (but does so on average).\n  Our largest model, named VC-1, outperforms all prior PVRs on average but does\nnot universally dominate either. Finally, we show that task or domain-specific\nadaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving\ncompetitive or superior performance than the best known results on all of the\nbenchmarks in CortexBench. These models required over 10,000 GPU-hours to train\nand can be found on our website for the benefit of the research community.\n","authors":["Arjun Majumdar","Karmesh Yadav","Sergio Arnaud","Yecheng Jason Ma","Claire Chen","Sneha Silwal","Aryan Jain","Vincent-Pierre Berges","Pieter Abbeel","Jitendra Malik","Dhruv Batra","Yixin Lin","Oleksandr Maksymets","Aravind Rajeswaran","Franziska Meier"],"pdf_url":"https://arxiv.org/pdf/2303.18240v1.pdf","comment":"Project website: https://eai-vc.github.io"},{"id":"http://arxiv.org/abs/2207.10435v2","updated":"2023-03-31T17:51:22Z","published":"2022-07-21T12:11:18Z","title":"Human Trajectory Prediction via Neural Social Physics","summary":"  Trajectory prediction has been widely pursued in many fields, and many\nmodel-based and model-free methods have been explored. The former include\nrule-based, geometric or optimization-based models, and the latter are mainly\ncomprised of deep learning approaches. In this paper, we propose a new method\ncombining both methodologies based on a new Neural Differential Equation model.\nOur new model (Neural Social Physics or NSP) is a deep neural network within\nwhich we use an explicit physics model with learnable parameters. The explicit\nphysics model serves as a strong inductive bias in modeling pedestrian\nbehaviors, while the rest of the network provides a strong data-fitting\ncapability in terms of system parameter estimation and dynamics stochasticity\nmodeling. We compare NSP with 15 recent deep learning methods on 6 datasets and\nimprove the state-of-the-art performance by 5.56%-70%. Besides, we show that\nNSP has better generalizability in predicting plausible trajectories in\ndrastically different scenarios where the density is 2-5 times as high as the\ntesting data. Finally, we show that the physics model in NSP can provide\nplausible explanations for pedestrian behaviors, as opposed to black-box deep\nlearning. Code is available:\nhttps://github.com/realcrane/Human-Trajectory-Prediction-via-Neural-Social-Physics.\n","authors":["Jiangbei Yue","Dinesh Manocha","He Wang"],"pdf_url":"https://arxiv.org/pdf/2207.10435v2.pdf","comment":"ECCV 2022"},{"id":"http://arxiv.org/abs/2303.18232v1","updated":"2023-03-31T17:47:23Z","published":"2023-03-31T17:47:23Z","title":"DIME-FM: DIstilling Multimodal and Efficient Foundation Models","summary":"  Large Vision-Language Foundation Models (VLFM), such as CLIP, ALIGN and\nFlorence, are trained on large-scale datasets of image-caption pairs and\nachieve superior transferability and robustness on downstream tasks, but they\nare difficult to use in many practical applications due to their large size,\nhigh latency and fixed architectures. Unfortunately, recent work shows training\na small custom VLFM for resource-limited applications is currently very\ndifficult using public and smaller-scale data. In this paper, we introduce a\nnew distillation mechanism (DIME-FM) that allows us to transfer the knowledge\ncontained in large VLFMs to smaller, customized foundation models using a\nrelatively small amount of inexpensive, unpaired images and sentences. We\ntransfer the knowledge from the pre-trained CLIP-ViTL/14 model to a ViT-B/32\nmodel, with only 40M public images and 28.4M unpaired public sentences. The\nresulting model \"Distill-ViT-B/32\" rivals the CLIP-ViT-B/32 model pre-trained\non its private WiT dataset (400M image-text pairs): Distill-ViT-B/32 achieves\nsimilar results in terms of zero-shot and linear-probing performance on both\nImageNet and the ELEVATER (20 image classification tasks) benchmarks. It also\ndisplays comparable robustness when evaluated on five datasets with natural\ndistribution shifts from ImageNet.\n","authors":["Ximeng Sun","Pengchuan Zhang","Peizhao Zhang","Hardik Shah","Kate Saenko","Xide Xia"],"pdf_url":"https://arxiv.org/pdf/2303.18232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18230v1","updated":"2023-03-31T17:41:31Z","published":"2023-03-31T17:41:31Z","title":"Procedure-Aware Pretraining for Instructional Video Understanding","summary":"  Our goal is to learn a video representation that is useful for downstream\nprocedure understanding tasks in instructional videos. Due to the small amount\nof available annotations, a key challenge in procedure understanding is to be\nable to extract from unlabeled videos the procedural knowledge such as the\nidentity of the task (e.g., 'make latte'), its steps (e.g., 'pour milk'), or\nthe potential next steps given partial progress in its execution. Our main\ninsight is that instructional videos depict sequences of steps that repeat\nbetween instances of the same or different tasks, and that this structure can\nbe well represented by a Procedural Knowledge Graph (PKG), where nodes are\ndiscrete steps and edges connect steps that occur sequentially in the\ninstructional activities. This graph can then be used to generate pseudo labels\nto train a video representation that encodes the procedural knowledge in a more\naccessible form to generalize to multiple procedure understanding tasks. We\nbuild a PKG by combining information from a text-based procedural knowledge\ndatabase and an unlabeled instructional video corpus and then use it to\ngenerate training pseudo labels with four novel pre-training objectives. We\ncall this PKG-based pre-training procedure and the resulting model Paprika,\nProcedure-Aware PRe-training for Instructional Knowledge Acquisition. We\nevaluate Paprika on COIN and CrossTask for procedure understanding tasks such\nas task recognition, step recognition, and step forecasting. Paprika yields a\nvideo representation that improves over the state of the art: up to 11.23%\ngains in accuracy in 12 evaluation settings. Implementation is available at\nhttps://github.com/salesforce/paprika.\n","authors":["Honglu Zhou","Roberto Martín-Martín","Mubbasir Kapadia","Silvio Savarese","Juan Carlos Niebles"],"pdf_url":"https://arxiv.org/pdf/2303.18230v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2111.12577v2","updated":"2023-03-31T17:33:24Z","published":"2021-11-24T15:58:10Z","title":"A Method for Evaluating Deep Generative Models of Images via Assessing\n  the Reproduction of High-order Spatial Context","summary":"  Deep generative models (DGMs) have the potential to revolutionize diagnostic\nimaging. Generative adversarial networks (GANs) are one kind of DGM which are\nwidely employed. The overarching problem with deploying GANs, and other DGMs,\nin any application that requires domain expertise in order to actually use the\ngenerated images is that there generally is not adequate or automatic means of\nassessing the domain-relevant quality of generated images. In this work, we\ndemonstrate several objective tests of images output by two popular GAN\narchitectures. We designed several stochastic context models (SCMs) of distinct\nimage features that can be recovered after generation by a trained GAN. Several\nof these features are high-order, algorithmic pixel-arrangement rules which are\nnot readily expressed in covariance matrices. We designed and validated\nstatistical classifiers to detect specific effects of the known arrangement\nrules. We then tested the rates at which two different GANs correctly\nreproduced the feature context under a variety of training scenarios, and\ndegrees of feature-class similarity. We found that ensembles of generated\nimages can appear largely accurate visually, and show high accuracy in ensemble\nmeasures, while not exhibiting the known spatial arrangements. Furthermore,\nGANs trained on a spectrum of distinct spatial orders did not respect the given\nprevalence of those orders in the training data. The main conclusion is that\nSCMs can be engineered to quantify numerous errors, per image, that may not be\ncaptured in ensemble statistics but plausibly can affect subsequent use of the\nGAN-generated images.\n","authors":["Rucha Deshpande","Mark A. Anastasio","Frank J. Brooks"],"pdf_url":"https://arxiv.org/pdf/2111.12577v2.pdf","comment":"The paper is under consideration at Pattern Recognition Letters.\n  Early version with preliminary results was accepted for poster presentation\n  at SPIE-MI 2022. This version on arXiv contains new and updated designs of\n  stochastic models, their mathematical representations and the corresponding\n  results. Data from the designed ensembles available at\n  https://doi.org/10.7910/DVN/HHF4AF"},{"id":"http://arxiv.org/abs/2303.18219v1","updated":"2023-03-31T17:20:27Z","published":"2023-03-31T17:20:27Z","title":"SemHint-MD: Learning from Noisy Semantic Labels for Self-Supervised\n  Monocular Depth Estimation","summary":"  Without ground truth supervision, self-supervised depth estimation can be\ntrapped in a local minimum due to the gradient-locality issue of the\nphotometric loss. In this paper, we present a framework to enhance depth by\nleveraging semantic segmentation to guide the network to jump out of the local\nminimum. Prior works have proposed to share encoders between these two tasks or\nexplicitly align them based on priors like the consistency between edges in the\ndepth and segmentation maps. Yet, these methods usually require ground truth or\nhigh-quality pseudo labels, which may not be easily accessible in real-world\napplications. In contrast, we investigate self-supervised depth estimation\nalong with a segmentation branch that is supervised with noisy labels provided\nby models pre-trained with limited data. We extend parameter sharing from the\nencoder to the decoder and study the influence of different numbers of shared\ndecoder parameters on model performance. Also, we propose to use cross-task\ninformation to refine current depth and segmentation predictions to generate\npseudo-depth and semantic labels for training. The advantages of the proposed\nmethod are demonstrated through extensive experiments on the KITTI benchmark\nand a downstream task for endoscopic tissue deformation tracking.\n","authors":["Shan Lin","Yuheng Zhi","Michael C. Yip"],"pdf_url":"https://arxiv.org/pdf/2303.18219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18210v1","updated":"2023-03-31T17:01:13Z","published":"2023-03-31T17:01:13Z","title":"A Closer Look at Few-Shot 3D Point Cloud Classification","summary":"  In recent years, research on few-shot learning (FSL) has been fast-growing in\nthe 2D image domain due to the less requirement for labeled training data and\ngreater generalization for novel classes. However, its application in 3D point\ncloud data is relatively under-explored. Not only need to distinguish unseen\nclasses as in the 2D domain, 3D FSL is more challenging in terms of irregular\nstructures, subtle inter-class differences, and high intra-class variances\n{when trained on a low number of data.} Moreover, different architectures and\nlearning algorithms make it difficult to study the effectiveness of existing 2D\nFSL algorithms when migrating to the 3D domain. In this work, for the first\ntime, we perform systematic and extensive investigations of directly applying\nrecent 2D FSL works to 3D point cloud related backbone networks and thus\nsuggest a strong learning baseline for few-shot 3D point cloud classification.\nFurthermore, we propose a new network, Point-cloud Correlation Interaction\n(PCIA), with three novel plug-and-play components called Salient-Part Fusion\n(SPF) module, Self-Channel Interaction Plus (SCI+) module, and Cross-Instance\nFusion Plus (CIF+) module to obtain more representative embeddings and improve\nthe feature distinction. These modules can be inserted into most FSL algorithms\nwith minor changes and significantly improve the performance. Experimental\nresults on three benchmark datasets, ModelNet40-FS, ShapeNet70-FS, and\nScanObjectNN-FS, demonstrate that our method achieves state-of-the-art\nperformance for the 3D FSL task. Code and datasets are available at\nhttps://github.com/cgye96/A_Closer_Look_At_3DFSL.\n","authors":["Chuangguan Ye","Hongyuan Zhu","Bo Zhang","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.18210v1.pdf","comment":"Accepted by IJCV 2023"},{"id":"http://arxiv.org/abs/2303.18193v1","updated":"2023-03-31T16:50:23Z","published":"2023-03-31T16:50:23Z","title":"GVP: Generative Volumetric Primitives","summary":"  Advances in 3D-aware generative models have pushed the boundary of image\nsynthesis with explicit camera control. To achieve high-resolution image\nsynthesis, several attempts have been made to design efficient generators, such\nas hybrid architectures with both 3D and 2D components. However, such a design\ncompromises multiview consistency, and the design of a pure 3D generator with\nhigh resolution is still an open problem. In this work, we present Generative\nVolumetric Primitives (GVP), the first pure 3D generative model that can sample\nand render 512-resolution images in real-time. GVP jointly models a number of\nvolumetric primitives and their spatial information, both of which can be\nefficiently generated via a 2D convolutional network. The mixture of these\nprimitives naturally captures the sparsity and correspondence in the 3D volume.\nThe training of such a generator with a high degree of freedom is made possible\nthrough a knowledge distillation technique. Experiments on several datasets\ndemonstrate superior efficiency and 3D consistency of GVP over the\nstate-of-the-art.\n","authors":["Mallikarjun B R","Xingang Pan","Mohamed Elgharib","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2303.18193v1.pdf","comment":"https://vcai.mpi-inf.mpg.de/projects/GVP/index.html"},{"id":"http://arxiv.org/abs/2303.16904v2","updated":"2023-03-31T16:48:02Z","published":"2023-03-23T22:35:37Z","title":"Severity classification of ground-glass opacity via 2-D convolutional\n  neural network and lung CT scans: a 3-day exploration","summary":"  Ground-glass opacity is a hallmark of numerous lung diseases, including\npatients with COVID19 and pneumonia, pulmonary fibrosis, and tuberculosis. This\nbrief note presents experimental results of a proof-of-concept framework that\ngot implemented and tested over three days as driven by the third challenge\nentitled \"COVID-19 Competition\", hosted at the AI-Enabled Medical Image\nAnalysis Workshop of the 2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP 2023). Using a newly built virtual\nenvironment (created on March 17, 2023), we investigated various pre-trained\ntwo-dimensional convolutional neural networks (CNN) such as Dense Neural\nNetwork, Residual Neural Networks (ResNet), and Vision Transformers, as well as\nthe extent of fine-tuning. Based on empirical experiments, we opted to\nfine-tune them using ADAM's optimization algorithm with a standard learning\nrate of 0.001 for all CNN architectures and apply early-stopping whenever the\nvalidation loss reached a plateau. For each trained CNN, the model state with\nthe best validation accuracy achieved during training was stored and later\nreloaded for new classifications of unseen samples drawn from the validation\nset provided by the challenge organizers. According to the organizers, few of\nthese 2D CNNs yielded performance comparable to an architecture that combined\nResNet and Recurrent Neural Network (Gated Recurrent Units). As part of the\nchallenge requirement, the source code produced during the course of this\nexercise is posted at https://github.com/lisatwyw/cov19. We also hope that\nother researchers may find this light prototype consisting of few Python files\nbased on PyTorch 1.13.1 and TorchVision 0.14.1 approachable.\n","authors":["Lisa Y. W. Tang"],"pdf_url":"https://arxiv.org/pdf/2303.16904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00939v5","updated":"2023-03-31T16:37:12Z","published":"2022-10-03T13:50:58Z","title":"Improving Sample Quality of Diffusion Models Using Self-Attention\n  Guidance","summary":"  Denoising diffusion models (DDMs) have attracted attention for their\nexceptional generation quality and diversity. This success is largely\nattributed to the use of class- or text-conditional diffusion guidance methods,\nsuch as classifier and classifier-free guidance. In this paper, we present a\nmore comprehensive perspective that goes beyond the traditional guidance\nmethods. From this generalized perspective, we introduce novel condition- and\ntraining-free strategies to enhance the quality of generated images. As a\nsimple solution, blur guidance improves the suitability of intermediate samples\nfor their fine-scale information and structures, enabling diffusion models to\ngenerate higher quality samples with a moderate guidance scale. Improving upon\nthis, Self-Attention Guidance (SAG) uses the intermediate self-attention maps\nof diffusion models to enhance their stability and efficacy. Specifically, SAG\nadversarially blurs only the regions that diffusion models attend to at each\niteration and guides them accordingly. Our experimental results show that our\nSAG improves the performance of various diffusion models, including ADM, IDDPM,\nStable Diffusion, and DiT. Moreover, combining SAG with conventional guidance\nmethods leads to further improvement.\n","authors":["Susung Hong","Gyuseong Lee","Wooseok Jang","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2210.00939v5.pdf","comment":"Project page: https://ku-cvlab.github.io/Self-Attention-Guidance"},{"id":"http://arxiv.org/abs/2302.10272v2","updated":"2023-03-31T16:33:22Z","published":"2023-01-23T12:48:08Z","title":"Is Autoencoder Truly Applicable for 3D CT Super-Resolution?","summary":"  Featured by a bottleneck structure, autoencoder (AE) and its variants have\nbeen largely applied in various medical image analysis tasks, such as\nsegmentation, reconstruction and de-noising. Despite of their promising\nperformances in aforementioned tasks, in this paper, we claim that AE models\nare not applicable to single image super-resolution (SISR) for 3D CT data. Our\nhypothesis is that the bottleneck architecture that resizes feature maps in AE\nmodels degrades the details of input images, thus can sabotage the performance\nof super-resolution. Although U-Net proposed skip connections that merge\ninformation from different levels, we claim that the degrading impact of\nfeature resizing operations could hardly be removed by skip connections. By\nconducting large-scale ablation experiments and comparing the performance\nbetween models with and without the bottleneck design on a public CT lung\ndataset , we have discovered that AE models, including U-Net, have failed to\nachieve a compatible SISR result ($p<0.05$ by Student's t-test) compared to the\nbaseline model. Our work is the first comparative study investigating the\nsuitability of AE architecture for 3D CT SISR tasks and brings a rationale for\nresearchers to re-think the choice of model architectures especially for 3D CT\nSISR tasks. The full implementation and trained models can be found at:\nhttps://github.com/Roldbach/Autoencoder-3D-CT-SISR\n","authors":["Weixun Luo","Xiaodan Xing","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2302.10272v2.pdf","comment":"ISBI 2023"},{"id":"http://arxiv.org/abs/2303.18181v1","updated":"2023-03-31T16:23:29Z","published":"2023-03-31T16:23:29Z","title":"A Closer Look at Parameter-Efficient Tuning in Diffusion Models","summary":"  Large-scale diffusion models like Stable Diffusion are powerful and find\nvarious real-world applications while customizing such models by fine-tuning is\nboth memory and time inefficient. Motivated by the recent progress in natural\nlanguage processing, we investigate parameter-efficient tuning in large\ndiffusion models by inserting small learnable modules (termed adapters). In\nparticular, we decompose the design space of adapters into orthogonal factors\n-- the input position, the output position as well as the function form, and\nperform Analysis of Variance (ANOVA), a classical statistical approach for\nanalyzing the correlation between discrete (design options) and continuous\nvariables (evaluation metrics). Our analysis suggests that the input position\nof adapters is the critical factor influencing the performance of downstream\ntasks. Then, we carefully study the choice of the input position, and we find\nthat putting the input position after the cross-attention block can lead to the\nbest performance, validated by additional visualization analyses. Finally, we\nprovide a recipe for parameter-efficient tuning in diffusion models, which is\ncomparable if not superior to the fully fine-tuned baseline (e.g., DreamBooth)\nwith only 0.75 \\% extra parameters, across various customized tasks.\n","authors":["Chendong Xiang","Fan Bao","Chongxuan Li","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.18181v1.pdf","comment":"8pages"},{"id":"http://arxiv.org/abs/2303.18177v1","updated":"2023-03-31T16:19:27Z","published":"2023-03-31T16:19:27Z","title":"STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action\n  Recognition","summary":"  We study the problem of human action recognition using motion capture (MoCap)\nsequences. Unlike existing techniques that take multiple manual steps to derive\nstandardized skeleton representations as model input, we propose a novel\nSpatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences.\nThe model uses a hierarchical transformer with intra-frame off-set attention\nand inter-frame self-attention. The attention mechanism allows the model to\nfreely attend between any two vertex patches to learn non-local relationships\nin the spatial-temporal domain. Masked vertex modeling and future frame\nprediction are used as two self-supervised tasks to fully activate the\nbi-directional and auto-regressive attention in our hierarchical transformer.\nThe proposed method achieves state-of-the-art performance compared to\nskeleton-based and point-cloud-based models on common MoCap benchmarks. Code is\navailable at https://github.com/zgzxy001/STMT.\n","authors":["Xiaoyu Zhu","Po-Yao Huang","Junwei Liang","Celso M. de Melo","Alexander Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2303.18177v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.18164v1","updated":"2023-03-31T16:01:03Z","published":"2023-03-31T16:01:03Z","title":"Single Image Depth Prediction Made Better: A Multivariate Gaussian Take","summary":"  Neural-network-based single image depth prediction (SIDP) is a challenging\ntask where the goal is to predict the scene's per-pixel depth at test time.\nSince the problem, by definition, is ill-posed, the fundamental goal is to come\nup with an approach that can reliably model the scene depth from a set of\ntraining examples. In the pursuit of perfect depth estimation, most existing\nstate-of-the-art learning techniques predict a single scalar depth value\nper-pixel. Yet, it is well-known that the trained model has accuracy limits and\ncan predict imprecise depth. Therefore, an SIDP approach must be mindful of the\nexpected depth variations in the model's prediction at test time. Accordingly,\nwe introduce an approach that performs continuous modeling of per-pixel depth,\nwhere we can predict and reason about the per-pixel depth and its distribution.\nTo this end, we model per-pixel scene depth using a multivariate Gaussian\ndistribution. Moreover, contrary to the existing uncertainty modeling methods\n-- in the same spirit, where per-pixel depth is assumed to be independent, we\nintroduce per-pixel covariance modeling that encodes its depth dependency w.r.t\nall the scene points. Unfortunately, per-pixel depth covariance modeling leads\nto a computationally expensive continuous loss function, which we solve\nefficiently using the learned low-rank approximation of the overall covariance\nmatrix. Notably, when tested on benchmark datasets such as KITTI, NYU, and\nSUN-RGB-D, the SIDP model obtained by optimizing our loss function shows\nstate-of-the-art results. Our method's accuracy (named MG) is among the top on\nthe KITTI depth-prediction benchmark leaderboard.\n","authors":["Ce Liu","Suryansh Kumar","Shuhang Gu","Radu Timofte","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2303.18164v1.pdf","comment":"Accepted to IEEE/CVF CVPR 2023. Draft info: 17 pages, 13 Figures, 9\n  Tables"},{"id":"http://arxiv.org/abs/2301.00794v2","updated":"2023-03-31T15:57:22Z","published":"2023-01-02T18:32:45Z","title":"STEPs: Self-Supervised Key Step Extraction from Unlabeled Procedural\n  Videos","summary":"  We address the problem of extracting key steps from unlabeled procedural\nvideos, motivated by the potential of Augmented Reality (AR) headsets to\nrevolutionize job training and performance. We decompose the problem into two\nsteps: representation learning and key steps extraction. We propose a training\nobjective, Bootstrapped Multi-Cue Contrastive (BMC2) loss to learn\ndisciriminative representations for various steps without any labels. Different\nfrom prior works, we develop techniques to train a light-weight temporal module\nwhich uses off-the-shelf features for self supervision. Our approach can\nseamlessly leverage information from multiple cues like optical flow, depth or\ngaze to learn discriminative features for key-steps making it amenable for AR\napplications. We finally extract key steps via a tunable algorithm that\nclusters the representations and samples. We show significant improvements over\nprior works for the task of key step localization and phase classification.\nQualitative results demonstrate that the extracted key steps are meaningful to\nsuccinctly represent various steps of the procedural tasks.\n","authors":["Anshul Shah","Benjamin Lundell","Harpreet Sawhney","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2301.00794v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18144v1","updated":"2023-03-31T15:29:25Z","published":"2023-03-31T15:29:25Z","title":"Siamese DETR","summary":"  Recent self-supervised methods are mainly designed for representation\nlearning with the base model, e.g., ResNets or ViTs. They cannot be easily\ntransferred to DETR, with task-specific Transformer modules. In this work, we\npresent Siamese DETR, a Siamese self-supervised pretraining approach for the\nTransformer architecture in DETR. We consider learning view-invariant and\ndetection-oriented representations simultaneously through two complementary\ntasks, i.e., localization and discrimination, in a novel multi-view learning\nframework. Two self-supervised pretext tasks are designed: (i) Multi-View\nRegion Detection aims at learning to localize regions-of-interest between\naugmented views of the input, and (ii) Multi-View Semantic Discrimination\nattempts to improve object-level discrimination for each region. The proposed\nSiamese DETR achieves state-of-the-art transfer performance on COCO and PASCAL\nVOC detection using different DETR variants in all setups. Code is available at\nhttps://github.com/Zx55/SiameseDETR.\n","authors":["Zeren Chen","Gengshi Huang","Wei Li","Jianing Teng","Kun Wang","Jing Shao","Chen Change Loy","Lu Sheng"],"pdf_url":"https://arxiv.org/pdf/2303.18144v1.pdf","comment":"10 pages, 11 figures. Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2212.04089v3","updated":"2023-03-31T15:27:01Z","published":"2022-12-08T05:50:53Z","title":"Editing Models with Task Arithmetic","summary":"  Changing how pre-trained models behave -- e.g., improving their performance\non a downstream task or mitigating biases learned during pre-training -- is a\ncommon practice when developing machine learning systems. In this work, we\npropose a new paradigm for steering the behavior of neural networks, centered\naround \\textit{task vectors}. A task vector specifies a direction in the weight\nspace of a pre-trained model, such that movement in that direction improves\nperformance on the task. We build task vectors by subtracting the weights of a\npre-trained model from the weights of the same model after fine-tuning on a\ntask. We show that these task vectors can be modified and combined together\nthrough arithmetic operations such as negation and addition, and the behavior\nof the resulting model is steered accordingly. Negating a task vector decreases\nperformance on the target task, with little change in model behavior on control\ntasks. Moreover, adding task vectors together can improve performance on\nmultiple tasks at once. Finally, when tasks are linked by an analogy\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\nthree of the tasks can improve performance on the fourth, even when no data\nfrom the fourth task is used for training. Overall, our experiments with\nseveral models, modalities and tasks show that task arithmetic is a simple,\nefficient and effective way of editing models.\n","authors":["Gabriel Ilharco","Marco Tulio Ribeiro","Mitchell Wortsman","Suchin Gururangan","Ludwig Schmidt","Hannaneh Hajishirzi","Ali Farhadi"],"pdf_url":"https://arxiv.org/pdf/2212.04089v3.pdf","comment":"In Proceedings of the 11th International Conference on Learning\n  Representations (ICLR 2023)"},{"id":"http://arxiv.org/abs/2303.18139v1","updated":"2023-03-31T15:23:35Z","published":"2023-03-31T15:23:35Z","title":"Efficient View Synthesis and 3D-based Multi-Frame Denoising with\n  Multiplane Feature Representations","summary":"  While current multi-frame restoration methods combine information from\nmultiple input images using 2D alignment techniques, recent advances in novel\nview synthesis are paving the way for a new paradigm relying on volumetric\nscene representations. In this work, we introduce the first 3D-based\nmulti-frame denoising method that significantly outperforms its 2D-based\ncounterparts with lower computational requirements. Our method extends the\nmultiplane image (MPI) framework for novel view synthesis by introducing a\nlearnable encoder-renderer pair manipulating multiplane representations in\nfeature space. The encoder fuses information across views and operates in a\ndepth-wise manner while the renderer fuses information across depths and\noperates in a view-wise manner. The two modules are trained end-to-end and\nlearn to separate depths in an unsupervised way, giving rise to Multiplane\nFeature (MPF) representations. Experiments on the Spaces and Real\nForward-Facing datasets as well as on raw burst data validate our approach for\nview synthesis, multi-frame denoising, and view synthesis under noisy\nconditions.\n","authors":["Thomas Tanay","Aleš Leonardis","Matteo Maggioni"],"pdf_url":"https://arxiv.org/pdf/2303.18139v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2209.01194v3","updated":"2023-03-31T15:21:48Z","published":"2022-09-02T17:44:50Z","title":"CLONeR: Camera-Lidar Fusion for Occupancy Grid-aided Neural\n  Representations","summary":"  Recent advances in neural radiance fields (NeRFs) achieve state-of-the-art\nnovel view synthesis and facilitate dense estimation of scene properties.\nHowever, NeRFs often fail for large, unbounded scenes that are captured under\nvery sparse views with the scene content concentrated far away from the camera,\nas is typical for field robotics applications. In particular, NeRF-style\nalgorithms perform poorly: (1) when there are insufficient views with little\npose diversity, (2) when scenes contain saturation and shadows, and (3) when\nfinely sampling large unbounded scenes with fine structures becomes\ncomputationally intensive.\n  This paper proposes CLONeR, which significantly improves upon NeRF by\nallowing it to model large outdoor driving scenes that are observed from sparse\ninput sensor views. This is achieved by decoupling occupancy and color learning\nwithin the NeRF framework into separate Multi-Layer Perceptrons (MLPs) trained\nusing LiDAR and camera data, respectively. In addition, this paper proposes a\nnovel method to build differentiable 3D Occupancy Grid Maps (OGM) alongside the\nNeRF model, and leverage this occupancy grid for improved sampling of points\nalong a ray for volumetric rendering in metric space.\n  Through extensive quantitative and qualitative experiments on scenes from the\nKITTI dataset, this paper demonstrates that the proposed method outperforms\nstate-of-the-art NeRF models on both novel view synthesis and dense depth\nprediction tasks when trained on sparse input data.\n","authors":["Alexandra Carlson","Manikandasriram Srinivasan Ramanagopal","Nathan Tseng","Matthew Johnson-Roberson","Ram Vasudevan","Katherine A. Skinner"],"pdf_url":"https://arxiv.org/pdf/2209.01194v3.pdf","comment":"first two authors equally contributed"},{"id":"http://arxiv.org/abs/2303.18125v1","updated":"2023-03-31T15:09:18Z","published":"2023-03-31T15:09:18Z","title":"Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter\n  Correction","summary":"  This paper addresses the problem of rolling shutter correction in complex\nnonlinear and dynamic scenes with extreme occlusion. Existing methods suffer\nfrom two main drawbacks. Firstly, they face challenges in estimating the\naccurate correction field due to the uniform velocity assumption, leading to\nsignificant image correction errors under complex motion. Secondly, the drastic\nocclusion in dynamic scenes prevents current solutions from achieving better\nimage quality because of the inherent difficulties in aligning and aggregating\nmultiple frames. To tackle these challenges, we model the curvilinear\ntrajectory of pixels analytically and propose a geometry-based Quadratic\nRolling Shutter (QRS) motion solver, which precisely estimates the high-order\ncorrection field of individual pixel. Besides, to reconstruct high-quality\nocclusion frames in dynamic scenes, we present a 3D video architecture that\neffectively Aligns and Aggregates multi-frame context, namely, RSA^2-Net. We\nevaluate our method across a broad range of cameras and video sequences,\ndemonstrating its significant superiority. Specifically, our method surpasses\nthe state-of-the-arts by +4.98, +0.77, and +4.33 of PSNR on Carla-RS,\nFastec-RS, and BS-RSC datasets, respectively.\n","authors":["Delin Qu","Yizhen Lao","Zhigang Wang","Dong Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2303.18125v1.pdf","comment":"8 pages, 11 figures, ICCV2023 under review"},{"id":"http://arxiv.org/abs/2205.13489v2","updated":"2023-03-31T15:07:28Z","published":"2022-05-26T16:57:04Z","title":"Measuring Perceptual Color Differences of Smartphone Photographs","summary":"  Measuring perceptual color differences (CDs) is of great importance in modern\nsmartphone photography. Despite the long history, most CD measures have been\nconstrained by psychophysical data of homogeneous color patches or a limited\nnumber of simplistic natural photographic images. It is thus questionable\nwhether existing CD measures generalize in the age of smartphone photography\ncharacterized by greater content complexities and learning-based image signal\nprocessors. In this paper, we put together so far the largest image dataset for\nperceptual CD assessment, in which the photographic images are 1) captured by\nsix flagship smartphones, 2) altered by Photoshop, 3) post-processed by\nbuilt-in filters of the smartphones, and 4) reproduced with incorrect color\nprofiles. We then conduct a large-scale psychophysical experiment to gather\nperceptual CDs of 30,000 image pairs in a carefully controlled laboratory\nenvironment. Based on the newly established dataset, we make one of the first\nattempts to construct an end-to-end learnable CD formula based on a lightweight\nneural network, as a generalization of several previous metrics. Extensive\nexperiments demonstrate that the optimized formula outperforms 33 existing CD\nmeasures by a large margin, offers reasonable local CD maps without the use of\ndense supervision, generalizes well to homogeneous color patch data, and\nempirically behaves as a proper metric in the mathematical sense. Our dataset\nand code are publicly available at https://github.com/hellooks/CDNet.\n","authors":["Zhihua Wang","Keshuo Xu","Yang Yang","Jianlei Dong","Shuhang Gu","Lihao Xu","Yuming Fang","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2205.13489v2.pdf","comment":"10 figures, 8 tables, 14 pages"},{"id":"http://arxiv.org/abs/2303.18119v1","updated":"2023-03-31T15:06:50Z","published":"2023-03-31T15:06:50Z","title":"Markerless 3D human pose tracking through multiple cameras and AI:\n  Enabling high accuracy, robustness, and real-time performance","summary":"  Tracking 3D human motion in real-time is crucial for numerous applications\nacross many fields. Traditional approaches involve attaching artificial\nfiducial objects or sensors to the body, limiting their usability and\ncomfort-of-use and consequently narrowing their application fields. Recent\nadvances in Artificial Intelligence (AI) have allowed for markerless solutions.\nHowever, most of these methods operate in 2D, while those providing 3D\nsolutions compromise accuracy and real-time performance. To address this\nchallenge and unlock the potential of visual pose estimation methods in\nreal-world scenarios, we propose a markerless framework that combines\nmulti-camera views and 2D AI-based pose estimation methods to track 3D human\nmotion. Our approach integrates a Weighted Least Square (WLS) algorithm that\ncomputes 3D human motion from multiple 2D pose estimations provided by an\nAI-driven method. The method is integrated within the Open-VICO framework\nallowing simulation and real-world execution. Several experiments have been\nconducted, which have shown high accuracy and real-time performance,\ndemonstrating the high level of readiness for real-world applications and the\npotential to revolutionize human motion capture.\n","authors":["Luca Fortini","Mattia Leonori","Juan M. Gandarias","Elena de Momi","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2303.18119v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.18118v1","updated":"2023-03-31T15:04:53Z","published":"2023-03-31T15:04:53Z","title":"A two-head loss function for deep Average-K classification","summary":"  Average-K classification is an alternative to top-K classification in which\nthe number of labels returned varies with the ambiguity of the input image but\nmust average to K over all the samples. A simple method to solve this task is\nto threshold the softmax output of a model trained with the cross-entropy loss.\nThis approach is theoretically proven to be asymptotically consistent, but it\nis not guaranteed to be optimal for a finite set of samples. In this paper, we\npropose a new loss function based on a multi-label classification head in\naddition to the classical softmax. This second head is trained using\npseudo-labels generated by thresholding the softmax head while guaranteeing\nthat K classes are returned on average. We show that this approach allows the\nmodel to better capture ambiguities between classes and, as a result, to return\nmore consistent sets of possible classes. Experiments on two datasets from the\nliterature demonstrate that our approach outperforms the softmax baseline, as\nwell as several other loss functions more generally designed for weakly\nsupervised multi-label classification. The gains are larger the higher the\nuncertainty, especially for classes with few samples.\n","authors":["Camille Garcin","Maximilien Servajean","Alexis Joly","Joseph Salmon"],"pdf_url":"https://arxiv.org/pdf/2303.18118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.02830v3","updated":"2023-03-31T14:59:51Z","published":"2022-05-05T17:58:06Z","title":"Interaction Replica: Tracking human-object interaction and scene changes\n  from human motion","summary":"  Humans naturally change their environment through interactions, e.g., by\nopening doors or moving furniture. To reproduce such interactions in virtual\nspaces (e.g., metaverse), we need to capture and model them, including changes\nin the scene geometry, ideally from egocentric input alone (head camera and\nbody-worn inertial sensors). While the head camera can be used to localize the\nperson in the scene, estimating dynamic object pose is much more challenging.\nAs the object is often not visible from the head camera (e.g., a human not\nlooking at a chair while sitting down), we can not rely on visual object pose\nestimation. Instead, our key observation is that human motion tells us a lot\nabout scene changes. Motivated by this, we present iReplica, the first\nhuman-object interaction reasoning method which can track objects and scene\nchanges based solely on human motion. iReplica is an essential first step\ntowards advanced AR/VR applications in immersive virtual universes and can\nprovide human-centric training data to teach machines to interact with their\nsurroundings. Our code, data and model will be available on our project page at\nhttp://virtualhumans.mpi-inf.mpg.de/ireplica/\n","authors":["Vladimir Guzov","Julian Chibane","Riccardo Marin","Yannan He","Torsten Sattler","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2205.02830v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18106v1","updated":"2023-03-31T14:53:56Z","published":"2023-03-31T14:53:56Z","title":"Automatic Detection of Out-of-body Frames in Surgical Videos for Privacy\n  Protection Using Self-supervised Learning and Minimal Labels","summary":"  Endoscopic video recordings are widely used in minimally invasive\nrobot-assisted surgery, but when the endoscope is outside the patient's body,\nit can capture irrelevant segments that may contain sensitive information. To\naddress this, we propose a framework that accurately detects out-of-body frames\nin surgical videos by leveraging self-supervision with minimal data labels. We\nuse a massive amount of unlabeled endoscopic images to learn meaningful\nrepresentations in a self-supervised manner. Our approach, which involves\npre-training on an auxiliary task and fine-tuning with limited supervision,\noutperforms previous methods for detecting out-of-body frames in surgical\nvideos captured from da Vinci X and Xi surgical systems. The average F1 scores\nrange from 96.00 to 98.02. Remarkably, using only 5% of the training labels,\nour approach still maintains an average F1 score performance above 97,\noutperforming fully-supervised methods with 95% fewer labels. These results\ndemonstrate the potential of our framework to facilitate the safe handling of\nsurgical video recordings and enhance data privacy protection in minimally\ninvasive surgery.\n","authors":["Ziheng Wang","Conor Perreault","Xi Liu","Anthony Jarc"],"pdf_url":"https://arxiv.org/pdf/2303.18106v1.pdf","comment":"A 15-page journal article submitted to Journal of Medical Robotics\n  Research (JMRR)"},{"id":"http://arxiv.org/abs/2302.04694v2","updated":"2023-03-31T14:50:55Z","published":"2023-02-09T15:25:52Z","title":"Partial Optimality in Cubic Correlation Clustering","summary":"  The higher-order correlation clustering problem is an expressive model, and\nrecently, local search heuristics have been proposed for several applications.\nCertifying optimality, however, is NP-hard and practically hampered already by\nthe complexity of the problem statement. Here, we focus on establishing partial\noptimality conditions for the special case of complete graphs and cubic\nobjective functions. In addition, we define and implement algorithms for\ntesting these conditions and examine their effect numerically, on two datasets.\n","authors":["David Stein","Silvia Di Gregorio","Bjoern Andres"],"pdf_url":"https://arxiv.org/pdf/2302.04694v2.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2303.18101v1","updated":"2023-03-31T14:46:31Z","published":"2023-03-31T14:46:31Z","title":"INoD: Injected Noise Discriminator for Self-Supervised Representation\n  Learning in Agricultural Fields","summary":"  Perception datasets for agriculture are limited both in quantity and\ndiversity which hinders effective training of supervised learning approaches.\nSelf-supervised learning techniques alleviate this problem, however, existing\nmethods are not optimized for dense prediction tasks in agriculture domains\nwhich results in degraded performance. In this work, we address this limitation\nwith our proposed Injected Noise Discriminator (INoD) which exploits principles\nof feature replacement and dataset discrimination for self-supervised\nrepresentation learning. INoD interleaves feature maps from two disjoint\ndatasets during their convolutional encoding and predicts the dataset\naffiliation of the resultant feature map as a pretext task. Our approach\nenables the network to learn unequivocal representations of objects seen in one\ndataset while observing them in conjunction with similar features from the\ndisjoint dataset. This allows the network to reason about higher-level\nsemantics of the entailed objects, thus improving its performance on various\ndownstream tasks. Additionally, we introduce the novel Fraunhofer Potato 2022\ndataset consisting of over 16,800 images for object detection in potato fields.\nExtensive evaluations of our proposed INoD pretraining strategy for the tasks\nof object detection, semantic segmentation, and instance segmentation on the\nSugar Beets 2016 and our potato dataset demonstrate that it achieves\nstate-of-the-art performance.\n","authors":["Julia Hindel","Nikhil Gosala","Kevin Bregler","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2303.18101v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2102.03176v2","updated":"2023-03-31T14:27:39Z","published":"2021-02-05T13:53:10Z","title":"Feature Representation in Deep Metric Embeddings","summary":"  In deep metric learning (DML), high-level input data are represented in a\nlower-level representation (embedding) space, such that samples from the same\nclass are mapped close together, while samples from disparate classes are\nmapped further apart. In this lower-level representation, only a single\ninference sample from each known class is required to discriminate between\nclasses accurately. The features a DML model uses to discriminate between\nclasses and the importance of each feature in the training process are unknown.\nTo investigate this, this study takes embeddings trained to discriminate faces\n(identities) and uses unsupervised clustering to identify the features involved\nin facial identity discrimination by examining their representation within the\nembedded space. This study is split into two cases; intra class\nsub-discrimination, where attributes that differ between a single identity are\nconsidered; such as beards and emotions; and extra class sub-discrimination,\nwhere attributes which differ between different identities/people, are\nconsidered; such as gender, skin tone and age. In the intra class scenario, the\ninference process distinguishes common attributes between single identities,\nachieving 90.0\\% and 76.0\\% accuracy for beards and glasses, respectively. The\nsystem can also perform extra class sub-discrimination with a high accuracy\nrate, notably 99.3\\%, 99.3\\% and 94.1\\% for gender, skin tone, and age,\nrespectively.\n","authors":["Ryan Furlong","Vincent O'Brien","James Garland","Daniel Palacios-Alonso","Francisco Dominguez-Mateos"],"pdf_url":"https://arxiv.org/pdf/2102.03176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18084v1","updated":"2023-03-31T14:22:32Z","published":"2023-03-31T14:22:32Z","title":"RDMNet: Reliable Dense Matching Based Point Cloud Registration for\n  Autonomous Driving","summary":"  Point cloud registration is an important task in robotics and autonomous\ndriving to estimate the ego-motion of the vehicle. Recent advances following\nthe coarse-to-fine manner show promising potential in point cloud registration.\nHowever, existing methods rely on good superpoint correspondences, which are\nhard to be obtained reliably and efficiently, thus resulting in less robust and\naccurate point cloud registration. In this paper, we propose a novel network,\nnamed RDMNet, to find dense point correspondences coarse-to-fine and improve\nfinal pose estimation based on such reliable correspondences. Our RDMNet uses a\ndevised 3D-RoFormer mechanism to first extract distinctive superpoints and\ngenerates reliable superpoints matches between two point clouds. The proposed\n3D-RoFormer fuses 3D position information into the transformer network,\nefficiently exploiting point clouds' contextual and geometric information to\ngenerate robust superpoint correspondences. RDMNet then propagates the sparse\nsuperpoints matches to dense point matches using the neighborhood information\nfor accurate point cloud registration. We extensively evaluate our method on\nmultiple datasets from different environments. The experimental results\ndemonstrate that our method outperforms existing state-of-the-art approaches in\nall tested datasets with a strong generalization ability.\n","authors":["Chenghao Shi","Xieyuanli Chen","Huimin Lu","Wenbang Deng","Junhao Xiao","Bin Dai"],"pdf_url":"https://arxiv.org/pdf/2303.18084v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2303.05240v2","updated":"2023-03-31T14:21:04Z","published":"2023-03-09T13:22:50Z","title":"Intriguing Property and Counterfactual Explanation of GAN for Remote\n  Sensing Image Generation","summary":"  Generative adversarial networks (GANs) have achieved remarkable progress in\nthe natural image field. However, when applying GANs in the remote sensing (RS)\nimage generation task, an extraordinary phenomenon is observed: the GAN model\nis more sensitive to the size of training data for RS image generation than for\nnatural image generation. In other words, the generation quality of RS images\nwill change significantly with the number of training categories or samples per\ncategory. In this paper, we first analyze this phenomenon from two kinds of toy\nexperiments and conclude that the amount of feature information contained in\nthe GAN model decreases with reduced training data. Then we establish a\nstructural causal model (SCM) of the data generation process and interpret the\ngenerated data as the counterfactuals. Based on this SCM, we theoretically\nprove that the quality of generated images is positively correlated with the\namount of feature information. This provides insights for enriching the feature\ninformation learned by the GAN model during training. Consequently, we propose\ntwo innovative adjustment schemes, namely Uniformity Regularization (UR) and\nEntropy Regularization (ER), to increase the information learned by the GAN\nmodel at the distributional and sample levels, respectively. We theoretically\nand empirically demonstrate the effectiveness and versatility of our methods.\nExtensive experiments on three RS datasets and two natural datasets show that\nour methods outperform the well-established models on RS image generation\ntasks. The source code is available at https://github.com/rootSue/Causal-RSGAN.\n","authors":["Xingzhe Su","Wenwen Qiang","Jie Hu","Fengge Wu","Changwen Zheng","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2303.05240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18080v1","updated":"2023-03-31T14:16:38Z","published":"2023-03-31T14:16:38Z","title":"One-shot Unsupervised Domain Adaptation with Personalized Diffusion\n  Models","summary":"  Adapting a segmentation model from a labeled source domain to a target\ndomain, where a single unlabeled datum is available, is one the most\nchallenging problems in domain adaptation and is otherwise known as one-shot\nunsupervised domain adaptation (OSUDA). Most of the prior works have addressed\nthe problem by relying on style transfer techniques, where the source images\nare stylized to have the appearance of the target domain. Departing from the\ncommon notion of transferring only the target ``texture'' information, we\nleverage text-to-image diffusion models (e.g., Stable Diffusion) to generate a\nsynthetic target dataset with photo-realistic images that not only faithfully\ndepict the style of the target domain, but are also characterized by novel\nscenes in diverse contexts. The text interface in our method Data AugmenTation\nwith diffUsion Models (DATUM) endows us with the possibility of guiding the\ngeneration of images towards desired semantic concepts while respecting the\noriginal spatial context of a single training image, which is not possible in\nexisting OSUDA methods. Extensive experiments on standard benchmarks show that\nour DATUM surpasses the state-of-the-art OSUDA methods by up to +7.1%. The\nimplementation is available at https://github.com/yasserben/DATUM\n","authors":["Yasser Benigmim","Subhankar Roy","Slim Essid","Vicky Kalogeiton","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2303.18080v1.pdf","comment":"13 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2303.14736v2","updated":"2023-03-31T13:36:44Z","published":"2023-03-26T14:32:02Z","title":"Disentangling Writer and Character Styles for Handwriting Generation","summary":"  Training machines to synthesize diverse handwritings is an intriguing task.\nRecently, RNN-based methods have been proposed to generate stylized online\nChinese characters. However, these methods mainly focus on capturing a person's\noverall writing style, neglecting subtle style inconsistencies between\ncharacters written by the same person. For example, while a person's\nhandwriting typically exhibits general uniformity (e.g., glyph slant and aspect\nratios), there are still small style variations in finer details (e.g., stroke\nlength and curvature) of characters. In light of this, we propose to\ndisentangle the style representations at both writer and character levels from\nindividual handwritings to synthesize realistic stylized online handwritten\ncharacters. Specifically, we present the style-disentangled Transformer (SDT),\nwhich employs two complementary contrastive objectives to extract the style\ncommonalities of reference samples and capture the detailed style patterns of\neach sample, respectively. Extensive experiments on various language scripts\ndemonstrate the effectiveness of SDT. Notably, our empirical findings reveal\nthat the two learned style representations provide information at different\nfrequency magnitudes, underscoring the importance of separate style extraction.\nOur source code is public at: https://github.com/dailenson/SDT.\n","authors":["Gang Dai","Yifan Zhang","Qingfeng Wang","Qing Du","Zhuliang Yu","Zhuoman Liu","Shuangping Huang"],"pdf_url":"https://arxiv.org/pdf/2303.14736v2.pdf","comment":"accepted by CVPR 2023. Source code: https://github.com/dailenson/SDT"},{"id":"http://arxiv.org/abs/2303.18044v1","updated":"2023-03-31T13:28:06Z","published":"2023-03-31T13:28:06Z","title":"Long-Short Temporal Co-Teaching for Weakly Supervised Video Anomaly\n  Detection","summary":"  Weakly supervised video anomaly detection (WS-VAD) is a challenging problem\nthat aims to learn VAD models only with video-level annotations. In this work,\nwe propose a Long-Short Temporal Co-teaching (LSTC) method to address the\nWS-VAD problem. It constructs two tubelet-based spatio-temporal transformer\nnetworks to learn from short- and long-term video clips respectively. Each\nnetwork is trained with respect to a multiple instance learning (MIL)-based\nranking loss, together with a cross-entropy loss when clip-level pseudo labels\nare available. A co-teaching strategy is adopted to train the two networks.\nThat is, clip-level pseudo labels generated from each network are used to\nsupervise the other one at the next training round, and the two networks are\nlearned alternatively and iteratively. Our proposed method is able to better\ndeal with the anomalies with varying durations as well as subtle anomalies.\nExtensive experiments on three public datasets demonstrate that our method\noutperforms state-of-the-art WS-VAD methods.\n","authors":["Shengyang Sun","Xiaojin Gong"],"pdf_url":"https://arxiv.org/pdf/2303.18044v1.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2206.06484v4","updated":"2023-03-31T13:20:56Z","published":"2022-06-13T21:30:29Z","title":"On Image Segmentation With Noisy Labels: Characterization and Volume\n  Properties of the Optimal Solutions to Accuracy and Dice","summary":"  We study two of the most popular performance metrics in medical image\nsegmentation, Accuracy and Dice, when the target labels are noisy. For both\nmetrics, several statements related to characterization and volume properties\nof the set of optimal segmentations are proved, and associated experiments are\nprovided. Our main insights are: (i) the volume of the solutions to both\nmetrics may deviate significantly from the expected volume of the target, (ii)\nthe volume of a solution to Accuracy is always less than or equal to the volume\nof a solution to Dice and (iii) the optimal solutions to both of these metrics\ncoincide when the set of feasible segmentations is constrained to the set of\nsegmentations with the volume equal to the expected volume of the target.\n","authors":["Marcus Nordström","Henrik Hult","Jonas Söderberg","Fredrik Löfman"],"pdf_url":"https://arxiv.org/pdf/2206.06484v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18037v1","updated":"2023-03-31T13:14:36Z","published":"2023-03-31T13:14:36Z","title":"Traffic Sign Recognition Dataset and Data Augmentation","summary":"  Although there are many datasets for traffic sign classification, there are\nfew datasets collected for traffic sign recognition and few of them obtain\nenough instances especially for training a model with the deep learning method.\nThe deep learning method is almost the only way to train a model for real-world\nusage that covers various highly similar classes compared with the traditional\nway such as through color, shape, etc. Also, for some certain sign classes,\ntheir sign meanings were destined to can't get enough instances in the dataset.\nTo solve this problem, we purpose a unique data augmentation method for the\ntraffic sign recognition dataset that takes advantage of the standard of the\ntraffic sign. We called it TSR dataset augmentation. We based on the benchmark\nTsinghua-Tencent 100K (TT100K) dataset to verify the unique data augmentation\nmethod. we performed the method on four main iteration version datasets based\non the TT100K dataset and the experimental results showed our method is\nefficacious. The iteration version datasets based on TT100K, data augmentation\nmethod source code and the training results introduced in this paper are\npublicly available.\n","authors":["Jingzhan Ge"],"pdf_url":"https://arxiv.org/pdf/2303.18037v1.pdf","comment":"14pages, 11 figures"},{"id":"http://arxiv.org/abs/2303.18031v1","updated":"2023-03-31T13:08:31Z","published":"2023-03-31T13:08:31Z","title":"Simple Domain Generalization Methods are Strong Baselines for Open\n  Domain Generalization","summary":"  In real-world applications, a machine learning model is required to handle an\nopen-set recognition (OSR), where unknown classes appear during the inference,\nin addition to a domain shift, where the distribution of data differs between\nthe training and inference phases. Domain generalization (DG) aims to handle\nthe domain shift situation where the target domain of the inference phase is\ninaccessible during model training. Open domain generalization (ODG) takes into\naccount both DG and OSR. Domain-Augmented Meta-Learning (DAML) is a method\ntargeting ODG but has a complicated learning process. On the other hand,\nalthough various DG methods have been proposed, they have not been evaluated in\nODG situations. This work comprehensively evaluates existing DG methods in ODG\nand shows that two simple DG methods, CORrelation ALignment (CORAL) and Maximum\nMean Discrepancy (MMD), are competitive with DAML in several cases. In\naddition, we propose simple extensions of CORAL and MMD by introducing the\ntechniques used in DAML, such as ensemble learning and Dirichlet mixup data\naugmentation. The experimental evaluation demonstrates that the extended CORAL\nand MMD can perform comparably to DAML with lower computational costs. This\nsuggests that the simple DG methods and their simple extensions are strong\nbaselines for ODG. The code used in the experiments is available at\nhttps://github.com/shiralab/OpenDG-Eval.\n","authors":["Masashi Noguchi","Shinichi Shirakawa"],"pdf_url":"https://arxiv.org/pdf/2303.18031v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17597v2","updated":"2023-03-31T13:03:55Z","published":"2023-03-30T17:59:17Z","title":"Robo3D: Towards Robust and Reliable 3D Perception against Corruptions","summary":"  The robustness of 3D perception systems under natural corruptions from\nenvironments and sensors is pivotal for safety-critical applications. Existing\nlarge-scale 3D perception datasets often contain data that are meticulously\ncleaned. Such configurations, however, cannot reflect the reliability of\nperception models during the deployment stage. In this work, we present Robo3D,\nthe first comprehensive benchmark heading toward probing the robustness of 3D\ndetectors and segmentors under out-of-distribution scenarios against natural\ncorruptions that occur in real-world environments. Specifically, we consider\neight corruption types stemming from adversarial weather conditions, external\ndisturbances, and internal sensor failure. We uncover that, although promising\nresults have been progressively achieved on standard benchmarks,\nstate-of-the-art 3D perception models are at risk of being vulnerable to\ncorruptions. We draw key observations on the use of data representations,\naugmentation schemes, and training strategies, that could severely affect the\nmodel's performance. To pursue better robustness, we propose a\ndensity-insensitive training framework along with a simple flexible\nvoxelization strategy to enhance the model resiliency. We hope our benchmark\nand approach could inspire future research in designing more robust and\nreliable 3D perception models. Our robustness benchmark suite is publicly\navailable.\n","authors":["Lingdong Kong","Youquan Liu","Xin Li","Runnan Chen","Wenwei Zhang","Jiawei Ren","Liang Pan","Kai Chen","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.17597v2.pdf","comment":"33 pages, 26 figures, 26 tables; code at\n  https://github.com/ldkong1205/Robo3D project page at\n  https://ldkong.com/Robo3D"},{"id":"http://arxiv.org/abs/2303.18022v1","updated":"2023-03-31T13:01:05Z","published":"2023-03-31T13:01:05Z","title":"The Topology-Overlap Trade-Off in Retinal Arteriole-Venule Segmentation","summary":"  Retinal fundus images can be an invaluable diagnosis tool for screening\nepidemic diseases like hypertension or diabetes. And they become especially\nuseful when the arterioles and venules they depict are clearly identified and\nannotated. However, manual annotation of these vessels is extremely time\ndemanding and taxing, which calls for automatic segmentation. Although\nconvolutional neural networks can achieve high overlap between predictions and\nexpert annotations, they often fail to produce topologically correct\npredictions of tubular structures. This situation is exacerbated by the\nbifurcation versus crossing ambiguity which causes classification mistakes.\nThis paper shows that including a topology preserving term in the loss function\nimproves the continuity of the segmented vessels, although at the expense of\nartery-vein misclassification and overall lower overlap metrics. However, we\nshow that by including an orientation score guided convolutional module, based\non the anisotropic single sided cake wavelet, we reduce such misclassification\nand further increase the topology correctness of the results. We evaluate our\nmodel on public datasets with conveniently chosen metrics to assess both\noverlap and topology correctness, showing that our model is able to produce\nresults on par with state-of-the-art from the point of view of overlap, while\nincreasing topological accuracy.\n","authors":["Angel Victor Juanco Muller","Joao F. C. Mota","Keith A. Goatman","Corne Hoogendoorn"],"pdf_url":"https://arxiv.org/pdf/2303.18022v1.pdf","comment":"To be published in proceedings of SPIE Medical Imaging 2023 Image\n  Processing"},{"id":"http://arxiv.org/abs/2303.18019v1","updated":"2023-03-31T12:52:24Z","published":"2023-03-31T12:52:24Z","title":"Live image-based neurosurgical guidance and roadmap generation using\n  unsupervised embedding","summary":"  Advanced minimally invasive neurosurgery navigation relies mainly on Magnetic\nResonance Imaging (MRI) guidance. MRI guidance, however, only provides\npre-operative information in the majority of the cases. Once the surgery\nbegins, the value of this guidance diminishes to some extent because of the\nanatomical changes due to surgery. Guidance with live image feedback coming\ndirectly from the surgical device, e.g., endoscope, can complement MRI-based\nnavigation or be an alternative if MRI guidance is not feasible. With this\nmotivation, we present a method for live image-only guidance leveraging a large\ndata set of annotated neurosurgical videos.First, we report the performance of\na deep learning-based object detection method, YOLO, on detecting anatomical\nstructures in neurosurgical images. Second, we present a method for generating\nneurosurgical roadmaps using unsupervised embedding without assuming exact\nanatomical matches between patients, presence of an extensive anatomical atlas,\nor the need for simultaneous localization and mapping. A generated roadmap\nencodes the common anatomical paths taken in surgeries in the training set. At\ninference, the roadmap can be used to map a surgeon's current location using\nlive image feedback on the path to provide guidance by being able to predict\nwhich structures should appear going forward or backward, much like a mapping\napplication. Even though the embedding is not supervised by position\ninformation, we show that it is correlated to the location inside the brain and\non the surgical path. We trained and evaluated the proposed method with a data\nset of 166 transsphenoidal adenomectomy procedures.\n","authors":["Gary Sarwin","Alessandro Carretta","Victor Staartjes","Matteo Zoli","Diego Mazzatenta","Luca Regli","Carlo Serra","Ender Konukoglu"],"pdf_url":"https://arxiv.org/pdf/2303.18019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18013v1","updated":"2023-03-31T12:38:08Z","published":"2023-03-31T12:38:08Z","title":"LaCViT: A Label-aware Contrastive Training Framework for Vision\n  Transformers","summary":"  Vision Transformers have been incredibly effective when tackling computer\nvision tasks due to their ability to model long feature dependencies. By using\nlarge-scale training data and various self-supervised signals (e.g., masked\nrandom patches), vision transformers provide state-of-the-art performance on\nseveral benchmarking datasets, such as ImageNet-1k and CIFAR-10. However, these\nvision transformers pretrained over general large-scale image corpora could\nonly produce an anisotropic representation space, limiting their\ngeneralizability and transferability to the target downstream tasks. In this\npaper, we propose a simple and effective Label-aware Contrastive Training\nframework LaCViT, which improves the isotropy of the pretrained representation\nspace for vision transformers, thereby enabling more effective transfer\nlearning amongst a wide range of image classification tasks. Through\nexperimentation over five standard image classification datasets, we\ndemonstrate that LaCViT-trained models outperform the original pretrained\nbaselines by around 9% absolute Accuracy@1, and consistent improvements can be\nobserved when applying LaCViT to our three evaluated vision transformers.\n","authors":["Zijun Long","Zaiqiao Meng","Gerardo Aragon Camarasa","Richard McCreadie"],"pdf_url":"https://arxiv.org/pdf/2303.18013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18001v1","updated":"2023-03-31T12:23:56Z","published":"2023-03-31T12:23:56Z","title":"You Only Train Once: Learning a General Anomaly Enhancement Network with\n  Random Masks for Hyperspectral Anomaly Detection","summary":"  In this paper, we introduce a new approach to address the challenge of\ngeneralization in hyperspectral anomaly detection (AD). Our method eliminates\nthe need for adjusting parameters or retraining on new test scenes as required\nby most existing methods. Employing an image-level training paradigm, we\nachieve a general anomaly enhancement network for hyperspectral AD that only\nneeds to be trained once. Trained on a set of anomaly-free hyperspectral images\nwith random masks, our network can learn the spatial context characteristics\nbetween anomalies and background in an unsupervised way. Additionally, a\nplug-and-play model selection module is proposed to search for a\nspatial-spectral transform domain that is more suitable for AD task than the\noriginal data. To establish a unified benchmark to comprehensively evaluate our\nmethod and existing methods, we develop a large-scale hyperspectral AD dataset\n(HAD100) that includes 100 real test scenes with diverse anomaly targets. In\ncomparison experiments, we combine our network with a parameter-free detector\nand achieve the optimal balance between detection accuracy and inference speed\namong state-of-the-art AD methods. Experimental results also show that our\nmethod still achieves competitive performance when the training and test set\nare captured by different sensor devices. Our code is available at\nhttps://github.com/ZhaoxuLi123/AETNet.\n","authors":["Zhaoxu Li","Yingqian Wang","Chao Xiao","Qiang Ling","Zaiping Lin","Wei An"],"pdf_url":"https://arxiv.org/pdf/2303.18001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15839v2","updated":"2023-03-31T12:16:03Z","published":"2023-03-28T09:15:55Z","title":"AutoKary2022: A Large-Scale Densely Annotated Dateset for Chromosome\n  Instance Segmentation","summary":"  Automated chromosome instance segmentation from metaphase cell microscopic\nimages is critical for the diagnosis of chromosomal disorders (i.e., karyotype\nanalysis). However, it is still a challenging task due to lacking of densely\nannotated datasets and the complicated morphologies of chromosomes, e.g., dense\ndistribution, arbitrary orientations, and wide range of lengths. To facilitate\nthe development of this area, we take a big step forward and manually construct\na large-scale densely annotated dataset named AutoKary2022, which contains over\n27,000 chromosome instances in 612 microscopic images from 50 patients.\nSpecifically, each instance is annotated with a polygonal mask and a class\nlabel to assist in precise chromosome detection and segmentation. On top of it,\nwe systematically investigate representative methods on this dataset and obtain\na number of interesting findings, which helps us have a deeper understanding of\nthe fundamental problems in chromosome instance segmentation. We hope this\ndataset could advance research towards medical understanding. The dataset can\nbe available at:\nhttps://github.com/wangjuncongyu/chromosome-instance-segmentation-dataset.\n","authors":["Dan You","Pengcheng Xia","Qiuzhu Chen","Minghui Wu","Suncheng Xiang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15839v2.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2207.08208v3","updated":"2023-03-31T12:12:24Z","published":"2022-07-17T15:53:24Z","title":"Unsupervised Medical Image Translation with Adversarial Diffusion Models","summary":"  Imputation of missing images via source-to-target modality translation can\nimprove diversity in medical imaging protocols. A pervasive approach for\nsynthesizing target images involves one-shot mapping through generative\nadversarial networks (GAN). Yet, GAN models that implicitly characterize the\nimage distribution can suffer from limited sample fidelity. Here, we propose a\nnovel method based on adversarial diffusion modeling, SynDiff, for improved\nperformance in medical image translation. To capture a direct correlate of the\nimage distribution, SynDiff leverages a conditional diffusion process that\nprogressively maps noise and source images onto the target image. For fast and\naccurate image sampling during inference, large diffusion steps are taken with\nadversarial projections in the reverse diffusion direction. To enable training\non unpaired datasets, a cycle-consistent architecture is devised with coupled\ndiffusive and non-diffusive modules that bilaterally translate between two\nmodalities. Extensive assessments are reported on the utility of SynDiff\nagainst competing GAN and diffusion models in multi-contrast MRI and MRI-CT\ntranslation. Our demonstrations indicate that SynDiff offers quantitatively and\nqualitatively superior performance against competing baselines.\n","authors":["Muzaffer Özbey","Onat Dalmaz","Salman UH Dar","Hasan A Bedel","Şaban Özturk","Alper Güngör","Tolga Çukur"],"pdf_url":"https://arxiv.org/pdf/2207.08208v3.pdf","comment":"M. Ozbey and O. Dalmaz contributed equally to this study"},{"id":"http://arxiv.org/abs/2303.17989v1","updated":"2023-03-31T12:07:23Z","published":"2023-03-31T12:07:23Z","title":"Unsupervised crack detection on complex stone masonry surfaces","summary":"  Computer vision for detecting building pathologies has interested researchers\nfor quite some time. Vision-based crack detection is a non-destructive\nassessment technique, which can be useful especially for Cultural Heritage (CH)\nwhere strict regulations apply and, even simple, interventions are not\npermitted. Recently, shallow and deep machine learning architectures applied on\nvarious types of imagery are gaining ground. In this article a crack detection\nmethodology for stone masonry walls is presented. In the proposed approach,\ncrack detection is approached as an unsupervised anomaly detection problem on\nRGB (Red Green Blue) image patches. Towards this direction, some of the most\npopular state of the art CNN (Convolutional Neural Network) architectures are\ndeployed and modified to binary classify the images or image patches by\npredicting a specific class for the tested imagery; 'Crack' or 'No crack', and\ndetect and localize those cracks on the RGB imagery with high accuracy. Testing\nof the model was performed on various test sites and random images retrieved\nfrom the internet and collected by the authors and results suggested the high\nperformance of specific networks compared to the rest, considering also the\nsmall numbers of epochs required for training. Those results met the accuracy\ndelivered by more complex and computationally heavy approaches, requiring a\nlarge amount of data for training. Source code is available on GitHub\nhttps://github.com/pagraf/Crack-detection while datasets are available on\nZenodo https://doi.org/10.5281/zenodo.6516913 .\n","authors":["Panagiotis Agrafiotis","Anastastios Doulamis","Andreas Georgopoulos"],"pdf_url":"https://arxiv.org/pdf/2303.17989v1.pdf","comment":"Submitted to the Journal of Cultural Heritage, Elsevier, under review\n  as of 31st of March 2023"},{"id":"http://arxiv.org/abs/2108.02980v2","updated":"2023-03-31T12:02:10Z","published":"2021-08-06T07:16:48Z","title":"Fine-grained Domain Adaptive Crowd Counting via Point-derived\n  Segmentation","summary":"  Due to domain shift, a large performance drop is usually observed when a\ntrained crowd counting model is deployed in the wild. While existing\ndomain-adaptive crowd counting methods achieve promising results, they\ntypically regard each crowd image as a whole and reduce domain discrepancies in\na holistic manner, thus limiting further improvement of domain adaptation\nperformance. To this end, we propose to untangle \\emph{domain-invariant} crowd\nand \\emph{domain-specific} background from crowd images and design a\nfine-grained domain adaption method for crowd counting. Specifically, to\ndisentangle crowd from background, we propose to learn crowd segmentation from\npoint-level crowd counting annotations in a weakly-supervised manner. Based on\nthe derived segmentation, we design a crowd-aware domain adaptation mechanism\nconsisting of two crowd-aware adaptation modules, i.e., Crowd Region Transfer\n(CRT) and Crowd Density Alignment (CDA). The CRT module is designed to guide\ncrowd features transfer across domains beyond background distractions. The CDA\nmodule dedicates to regularising target-domain crowd density generation by its\nown crowd density distribution. Our method outperforms previous approaches\nconsistently in the widely-used adaptation scenarios.\n","authors":["Yongtuo Liu","Dan Xu","Sucheng Ren","Hanjie Wu","Hongmin Cai","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2108.02980v2.pdf","comment":"10 pages, 5 figures, and 9 tables"},{"id":"http://arxiv.org/abs/2303.11674v2","updated":"2023-03-31T11:55:55Z","published":"2023-03-21T08:36:34Z","title":"ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency\n  Transform for Domain Generalization","summary":"  Domain generalization (DG) aims to learn a model that generalizes well to\nunseen target domains utilizing multiple source domains without re-training.\nMost existing DG works are based on convolutional neural networks (CNNs).\nHowever, the local operation of the convolution kernel makes the model focus\ntoo much on local representations (e.g., texture), which inherently causes the\nmodel more prone to overfit to the source domains and hampers its\ngeneralization ability. Recently, several MLP-based methods have achieved\npromising results in supervised learning tasks by learning global interactions\namong different patches of the image. Inspired by this, in this paper, we first\nanalyze the difference between CNN and MLP methods in DG and find that MLP\nmethods exhibit a better generalization ability because they can better capture\nthe global representations (e.g., structure) than CNN methods. Then, based on a\nrecent lightweight MLP method, we obtain a strong baseline that outperforms\nmost state-of-the-art CNN-based methods. The baseline can learn global\nstructure representations with a filter to suppress structure irrelevant\ninformation in the frequency space. Moreover, we propose a dynAmic\nLOw-Frequency spectrum Transform (ALOFT) that can perturb local texture\nfeatures while preserving global structure features, thus enabling the filter\nto remove structure-irrelevant information sufficiently. Extensive experiments\non four benchmarks have demonstrated that our method can achieve great\nperformance improvement with a small number of parameters compared to SOTA\nCNN-based DG methods. Our code is available at\nhttps://github.com/lingeringlight/ALOFT/.\n","authors":["Jintao Guo","Na Wang","Lei Qi","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2303.11674v2.pdf","comment":"Accepted by CVPR2023. The code is available at\n  https://github.com/lingeringlight/ALOFT/"},{"id":"http://arxiv.org/abs/2211.03726v2","updated":"2023-03-31T11:51:40Z","published":"2022-11-07T17:57:02Z","title":"TAP-Vid: A Benchmark for Tracking Any Point in a Video","summary":"  Generic motion understanding from video involves not only tracking objects,\nbut also perceiving how their surfaces deform and move. This information is\nuseful to make inferences about 3D shape, physical properties and object\ninteractions. While the problem of tracking arbitrary physical points on\nsurfaces over longer video clips has received some attention, no dataset or\nbenchmark for evaluation existed, until now. In this paper, we first formalize\nthe problem, naming it tracking any point (TAP). We introduce a companion\nbenchmark, TAP-Vid, which is composed of both real-world videos with accurate\nhuman annotations of point tracks, and synthetic videos with perfect\nground-truth point tracks. Central to the construction of our benchmark is a\nnovel semi-automatic crowdsourced pipeline which uses optical flow estimates to\ncompensate for easier, short-term motion like camera shake, allowing annotators\nto focus on harder sections of video. We validate our pipeline on synthetic\ndata and propose a simple end-to-end point tracking model TAP-Net, showing that\nit outperforms all prior methods on our benchmark when trained on synthetic\ndata.\n","authors":["Carl Doersch","Ankush Gupta","Larisa Markeeva","Adrià Recasens","Lucas Smaira","Yusuf Aytar","João Carreira","Andrew Zisserman","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2211.03726v2.pdf","comment":"Published in NeurIPS Datasets and Benchmarks track, 2022"},{"id":"http://arxiv.org/abs/2302.02790v2","updated":"2023-03-31T11:43:42Z","published":"2023-02-06T14:07:13Z","title":"Perception Datasets for Anomaly Detection in Autonomous Driving: A\n  Survey","summary":"  Deep neural networks (DNN) which are employed in perception systems for\nautonomous driving require a huge amount of data to train on, as they must\nreliably achieve high performance in all kinds of situations. However, these\nDNN are usually restricted to a closed set of semantic classes available in\ntheir training data, and are therefore unreliable when confronted with\npreviously unseen instances. Thus, multiple perception datasets have been\ncreated for the evaluation of anomaly detection methods, which can be\ncategorized into three groups: real anomalies in real-world, synthetic\nanomalies augmented into real-world and completely synthetic scenes. This\nsurvey provides a structured and, to the best of our knowledge, complete\noverview and comparison of perception datasets for anomaly detection in\nautonomous driving. Each chapter provides information about tasks and ground\ntruth, context information, and licenses. Additionally, we discuss current\nweaknesses and gaps in existing datasets to underline the importance of\ndeveloping further data.\n","authors":["Daniel Bogdoll","Svenja Uhlemeyer","Kamil Kowol","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2302.02790v2.pdf","comment":"Accepted for publication at IV 2023"},{"id":"http://arxiv.org/abs/2211.13551v2","updated":"2023-03-31T11:37:12Z","published":"2022-11-24T12:02:13Z","title":"SfM-TTR: Using Structure from Motion for Test-Time Refinement of\n  Single-View Depth Networks","summary":"  Estimating a dense depth map from a single view is geometrically ill-posed,\nand state-of-the-art methods rely on learning depth's relation with visual\nappearance using deep neural networks. On the other hand, Structure from Motion\n(SfM) leverages multi-view constraints to produce very accurate but sparse\nmaps, as matching across images is typically limited by locally discriminative\ntexture. In this work, we combine the strengths of both approaches by proposing\na novel test-time refinement (TTR) method, denoted as SfM-TTR, that boosts the\nperformance of single-view depth networks at test time using SfM multi-view\ncues. Specifically, and differently from the state of the art, we use sparse\nSfM point clouds as test-time self-supervisory signal, fine-tuning the network\nencoder to learn a better representation of the test scene. Our results show\nhow the addition of SfM-TTR to several state-of-the-art self-supervised and\nsupervised networks improves significantly their performance, outperforming\nprevious TTR baselines mainly based on photometric multi-view consistency. The\ncode is available at https://github.com/serizba/SfM-TTR.\n","authors":["Sergio Izquierdo","Javier Civera"],"pdf_url":"https://arxiv.org/pdf/2211.13551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17981v1","updated":"2023-03-31T11:33:21Z","published":"2023-03-31T11:33:21Z","title":"Knowledge Distillation for Feature Extraction in Underwater VSLAM","summary":"  In recent years, learning-based feature detection and matching have\noutperformed manually-designed methods in in-air cases. However, it is\nchallenging to learn the features in the underwater scenario due to the absence\nof annotated underwater datasets. This paper proposes a cross-modal knowledge\ndistillation framework for training an underwater feature detection and\nmatching network (UFEN). In particular, we use in-air RGBD data to generate\nsynthetic underwater images based on a physical underwater imaging formation\nmodel and employ these as the medium to distil knowledge from a teacher model\nSuperPoint pretrained on in-air images. We embed UFEN into the ORB-SLAM3\nframework to replace the ORB feature by introducing an additional binarization\nlayer. To test the effectiveness of our method, we built a new underwater\ndataset with groundtruth measurements named EASI\n(https://github.com/Jinghe-mel/UFEN-SLAM), recorded in an indoor water tank for\ndifferent turbidity levels. The experimental results on the existing dataset\nand our new dataset demonstrate the effectiveness of our method.\n","authors":["Jinghe Yang","Mingming Gong","Girish Nair","Jung Hoon Lee","Jason Monty","Ye Pu"],"pdf_url":"https://arxiv.org/pdf/2303.17981v1.pdf","comment":"Accepted by IEEE International Conference on Robotics and Automation\n  (ICRA 2023),6 pages"},{"id":"http://arxiv.org/abs/2303.17968v1","updated":"2023-03-31T11:13:17Z","published":"2023-03-31T11:13:17Z","title":"VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence\n  Normalization","summary":"  We propose VDN-NeRF, a method to train neural radiance fields (NeRFs) for\nbetter geometry under non-Lambertian surface and dynamic lighting conditions\nthat cause significant variation in the radiance of a point when viewed from\ndifferent angles. Instead of explicitly modeling the underlying factors that\nresult in the view-dependent phenomenon, which could be complex yet not\ninclusive, we develop a simple and effective technique that normalizes the\nview-dependence by distilling invariant information already encoded in the\nlearned NeRFs. We then jointly train NeRFs for view synthesis with\nview-dependence normalization to attain quality geometry. Our experiments show\nthat even though shape-radiance ambiguity is inevitable, the proposed\nnormalization can minimize its effect on geometry, which essentially aligns the\noptimal capacity needed for explaining view-dependent variations. Our method\napplies to various baselines and significantly improves geometry without\nchanging the volume rendering pipeline, even if the data is captured under a\nmoving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF.\n","authors":["Bingfan Zhu","Yanchao Yang","Xulong Wang","Youyi Zheng","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2303.17968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17967v1","updated":"2023-03-31T11:12:35Z","published":"2023-03-31T11:12:35Z","title":"Learning with Explicit Shape Priors for Medical Image Segmentation","summary":"  Medical image segmentation is considered as the basic step for medical image\nanalysis and surgical intervention. And many previous works attempted to\nincorporate shape priors for designing segmentation models, which is beneficial\nto attain finer masks with anatomical shape information. Here in our work, we\ndetailedly discuss three types of segmentation models with shape priors, which\nconsist of atlas-based models, statistical-based models and UNet-based models.\nOn the ground that the former two kinds of methods show a poor generalization\nability, UNet-based models have dominated the field of medical image\nsegmentation in recent years. However, existing UNet-based models tend to\nemploy implicit shape priors, which do not have a good interpretability and\ngeneralization ability on different organs with distinctive shapes. Thus, we\nproposed a novel shape prior module (SPM), which could explicitly introduce\nshape priors to promote the segmentation performance of UNet-based models. To\nevaluate the effectiveness of SPM, we conduct experiments on three challenging\npublic datasets. And our proposed model achieves state-of-the-art performance.\nFurthermore, SPM shows an outstanding generalization ability on different\nclassic convolution-neural-networks (CNNs) and recent Transformer-based\nbackbones, which can serve as a plug-and-play structure for the segmentation\ntask of different datasets.\n","authors":["Xin You","Junjun He","Jie Yang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2303.17967v1.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2204.03245v2","updated":"2023-03-31T11:04:55Z","published":"2022-04-07T06:23:02Z","title":"HIT-UAV: A high-altitude infrared thermal dataset for Unmanned Aerial\n  Vehicle-based object detection","summary":"  We present the HIT-UAV dataset, a high-altitude infrared thermal dataset for\nobject detection applications on Unmanned Aerial Vehicles (UAVs). The dataset\ncomprises 2,898 infrared thermal images extracted from 43,470 frames in\nhundreds of videos captured by UAVs in various scenarios including schools,\nparking lots, roads, and playgrounds. Moreover, the HIT-UAV provides essential\nflight data for each image, such as flight altitude, camera perspective, date,\nand daylight intensity. For each image, we have manually annotated object\ninstances with bounding boxes of two types (oriented and standard) to tackle\nthe challenge of significant overlap of object instances in aerial images. To\nthe best of our knowledge, the HIT-UAV is the first publicly available\nhigh-altitude UAV-based infrared thermal dataset for detecting persons and\nvehicles. We have trained and evaluated well-established object detection\nalgorithms on the HIT-UAV. Our results demonstrate that the detection\nalgorithms perform exceptionally well on the HIT-UAV compared to visual light\ndatasets since infrared thermal images do not contain significant irrelevant\ninformation about objects. We believe that the HIT-UAV will contribute to\nvarious UAV-based applications and researches. The dataset is freely available\nat https://github.com/suojiashun/HIT-UAV-Infrared-Thermal-Dataset.\n","authors":["Jiashun Suo","Tianyi Wang","Xingzhou Zhang","Haiyang Chen","Wei Zhou","Weisong Shi"],"pdf_url":"https://arxiv.org/pdf/2204.03245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17959v1","updated":"2023-03-31T10:53:24Z","published":"2023-03-31T10:53:24Z","title":"Diffusion Action Segmentation","summary":"  Temporal action segmentation is crucial for understanding long-form videos.\nPrevious works on this task commonly adopt an iterative refinement paradigm by\nusing multi-stage models. Our paper proposes an essentially different framework\nvia denoising diffusion models, which nonetheless shares the same inherent\nspirit of such iterative refinement. In this framework, action predictions are\nprogressively generated from random noise with input video features as\nconditions. To enhance the modeling of three striking characteristics of human\nactions, including the position prior, the boundary ambiguity, and the\nrelational dependency, we devise a unified masking strategy for the\nconditioning inputs in our framework. Extensive experiments on three benchmark\ndatasets, i.e., GTEA, 50Salads, and Breakfast, are performed and the proposed\nmethod achieves superior or comparable results to state-of-the-art methods,\nshowing the effectiveness of a generative approach for action segmentation. Our\ncodes will be made available.\n","authors":["Daochang Liu","Qiyue Li","AnhDung Dinh","Tingting Jiang","Mubarak Shah","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2303.17959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17956v1","updated":"2023-03-31T10:37:19Z","published":"2023-03-31T10:37:19Z","title":"Ensemble Methods for Multi-Organ Segmentation in CT Series","summary":"  In the medical images field, semantic segmentation is one of the most\nimportant, yet difficult and time-consuming tasks to be performed by\nphysicians. Thanks to the recent advancement in the Deep Learning models\nregarding Computer Vision, the promise to automate this kind of task is getting\nmore and more realistic. However, many problems are still to be solved, like\nthe scarce availability of data and the difficulty to extend the efficiency of\nhighly specialised models to general scenarios. Organs at risk segmentation for\nradiotherapy treatment planning falls in this category, as the limited data\navailable negatively affects the possibility to develop general-purpose models;\nin this work, we focus on the possibility to solve this problem by presenting\nthree types of ensembles of single-organ models able to produce multi-organ\nmasks exploiting the different specialisations of their components. The results\nobtained are promising and prove that this is a possible solution to finding\nefficient multi-organ segmentation methods.\n","authors":["Leonardo Crespi","Paolo Roncaglioni","Damiano Dei","Ciro Franzese","Nicola Lambri","Daniele Loiacono","Pietro Mancosu","Marta Scorsetti"],"pdf_url":"https://arxiv.org/pdf/2303.17956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17948v1","updated":"2023-03-31T10:26:47Z","published":"2023-03-31T10:26:47Z","title":"CIMI4D: A Large Multimodal Climbing Motion Dataset under Human-scene\n  Interactions","summary":"  Motion capture is a long-standing research problem. Although it has been\nstudied for decades, the majority of research focus on ground-based movements\nsuch as walking, sitting, dancing, etc. Off-grounded actions such as climbing\nare largely overlooked. As an important type of action in sports and\nfirefighting field, the climbing movements is challenging to capture because of\nits complex back poses, intricate human-scene interactions, and difficult\nglobal localization. The research community does not have an in-depth\nunderstanding of the climbing action due to the lack of specific datasets. To\naddress this limitation, we collect CIMI4D, a large rock\n\\textbf{C}l\\textbf{I}mbing \\textbf{M}ot\\textbf{I}on dataset from 12 persons\nclimbing 13 different climbing walls. The dataset consists of around 180,000\nframes of pose inertial measurements, LiDAR point clouds, RGB videos,\nhigh-precision static point cloud scenes, and reconstructed scene meshes.\nMoreover, we frame-wise annotate touch rock holds to facilitate a detailed\nexploration of human-scene interaction. The core of this dataset is a blending\noptimization process, which corrects for the pose as it drifts and is affected\nby the magnetic conditions. To evaluate the merit of CIMI4D, we perform four\ntasks which include human pose estimations (with/without scene constraints),\npose prediction, and pose generation. The experimental results demonstrate that\nCIMI4D presents great challenges to existing methods and enables extensive\nresearch opportunities. We share the dataset with the research community in\nhttp://www.lidarhumanmotion.net/cimi4d/.\n","authors":["Ming Yan","Xin Wang","Yudi Dai","Siqi Shen","Chenglu Wen","Lan Xu","Yuexin Ma","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2303.17948v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17942v1","updated":"2023-03-31T10:13:01Z","published":"2023-03-31T10:13:01Z","title":"Benchmarking FedAvg and FedCurv for Image Classification Tasks","summary":"  Classic Machine Learning techniques require training on data available in a\nsingle data lake. However, aggregating data from different owners is not always\nconvenient for different reasons, including security, privacy and secrecy. Data\ncarry a value that might vanish when shared with others; the ability to avoid\nsharing the data enables industrial applications where security and privacy are\nof paramount importance, making it possible to train global models by\nimplementing only local policies which can be run independently and even on\nair-gapped data centres. Federated Learning (FL) is a distributed machine\nlearning approach which has emerged as an effective way to address privacy\nconcerns by only sharing local AI models while keeping the data decentralized.\nTwo critical challenges of Federated Learning are managing the heterogeneous\nsystems in the same federated network and dealing with real data, which are\noften not independently and identically distributed (non-IID) among the\nclients. In this paper, we focus on the second problem, i.e., the problem of\nstatistical heterogeneity of the data in the same federated network. In this\nsetting, local models might be strayed far from the local optimum of the\ncomplete dataset, thus possibly hindering the convergence of the federated\nmodel. Several Federated Learning algorithms, such as FedAvg, FedProx and\nFederated Curvature (FedCurv), aiming at tackling the non-IID setting, have\nalready been proposed. This work provides an empirical assessment of the\nbehaviour of FedAvg and FedCurv in common non-IID scenarios. Results show that\nthe number of epochs per round is an important hyper-parameter that, when tuned\nappropriately, can lead to significant performance gains while reducing the\ncommunication cost. As a side product of this work, we release the non-IID\nversion of the datasets we used so to facilitate further comparisons from the\nFL community.\n","authors":["Bruno Casella","Roberto Esposito","Carlo Cavazzoni","Marco Aldinucci"],"pdf_url":"https://arxiv.org/pdf/2303.17942v1.pdf","comment":"12 pages, Proceedings of ITADATA22, The 1st Italian Conference on Big\n  Data and Data Science; Published on CEUR Workshop Proceedings (CEUR-WS.org,\n  ISSN 1613-0073), Vol. 3340, pp. 99-110, 2022"},{"id":"http://arxiv.org/abs/2303.17941v1","updated":"2023-03-31T10:10:05Z","published":"2023-03-31T10:10:05Z","title":"Comparing Adversarial and Supervised Learning for Organs at Risk\n  Segmentation in CT images","summary":"  Organ at Risk (OAR) segmentation from CT scans is a key component of the\nradiotherapy treatment workflow. In recent years, deep learning techniques have\nshown remarkable potential in automating this process. In this paper, we\ninvestigate the performance of Generative Adversarial Networks (GANs) compared\nto supervised learning approaches for segmenting OARs from CT images. We\npropose three GAN-based models with identical generator architectures but\ndifferent discriminator networks. These models are compared with\nwell-established CNN models, such as SE-ResUnet and DeepLabV3, using the\nStructSeg dataset, which consists of 50 annotated CT scans containing contours\nof six OARs. Our work aims to provide insight into the advantages and\ndisadvantages of adversarial training in the context of OAR segmentation. The\nresults are very promising and show that the proposed GAN-based approaches are\nsimilar or superior to their CNN-based counterparts, particularly when\nsegmenting more challenging target organs.\n","authors":["Leonardo Crespi","Mattia Portanti","Daniele Loiacono"],"pdf_url":"https://arxiv.org/pdf/2303.17941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17937v1","updated":"2023-03-31T10:04:44Z","published":"2023-03-31T10:04:44Z","title":"STFAR: Improving Object Detection Robustness at Test-Time by\n  Self-Training with Feature Alignment Regularization","summary":"  Domain adaptation helps generalizing object detection models to target domain\ndata with distribution shift. It is often achieved by adapting with access to\nthe whole target domain data. In a more realistic scenario, target distribution\nis often unpredictable until inference stage. This motivates us to explore\nadapting an object detection model at test-time, a.k.a. test-time adaptation\n(TTA). In this work, we approach test-time adaptive object detection (TTAOD)\nfrom two perspective. First, we adopt a self-training paradigm to generate\npseudo labeled objects with an exponential moving average model. The pseudo\nlabels are further used to supervise adapting source domain model. As\nself-training is prone to incorrect pseudo labels, we further incorporate\naligning feature distributions at two output levels as regularizations to\nself-training. To validate the performance on TTAOD, we create benchmarks based\non three standard object detection datasets and adapt generic TTA methods to\nobject detection task. Extensive evaluations suggest our proposed method sets\nthe state-of-the-art on test-time adaptive object detection task.\n","authors":["Yijin Chen","Xun Xu","Yongyi Su","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2303.17937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.14682v3","updated":"2023-03-31T09:56:52Z","published":"2022-07-29T13:57:16Z","title":"Towards Unconstrained Audio Splicing Detection and Localization with\n  Neural Networks","summary":"  Freely available and easy-to-use audio editing tools make it straightforward\nto perform audio splicing. Convincing forgeries can be created by combining\nvarious speech samples from the same person. Detection of such splices is\nimportant both in the public sector when considering misinformation, and in a\nlegal context to verify the integrity of evidence. Unfortunately, most existing\ndetection algorithms for audio splicing use handcrafted features and make\nspecific assumptions. However, criminal investigators are often faced with\naudio samples from unconstrained sources with unknown characteristics, which\nraises the need for more generally applicable methods.\n  With this work, we aim to take a first step towards unconstrained audio\nsplicing detection to address this need. We simulate various attack scenarios\nin the form of post-processing operations that may disguise splicing. We\npropose a Transformer sequence-to-sequence (seq2seq) network for splicing\ndetection and localization. Our extensive evaluation shows that the proposed\nmethod outperforms existing dedicated approaches for splicing detection [3, 10]\nas well as the general-purpose networks EfficientNet [28] and RegNet [25].\n","authors":["Denise Moussa","Germans Hirsch","Christian Riess"],"pdf_url":"https://arxiv.org/pdf/2207.14682v3.pdf","comment":"Accepted at MMFORWILD 2022, ICPR Workshops - Code:\n  https://faui1-gitlab.cs.fau.de/denise.moussa/audio-splicing-localization"},{"id":"http://arxiv.org/abs/2303.16066v2","updated":"2023-03-31T09:54:13Z","published":"2023-03-27T05:29:53Z","title":"Neural Collapse Inspired Federated Learning with Non-iid Data","summary":"  One of the challenges in federated learning is the non-independent and\nidentically distributed (non-iid) characteristics between heterogeneous\ndevices, which cause significant differences in local updates and affect the\nperformance of the central server. Although many studies have been proposed to\naddress this challenge, they only focus on local training and aggregation\nprocesses to smooth the changes and fail to achieve high performance with deep\nlearning models. Inspired by the phenomenon of neural collapse, we force each\nclient to be optimized toward an optimal global structure for classification.\nSpecifically, we initialize it as a random simplex Equiangular Tight Frame\n(ETF) and fix it as the unit optimization target of all clients during the\nlocal updating. After guaranteeing all clients are learning to converge to the\nglobal optimum, we propose to add a global memory vector for each category to\nremedy the parameter fluctuation caused by the bias of the intra-class\ncondition distribution among clients. Our experimental results show that our\nmethod can improve the performance with faster convergence speed on\ndifferent-size datasets.\n","authors":["Chenxi Huang","Liang Xie","Yibo Yang","Wenxiao Wang","Binbin Lin","Deng Cai"],"pdf_url":"https://arxiv.org/pdf/2303.16066v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17921v1","updated":"2023-03-31T09:31:29Z","published":"2023-03-31T09:31:29Z","title":"IC-FPS: Instance-Centroid Faster Point Sampling Module for 3D Point-base\n  Object Detection","summary":"  3D object detection is one of the most important tasks in autonomous driving\nand robotics. Our research focuses on tackling low efficiency issue of\npoint-based methods on large-scale point clouds. Existing point-based methods\nadopt farthest point sampling (FPS) strategy for downsampling, which is\ncomputationally expensive in terms of inference time and memory consumption\nwhen the number of point cloud increases. In order to improve efficiency, we\npropose a novel Instance-Centroid Faster Point Sampling Module (IC-FPS) , which\neffectively replaces the first Set Abstraction (SA) layer that is extremely\ntedious. IC-FPS module is comprised of two methods, local feature diffusion\nbased background point filter (LFDBF) and Centroid-Instance Sampling Strategy\n(CISS). LFDBF is constructed to exclude most invalid background points, while\nCISS substitutes FPS strategy by fast sampling centroids and instance points.\nIC-FPS module can be inserted to almost every point-based models. Extensive\nexperiments on multiple public benchmarks have demonstrated the superiority of\nIC-FPS. On Waymo dataset, the proposed module significantly improves\nperformance of baseline model and accelerates inference speed by 3.8 times. For\nthe first time, real-time detection of point-based models in large-scale point\ncloud scenario is realized.\n","authors":["Hu Haotian","Wang Fanyi","Su Jingwen","Gao Shiyu","Zhang Zhiwang"],"pdf_url":"https://arxiv.org/pdf/2303.17921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.13137v4","updated":"2023-03-31T09:26:28Z","published":"2022-05-26T04:00:42Z","title":"MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of\n  Hierarchical Vision Transformers","summary":"  In this paper, we propose Mixed and Masked AutoEncoder (MixMAE), a simple but\nefficient pretraining method that is applicable to various hierarchical Vision\nTransformers. Existing masked image modeling (MIM) methods for hierarchical\nVision Transformers replace a random subset of input tokens with a special\n[MASK] symbol and aim at reconstructing original image tokens from the\ncorrupted image. However, we find that using the [MASK] symbol greatly slows\ndown the training and causes pretraining-finetuning inconsistency, due to the\nlarge masking ratio (e.g., 60% in SimMIM). On the other hand, MAE does not\nintroduce [MASK] tokens at its encoder at all but is not applicable for\nhierarchical Vision Transformers. To solve the issue and accelerate the\npretraining of hierarchical models, we replace the masked tokens of one image\nwith visible tokens of another image, i.e., creating a mixed image. We then\nconduct dual reconstruction to reconstruct the two original images from the\nmixed input, which significantly improves efficiency. While MixMAE can be\napplied to various hierarchical Transformers, this paper explores using Swin\nTransformer with a large window size and scales up to huge model size (to reach\n600M parameters). Empirical results demonstrate that MixMAE can learn\nhigh-quality visual representations efficiently. Notably, MixMAE with\nSwin-B/W14 achieves 85.1% top-1 accuracy on ImageNet-1K by pretraining for 600\nepochs. Besides, its transfer performances on the other 6 datasets show that\nMixMAE has better FLOPs / performance tradeoff than previous popular MIM\nmethods. Code is available at https://github.com/Sense-X/MixMIM.\n","authors":["Jihao Liu","Xin Huang","Jinliang Zheng","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2205.13137v4.pdf","comment":"CVPR2023. Code: https://github.com/Sense-X/MixMIM"},{"id":"http://arxiv.org/abs/2303.17915v1","updated":"2023-03-31T09:23:27Z","published":"2023-03-31T09:23:27Z","title":"Multiple Instance Ensembling For Paranasal Anomaly Classification In The\n  Maxillary Sinus","summary":"  Paranasal anomalies are commonly discovered during routine radiological\nscreenings and can present with a wide range of morphological features. This\ndiversity can make it difficult for convolutional neural networks (CNNs) to\naccurately classify these anomalies, especially when working with limited\ndatasets. Additionally, current approaches to paranasal anomaly classification\nare constrained to identifying a single anomaly at a time. These challenges\nnecessitate the need for further research and development in this area.\n  In this study, we investigate the feasibility of using a 3D convolutional\nneural network (CNN) to classify healthy maxillary sinuses (MS) and MS with\npolyps or cysts. The task of accurately identifying the relevant MS volume\nwithin larger head and neck Magnetic Resonance Imaging (MRI) scans can be\ndifficult, but we develop a straightforward strategy to tackle this challenge.\nOur end-to-end solution includes the use of a novel sampling technique that not\nonly effectively localizes the relevant MS volume, but also increases the size\nof the training dataset and improves classification results. Additionally, we\nemploy a multiple instance ensemble prediction method to further boost\nclassification performance. Finally, we identify the optimal size of MS volumes\nto achieve the highest possible classification performance on our dataset.\n  With our multiple instance ensemble prediction strategy and sampling\nstrategy, our 3D CNNs achieve an F1 of 0.85 whereas without it, they achieve an\nF1 of 0.70.\n  We demonstrate the feasibility of classifying anomalies in the MS. We propose\na data enlarging strategy alongside a novel ensembling strategy that proves to\nbe beneficial for paranasal anomaly classification in the MS.\n","authors":["Debayan Bhattacharya","Finn Behrendt","Benjamin Tobias Becker","Dirk Beyersdorff","Elina Petersen","Marvin Petersen","Bastian Cheng","Dennis Eggert","Christian Betz","Anna Sophie Hoffmann","Alexander Schlaefer"],"pdf_url":"https://arxiv.org/pdf/2303.17915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17912v1","updated":"2023-03-31T09:18:12Z","published":"2023-03-31T09:18:12Z","title":"CIRCLE: Capture In Rich Contextual Environments","summary":"  Synthesizing 3D human motion in a contextual, ecological environment is\nimportant for simulating realistic activities people perform in the real world.\nHowever, conventional optics-based motion capture systems are not suited for\nsimultaneously capturing human movements and complex scenes. The lack of rich\ncontextual 3D human motion datasets presents a roadblock to creating\nhigh-quality generative human motion models. We propose a novel motion\nacquisition system in which the actor perceives and operates in a highly\ncontextual virtual world while being motion captured in the real world. Our\nsystem enables rapid collection of high-quality human motion in highly diverse\nscenes, without the concern of occlusion or the need for physical scene\nconstruction in the real world. We present CIRCLE, a dataset containing 10\nhours of full-body reaching motion from 5 subjects across nine scenes, paired\nwith ego-centric information of the environment represented in various forms,\nsuch as RGBD videos. We use this dataset to train a model that generates human\nmotion conditioned on scene information. Leveraging our dataset, the model\nlearns to use ego-centric scene information to achieve nontrivial reaching\ntasks in the context of complex 3D scenes. To download the data please visit\nhttps://stanford-tml.github.io/circle_dataset/.\n","authors":["Joao Pedro Araujo","Jiaman Li","Karthik Vetrivel","Rishi Agarwal","Deepak Gopinath","Jiajun Wu","Alexander Clegg","C. Karen Liu"],"pdf_url":"https://arxiv.org/pdf/2303.17912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05936v2","updated":"2023-03-31T09:16:11Z","published":"2022-12-07T08:16:26Z","title":"Encoder-Decoder Network with Guided Transmission Map: Architecture","summary":"  An insight into the architecture of the Encoder-Decoder Network with Guided\nTransmission Map (EDN-GTM), a novel and effective single image dehazing scheme,\nis presented in this paper. The EDN-GTM takes a conventional RGB hazy image in\nconjunction with the corresponding transmission map estimated by the dark\nchannel prior (DCP) approach as inputs of the network. The EDN-GTM adopts an\nenhanced structure of U-Net developed for dehazing tasks and the resulting\nEDN-GDM has shown state-of-the-art performances on benchmark dehazing datasets\nin terms of PSNR and SSIM metrics. In order to give an in-depth understanding\nof the well-designed architecture which largely contributes to the success of\nthe EDN-GTM, extensive experiments and analysis from selecting the core\nstructure of the scheme to investigating advanced network designs are presented\nin this paper.\n","authors":["Le-Anh Tran","Dong-Chul Park"],"pdf_url":"https://arxiv.org/pdf/2212.05936v2.pdf","comment":"3 pages, 2 figures, ASPAI 2022"},{"id":"http://arxiv.org/abs/2203.12612v6","updated":"2023-03-31T09:11:53Z","published":"2022-03-23T17:58:31Z","title":"StructToken : Rethinking Semantic Segmentation with Structural Prior","summary":"  In previous deep-learning-based methods, semantic segmentation has been\nregarded as a static or dynamic per-pixel classification task, \\textit{i.e.,}\nclassify each pixel representation to a specific category. However, these\nmethods only focus on learning better pixel representations or classification\nkernels while ignoring the structural information of objects, which is critical\nto human decision-making mechanism. In this paper, we present a new paradigm\nfor semantic segmentation, named structure-aware extraction. Specifically, it\ngenerates the segmentation results via the interactions between a set of\nlearned structure tokens and the image feature, which aims to progressively\nextract the structural information of each category from the feature. Extensive\nexperiments show that our StructToken outperforms the state-of-the-art on three\nwidely-used benchmarks, including ADE20K, Cityscapes, and COCO-Stuff-10K.\n","authors":["Fangjian Lin","Zhanhao Liang","Sitong Wu","Junjun He","Kai Chen","Shengwei Tian"],"pdf_url":"https://arxiv.org/pdf/2203.12612v6.pdf","comment":"Accept by IEEE TCSVT"},{"id":"http://arxiv.org/abs/2303.17908v1","updated":"2023-03-31T09:11:26Z","published":"2023-03-31T09:11:26Z","title":"Pay Attention: Accuracy Versus Interpretability Trade-off in Fine-tuned\n  Diffusion Models","summary":"  The recent progress of diffusion models in terms of image quality has led to\na major shift in research related to generative models. Current approaches\noften fine-tune pre-trained foundation models using domain-specific\ntext-to-image pairs. This approach is straightforward for X-ray image\ngeneration due to the high availability of radiology reports linked to specific\nimages. However, current approaches hardly ever look at attention layers to\nverify whether the models understand what they are generating. In this paper,\nwe discover an important trade-off between image fidelity and interpretability\nin generative diffusion models. In particular, we show that fine-tuning\ntext-to-image models with learnable text encoder leads to a lack of\ninterpretability of diffusion models. Finally, we demonstrate the\ninterpretability of diffusion models by showing that keeping the language\nencoder frozen, enables diffusion models to achieve state-of-the-art phrase\ngrounding performance on certain diseases for a challenging multi-label\nsegmentation task, without any additional training. Code and models will be\navailable at https://github.com/MischaD/chest-distillation.\n","authors":["Mischa Dombrowski","Hadrien Reynaud","Johanna P. Müller","Matthew Baugh","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2303.17908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17905v1","updated":"2023-03-31T09:03:18Z","published":"2023-03-31T09:03:18Z","title":"3D-aware Image Generation using 2D Diffusion Models","summary":"  In this paper, we introduce a novel 3D-aware image generation method that\nleverages 2D diffusion models. We formulate the 3D-aware image generation task\nas multiview 2D image set generation, and further to a sequential\nunconditional-conditional multiview image generation process. This allows us to\nutilize 2D diffusion models to boost the generative modeling power of the\nmethod. Additionally, we incorporate depth information from monocular depth\nestimators to construct the training data for the conditional diffusion model\nusing only still images. We train our method on a large-scale dataset, i.e.,\nImageNet, which is not addressed by previous methods. It produces high-quality\nimages that significantly outperform prior methods. Furthermore, our approach\nshowcases its capability to generate instances with large view angles, even\nthough the training images are diverse and unaligned, gathered from\n\"in-the-wild\" real-world environments.\n","authors":["Jianfeng Xiang","Jiaolong Yang","Binbin Huang","Xin Tong"],"pdf_url":"https://arxiv.org/pdf/2303.17905v1.pdf","comment":"Website: https://jeffreyxiang.github.io/ivid/"},{"id":"http://arxiv.org/abs/2303.17895v1","updated":"2023-03-31T08:56:29Z","published":"2023-03-31T08:56:29Z","title":"EA-BEV: Edge-aware Bird' s-Eye-View Projector for 3D Object Detection","summary":"  In recent years, great progress has been made in the Lift-Splat-Shot-based\n(LSS-based) 3D object detection method, which converts features of 2D camera\nview and 3D lidar view to Bird's-Eye-View (BEV) for feature fusion. However,\ninaccurate depth estimation (e.g. the 'depth jump' problem) is an obstacle to\ndevelop LSS-based methods. To alleviate the 'depth jump' problem, we proposed\nEdge-Aware Bird's-Eye-View (EA-BEV) projector. By coupling proposed edge-aware\ndepth fusion module and depth estimate module, the proposed EA-BEV projector\nsolves the problem and enforces refined supervision on depth. Besides, we\npropose sparse depth supervision and gradient edge depth supervision, for\nconstraining learning on global depth and local marginal depth information. Our\nEA-BEV projector is a plug-and-play module for any LSS-based 3D object\ndetection models, and effectively improves the baseline performance. We\ndemonstrate the effectiveness on the nuScenes benchmark. On the nuScenes 3D\nobject detection validation dataset, our proposed EA-BEV projector can boost\nseveral state-of-the-art LLS-based baselines on nuScenes 3D object detection\nbenchmark and nuScenes BEV map segmentation benchmark with negligible increment\nof inference time.\n","authors":[" Haotian"," Hu"," Fanyi"," Wang"," Jingwen"," Su"," Laifeng"," Hu"," Tianpeng"," Feng"," Zhaokai"," Zhang"," Wangzhi"," Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.17895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17896v1","updated":"2023-03-31T08:56:29Z","published":"2023-03-31T08:56:29Z","title":"Exploring the Limits of Deep Image Clustering using Pretrained Models","summary":"  We present a general methodology that learns to classify images without\nlabels by leveraging pretrained feature extractors. Our approach involves\nself-distillation training of clustering heads, based on the fact that nearest\nneighbors in the pretrained feature space are likely to share the same label.\nWe propose a novel objective to learn associations between images by\nintroducing a variant of pointwise mutual information together with instance\nweighting. We demonstrate that the proposed objective is able to attenuate the\neffect of false positive pairs while efficiently exploiting the structure in\nthe pretrained feature space. As a result, we improve the clustering accuracy\nover $k$-means on $17$ different pretrained models by $6.1$\\% and $12.2$\\% on\nImageNet and CIFAR100, respectively. Finally, using self-supervised pretrained\nvision transformers we push the clustering accuracy on ImageNet to $61.6$\\%.\nThe code will be open-sourced.\n","authors":["Nikolas Adaloglou","Felix Michels","Hamza Kalisch","Markus Kollmann"],"pdf_url":"https://arxiv.org/pdf/2303.17896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12277v3","updated":"2023-03-31T08:52:12Z","published":"2022-11-22T13:49:10Z","title":"Semantic Guided Level-Category Hybrid Prediction Network for\n  Hierarchical Image Classification","summary":"  Hierarchical classification (HC) assigns each object with multiple labels\norganized into a hierarchical structure. The existing deep learning based HC\nmethods usually predict an instance starting from the root node until a leaf\nnode is reached. However, in the real world, images interfered by noise,\nocclusion, blur, or low resolution may not provide sufficient information for\nthe classification at subordinate levels. To address this issue, we propose a\nnovel semantic guided level-category hybrid prediction network (SGLCHPN) that\ncan jointly perform the level and category prediction in an end-to-end manner.\nSGLCHPN comprises two modules: a visual transformer that extracts feature\nvectors from the input images, and a semantic guided cross-attention module\nthat uses categories word embeddings as queries to guide learning\ncategory-specific representations. In order to evaluate the proposed method, we\nconstruct two new datasets in which images are at a broad range of quality and\nthus are labeled to different levels (depths) in the hierarchy according to\ntheir individual quality. Experimental results demonstrate the effectiveness of\nour proposed HC method.\n","authors":["Peng Wang","Jingzhou Chen","Yuntao Qian"],"pdf_url":"https://arxiv.org/pdf/2211.12277v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17890v1","updated":"2023-03-31T08:48:57Z","published":"2023-03-31T08:48:57Z","title":"Fooling Polarization-based Vision using Locally Controllable Polarizing\n  Projection","summary":"  Polarization is a fundamental property of light that encodes abundant\ninformation regarding surface shape, material, illumination and viewing\ngeometry. The computer vision community has witnessed a blossom of\npolarization-based vision applications, such as reflection removal,\nshape-from-polarization, transparent object segmentation and color constancy,\npartially due to the emergence of single-chip mono/color polarization sensors\nthat make polarization data acquisition easier than ever. However, is\npolarization-based vision vulnerable to adversarial attacks? If so, is that\npossible to realize these adversarial attacks in the physical world, without\nbeing perceived by human eyes? In this paper, we warn the community of the\nvulnerability of polarization-based vision, which can be more serious than\nRGB-based vision. By adapting a commercial LCD projector, we achieve locally\ncontrollable polarizing projection, which is successfully utilized to fool\nstate-of-the-art polarization-based vision algorithms for glass segmentation\nand color constancy. Compared with existing physical attacks on RGB-based\nvision, which always suffer from the trade-off between attack efficacy and eye\nconceivability, the adversarial attackers based on polarizing projection are\ncontact-free and visually imperceptible, since naked human eyes can rarely\nperceive the difference of viciously manipulated polarizing light and ordinary\nillumination. This poses unprecedented risks on polarization-based vision, both\nin the monochromatic and trichromatic domain, for which due attentions should\nbe paid and counter measures be considered.\n","authors":["Zhuoxiao Li","Zhihang Zhong","Shohei Nobuhara","Ko Nishino","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2303.17890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17882v1","updated":"2023-03-31T08:34:07Z","published":"2023-03-31T08:34:07Z","title":"Visual Anomaly Detection via Dual-Attention Transformer and\n  Discriminative Flow","summary":"  In this paper, we introduce the novel state-of-the-art Dual-attention\nTransformer and Discriminative Flow (DADF) framework for visual anomaly\ndetection. Based on only normal knowledge, visual anomaly detection has wide\napplications in industrial scenarios and has attracted significant attention.\nHowever, most existing methods fail to meet the requirements. In contrast, the\nproposed DTDF presents a new paradigm: it firstly leverages a pre-trained\nnetwork to acquire multi-scale prior embeddings, followed by the development of\na vision Transformer with dual attention mechanisms, namely self-attention and\nmemorial-attention, to achieve two-level reconstruction for prior embeddings\nwith the sequential and normality association. Additionally, we propose using\nnormalizing flow to establish discriminative likelihood for the joint\ndistribution of prior and reconstructions at each scale. The DADF achieves\n98.3/98.4 of image/pixel AUROC on Mvtec AD; 83.7 of image AUROC and 67.4 of\npixel sPRO on Mvtec LOCO AD benchmarks, demonstrating the effectiveness of our\nproposed approach.\n","authors":["Haiming Yao","Wei Luo","Wenyong Yu"],"pdf_url":"https://arxiv.org/pdf/2303.17882v1.pdf","comment":"Submission to IEEE Transactions On Industrial Informatics"},{"id":"http://arxiv.org/abs/2303.17870v1","updated":"2023-03-31T08:06:33Z","published":"2023-03-31T08:06:33Z","title":"GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models\n  Coherently","summary":"  Recent breakthroughs in the field of language-guided image generation have\nyielded impressive achievements, enabling the creation of high-quality and\ndiverse images based on user instructions. Although the synthesis performance\nis fascinating, one significant limitation of current image generation models\nis their insufficient ability to generate coherent text within images,\nparticularly for complex glyph structures like Chinese characters. To address\nthis problem, we introduce GlyphDraw, a general learning framework aiming at\nendowing image generation models with the capacity to generate images embedded\nwith coherent text. To the best of our knowledge, this is the first work in the\nfield of image synthesis to address the generation of Chinese characters. % we\nfirst adopt the OCR technique to collect images with Chinese characters as\ntraining samples, and extract the text and locations as auxiliary information.\nWe first sophisticatedly design the image-text dataset's construction strategy,\nthen build our model specifically on a diffusion-based image generator and\ncarefully modify the network structure to allow the model to learn drawing\nChinese characters with the help of glyph and position information.\nFurthermore, we maintain the model's open-domain image synthesis capability by\npreventing catastrophic forgetting by using a variety of training techniques.\nExtensive qualitative and quantitative experiments demonstrate that our method\nnot only produces accurate Chinese characters as in prompts, but also naturally\nblends the generated text into the background. Please refer to\nhttps://1073521013.github.io/glyph-draw.github.io\n","authors":["Jian Ma","Mingjun Zhao","Chen Chen","Ruichen Wang","Di Niu","Haonan Lu","Xiaodong Lin"],"pdf_url":"https://arxiv.org/pdf/2303.17870v1.pdf","comment":"24 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.06859v2","updated":"2023-03-31T08:02:01Z","published":"2023-03-13T05:04:18Z","title":"Learning Distortion Invariant Representation for Image Restoration from\n  A Causality Perspective","summary":"  In recent years, we have witnessed the great advancement of Deep neural\nnetworks (DNNs) in image restoration. However, a critical limitation is that\nthey cannot generalize well to real-world degradations with different degrees\nor types. In this paper, we are the first to propose a novel training strategy\nfor image restoration from the causality perspective, to improve the\ngeneralization ability of DNNs for unknown degradations. Our method, termed\nDistortion Invariant representation Learning (DIL), treats each distortion type\nand degree as one specific confounder, and learns the distortion-invariant\nrepresentation by eliminating the harmful confounding effect of each\ndegradation. We derive our DIL with the back-door criterion in causality by\nmodeling the interventions of different distortions from the optimization\nperspective. Particularly, we introduce counterfactual distortion augmentation\nto simulate the virtual distortion types and degrees as the confounders. Then,\nwe instantiate the intervention of each distortion with a virtual model\nupdating based on corresponding distorted images, and eliminate them from the\nmeta-learning perspective. Extensive experiments demonstrate the effectiveness\nof our DIL on the generalization capability for unseen distortion types and\ndegrees. Our code will be available at\nhttps://github.com/lixinustc/Causal-IR-DIL.\n","authors":["Xin Li","Bingchen Li","Xin Jin","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06859v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.17867v1","updated":"2023-03-31T08:01:21Z","published":"2023-03-31T08:01:21Z","title":"CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer","summary":"  Content affinity loss including feature and pixel affinity is a main problem\nwhich leads to artifacts in photorealistic and video style transfer. This paper\nproposes a new framework named CAP-VSTNet, which consists of a new reversible\nresidual network and an unbiased linear transform module, for versatile style\ntransfer. This reversible residual network can not only preserve content\naffinity but not introduce redundant information as traditional reversible\nnetworks, and hence facilitate better stylization. Empowered by Matting\nLaplacian training loss which can address the pixel affinity loss problem led\nby the linear transform, the proposed framework is applicable and effective on\nversatile style transfer. Extensive experiments show that CAP-VSTNet can\nproduce better qualitative and quantitative results in comparison with the\nstate-of-the-art methods.\n","authors":["Linfeng Wen","Chengying Gao","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2303.17867v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15999v3","updated":"2023-03-31T07:54:40Z","published":"2023-03-28T14:15:13Z","title":"Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised\n  Regression Deep Learning Models","summary":"  In this work, the authors develop regression approaches based on deep\nlearning to perform thread density estimation for plain weave canvas analysis.\nPrevious approaches were based on Fourier analysis, which is quite robust for\nsome scenarios but fails in some others, in machine learning tools, that\ninvolve pre-labeling of the painting at hand, or the segmentation of thread\ncrossing points, that provides good estimations in all scenarios with no need\nof pre-labeling. The segmentation approach is time-consuming as the estimation\nof the densities is performed after locating the crossing points. In this novel\nproposal, we avoid this step by computing the density of threads directly from\nthe image with a regression deep learning model. We also incorporate some\nimprovements in the initial preprocessing of the input image with an impact on\nthe final error. Several models are proposed and analyzed to retain the best\none. Furthermore, we further reduce the density estimation error by introducing\na semi-supervised approach. The performance of our novel algorithm is analyzed\nwith works by Ribera, Vel\\'azquez, and Poussin where we compare our results to\nthe ones of previous approaches. Finally, the method is put into practice to\nsupport the change of authorship or a masterpiece at the Museo del Prado.\n","authors":["A. D. Bejarano","Juan J. Murillo-Fuentes","Laura Alba-Carcelen"],"pdf_url":"https://arxiv.org/pdf/2303.15999v3.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2303.17859v1","updated":"2023-03-31T07:39:12Z","published":"2023-03-31T07:39:12Z","title":"MapFormer: Boosting Change Detection by Using Pre-change Information","summary":"  Change detection in remote sensing imagery is essential for a variety of\napplications such as urban planning, disaster management, and climate research.\nHowever, existing methods for identifying semantically changed areas overlook\nthe availability of semantic information in the form of existing maps\ndescribing features of the earth's surface. In this paper, we leverage this\ninformation for change detection in bi-temporal images. We show that the simple\nintegration of the additional information via concatenation of latent\nrepresentations suffices to significantly outperform state-of-the-art change\ndetection methods. Motivated by this observation, we propose the new task of\nConditional Change Detection, where pre-change semantic information is used as\ninput next to bi-temporal images. To fully exploit the extra information, we\npropose MapFormer, a novel architecture based on a multi-modal feature fusion\nmodule that allows for feature processing conditioned on the available semantic\ninformation. We further employ a supervised, cross-modal contrastive loss to\nguide the learning of visual representations. Our approach outperforms existing\nchange detection methods by an absolute 11.7% and 18.4% in terms of binary\nchange IoU on DynamicEarthNet and HRSCD, respectively. Furthermore, we\ndemonstrate the robustness of our approach to the quality of the pre-change\nsemantic information and the absence pre-change imagery. The code will be made\npublicly available.\n","authors":["Maximilian Bernhard","Niklas Strauß","Matthias Schubert"],"pdf_url":"https://arxiv.org/pdf/2303.17859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13241v2","updated":"2023-03-31T07:30:23Z","published":"2023-03-23T13:18:05Z","title":"6D Object Pose Estimation from Approximate 3D Models for Orbital\n  Robotics","summary":"  We present a novel technique to estimate the 6D pose of objects from single\nimages where the 3D geometry of the object is only given approximately and not\nas a precise 3D model. To achieve this, we employ a dense 2D-to-3D\ncorrespondence predictor that regresses 3D model coordinates for every pixel.\nIn addition to the 3D coordinates, our model also estimates the pixel-wise\ncoordinate error to discard correspondences that are likely wrong. This allows\nus to generate multiple 6D pose hypotheses of the object, which we then refine\niteratively using a highly efficient region-based approach. We also introduce a\nnovel pixel-wise posterior formulation by which we can estimate the probability\nfor each hypothesis and select the most likely one. As we show in experiments,\nour approach is capable of dealing with extreme visual conditions including\noverexposure, high contrast, or low signal-to-noise ratio. This makes it a\npowerful technique for the particularly challenging task of estimating the pose\nof tumbling satellites for in-orbit robotic applications. Our method achieves\nstate-of-the-art performance on the SPEED+ dataset and has won the SPEC2021\npost-mortem competition.\n","authors":["Maximilian Ulmer","Maximilian Durner","Martin Sundermeyer","Manuel Stoiber","Rudolph Triebel"],"pdf_url":"https://arxiv.org/pdf/2303.13241v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2303.17852v1","updated":"2023-03-31T07:29:36Z","published":"2023-03-31T07:29:36Z","title":"Maximum Covariance Unfolding Regression: A Novel Covariate-based\n  Manifold Learning Approach for Point Cloud Data","summary":"  Point cloud data are widely used in manufacturing applications for process\ninspection, modeling, monitoring and optimization. The state-of-art tensor\nregression techniques have effectively been used for analysis of structured\npoint cloud data, where the measurements on a uniform grid can be formed into a\ntensor. However, these techniques are not capable of handling unstructured\npoint cloud data that are often in the form of manifolds. In this paper, we\npropose a nonlinear dimension reduction approach named Maximum Covariance\nUnfolding Regression that is able to learn the low-dimensional (LD) manifold of\npoint clouds with the highest correlation with explanatory covariates. This LD\nmanifold is then used for regression modeling and process optimization based on\nprocess variables. The performance of the proposed method is subsequently\nevaluated and compared with benchmark methods through simulations and a case\nstudy of steel bracket manufacturing.\n","authors":["Qian Wang","Kamran Paynabar"],"pdf_url":"https://arxiv.org/pdf/2303.17852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10551v2","updated":"2023-03-31T07:27:41Z","published":"2022-11-19T01:01:53Z","title":"A Practical Stereo Depth System for Smart Glasses","summary":"  We present the design of a productionized end-to-end stereo depth sensing\nsystem that does pre-processing, online stereo rectification, and stereo depth\nestimation with a fallback to monocular depth estimation when rectification is\nunreliable. The output of our depth sensing system is then used in a novel view\ngeneration pipeline to create 3D computational photography effects using\npoint-of-view images captured by smart glasses. All these steps are executed\non-device on the stringent compute budget of a mobile phone, and because we\nexpect the users can use a wide range of smartphones, our design needs to be\ngeneral and cannot be dependent on a particular hardware or ML accelerator such\nas a smartphone GPU. Although each of these steps is well studied, a\ndescription of a practical system is still lacking. For such a system, all\nthese steps need to work in tandem with one another and fallback gracefully on\nfailures within the system or less than ideal input data. We show how we handle\nunforeseen changes to calibration, e.g., due to heat, robustly support depth\nestimation in the wild, and still abide by the memory and latency constraints\nrequired for a smooth user experience. We show that our trained models are\nfast, and run in less than 1s on a six-year-old Samsung Galaxy S8 phone's CPU.\nOur models generalize well to unseen data and achieve good results on\nMiddlebury and in-the-wild images captured from the smart glasses.\n","authors":["Jialiang Wang","Daniel Scharstein","Akash Bapat","Kevin Blackburn-Matzen","Matthew Yu","Jonathan Lehman","Suhib Alsisan","Yanghan Wang","Sam Tsai","Jan-Michael Frahm","Zijian He","Peter Vajda","Michael F. Cohen","Matt Uyttendaele"],"pdf_url":"https://arxiv.org/pdf/2211.10551v2.pdf","comment":"Accepted at CVPR2023"},{"id":"http://arxiv.org/abs/2303.17845v1","updated":"2023-03-31T07:12:58Z","published":"2023-03-31T07:12:58Z","title":"WSense: A Robust Feature Learning Module for Lightweight Human Activity\n  Recognition","summary":"  In recent times, various modules such as squeeze-and-excitation, and others\nhave been proposed to improve the quality of features learned from wearable\nsensor signals. However, these modules often cause the number of parameters to\nbe large, which is not suitable for building lightweight human activity\nrecognition models which can be easily deployed on end devices. In this\nresearch, we propose a feature learning module, termed WSense, which uses two\n1D CNN and global max pooling layers to extract similar quality features from\nwearable sensor data while ignoring the difference in activity recognition\nmodels caused by the size of the sliding window. Experiments were carried out\nusing CNN and ConvLSTM feature learning pipelines on a dataset obtained with a\nsingle accelerometer (WISDM) and another obtained using the fusion of\naccelerometers, gyroscopes, and magnetometers (PAMAP2) under various sliding\nwindow sizes. A total of nine hundred sixty (960) experiments were conducted to\nvalidate the WSense module against baselines and existing methods on the two\ndatasets. The results showed that the WSense module aided pipelines in learning\nsimilar quality features and outperformed the baselines and existing models\nwith a minimal and uniform model size across all sliding window segmentations.\nThe code is available at https://github.com/AOige/WSense.\n","authors":["Ayokunle Olalekan Ige","Mohd Halim Mohd Noor"],"pdf_url":"https://arxiv.org/pdf/2303.17845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17842v1","updated":"2023-03-31T07:07:29Z","published":"2023-03-31T07:07:29Z","title":"Shepherding Slots to Objects: Towards Stable and Robust Object-Centric\n  Learning","summary":"  Object-centric learning (OCL) aspires general and compositional understanding\nof scenes by representing a scene as a collection of object-centric\nrepresentations. OCL has also been extended to multi-view image and video\ndatasets to apply various data-driven inductive biases by utilizing geometric\nor temporal information in the multi-image data. Single-view images carry less\ninformation about how to disentangle a given scene than videos or multi-view\nimages do. Hence, owing to the difficulty of applying inductive biases, OCL for\nsingle-view images remains challenging, resulting in inconsistent learning of\nobject-centric representation. To this end, we introduce a novel OCL framework\nfor single-view images, SLot Attention via SHepherding (SLASH), which consists\nof two simple-yet-effective modules on top of Slot Attention. The new modules,\nAttention Refining Kernel (ARK) and Intermediate Point Predictor and Encoder\n(IPPE), respectively, prevent slots from being distracted by the background\nnoise and indicate locations for slots to focus on to facilitate learning of\nobject-centric representation. We also propose a weak semi-supervision approach\nfor OCL, whilst our proposed framework can be used without any assistant\nannotation during the inference. Experiments show that our proposed method\nenables consistent learning of object-centric representation and achieves\nstrong performance across four datasets. Code is available at\n\\url{https://github.com/object-understanding/SLASH}.\n","authors":["Jinwoo Kim","Janghyuk Choi","Ho-Jin Choi","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2303.17842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12429v2","updated":"2023-03-31T07:05:35Z","published":"2023-01-29T11:53:55Z","title":"Debiased Fine-Tuning for Vision-language Models by Prompt Regularization","summary":"  We present a new paradigm for fine-tuning large-scale visionlanguage\npre-trained models on downstream task, dubbed Prompt Regularization (ProReg).\nDifferent from traditional fine-tuning which easily overfits to the downstream\ntask data, ProReg uses the prediction by prompting the pretrained model to\nregularize the fine-tuning. The motivation is: by prompting the large model \"a\nphoto of a [CLASS]\", the fil-lin answer is only dependent on the pretraining\nencyclopedic knowledge while independent of the task data distribution, which\nis usually biased. Specifically, given a training sample prediction during\nfine-tuning, we first calculate its KullbackLeibler loss of the prompt\nprediction and Cross-Entropy loss of the ground-truth label, and then combine\nthem with a proposed sample-wise adaptive trade-off weight, which automatically\nadjusts the transfer between the pretrained and downstream domains. On various\nout-of-distribution benchmarks, we show the consistently strong performance of\nProReg compared with conventional fine-tuning, zero-shot prompt, prompt tuning,\nand other state-of-the-art methods.\n","authors":["Beier Zhu","Yulei Niu","Saeil Lee","Minhoe Hur","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.12429v2.pdf","comment":"AAAI2023 accepted"},{"id":"http://arxiv.org/abs/2207.13378v2","updated":"2023-03-31T07:03:13Z","published":"2022-07-27T09:03:03Z","title":"Identifying Hard Noise in Long-Tailed Sample Distribution","summary":"  Conventional de-noising methods rely on the assumption that all samples are\nindependent and identically distributed, so the resultant classifier, though\ndisturbed by noise, can still easily identify the noises as the outliers of\ntraining distribution. However, the assumption is unrealistic in large-scale\ndata that is inevitably long-tailed. Such imbalanced training data makes a\nclassifier less discriminative for the tail classes, whose previously \"easy\"\nnoises are now turned into \"hard\" ones -- they are almost as outliers as the\nclean tail samples. We introduce this new challenge as Noisy Long-Tailed\nClassification (NLT). Not surprisingly, we find that most de-noising methods\nfail to identify the hard noises, resulting in significant performance drop on\nthe three proposed NLT benchmarks: ImageNet-NLT, Animal10-NLT, and Food101-NLT.\nTo this end, we design an iterative noisy learning framework called\nHard-to-Easy (H2E). Our bootstrapping philosophy is to first learn a classifier\nas noise identifier invariant to the class and context distributional changes,\nreducing \"hard\" noises to \"easy\" ones, whose removal further improves the\ninvariance. Experimental results show that our H2E outperforms state-of-the-art\nde-noising methods and their ablations on long-tailed settings while\nmaintaining a stable performance on the conventional balanced settings.\nDatasets and codes are available at https://github.com/yxymessi/H2E-Framework\n","authors":["Xuanyu Yi","Kaihua Tang","Xian-Sheng Hua","Joo-Hwee Lim","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.13378v2.pdf","comment":"Accepted to ECCV2022(Oral) ; Datasets and codes are available at\n  https://github.com/yxymessi/H2E-Framework"},{"id":"http://arxiv.org/abs/2303.17839v1","updated":"2023-03-31T07:02:26Z","published":"2023-03-31T07:02:26Z","title":"Learning Procedure-aware Video Representation from Instructional Videos\n  and Their Narrations","summary":"  The abundance of instructional videos and their narrations over the Internet\noffers an exciting avenue for understanding procedural activities. In this\nwork, we propose to learn video representation that encodes both action steps\nand their temporal ordering, based on a large-scale dataset of web\ninstructional videos and their narrations, without using human annotations. Our\nmethod jointly learns a video representation to encode individual step\nconcepts, and a deep probabilistic model to capture both temporal dependencies\nand immense individual variations in the step ordering. We empirically\ndemonstrate that learning temporal ordering not only enables new capabilities\nfor procedure reasoning, but also reinforces the recognition of individual\nsteps. Our model significantly advances the state-of-the-art results on step\nclassification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting\n(+7.4% on COIN). Moreover, our model attains promising results in zero-shot\ninference for step classification and forecasting, as well as in predicting\ndiverse and plausible steps for incomplete procedures. Our code is available at\nhttps://github.com/facebookresearch/ProcedureVRL.\n","authors":["Yiwu Zhong","Licheng Yu","Yang Bai","Shangwen Li","Xueting Yan","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2303.17839v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17836v1","updated":"2023-03-31T06:58:45Z","published":"2023-03-31T06:58:45Z","title":"Rethinking interpretation: Input-agnostic saliency mapping of deep\n  visual classifiers","summary":"  Saliency methods provide post-hoc model interpretation by attributing input\nfeatures to the model outputs. Current methods mainly achieve this using a\nsingle input sample, thereby failing to answer input-independent inquiries\nabout the model. We also show that input-specific saliency mapping is\nintrinsically susceptible to misleading feature attribution. Current attempts\nto use 'general' input features for model interpretation assume access to a\ndataset containing those features, which biases the interpretation. Addressing\nthe gap, we introduce a new perspective of input-agnostic saliency mapping that\ncomputationally estimates the high-level features attributed by the model to\nits outputs. These features are geometrically correlated, and are computed by\naccumulating model's gradient information with respect to an unrestricted data\ndistribution. To compute these features, we nudge independent data points over\nthe model loss surface towards the local minima associated by a\nhuman-understandable concept, e.g., class label for classifiers. With a\nsystematic projection, scaling and refinement process, this information is\ntransformed into an interpretable visualization without compromising its\nmodel-fidelity. The visualization serves as a stand-alone qualitative\ninterpretation. With an extensive evaluation, we not only demonstrate\nsuccessful visualizations for a variety of concepts for large-scale models, but\nalso showcase an interesting utility of this new form of saliency mapping by\nidentifying backdoor signatures in compromised classifiers.\n","authors":["Naveed Akhtar","Mohammad A. A. K. Jalwana"],"pdf_url":"https://arxiv.org/pdf/2303.17836v1.pdf","comment":"Accepted for publication in AAAI 2023"},{"id":"http://arxiv.org/abs/2303.17835v1","updated":"2023-03-31T06:57:34Z","published":"2023-03-31T06:57:34Z","title":"Improved Difference Images for Change Detection Classifiers in SAR\n  Imagery Using Deep Learning","summary":"  Satellite-based Synthetic Aperture Radar (SAR) images can be used as a source\nof remote sensed imagery regardless of cloud cover and day-night cycle.\nHowever, the speckle noise and varying image acquisition conditions pose a\nchallenge for change detection classifiers. This paper proposes a new method of\nimproving SAR image processing to produce higher quality difference images for\nthe classification algorithms. The method is built on a neural network-based\nmapping transformation function that produces artificial SAR images from a\nlocation in the requested acquisition conditions. The inputs for the model are:\nprevious SAR images from the location, imaging angle information from the SAR\nimages, digital elevation model, and weather conditions. The method was tested\nwith data from a location in North-East Finland by using Sentinel-1 SAR images\nfrom European Space Agency, weather data from Finnish Meteorological Institute,\nand a digital elevation model from National Land Survey of Finland. In order to\nverify the method, changes to the SAR images were simulated, and the\nperformance of the proposed method was measured using experimentation where it\ngave substantial improvements to performance when compared to a more\nconventional method of creating difference images.\n","authors":["Janne Alatalo","Tuomo Sipola","Mika Rantonen"],"pdf_url":"https://arxiv.org/pdf/2303.17835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10229v2","updated":"2023-03-31T06:55:37Z","published":"2022-12-20T13:07:20Z","title":"StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for\n  One-shot and Few-shot Domain Adaptation","summary":"  Domain adaptation of GANs is a problem of fine-tuning the state-of-the-art\nGAN models (e.g. StyleGAN) pretrained on a large dataset to a specific domain\nwith few samples (e.g. painting faces, sketches, etc.). While there are a great\nnumber of methods that tackle this problem in different ways, there are still\nmany important questions that remain unanswered.\n  In this paper, we provide a systematic and in-depth analysis of the domain\nadaptation problem of GANs, focusing on the StyleGAN model. First, we perform a\ndetailed exploration of the most important parts of StyleGAN that are\nresponsible for adapting the generator to a new domain depending on the\nsimilarity between the source and target domains. As a result of this in-depth\nstudy, we propose new efficient and lightweight parameterizations of StyleGAN\nfor domain adaptation. Particularly, we show there exist directions in\nStyleSpace (StyleDomain directions) that are sufficient for adapting to similar\ndomains and they can be reduced further. For dissimilar domains, we propose\nAffine$+$ and AffineLight$+$ parameterizations that allows us to outperform\nexisting baselines in few-shot adaptation with low data regime. Finally, we\nexamine StyleDomain directions and discover their many surprising properties\nthat we apply for domain mixing and cross-domain image morphing.\n","authors":["Aibek Alanov","Vadim Titov","Maksim Nakhodnov","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2212.10229v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2202.10324v3","updated":"2023-03-31T06:41:29Z","published":"2022-02-17T09:51:32Z","title":"VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning","summary":"  We propose VRL3, a powerful data-driven framework with a simple design for\nsolving challenging visual deep reinforcement learning (DRL) tasks. We analyze\na number of major obstacles in taking a data-driven approach, and present a\nsuite of design principles, novel findings, and critical insights about\ndata-driven visual DRL. Our framework has three stages: in stage 1, we leverage\nnon-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations;\nin stage 2, we use offline RL data (e.g. a limited number of expert\ndemonstrations) to convert the task-agnostic representations into more powerful\ntask-specific representations; in stage 3, we fine-tune the agent with online\nRL. On a set of challenging hand manipulation tasks with sparse reward and\nrealistic visual inputs, compared to the previous SOTA, VRL3 achieves an\naverage of 780% better sample efficiency. And on the hardest task, VRL3 is\n1220% more sample efficient (2440% when using a wider encoder) and solves the\ntask with only 10% of the computation. These significant results clearly\ndemonstrate the great potential of data-driven deep reinforcement learning.\n","authors":["Che Wang","Xufang Luo","Keith Ross","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2202.10324v3.pdf","comment":"41 pages, camera-ready final version, accepted to NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.17815v1","updated":"2023-03-31T06:11:02Z","published":"2023-03-31T06:11:02Z","title":"APPT : Asymmetric Parallel Point Transformer for 3D Point Cloud\n  Understanding","summary":"  Transformer-based networks have achieved impressive performance in 3D point\ncloud understanding. However, most of them concentrate on aggregating local\nfeatures, but neglect to directly model global dependencies, which results in a\nlimited effective receptive field. Besides, how to effectively incorporate\nlocal and global components also remains challenging. To tackle these problems,\nwe propose Asymmetric Parallel Point Transformer (APPT). Specifically, we\nintroduce Global Pivot Attention to extract global features and enlarge the\neffective receptive field. Moreover, we design the Asymmetric Parallel\nstructure to effectively integrate local and global information. Combined with\nthese designs, APPT is able to capture features globally throughout the entire\nnetwork while focusing on local-detailed features. Extensive experiments show\nthat our method outperforms the priors and achieves state-of-the-art on several\nbenchmarks for 3D point cloud understanding, such as 3D semantic segmentation\non S3DIS, 3D shape classification on ModelNet40, and 3D part segmentation on\nShapeNet.\n","authors":["Hengjia Li","Tu Zheng","Zhihao Chi","Zheng Yang","Wenxiao Wang","Boxi Wu","Binbin Lin","Deng Cai"],"pdf_url":"https://arxiv.org/pdf/2303.17815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17811v1","updated":"2023-03-31T06:00:50Z","published":"2023-03-31T06:00:50Z","title":"Zero-shot Referring Image Segmentation with Global-Local Context\n  Features","summary":"  Referring image segmentation (RIS) aims to find a segmentation mask given a\nreferring expression grounded to a region of the input image. Collecting\nlabelled datasets for this task, however, is notoriously costly and\nlabor-intensive. To overcome this issue, we propose a simple yet effective\nzero-shot referring image segmentation method by leveraging the pre-trained\ncross-modal knowledge from CLIP. In order to obtain segmentation masks grounded\nto the input text, we propose a mask-guided visual encoder that captures global\nand local contextual information of an input image. By utilizing instance masks\nobtained from off-the-shelf mask proposal techniques, our method is able to\nsegment fine-detailed Istance-level groundings. We also introduce a\nglobal-local text encoder where the global feature captures complex\nsentence-level semantics of the entire input expression while the local feature\nfocuses on the target noun phrase extracted by a dependency parser. In our\nexperiments, the proposed method outperforms several zero-shot baselines of the\ntask and even the weakly supervised referring expression segmentation method\nwith substantial margins. Our code is available at\nhttps://github.com/Seonghoon-Yu/Zero-shot-RIS.\n","authors":["Seonghoon Yu","Paul Hongsuch Seo","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2303.17811v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2208.03462v2","updated":"2023-03-31T05:56:37Z","published":"2022-08-06T08:09:54Z","title":"Class Is Invariant to Context and Vice Versa: On Learning Invariance for\n  Out-Of-Distribution Generalization","summary":"  Out-Of-Distribution generalization (OOD) is all about learning invariance\nagainst environmental changes. If the context in every class is evenly\ndistributed, OOD would be trivial because the context can be easily removed due\nto an underlying principle: class is invariant to context. However, collecting\nsuch a balanced dataset is impractical. Learning on imbalanced data makes the\nmodel bias to context and thus hurts OOD. Therefore, the key to OOD is context\nbalance. We argue that the widely adopted assumption in prior work, the context\nbias can be directly annotated or estimated from biased class prediction,\nrenders the context incomplete or even incorrect. In contrast, we point out the\neveroverlooked other side of the above principle: context is also invariant to\nclass, which motivates us to consider the classes (which are already labeled)\nas the varying environments to resolve context bias (without context labels).\nWe implement this idea by minimizing the contrastive loss of intra-class sample\nsimilarity while assuring this similarity to be invariant across all classes.\nOn benchmarks with various context biases and domain gaps, we show that a\nsimple re-weighting based classifier equipped with our context estimation\nachieves state-of-the-art performance. We provide the theoretical\njustifications in Appendix and codes on\nhttps://github.com/simpleshinobu/IRMCon.\n","authors":["Jiaxin Qi","Kaihua Tang","Qianru Sun","Xian-Sheng Hua","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2208.03462v2.pdf","comment":"Accepted by ECCV 2022"},{"id":"http://arxiv.org/abs/2303.17137v2","updated":"2023-03-31T05:44:48Z","published":"2023-03-30T04:01:48Z","title":"Online Camera-to-ground Calibration for Autonomous Driving","summary":"  Online camera-to-ground calibration is to generate a non-rigid body\ntransformation between the camera and the road surface in a real-time manner.\nExisting solutions utilize static calibration, suffering from environmental\nvariations such as tire pressure changes, vehicle loading volume variations,\nand road surface diversity. Other online solutions exploit the usage of road\nelements or photometric consistency between overlapping views across images,\nwhich require continuous detection of specific targets on the road or\nassistance with multiple cameras to facilitate calibration. In our work, we\npropose an online monocular camera-to-ground calibration solution that does not\nutilize any specific targets while driving. We perform a coarse-to-fine\napproach for ground feature extraction through wheel odometry and estimate the\ncamera-to-ground calibration parameters through a sliding-window-based factor\ngraph optimization. Considering the non-rigid transformation of\ncamera-to-ground while driving, we provide metrics to quantify calibration\nperformance and stopping criteria to report/broadcast our satisfying\ncalibration results. Extensive experiments using real-world data demonstrate\nthat our algorithm is effective and outperforms state-of-the-art techniques.\n","authors":["Binbin Li","Xinyu Du","Yao Hu","Hao Yu","Wende Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.17137v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17806v1","updated":"2023-03-31T05:38:13Z","published":"2023-03-31T05:38:13Z","title":"Neural Microfacet Fields for Inverse Rendering","summary":"  We present Neural Microfacet Fields, a method for recovering materials,\ngeometry, and environment illumination from images of a scene. Our method uses\na microfacet reflectance model within a volumetric setting by treating each\nsample along the ray as a (potentially non-opaque) surface. Using surface-based\nMonte Carlo rendering in a volumetric setting enables our method to perform\ninverse rendering efficiently by combining decades of research in surface-based\nlight transport with recent advances in volume rendering for view synthesis.\nOur approach outperforms prior work in inverse rendering, capturing high\nfidelity geometry and high frequency illumination details; its novel view\nsynthesis results are on par with state-of-the-art methods that do not recover\nillumination or materials.\n","authors":["Alexander Mai","Dor Verbin","Falko Kuester","Sara Fridovich-Keil"],"pdf_url":"https://arxiv.org/pdf/2303.17806v1.pdf","comment":"Project page: https://half-potato.gitlab.io/posts/nmf/"},{"id":"http://arxiv.org/abs/2303.17803v1","updated":"2023-03-31T05:25:32Z","published":"2023-03-31T05:25:32Z","title":"Rethinking Local Perception in Lightweight Vision Transformer","summary":"  Vision Transformers (ViTs) have been shown to be effective in various vision\ntasks. However, resizing them to a mobile-friendly size leads to significant\nperformance degradation. Therefore, developing lightweight vision transformers\nhas become a crucial area of research. This paper introduces CloFormer, a\nlightweight vision transformer that leverages context-aware local enhancement.\nCloFormer explores the relationship between globally shared weights often used\nin vanilla convolutional operators and token-specific context-aware weights\nappearing in attention, then proposes an effective and straightforward module\nto capture high-frequency local information. In CloFormer, we introduce\nAttnConv, a convolution operator in attention's style. The proposed AttnConv\nuses shared weights to aggregate local information and deploys carefully\ndesigned context-aware weights to enhance local features. The combination of\nthe AttnConv and vanilla attention which uses pooling to reduce FLOPs in\nCloFormer enables the model to perceive high-frequency and low-frequency\ninformation. Extensive experiments were conducted in image classification,\nobject detection, and semantic segmentation, demonstrating the superiority of\nCloFormer.\n","authors":["Qihang Fan","Huaibo Huang","Jiyang Guan","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.17803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17802v1","updated":"2023-03-31T05:22:56Z","published":"2023-03-31T05:22:56Z","title":"Time-series Anomaly Detection based on Difference Subspace between\n  Signal Subspaces","summary":"  This paper proposes a new method for anomaly detection in time-series data by\nincorporating the concept of difference subspace into the singular spectrum\nanalysis (SSA). The key idea is to monitor slight temporal variations of the\ndifference subspace between two signal subspaces corresponding to the past and\npresent time-series data, as anomaly score. It is a natural generalization of\nthe conventional SSA-based method which measures the minimum angle between the\ntwo signal subspaces as the degree of changes. By replacing the minimum angle\nwith the difference subspace, our method boosts the performance while using the\nSSA-based framework as it can capture the whole structural difference between\nthe two subspaces in its magnitude and direction. We demonstrate our method's\neffectiveness through performance evaluations on public time-series datasets.\n","authors":["Takumi Kanai","Naoya Sogi","Atsuto Maki","Kazuhiro Fukui"],"pdf_url":"https://arxiv.org/pdf/2303.17802v1.pdf","comment":"8pages"},{"id":"http://arxiv.org/abs/2212.04196v2","updated":"2023-03-31T04:12:25Z","published":"2022-12-08T11:23:24Z","title":"Learning Domain Invariant Prompt for Vision-Language Models","summary":"  Prompt learning is one of the most effective and trending ways to adapt\npowerful vision-language foundation models like CLIP to downstream datasets by\ntuning learnable prompt vectors with very few samples. However, although prompt\nlearning achieves excellent performance over in-domain data, it still faces the\nmajor challenge of generalizing to unseen classes and domains. Some existing\nprompt learning methods tackle this issue by adaptively generating different\nprompts for different tokens or domains but neglecting the ability of learned\nprompts to generalize to unseen domains. In this paper, we propose a novel\nprompt learning paradigm that directly generates \\emph{domain invariant} prompt\nthat can be generalized to unseen domains, called MetaPrompt. Specifically, a\ndual-modality prompt tuning network is proposed to generate prompts for input\nfrom both image and text modalities. With a novel asymmetric contrastive loss,\nthe representation from the original pre-trained vision-language model acts as\nsupervision to enhance the generalization ability of the learned prompt. More\nimportantly, we propose a meta-learning-based prompt tuning algorithm that\nexplicitly constrains the task-specific prompt tuned for one domain or class to\nalso achieve good performance in another domain or class. Extensive experiments\non 11 datasets for base-to-new generalization and 4 datasets for domain\ngeneralization demonstrate that our method consistently and significantly\noutperforms existing methods.\n","authors":["Cairong Zhao","Yubin Wang","Xinyang Jiang","Yifei Shen","Kaitao Song","Dongsheng Li","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2212.04196v2.pdf","comment":"12 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2212.00338v3","updated":"2023-03-31T03:49:07Z","published":"2022-12-01T07:55:56Z","title":"3D-Aware Object Goal Navigation via Simultaneous Exploration and\n  Identification","summary":"  Object goal navigation (ObjectNav) in unseen environments is a fundamental\ntask for Embodied AI. Agents in existing works learn ObjectNav policies based\non 2D maps, scene graphs, or image sequences. Considering this task happens in\n3D space, a 3D-aware agent can advance its ObjectNav capability via learning\nfrom fine-grained spatial information. However, leveraging 3D scene\nrepresentation can be prohibitively unpractical for policy learning in this\nfloor-level task, due to low sample efficiency and expensive computational\ncost. In this work, we propose a framework for the challenging 3D-aware\nObjectNav based on two straightforward sub-policies. The two sub-polices,\nnamely corner-guided exploration policy and category-aware identification\npolicy, simultaneously perform by utilizing online fused 3D points as\nobservation. Through extensive experiments, we show that this framework can\ndramatically improve the performance in ObjectNav through learning from 3D\nscene representation. Our framework achieves the best performance among all\nmodular-based methods on the Matterport3D and Gibson datasets, while requiring\n(up to 30x) less computational cost for training.\n","authors":["Jiazhao Zhang","Liu Dai","Fanpeng Meng","Qingnan Fan","Xuelin Chen","Kai Xu","He Wang"],"pdf_url":"https://arxiv.org/pdf/2212.00338v3.pdf","comment":"To appear in CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17789v1","updated":"2023-03-31T03:25:06Z","published":"2023-03-31T03:25:06Z","title":"FONT: Flow-guided One-shot Talking Head Generation with Natural Head\n  Motions","summary":"  One-shot talking head generation has received growing attention in recent\nyears, with various creative and practical applications. An ideal natural and\nvivid generated talking head video should contain natural head pose changes.\nHowever, it is challenging to map head pose sequences from driving audio since\nthere exists a natural gap between audio-visual modalities. In this work, we\npropose a Flow-guided One-shot model that achieves NaTural head motions(FONT)\nover generated talking heads. Specifically, the head pose prediction module is\ndesigned to generate head pose sequences from the source face and driving\naudio. We add the random sampling operation and the structural similarity\nconstraint to model the diversity in the one-to-many mapping between\naudio-visual modality, thus predicting natural head poses. Then we develop a\nkeypoint predictor that produces unsupervised keypoints from the source face,\ndriving audio and pose sequences to describe the facial structure information.\nFinally, a flow-guided occlusion-aware generator is employed to produce\nphoto-realistic talking head videos from the estimated keypoints and source\nface. Extensive experimental results prove that FONT generates talking heads\nwith natural head poses and synchronized mouth shapes, outperforming other\ncompared methods.\n","authors":["Jin Liu","Xi Wang","Xiaomeng Fu","Yesheng Chai","Cai Yu","Jiao Dai","Jizhong Han"],"pdf_url":"https://arxiv.org/pdf/2303.17789v1.pdf","comment":"Accepted by ICME2023"},{"id":"http://arxiv.org/abs/2303.17783v1","updated":"2023-03-31T03:14:44Z","published":"2023-03-31T03:14:44Z","title":"SOSR: Source-Free Image Super-Resolution with Wavelet Augmentation\n  Transformer","summary":"  Real-world images taken by different cameras with different degradation\nkernels often result in a cross-device domain gap in image super-resolution. A\nprevalent attempt to this issue is unsupervised domain adaptation (UDA) that\nneeds to access source data. Considering privacy policies or transmission\nrestrictions of data in many practical applications, we propose a SOurce-free\nimage Super-Resolution framework (SOSR) to address this issue, i.e., adapt a\nmodel pre-trained on labeled source data to a target domain with only unlabeled\ntarget data. SOSR leverages the source model to generate refined pseudo-labels\nfor teacher-student learning. To better utilize the pseudo-labels, this paper\nproposes a novel wavelet-based augmentation method, named Wavelet Augmentation\nTransformer (WAT), which can be flexibly incorporated with existing networks,\nto implicitly produce useful augmented data. WAT learns low-frequency\ninformation of varying levels across diverse samples, which is aggregated\nefficiently via deformable attention. Furthermore, an uncertainty-aware\nself-training mechanism is proposed to improve the accuracy of pseudo-labels,\nwith inaccurate predictions being rectified by uncertainty estimation. To\nacquire better SR results and avoid overfitting pseudo-labels, several\nregularization losses are proposed to constrain the frequency information\nbetween target LR and SR images. Experiments show that without accessing source\ndata, SOSR achieves superior results to the state-of-the-art UDA methods.\n","authors":["Yuang Ai","Xiaoqiang Zhou","Huaibo Huang","Lei Zhang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.17783v1.pdf","comment":"15 pages, 9 figures, 10 tables"},{"id":"http://arxiv.org/abs/2303.17778v1","updated":"2023-03-31T02:50:52Z","published":"2023-03-31T02:50:52Z","title":"CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition","summary":"  We present CrossLoc3D, a novel 3D place recognition method that solves a\nlarge-scale point matching problem in a cross-source setting. Cross-source\npoint cloud data corresponds to point sets captured by depth sensors with\ndifferent accuracies or from different distances and perspectives. We address\nthe challenges in terms of developing 3D place recognition methods that account\nfor the representation gap between points captured by different sources. Our\nmethod handles cross-source data by utilizing multi-grained features and\nselecting convolution kernel sizes that correspond to most prominent features.\nInspired by the diffusion models, our method uses a novel iterative refinement\nprocess that gradually shifts the embedding spaces from different sources to a\nsingle canonical space for better metric learning. In addition, we present\nCS-Campus3D, the first 3D aerial-ground cross-source dataset consisting of\npoint cloud data from both aerial and ground LiDAR scans. The point clouds in\nCS-Campus3D have representation gaps and other features like different views,\npoint densities, and noise patterns. We show that our CrossLoc3D algorithm can\nachieve an improvement of 4.74% - 15.37% in terms of the top 1 average recall\non our CS-Campus3D benchmark and achieves performance comparable to\nstate-of-the-art 3D place recognition method on the Oxford RobotCar. We will\nrelease the code and CS-Campus3D benchmark.\n","authors":["Tianrui Guan","Aswath Muthuselvam","Montana Hoover","Xijun Wang","Jing Liang","Adarsh Jagan Sathyamoorthy","Damon Conover","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2303.17778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17776v1","updated":"2023-03-31T02:43:01Z","published":"2023-03-31T02:43:01Z","title":"Learning Internal Representations of 3D Transformations from 2D\n  Projected Inputs","summary":"  When interacting in a three dimensional world, humans must estimate 3D\nstructure from visual inputs projected down to two dimensional retinal images.\nIt has been shown that humans use the persistence of object shape over\nmotion-induced transformations as a cue to resolve depth ambiguity when solving\nthis underconstrained problem. With the aim of understanding how biological\nvision systems may internally represent 3D transformations, we propose a\ncomputational model, based on a generative manifold model, which can be used to\ninfer 3D structure from the motion of 2D points. Our model can also learn\nrepresentations of the transformations with minimal supervision, providing a\nproof of concept for how humans may develop internal representations on a\ndevelopmental or evolutionary time scale. Focused on rotational motion, we show\nhow our model infers depth from moving 2D projected points, learns 3D\nrotational transformations from 2D training stimuli, and compares to human\nperformance on psychophysical structure-from-motion experiments.\n","authors":["Marissa Connor","Bruno Olshausen","Christopher Rozell"],"pdf_url":"https://arxiv.org/pdf/2303.17776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17774v1","updated":"2023-03-31T02:37:36Z","published":"2023-03-31T02:37:36Z","title":"Semi-Weakly Supervised Object Kinematic Motion Prediction","summary":"  Given a 3D object, kinematic motion prediction aims to identify the mobile\nparts as well as the corresponding motion parameters. Due to the large\nvariations in both topological structure and geometric details of 3D objects,\nthis remains a challenging task and the lack of large scale labeled data also\nconstrain the performance of deep learning based approaches. In this paper, we\ntackle the task of object kinematic motion prediction problem in a semi-weakly\nsupervised manner. Our key observations are two-fold. First, although 3D\ndataset with fully annotated motion labels is limited, there are existing\ndatasets and methods for object part semantic segmentation at large scale.\nSecond, semantic part segmentation and mobile part segmentation is not always\nconsistent but it is possible to detect the mobile parts from the underlying 3D\nstructure. Towards this end, we propose a graph neural network to learn the map\nbetween hierarchical part-level segmentation and mobile parts parameters, which\nare further refined based on geometric alignment. This network can be first\ntrained on PartNet-Mobility dataset with fully labeled mobility information and\nthen applied on PartNet dataset with fine-grained and hierarchical part-level\nsegmentation. The network predictions yield a large scale of 3D objects with\npseudo labeled mobility information and can further be used for\nweakly-supervised learning with pre-existing segmentation. Our experiments show\nthere are significant performance boosts with the augmented data for previous\nmethod designed for kinematic motion prediction on 3D partial scans.\n","authors":["Gengxin Liu","Qian Sun","Haibin Huang","Chongyang Ma","Yulan Guo","Li Yi","Hui Huang","Ruizhen Hu"],"pdf_url":"https://arxiv.org/pdf/2303.17774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09681v2","updated":"2023-03-31T02:31:52Z","published":"2023-03-16T22:56:12Z","title":"Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer","summary":"  Event camera, as an emerging biologically-inspired vision sensor for\ncapturing motion dynamics, presents new potential for 3D human pose tracking,\nor video-based 3D human pose estimation. However, existing works in pose\ntracking either require the presence of additional gray-scale images to\nestablish a solid starting pose, or ignore the temporal dependencies all\ntogether by collapsing segments of event streams to form static event frames.\nMeanwhile, although the effectiveness of Artificial Neural Networks (ANNs,\na.k.a. dense deep learning) has been showcased in many event-based tasks, the\nuse of ANNs tends to neglect the fact that compared to the dense frame-based\nimage sequences, the occurrence of events from an event camera is\nspatiotemporally much sparser. Motivated by the above mentioned issues, we\npresent in this paper a dedicated end-to-end sparse deep learning approach for\nevent-based pose tracking: 1) to our knowledge this is the first time that 3D\nhuman pose tracking is obtained from events only, thus eliminating the need of\naccessing to any frame-based images as part of input; 2) our approach is based\nentirely upon the framework of Spiking Neural Networks (SNNs), which consists\nof Spike-Element-Wise (SEW) ResNet and a novel Spiking Spatiotemporal\nTransformer; 3) a large-scale synthetic dataset is constructed that features a\nbroad and diverse set of annotated 3D human motions, as well as longer hours of\nevent stream data, named SynEventHPD. Empirical experiments demonstrate that,\nwith superior performance over the state-of-the-art (SOTA) ANNs counterparts,\nour approach also achieves a significant computation reduction of 80% in FLOPS.\nFurthermore, our proposed method also outperforms SOTA SNNs in the regression\ntask of human pose tracking. Our implementation is available at\nhttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be released\nupon paper acceptance.\n","authors":["Shihao Zou","Yuxuan Mu","Xinxin Zuo","Sen Wang","Li Cheng"],"pdf_url":"https://arxiv.org/pdf/2303.09681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16374v2","updated":"2023-03-31T02:15:49Z","published":"2022-11-29T16:54:34Z","title":"DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image\n  Diffusion for 3D Generative Model","summary":"  Recent 3D generative models have achieved remarkable performance in\nsynthesizing high resolution photorealistic images with view consistency and\ndetailed 3D shapes, but training them for diverse domains is challenging since\nit requires massive training images and their camera distribution information.\nText-guided domain adaptation methods have shown impressive performance on\nconverting the 2D generative model on one domain into the models on other\ndomains with different styles by leveraging the CLIP (Contrastive\nLanguage-Image Pre-training), rather than collecting massive datasets for those\ndomains. However, one drawback of them is that the sample diversity in the\noriginal generative model is not well-preserved in the domain-adapted\ngenerative models due to the deterministic nature of the CLIP text encoder.\nText-guided domain adaptation will be even more challenging for 3D generative\nmodels not only because of catastrophic diversity loss, but also because of\ninferior text-image correspondence and poor image quality. Here we propose\nDATID-3D, a domain adaptation method tailored for 3D generative models using\ntext-to-image diffusion models that can synthesize diverse images per text\nprompt without collecting additional images and camera information for the\ntarget domain. Unlike 3D extensions of prior text-guided domain adaptation\nmethods, our novel pipeline was able to fine-tune the state-of-the-art 3D\ngenerator of the source domain to synthesize high resolution, multi-view\nconsistent images in text-guided targeted domains without additional data,\noutperforming the existing text-guided domain adaptation methods in diversity\nand text-image correspondence. Furthermore, we propose and demonstrate diverse\n3D image manipulations such as one-shot instance-selected adaptation and\nsingle-view manipulated 3D reconstruction to fully enjoy diversity in text.\n","authors":["Gwanghyun Kim","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2211.16374v2.pdf","comment":"Accepted to CVPR 2023, Project page:\n  https://gwang-kim.github.io/datid_3d/"},{"id":"http://arxiv.org/abs/2303.17766v1","updated":"2023-03-31T02:05:45Z","published":"2023-03-31T02:05:45Z","title":"Joint Depth Estimation and Mixture of Rain Removal From a Single Image","summary":"  Rainy weather significantly deteriorates the visibility of scene objects,\nparticularly when images are captured through outdoor camera lenses or\nwindshields. Through careful observation of numerous rainy photos, we have\nfound that the images are generally affected by various rainwater artifacts\nsuch as raindrops, rain streaks, and rainy haze, which impact the image quality\nfrom both near and far distances, resulting in a complex and intertwined\nprocess of image degradation. However, current deraining techniques are limited\nin their ability to address only one or two types of rainwater, which poses a\nchallenge in removing the mixture of rain (MOR). In this study, we propose an\neffective image deraining paradigm for Mixture of rain REmoval, called\nDEMore-Net, which takes full account of the MOR effect. Going beyond the\nexisting deraining wisdom, DEMore-Net is a joint learning paradigm that\nintegrates depth estimation and MOR removal tasks to achieve superior rain\nremoval. The depth information can offer additional meaningful guidance\ninformation based on distance, thus better helping DEMore-Net remove different\ntypes of rainwater. Moreover, this study explores normalization approaches in\nimage deraining tasks and introduces a new Hybrid Normalization Block (HNB) to\nenhance the deraining performance of DEMore-Net. Extensive experiments\nconducted on synthetic datasets and real-world MOR photos fully validate the\nsuperiority of the proposed DEMore-Net. Code is available at\nhttps://github.com/yz-wang/DEMore-Net.\n","authors":["Yongzhen Wang","Xuefeng Yan","Yanbiao Niu","Lina Gong","Yanwen Guo","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2303.17766v1.pdf","comment":"11 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2211.12497v2","updated":"2023-03-31T01:39:56Z","published":"2022-11-22T18:59:31Z","title":"MagicPony: Learning Articulated 3D Animals in the Wild","summary":"  We consider the problem of predicting the 3D shape, articulation, viewpoint,\ntexture, and lighting of an articulated animal like a horse given a single test\nimage as input. We present a new method, dubbed MagicPony, that learns this\npredictor purely from in-the-wild single-view images of the object category,\nwith minimal assumptions about the topology of deformation. At its core is an\nimplicit-explicit representation of articulated shape and appearance, combining\nthe strengths of neural fields and meshes. In order to help the model\nunderstand an object's shape and pose, we distil the knowledge captured by an\noff-the-shelf self-supervised vision transformer and fuse it into the 3D model.\nTo overcome local optima in viewpoint estimation, we further introduce a new\nviewpoint sampling scheme that comes at no additional training cost. MagicPony\noutperforms prior work on this challenging task and demonstrates excellent\ngeneralisation in reconstructing art, despite the fact that it is only trained\non real images.\n","authors":["Shangzhe Wu","Ruining Li","Tomas Jakab","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2211.12497v2.pdf","comment":"CVPR 2023. Project Page: https://3dmagicpony.github.io/"},{"id":"http://arxiv.org/abs/2209.06015v2","updated":"2023-03-31T01:11:50Z","published":"2022-08-04T05:32:20Z","title":"Black-box Dataset Ownership Verification via Backdoor Watermarking","summary":"  Deep learning, especially deep neural networks (DNNs), has been widely and\nsuccessfully adopted in many critical applications for its high effectiveness\nand efficiency. The rapid development of DNNs has benefited from the existence\nof some high-quality datasets ($e.g.$, ImageNet), which allow researchers and\ndevelopers to easily verify the performance of their methods. Currently, almost\nall existing released datasets require that they can only be adopted for\nacademic or educational purposes rather than commercial purposes without\npermission. However, there is still no good way to ensure that. In this paper,\nwe formulate the protection of released datasets as verifying whether they are\nadopted for training a (suspicious) third-party model, where defenders can only\nquery the model while having no information about its parameters and training\ndetails. Based on this formulation, we propose to embed external patterns via\nbackdoor watermarking for the ownership verification to protect them. Our\nmethod contains two main parts, including dataset watermarking and dataset\nverification. Specifically, we exploit poison-only backdoor attacks ($e.g.$,\nBadNets) for dataset watermarking and design a hypothesis-test-guided method\nfor dataset verification. We also provide some theoretical analyses of our\nmethods. Experiments on multiple benchmark datasets of different tasks are\nconducted, which verify the effectiveness of our method. The code for\nreproducing main experiments is available at\n\\url{https://github.com/THUYimingLi/DVBW}.\n","authors":["Yiming Li","Mingyan Zhu","Xue Yang","Yong Jiang","Tao Wei","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2209.06015v2.pdf","comment":"This paper is accepted by IEEE TIFS. 15 pages. The preliminary short\n  version of this paper was posted on arXiv (arXiv:2010.05821) and presented in\n  a non-archival NeurIPS Workshop (2020)"},{"id":"http://arxiv.org/abs/2303.17748v1","updated":"2023-03-31T00:15:22Z","published":"2023-03-31T00:15:22Z","title":"MLGCN: An Ultra Efficient Graph Convolution Neural Model For 3D Point\n  Cloud Analysis","summary":"  The analysis of 3D point clouds has diverse applications in robotics, vision\nand graphics. Processing them presents specific challenges since they are\nnaturally sparse, can vary in spatial resolution and are typically unordered.\nGraph-based networks to abstract features have emerged as a promising\nalternative to convolutional neural networks for their analysis, but these can\nbe computationally heavy as well as memory inefficient. To address these\nlimitations we introduce a novel Multi-level Graph Convolution Neural (MLGCN)\nmodel, which uses Graph Neural Networks (GNN) blocks to extract features from\n3D point clouds at specific locality levels. Our approach employs precomputed\ngraph KNNs, where each KNN graph is shared between GCN blocks inside a GNN\nblock, making it both efficient and effective compared to present models. We\ndemonstrate the efficacy of our approach on point cloud based object\nclassification and part segmentation tasks on benchmark datasets, showing that\nit produces comparable results to those of state-of-the-art models while\nrequiring up to a thousand times fewer floating-point operations (FLOPs) and\nhaving significantly reduced storage requirements. Thus, our MLGCN model could\nbe particular relevant to point cloud based 3D shape analysis in industrial\napplications when computing resources are scarce.\n","authors":["Mohammad Khodadad","Morteza Rezanejad","Ali Shiraee Kasmaee","Kaleem Siddiqi","Dirk Walther","Hamidreza Mahyar"],"pdf_url":"https://arxiv.org/pdf/2303.17748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06441v2","updated":"2023-03-31T00:08:46Z","published":"2022-10-12T17:42:01Z","title":"How Much Data Are Augmentations Worth? An Investigation into Scaling\n  Laws, Invariance, and Implicit Regularization","summary":"  Despite the clear performance benefits of data augmentations, little is known\nabout why they are so effective. In this paper, we disentangle several key\nmechanisms through which data augmentations operate. Establishing an exchange\nrate between augmented and additional real data, we find that in\nout-of-distribution testing scenarios, augmentations which yield samples that\nare diverse, but inconsistent with the data distribution can be even more\nvaluable than additional training data. Moreover, we find that data\naugmentations which encourage invariances can be more valuable than invariance\nalone, especially on small and medium sized training sets. Following this\nobservation, we show that augmentations induce additional stochasticity during\ntraining, effectively flattening the loss landscape.\n","authors":["Jonas Geiping","Micah Goldblum","Gowthami Somepalli","Ravid Shwartz-Ziv","Tom Goldstein","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2210.06441v2.pdf","comment":"31 pages, 29 figures. To be presented at ICLR 2023. Code at\n  https://github.com/JonasGeiping/dataaugs"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2209.12638v2","updated":"2023-03-31T06:59:28Z","published":"2022-09-26T12:37:37Z","title":"Bounded Simplex-Structured Matrix Factorization: Algorithms,\n  Identifiability and Applications","summary":"  In this paper, we propose a new low-rank matrix factorization model dubbed\nbounded simplex-structured matrix factorization (BSSMF). Given an input matrix\n$X$ and a factorization rank $r$, BSSMF looks for a matrix $W$ with $r$ columns\nand a matrix $H$ with $r$ rows such that $X \\approx WH$ where the entries in\neach column of $W$ are bounded, that is, they belong to given intervals, and\nthe columns of $H$ belong to the probability simplex, that is, $H$ is column\nstochastic. BSSMF generalizes nonnegative matrix factorization (NMF), and\nsimplex-structured matrix factorization (SSMF). BSSMF is particularly well\nsuited when the entries of the input matrix $X$ belong to a given interval; for\nexample when the rows of $X$ represent images, or $X$ is a rating matrix such\nas in the Netflix and MovieLens datasets where the entries of $X$ belong to the\ninterval $[1,5]$. The simplex-structured matrix $H$ not only leads to an easily\nunderstandable decomposition providing a soft clustering of the columns of $X$,\nbut implies that the entries of each column of $WH$ belong to the same\nintervals as the columns of $W$. In this paper, we first propose a fast\nalgorithm for BSSMF, even in the presence of missing data in $X$. Then we\nprovide identifiability conditions for BSSMF, that is, we provide conditions\nunder which BSSMF admits a unique decomposition, up to trivial ambiguities.\nFinally, we illustrate the effectiveness of BSSMF on two applications:\nextraction of features in a set of images, and the matrix completion problem\nfor recommender systems.\n","authors":["Olivier Vu Thanh","Nicolas Gillis","Fabian Lecron"],"pdf_url":"https://arxiv.org/pdf/2209.12638v2.pdf","comment":"14 pages, new title, new numerical experiments on synthetic data,\n  clarifications of several parts of the paper, run times added"},{"id":"http://arxiv.org/abs/2304.00116v1","updated":"2023-03-31T20:24:14Z","published":"2023-03-31T20:24:14Z","title":"Enhancing Large Language Models with Climate Resources","summary":"  Large language models (LLMs) have significantly transformed the landscape of\nartificial intelligence by demonstrating their ability in generating human-like\ntext across diverse topics. However, despite their impressive capabilities,\nLLMs lack recent information and often employ imprecise language, which can be\ndetrimental in domains where accuracy is crucial, such as climate change. In\nthis study, we make use of recent ideas to harness the potential of LLMs by\nviewing them as agents that access multiple sources, including databases\ncontaining recent and precise information about organizations, institutions,\nand companies. We demonstrate the effectiveness of our method through a\nprototype agent that retrieves emission data from ClimateWatch\n(https://www.climatewatchdata.org/) and leverages general Google search. By\nintegrating these resources with LLMs, our approach overcomes the limitations\nassociated with imprecise language and delivers more reliable and accurate\ninformation in the critical domain of climate change. This work paves the way\nfor future advancements in LLMs and their application in domains where\nprecision is of paramount importance.\n","authors":["Mathias Kraus","Julia Anna Bingler","Markus Leippold","Tobias Schimanski","Chiara Colesanti Senni","Dominik Stammbach","Saeid Ashraf Vaghefi","Nicolas Webersinke"],"pdf_url":"https://arxiv.org/pdf/2304.00116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00114v1","updated":"2023-03-31T20:21:32Z","published":"2023-03-31T20:21:32Z","title":"Dense Sparse Retrieval: Using Sparse Language Models for Inference\n  Efficient Dense Retrieval","summary":"  Vector-based retrieval systems have become a common staple for academic and\nindustrial search applications because they provide a simple and scalable way\nof extending the search to leverage contextual representations for documents\nand queries. As these vector-based systems rely on contextual language models,\ntheir usage commonly requires GPUs, which can be expensive and difficult to\nmanage. Given recent advances in introducing sparsity into language models for\nimproved inference efficiency, in this paper, we study how sparse language\nmodels can be used for dense retrieval to improve inference efficiency. Using\nthe popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQA\ndatasets, we find that sparse language models can be used as direct\nreplacements with little to no drop in accuracy and up to 4.3x improved\ninference speeds\n","authors":["Daniel Campos","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2304.00114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01016v1","updated":"2023-03-31T15:44:13Z","published":"2023-03-31T15:44:13Z","title":"Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler\n  Alignment of Embeddings for Asymmetrical dual encoders","summary":"  In this paper, we consider the problem of improving the inference latency of\nlanguage model-based dense retrieval systems by introducing structural\ncompression and model size asymmetry between the context and query encoders.\nFirst, we investigate the impact of pre and post-training compression on the\nMSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that\nasymmetry in the dual encoders in dense retrieval can lead to improved\ninference efficiency. Knowing this, we introduce Kullback Leibler Alignment of\nEmbeddings (KALE), an efficient and accurate method for increasing the\ninference efficiency of dense retrieval methods by pruning and aligning the\nquery encoder after training. Specifically, KALE extends traditional Knowledge\nDistillation after bi-encoder training, allowing for effective query encoder\ncompression without full retraining or index generation. Using KALE and\nasymmetric training, we can generate models which exceed the performance of\nDistilBERT despite having 3x faster inference.\n","authors":["Daniel Campos","Alessandro Magnani","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2304.01016v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2303.09483v3","updated":"2023-03-31T17:58:40Z","published":"2023-03-16T17:00:42Z","title":"Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks\n  in Continual Learning","summary":"  In contrast to the natural capabilities of humans to learn new tasks in a\nsequential fashion, neural networks are known to suffer from catastrophic\nforgetting, where the model's performances on old tasks drop dramatically after\nbeing optimized for a new task. Since then, the continual learning (CL)\ncommunity has proposed several solutions aiming to equip the neural network\nwith the ability to learn the current task (plasticity) while still achieving\nhigh accuracy on the previous tasks (stability). Despite remarkable\nimprovements, the plasticity-stability trade-off is still far from being solved\nand its underlying mechanism is poorly understood. In this work, we propose\nAuxiliary Network Continual Learning (ANCL), a novel method that applies an\nadditional auxiliary network which promotes plasticity to the continually\nlearned model which mainly focuses on stability. More concretely, the proposed\nframework materializes in a regularizer that naturally interpolates between\nplasticity and stability, surpassing strong baselines on task incremental and\nclass incremental scenarios. Through extensive analyses on ANCL solutions, we\nidentify some essential principles beneath the stability-plasticity trade-off.\n","authors":["Sanghwan Kim","Lorenzo Noci","Antonio Orvieto","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2303.09483v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2202.06861v2","updated":"2023-03-31T17:58:37Z","published":"2022-02-14T16:45:36Z","title":"Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural\n  Network Explanations and Beyond","summary":"  The evaluation of explanation methods is a research topic that has not yet\nbeen explored deeply, however, since explainability is supposed to strengthen\ntrust in artificial intelligence, it is necessary to systematically review and\ncompare explanation methods in order to confirm their correctness. Until now,\nno tool with focus on XAI evaluation exists that exhaustively and speedily\nallows researchers to evaluate the performance of explanations of neural\nnetwork predictions. To increase transparency and reproducibility in the field,\nwe therefore built Quantus -- a comprehensive, evaluation toolkit in Python\nthat includes a growing, well-organised collection of evaluation metrics and\ntutorials for evaluating explainable methods. The toolkit has been thoroughly\ntested and is available under an open-source license on PyPi (or on\nhttps://github.com/understandable-machine-intelligence-lab/Quantus/).\n","authors":["Anna Hedström","Leander Weber","Dilyara Bareeva","Franz Motzkus","Wojciech Samek","Sebastian Lapuschkin","Marina M. -C. Höhne"],"pdf_url":"https://arxiv.org/pdf/2202.06861v2.pdf","comment":"4 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2303.18242v1","updated":"2023-03-31T17:58:08Z","published":"2023-03-31T17:58:08Z","title":"$\\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified\n  States","summary":"  We introduce $\\infty$-Diff, a generative diffusion model which directly\noperates on infinite resolution data. By randomly sampling subsets of\ncoordinates during training and learning to denoise the content at those\ncoordinates, a continuous function is learned that allows sampling at arbitrary\nresolutions. In contrast to other recent infinite resolution generative models,\nour approach operates directly on the raw data, not requiring latent vector\ncompression for context, using hypernetworks, nor relying on discrete\ncomponents. As such, our approach achieves significantly higher sample quality,\nas evidenced by lower FID scores, as well as being able to effectively scale to\nhigher resolutions than the training data while retaining detail.\n","authors":["Sam Bond-Taylor","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2303.18242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18240v1","updated":"2023-03-31T17:56:33Z","published":"2023-03-31T17:56:33Z","title":"Where are we in the search for an Artificial Visual Cortex for Embodied\n  Intelligence?","summary":"  We present the largest and most comprehensive empirical study of pre-trained\nvisual representations (PVRs) or visual 'foundation models' for Embodied AI.\nFirst, we curate CortexBench, consisting of 17 different tasks spanning\nlocomotion, navigation, dexterous, and mobile manipulation. Next, we\nsystematically evaluate existing PVRs and find that none are universally\ndominant.\n  To study the effect of pre-training data scale and diversity, we combine over\n4,000 hours of egocentric videos from 7 different sources (over 5.6M images)\nand ImageNet to train different-sized vision transformers using Masked\nAuto-Encoding (MAE) on slices of this data. Contrary to inferences from prior\nwork, we find that scaling dataset size and diversity does not improve\nperformance universally (but does so on average).\n  Our largest model, named VC-1, outperforms all prior PVRs on average but does\nnot universally dominate either. Finally, we show that task or domain-specific\nadaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving\ncompetitive or superior performance than the best known results on all of the\nbenchmarks in CortexBench. These models required over 10,000 GPU-hours to train\nand can be found on our website for the benefit of the research community.\n","authors":["Arjun Majumdar","Karmesh Yadav","Sergio Arnaud","Yecheng Jason Ma","Claire Chen","Sneha Silwal","Aryan Jain","Vincent-Pierre Berges","Pieter Abbeel","Jitendra Malik","Dhruv Batra","Yixin Lin","Oleksandr Maksymets","Aravind Rajeswaran","Franziska Meier"],"pdf_url":"https://arxiv.org/pdf/2303.18240v1.pdf","comment":"Project website: https://eai-vc.github.io"},{"id":"http://arxiv.org/abs/2211.16886v2","updated":"2023-03-31T17:48:47Z","published":"2022-11-30T10:38:24Z","title":"A Unifying Theory of Distance from Calibration","summary":"  We study the fundamental question of how to define and measure the distance\nfrom calibration for probabilistic predictors. While the notion of perfect\ncalibration is well-understood, there is no consensus on how to quantify the\ndistance from perfect calibration. Numerous calibration measures have been\nproposed in the literature, but it is unclear how they compare to each other,\nand many popular measures such as Expected Calibration Error (ECE) fail to\nsatisfy basic properties like continuity.\n  We present a rigorous framework for analyzing calibration measures, inspired\nby the literature on property testing. We propose a ground-truth notion of\ndistance from calibration: the $\\ell_1$ distance to the nearest perfectly\ncalibrated predictor. We define a consistent calibration measure as one that is\npolynomially related to this distance. Applying our framework, we identify\nthree calibration measures that are consistent and can be estimated\nefficiently: smooth calibration, interval calibration, and Laplace kernel\ncalibration. The former two give quadratic approximations to the ground truth\ndistance, which we show is information-theoretically optimal in a natural model\nfor measuring calibration which we term the prediction-only access model. Our\nwork thus establishes fundamental lower and upper bounds on measuring the\ndistance to calibration, and also provides theoretical justification for\npreferring certain metrics (like Laplace kernel calibration) in practice.\n","authors":["Jarosław Błasiok","Parikshit Gopalan","Lunjia Hu","Preetum Nakkiran"],"pdf_url":"https://arxiv.org/pdf/2211.16886v2.pdf","comment":"In STOC 2023"},{"id":"http://arxiv.org/abs/2106.02780v2","updated":"2023-03-31T17:35:23Z","published":"2021-06-05T02:42:46Z","title":"Learning Treatment Effects in Panels with General Intervention Patterns","summary":"  The problem of causal inference with panel data is a central econometric\nquestion. The following is a fundamental version of this problem: Let $M^*$ be\na low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix\n$Z$ with entries in $\\{0,1\\}$ we observe the matrix $O$ with entries $O_{ij} :=\nM^*_{ij} + E_{ij} + \\mathcal{T}_{ij} Z_{ij}$ where $\\mathcal{T}_{ij} $ are\nunknown, heterogenous treatment effects. The problem requires we estimate the\naverage treatment effect $\\tau^* := \\sum_{ij} \\mathcal{T}_{ij} Z_{ij} /\n\\sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach to\nestimating $\\tau^*$ when $Z$ places support on a single row. This paper extends\nthat framework to allow rate-optimal recovery of $\\tau^*$ for general $Z$, thus\nbroadly expanding its applicability. Our guarantees are the first of their type\nin this general setting. Computational experiments on synthetic and real-world\ndata show a substantial advantage over competing estimators.\n","authors":["Vivek F. Farias","Andrew A. Li","Tianyi Peng"],"pdf_url":"https://arxiv.org/pdf/2106.02780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.12577v2","updated":"2023-03-31T17:33:24Z","published":"2021-11-24T15:58:10Z","title":"A Method for Evaluating Deep Generative Models of Images via Assessing\n  the Reproduction of High-order Spatial Context","summary":"  Deep generative models (DGMs) have the potential to revolutionize diagnostic\nimaging. Generative adversarial networks (GANs) are one kind of DGM which are\nwidely employed. The overarching problem with deploying GANs, and other DGMs,\nin any application that requires domain expertise in order to actually use the\ngenerated images is that there generally is not adequate or automatic means of\nassessing the domain-relevant quality of generated images. In this work, we\ndemonstrate several objective tests of images output by two popular GAN\narchitectures. We designed several stochastic context models (SCMs) of distinct\nimage features that can be recovered after generation by a trained GAN. Several\nof these features are high-order, algorithmic pixel-arrangement rules which are\nnot readily expressed in covariance matrices. We designed and validated\nstatistical classifiers to detect specific effects of the known arrangement\nrules. We then tested the rates at which two different GANs correctly\nreproduced the feature context under a variety of training scenarios, and\ndegrees of feature-class similarity. We found that ensembles of generated\nimages can appear largely accurate visually, and show high accuracy in ensemble\nmeasures, while not exhibiting the known spatial arrangements. Furthermore,\nGANs trained on a spectrum of distinct spatial orders did not respect the given\nprevalence of those orders in the training data. The main conclusion is that\nSCMs can be engineered to quantify numerous errors, per image, that may not be\ncaptured in ensemble statistics but plausibly can affect subsequent use of the\nGAN-generated images.\n","authors":["Rucha Deshpande","Mark A. Anastasio","Frank J. Brooks"],"pdf_url":"https://arxiv.org/pdf/2111.12577v2.pdf","comment":"The paper is under consideration at Pattern Recognition Letters.\n  Early version with preliminary results was accepted for poster presentation\n  at SPIE-MI 2022. This version on arXiv contains new and updated designs of\n  stochastic models, their mathematical representations and the corresponding\n  results. Data from the designed ensembles available at\n  https://doi.org/10.7910/DVN/HHF4AF"},{"id":"http://arxiv.org/abs/2009.03825v5","updated":"2023-03-31T17:15:54Z","published":"2020-09-08T15:45:44Z","title":"Optimal training of integer-valued neural networks with mixed integer\n  programming","summary":"  Recent work has shown potential in using Mixed Integer Programming (MIP)\nsolvers to optimize certain aspects of neural networks (NNs). However the\nintriguing approach of training NNs with MIP solvers is under-explored.\nState-of-the-art-methods to train NNs are typically gradient-based and require\nsignificant data, computation on GPUs, and extensive hyper-parameter tuning. In\ncontrast, training with MIP solvers does not require GPUs or heavy\nhyper-parameter tuning, but currently cannot handle anything but small amounts\nof data. This article builds on recent advances that train binarized NNs using\nMIP solvers. We go beyond current work by formulating new MIP models which\nimprove training efficiency and which can train the important class of\ninteger-valued neural networks (INNs). We provide two novel methods to further\nthe potential significance of using MIP to train NNs. The first method\noptimizes the number of neurons in the NN while training. This reduces the need\nfor deciding on network architecture before training. The second method\naddresses the amount of training data which MIP can feasibly handle: we provide\na batch training method that dramatically increases the amount of data that MIP\nsolvers can use to train. We thus provide a promising step towards using much\nmore data than before when training NNs using MIP models. Experimental results\non two real-world data-limited datasets demonstrate that our approach strongly\noutperforms the previous state of the art in training NN with MIP, in terms of\naccuracy, training time and amount of data. Our methodology is proficient at\ntraining NNs when minimal training data is available, and at training with\nminimal memory requirements -- which is potentially valuable for deploying to\nlow-memory devices.\n","authors":["Tómas Thorbjarnarson","Neil Yorke-Smith"],"pdf_url":"https://arxiv.org/pdf/2009.03825v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18211v1","updated":"2023-03-31T17:05:46Z","published":"2023-03-31T17:05:46Z","title":"Simple Sorting Criteria Help Find the Causal Order in Additive Noise\n  Models","summary":"  Additive Noise Models (ANM) encode a popular functional assumption that\nenables learning causal structure from observational data. Due to a lack of\nreal-world data meeting the assumptions, synthetic ANM data are often used to\nevaluate causal discovery algorithms. Reisach et al. (2021) show that, for\ncommon simulation parameters, a variable ordering by increasing variance is\nclosely aligned with a causal order and introduce var-sortability to quantify\nthe alignment. Here, we show that not only variance, but also the fraction of a\nvariable's variance explained by all others, as captured by the coefficient of\ndetermination $R^2$, tends to increase along the causal order. Simple baseline\nalgorithms can use $R^2$-sortability to match the performance of established\nmethods. Since $R^2$-sortability is invariant under data rescaling, these\nalgorithms perform equally well on standardized or rescaled data, addressing a\nkey limitation of algorithms exploiting var-sortability. We characterize and\nempirically assess $R^2$-sortability for different simulation parameters. We\nshow that all simulation parameters can affect $R^2$-sortability and must be\nchosen deliberately to control the difficulty of the causal discovery task and\nthe real-world plausibility of the simulated data. We provide an implementation\nof the sortability measures and sortability-based algorithms in our library\nCausalDisco (https://github.com/CausalDisco/CausalDisco).\n","authors":["Alexander G. Reisach","Myriam Tami","Christof Seiler","Antoine Chambaz","Sebastian Weichwald"],"pdf_url":"https://arxiv.org/pdf/2303.18211v1.pdf","comment":"See https://github.com/CausalDisco/CausalDisco for implementations"},{"id":"http://arxiv.org/abs/2303.18205v1","updated":"2023-03-31T16:59:40Z","published":"2023-03-31T16:59:40Z","title":"SimTS: Rethinking Contrastive Representation Learning for Time Series\n  Forecasting","summary":"  Contrastive learning methods have shown an impressive ability to learn\nmeaningful representations for image or time series classification. However,\nthese methods are less effective for time series forecasting, as optimization\nof instance discrimination is not directly applicable to predicting the future\nstate from the history context. Moreover, the construction of positive and\nnegative pairs in current technologies strongly relies on specific time series\ncharacteristics, restricting their generalization across diverse types of time\nseries data. To address these limitations, we propose SimTS, a simple\nrepresentation learning approach for improving time series forecasting by\nlearning to predict the future from the past in the latent space. SimTS does\nnot rely on negative pairs or specific assumptions about the characteristics of\nthe particular time series. Our extensive experiments on several benchmark time\nseries forecasting datasets show that SimTS achieves competitive performance\ncompared to existing contrastive learning methods. Furthermore, we show the\nshortcomings of the current contrastive learning framework used for time series\nforecasting through a detailed ablation study. Overall, our work suggests that\nSimTS is a promising alternative to other contrastive learning approaches for\ntime series forecasting.\n","authors":["Xiaochen Zheng","Xingyu Chen","Manuel Schürch","Amina Mollaysa","Ahmed Allam","Michael Krauthammer"],"pdf_url":"https://arxiv.org/pdf/2303.18205v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.18193v1","updated":"2023-03-31T16:50:23Z","published":"2023-03-31T16:50:23Z","title":"GVP: Generative Volumetric Primitives","summary":"  Advances in 3D-aware generative models have pushed the boundary of image\nsynthesis with explicit camera control. To achieve high-resolution image\nsynthesis, several attempts have been made to design efficient generators, such\nas hybrid architectures with both 3D and 2D components. However, such a design\ncompromises multiview consistency, and the design of a pure 3D generator with\nhigh resolution is still an open problem. In this work, we present Generative\nVolumetric Primitives (GVP), the first pure 3D generative model that can sample\nand render 512-resolution images in real-time. GVP jointly models a number of\nvolumetric primitives and their spatial information, both of which can be\nefficiently generated via a 2D convolutional network. The mixture of these\nprimitives naturally captures the sparsity and correspondence in the 3D volume.\nThe training of such a generator with a high degree of freedom is made possible\nthrough a knowledge distillation technique. Experiments on several datasets\ndemonstrate superior efficiency and 3D consistency of GVP over the\nstate-of-the-art.\n","authors":["Mallikarjun B R","Xingang Pan","Mohamed Elgharib","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2303.18193v1.pdf","comment":"https://vcai.mpi-inf.mpg.de/projects/GVP/index.html"},{"id":"http://arxiv.org/abs/2303.16904v2","updated":"2023-03-31T16:48:02Z","published":"2023-03-23T22:35:37Z","title":"Severity classification of ground-glass opacity via 2-D convolutional\n  neural network and lung CT scans: a 3-day exploration","summary":"  Ground-glass opacity is a hallmark of numerous lung diseases, including\npatients with COVID19 and pneumonia, pulmonary fibrosis, and tuberculosis. This\nbrief note presents experimental results of a proof-of-concept framework that\ngot implemented and tested over three days as driven by the third challenge\nentitled \"COVID-19 Competition\", hosted at the AI-Enabled Medical Image\nAnalysis Workshop of the 2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP 2023). Using a newly built virtual\nenvironment (created on March 17, 2023), we investigated various pre-trained\ntwo-dimensional convolutional neural networks (CNN) such as Dense Neural\nNetwork, Residual Neural Networks (ResNet), and Vision Transformers, as well as\nthe extent of fine-tuning. Based on empirical experiments, we opted to\nfine-tune them using ADAM's optimization algorithm with a standard learning\nrate of 0.001 for all CNN architectures and apply early-stopping whenever the\nvalidation loss reached a plateau. For each trained CNN, the model state with\nthe best validation accuracy achieved during training was stored and later\nreloaded for new classifications of unseen samples drawn from the validation\nset provided by the challenge organizers. According to the organizers, few of\nthese 2D CNNs yielded performance comparable to an architecture that combined\nResNet and Recurrent Neural Network (Gated Recurrent Units). As part of the\nchallenge requirement, the source code produced during the course of this\nexercise is posted at https://github.com/lisatwyw/cov19. We also hope that\nother researchers may find this light prototype consisting of few Python files\nbased on PyTorch 1.13.1 and TorchVision 0.14.1 approachable.\n","authors":["Lisa Y. W. Tang"],"pdf_url":"https://arxiv.org/pdf/2303.16904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00939v5","updated":"2023-03-31T16:37:12Z","published":"2022-10-03T13:50:58Z","title":"Improving Sample Quality of Diffusion Models Using Self-Attention\n  Guidance","summary":"  Denoising diffusion models (DDMs) have attracted attention for their\nexceptional generation quality and diversity. This success is largely\nattributed to the use of class- or text-conditional diffusion guidance methods,\nsuch as classifier and classifier-free guidance. In this paper, we present a\nmore comprehensive perspective that goes beyond the traditional guidance\nmethods. From this generalized perspective, we introduce novel condition- and\ntraining-free strategies to enhance the quality of generated images. As a\nsimple solution, blur guidance improves the suitability of intermediate samples\nfor their fine-scale information and structures, enabling diffusion models to\ngenerate higher quality samples with a moderate guidance scale. Improving upon\nthis, Self-Attention Guidance (SAG) uses the intermediate self-attention maps\nof diffusion models to enhance their stability and efficacy. Specifically, SAG\nadversarially blurs only the regions that diffusion models attend to at each\niteration and guides them accordingly. Our experimental results show that our\nSAG improves the performance of various diffusion models, including ADM, IDDPM,\nStable Diffusion, and DiT. Moreover, combining SAG with conventional guidance\nmethods leads to further improvement.\n","authors":["Susung Hong","Gyuseong Lee","Wooseok Jang","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2210.00939v5.pdf","comment":"Project page: https://ku-cvlab.github.io/Self-Attention-Guidance"},{"id":"http://arxiv.org/abs/2301.00497v3","updated":"2023-03-31T16:29:36Z","published":"2023-01-02T01:12:29Z","title":"Efficient Online Learning with Memory via Frank-Wolfe Optimization:\n  Algorithms with Bounded Dynamic Regret and Applications to Control","summary":"  Projection operations are a typical computation bottleneck in online\nlearning. In this paper, we enable projection-free online learning within the\nframework of Online Convex Optimization with Memory (OCO-M) -- OCO-M captures\nhow the history of decisions affects the current outcome by allowing the online\nlearning loss functions to depend on both current and past decisions.\nParticularly, we introduce the first projection-free meta-base learning\nalgorithm with memory that minimizes dynamic regret, i.e., that minimizes the\nsuboptimality against any sequence of time-varying decisions. We are motivated\nby artificial intelligence applications where autonomous agents need to adapt\nto time-varying environments in real-time, accounting for how past decisions\naffect the present. Examples of such applications are: online control of\ndynamical systems; statistical arbitrage; and time series prediction. The\nalgorithm builds on the Online Frank-Wolfe (OFW) and Hedge algorithms. We\ndemonstrate how our algorithm can be applied to the online control of linear\ntime-varying systems in the presence of unpredictable process noise. To this\nend, we develop a controller with memory and bounded dynamic regret against any\noptimal time-varying linear feedback control policy. We validate our algorithm\nin simulated scenarios of online control of linear time-invariant systems.\n","authors":["Hongyu Zhou","Zirui Xu","Vasileios Tzoumas"],"pdf_url":"https://arxiv.org/pdf/2301.00497v3.pdf","comment":"The version corrects proofs and updates presentation"},{"id":"http://arxiv.org/abs/2303.18181v1","updated":"2023-03-31T16:23:29Z","published":"2023-03-31T16:23:29Z","title":"A Closer Look at Parameter-Efficient Tuning in Diffusion Models","summary":"  Large-scale diffusion models like Stable Diffusion are powerful and find\nvarious real-world applications while customizing such models by fine-tuning is\nboth memory and time inefficient. Motivated by the recent progress in natural\nlanguage processing, we investigate parameter-efficient tuning in large\ndiffusion models by inserting small learnable modules (termed adapters). In\nparticular, we decompose the design space of adapters into orthogonal factors\n-- the input position, the output position as well as the function form, and\nperform Analysis of Variance (ANOVA), a classical statistical approach for\nanalyzing the correlation between discrete (design options) and continuous\nvariables (evaluation metrics). Our analysis suggests that the input position\nof adapters is the critical factor influencing the performance of downstream\ntasks. Then, we carefully study the choice of the input position, and we find\nthat putting the input position after the cross-attention block can lead to the\nbest performance, validated by additional visualization analyses. Finally, we\nprovide a recipe for parameter-efficient tuning in diffusion models, which is\ncomparable if not superior to the fully fine-tuned baseline (e.g., DreamBooth)\nwith only 0.75 \\% extra parameters, across various customized tasks.\n","authors":["Chendong Xiang","Fan Bao","Chongxuan Li","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.18181v1.pdf","comment":"8pages"},{"id":"http://arxiv.org/abs/2303.18158v1","updated":"2023-03-31T15:51:56Z","published":"2023-03-31T15:51:56Z","title":"Constrained Optimization of Rank-One Functions with Indicator Variables","summary":"  Optimization problems involving minimization of a rank-one convex function\nover constraints modeling restrictions on the support of the decision variables\nemerge in various machine learning applications. These problems are often\nmodeled with indicator variables for identifying the support of the continuous\nvariables. In this paper we investigate compact extended formulations for such\nproblems through perspective reformulation techniques. In contrast to the\nmajority of previous work that relies on support function arguments and\ndisjunctive programming techniques to provide convex hull results, we propose a\nconstructive approach that exploits a hidden conic structure induced by\nperspective functions. To this end, we first establish a convex hull result for\na general conic mixed-binary set in which each conic constraint involves a\nlinear function of independent continuous variables and a set of binary\nvariables. We then demonstrate that extended representations of sets associated\nwith epigraphs of rank-one convex functions over constraints modeling indicator\nrelations naturally admit such a conic representation. This enables us to\nsystematically give perspective formulations for the convex hull descriptions\nof these sets with nonlinear separable or non-separable objective functions,\nsign constraints on continuous variables, and combinatorial constraints on\nindicator variables. We illustrate the efficacy of our results on sparse\nnonnegative logistic regression problems.\n","authors":["Soroosh Shafieezadeh-Abadeh","Fatma Kılınç-Karzan"],"pdf_url":"https://arxiv.org/pdf/2303.18158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18157v1","updated":"2023-03-31T15:47:49Z","published":"2023-03-31T15:47:49Z","title":"MAGNNETO: A Graph Neural Network-based Multi-Agent system for Traffic\n  Engineering","summary":"  Current trends in networking propose the use of Machine Learning (ML) for a\nwide variety of network optimization tasks. As such, many efforts have been\nmade to produce ML-based solutions for Traffic Engineering (TE), which is a\nfundamental problem in ISP networks. Nowadays, state-of-the-art TE optimizers\nrely on traditional optimization techniques, such as Local search, Constraint\nProgramming, or Linear programming. In this paper, we present MAGNNETO, a\ndistributed ML-based framework that leverages Multi-Agent Reinforcement\nLearning and Graph Neural Networks for distributed TE optimization. MAGNNETO\ndeploys a set of agents across the network that learn and communicate in a\ndistributed fashion via message exchanges between neighboring agents.\nParticularly, we apply this framework to optimize link weights in OSPF, with\nthe goal of minimizing network congestion. In our evaluation, we compare\nMAGNNETO against several state-of-the-art TE optimizers in more than 75\ntopologies (up to 153 nodes and 354 links), including realistic traffic loads.\nOur experimental results show that, thanks to its distributed nature, MAGNNETO\nachieves comparable performance to state-of-the-art TE optimizers with\nsignificantly lower execution times. Moreover, our ML-based solution\ndemonstrates a strong generalization capability to successfully operate in new\nnetworks unseen during training.\n","authors":["Guillermo Bernárdez","José Suárez-Varela","Albert López","Xiang Shi","Shihan Xiao","Xiangle Cheng","Pere Barlet-Ros","Albert Cabellos-Aparicio"],"pdf_url":"https://arxiv.org/pdf/2303.18157v1.pdf","comment":"IEEE Transactions on Cognitive Communications and Networking (2023).\n  arXiv admin note: text overlap with arXiv:2109.01445"},{"id":"http://arxiv.org/abs/2212.04089v3","updated":"2023-03-31T15:27:01Z","published":"2022-12-08T05:50:53Z","title":"Editing Models with Task Arithmetic","summary":"  Changing how pre-trained models behave -- e.g., improving their performance\non a downstream task or mitigating biases learned during pre-training -- is a\ncommon practice when developing machine learning systems. In this work, we\npropose a new paradigm for steering the behavior of neural networks, centered\naround \\textit{task vectors}. A task vector specifies a direction in the weight\nspace of a pre-trained model, such that movement in that direction improves\nperformance on the task. We build task vectors by subtracting the weights of a\npre-trained model from the weights of the same model after fine-tuning on a\ntask. We show that these task vectors can be modified and combined together\nthrough arithmetic operations such as negation and addition, and the behavior\nof the resulting model is steered accordingly. Negating a task vector decreases\nperformance on the target task, with little change in model behavior on control\ntasks. Moreover, adding task vectors together can improve performance on\nmultiple tasks at once. Finally, when tasks are linked by an analogy\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\nthree of the tasks can improve performance on the fourth, even when no data\nfrom the fourth task is used for training. Overall, our experiments with\nseveral models, modalities and tasks show that task arithmetic is a simple,\nefficient and effective way of editing models.\n","authors":["Gabriel Ilharco","Marco Tulio Ribeiro","Mitchell Wortsman","Suchin Gururangan","Ludwig Schmidt","Hannaneh Hajishirzi","Ali Farhadi"],"pdf_url":"https://arxiv.org/pdf/2212.04089v3.pdf","comment":"In Proceedings of the 11th International Conference on Learning\n  Representations (ICLR 2023)"},{"id":"http://arxiv.org/abs/2303.18118v1","updated":"2023-03-31T15:04:53Z","published":"2023-03-31T15:04:53Z","title":"A two-head loss function for deep Average-K classification","summary":"  Average-K classification is an alternative to top-K classification in which\nthe number of labels returned varies with the ambiguity of the input image but\nmust average to K over all the samples. A simple method to solve this task is\nto threshold the softmax output of a model trained with the cross-entropy loss.\nThis approach is theoretically proven to be asymptotically consistent, but it\nis not guaranteed to be optimal for a finite set of samples. In this paper, we\npropose a new loss function based on a multi-label classification head in\naddition to the classical softmax. This second head is trained using\npseudo-labels generated by thresholding the softmax head while guaranteeing\nthat K classes are returned on average. We show that this approach allows the\nmodel to better capture ambiguities between classes and, as a result, to return\nmore consistent sets of possible classes. Experiments on two datasets from the\nliterature demonstrate that our approach outperforms the softmax baseline, as\nwell as several other loss functions more generally designed for weakly\nsupervised multi-label classification. The gains are larger the higher the\nuncertainty, especially for classes with few samples.\n","authors":["Camille Garcin","Maximilien Servajean","Alexis Joly","Joseph Salmon"],"pdf_url":"https://arxiv.org/pdf/2303.18118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03421v3","updated":"2023-03-31T14:59:56Z","published":"2023-02-07T12:11:59Z","title":"A unified recipe for deriving (time-uniform) PAC-Bayes bounds","summary":"  We present a unified framework for deriving PAC-Bayesian generalization\nbounds. Unlike most previous literature on this topic, our bounds are\nanytime-valid (i.e., time-uniform), meaning that they hold at all stopping\ntimes, not only for a fixed sample size. Our approach combines four tools in\nthe following order: (a) nonnegative supermartingales or reverse\nsubmartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula\n(or other convex duality principles), and (d) Ville's inequality. Our main\nresult is a PAC-Bayes theorem which holds for a wide class of discrete\nstochastic processes. We show how this result implies time-uniform versions of\nwell-known classical PAC-Bayes bounds, such as those of Seeger, McAllester,\nMaurer, and Catoni, in addition to many recent bounds. We also present several\nnovel bounds. Our framework also enables us to relax traditional assumptions;\nin particular, we consider nonstationary loss functions and non-i.i.d. data. In\nsum, we unify the derivation of past bounds and ease the search for future\nbounds: one may simply check if our supermartingale or submartingale conditions\nare met and, if so, be guaranteed a (time-uniform) PAC-Bayes bound.\n","authors":["Ben Chugg","Hongjian Wang","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2302.03421v3.pdf","comment":"46 pages"},{"id":"http://arxiv.org/abs/2303.18110v1","updated":"2023-03-31T14:56:54Z","published":"2023-03-31T14:56:54Z","title":"The Edinburgh International Accents of English Corpus: Towards the\n  Democratization of English ASR","summary":"  English is the most widely spoken language in the world, used daily by\nmillions of people as a first or second language in many different contexts. As\na result, there are many varieties of English. Although the great many advances\nin English automatic speech recognition (ASR) over the past decades, results\nare usually reported based on test datasets which fail to represent the\ndiversity of English as spoken today around the globe. We present the first\nrelease of The Edinburgh International Accents of English Corpus (EdAcc). This\ndataset attempts to better represent the wide diversity of English,\nencompassing almost 40 hours of dyadic video call conversations between\nfriends. Unlike other datasets, EdAcc includes a wide range of first and\nsecond-language varieties of English and a linguistic background profile of\neach speaker. Results on latest public, and commercial models show that EdAcc\nhighlights shortcomings of current English ASR models. The best performing\nmodel, trained on 680 thousand hours of transcribed data, obtains an average of\n19.7% word error rate (WER) -- in contrast to the 2.7% WER obtained when\nevaluated on US English clean read speech. Across all models, we observe a drop\nin performance on Indian, Jamaican, and Nigerian English speakers. Recordings,\nlinguistic backgrounds, data statement, and evaluation scripts are released on\nour website (https://groups.inf.ed.ac.uk/edacc/) under CC-BY-SA license.\n","authors":["Ramon Sanabria","Nikolay Bogoychev","Nina Markl","Andrea Carmantini","Ondrej Klejch","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2303.18110v1.pdf","comment":"Accepted to IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2302.04694v2","updated":"2023-03-31T14:50:55Z","published":"2023-02-09T15:25:52Z","title":"Partial Optimality in Cubic Correlation Clustering","summary":"  The higher-order correlation clustering problem is an expressive model, and\nrecently, local search heuristics have been proposed for several applications.\nCertifying optimality, however, is NP-hard and practically hampered already by\nthe complexity of the problem statement. Here, we focus on establishing partial\noptimality conditions for the special case of complete graphs and cubic\nobjective functions. In addition, we define and implement algorithms for\ntesting these conditions and examine their effect numerically, on two datasets.\n","authors":["David Stein","Silvia Di Gregorio","Bjoern Andres"],"pdf_url":"https://arxiv.org/pdf/2302.04694v2.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2110.04829v2","updated":"2023-03-31T14:31:10Z","published":"2021-10-10T15:51:01Z","title":"Adaptive joint distribution learning","summary":"  We develop a new framework for embedding joint probability distributions in\ntensor product reproducing kernel Hilbert spaces (RKHS). Our framework\naccommodates a low-dimensional, normalized and positive model of a\nRadon-Nikodym derivative, which we estimate from sample sizes of up to several\nmillion data points, alleviating the inherent limitations of RKHS modeling.\nWell-defined normalized and positive conditional distributions are natural\nby-products to our approach. The embedding is fast to compute and accommodates\nlearning problems ranging from prediction to classification. Our theoretical\nfindings are supplemented by favorable numerical results.\n","authors":["Damir Filipovic","Michael Multerer","Paul Schneider"],"pdf_url":"https://arxiv.org/pdf/2110.04829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.17101v2","updated":"2023-03-31T14:29:44Z","published":"2022-10-31T07:05:44Z","title":"Unrolled Graph Learning for Multi-Agent Collaboration","summary":"  Multi-agent learning has gained increasing attention to tackle distributed\nmachine learning scenarios under constrictions of data exchanging. However,\nexisting multi-agent learning models usually consider data fusion under fixed\nand compulsory collaborative relations among agents, which is not as flexible\nand autonomous as human collaboration. To fill this gap, we propose a\ndistributed multi-agent learning model inspired by human collaboration, in\nwhich the agents can autonomously detect suitable collaborators and refer to\ncollaborators' model for better performance. To implement such adaptive\ncollaboration, we use a collaboration graph to indicate the pairwise\ncollaborative relation. The collaboration graph can be obtained by graph\nlearning techniques based on model similarity between different agents. Since\nmodel similarity can not be formulated by a fixed graphical optimization, we\ndesign a graph learning network by unrolling, which can learn underlying\nsimilar features among potential collaborators. By testing on both regression\nand classification tasks, we validate that our proposed collaboration model can\nfigure out accurate collaborative relationship and greatly improve agents'\nlearning performance.\n","authors":["Enpei Zhang","Shuo Tang","Xiaowen Dong","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2210.17101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18087v1","updated":"2023-03-31T14:24:06Z","published":"2023-03-31T14:24:06Z","title":"Evaluation Challenges for Geospatial ML","summary":"  As geospatial machine learning models and maps derived from their predictions\nare increasingly used for downstream analyses in science and policy, it is\nimperative to evaluate their accuracy and applicability. Geospatial machine\nlearning has key distinctions from other learning paradigms, and as such, the\ncorrect way to measure performance of spatial machine learning outputs has been\na topic of debate. In this paper, I delineate unique challenges of model\nevaluation for geospatial machine learning with global or remotely sensed\ndatasets, culminating in concrete takeaways to improve evaluations of\ngeospatial model performance.\n","authors":["Esther Rolf"],"pdf_url":"https://arxiv.org/pdf/2303.18087v1.pdf","comment":"ICLR 2023 Workshop on Machine Learning for Remote Sensing"},{"id":"http://arxiv.org/abs/2303.18083v1","updated":"2023-03-31T14:21:53Z","published":"2023-03-31T14:21:53Z","title":"Analysis and Comparison of Two-Level KFAC Methods for Training Deep\n  Neural Networks","summary":"  As a second-order method, the Natural Gradient Descent (NGD) has the ability\nto accelerate training of neural networks. However, due to the prohibitive\ncomputational and memory costs of computing and inverting the Fisher\nInformation Matrix (FIM), efficient approximations are necessary to make NGD\nscalable to Deep Neural Networks (DNNs). Many such approximations have been\nattempted. The most sophisticated of these is KFAC, which approximates the FIM\nas a block-diagonal matrix, where each block corresponds to a layer of the\nneural network. By doing so, KFAC ignores the interactions between different\nlayers. In this work, we investigate the interest of restoring some\nlow-frequency interactions between the layers by means of two-level methods.\nInspired from domain decomposition, several two-level corrections to KFAC using\ndifferent coarse spaces are proposed and assessed. The obtained results show\nthat incorporating the layer interactions in this fashion does not really\nimprove the performance of KFAC. This suggests that it is safe to discard the\noff-diagonal blocks of the FIM, since the block-diagonal approach is\nsufficiently robust, accurate and economical in computation time.\n","authors":["Abdoulaye Koroko","Ani Anciaux-Sedrakian","Ibtihel Ben Gharbia","Valérie Garès","Mounir Haddou","Quang Huy Tran"],"pdf_url":"https://arxiv.org/pdf/2303.18083v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2210.12054v2","updated":"2023-03-31T14:06:30Z","published":"2022-10-21T15:48:22Z","title":"Towards Global Neural Network Abstractions with Locally-Exact\n  Reconstruction","summary":"  Neural networks are a powerful class of non-linear functions. However, their\nblack-box nature makes it difficult to explain their behaviour and certify\ntheir safety. Abstraction techniques address this challenge by transforming the\nneural network into a simpler, over-approximated function. Unfortunately,\nexisting abstraction techniques are slack, which limits their applicability to\nsmall local regions of the input domain. In this paper, we propose Global\nInterval Neural Network Abstractions with Center-Exact Reconstruction\n(GINNACER). Our novel abstraction technique produces sound over-approximation\nbounds over the whole input domain while guaranteeing exact reconstructions for\nany given local input. Our experiments show that GINNACER is several orders of\nmagnitude tighter than state-of-the-art global abstraction techniques, while\nbeing competitive with local ones.\n","authors":["Edoardo Manino","Iury Bessa","Lucas Cordeiro"],"pdf_url":"https://arxiv.org/pdf/2210.12054v2.pdf","comment":"Under submission to the Neural Networks Journal (revised version).\n  Sections 2, 4.7, 5.4, Appendix A and B have been added"},{"id":"http://arxiv.org/abs/2211.16566v3","updated":"2023-03-31T13:31:33Z","published":"2022-11-29T20:00:11Z","title":"Relative Sparsity for Medical Decision Problems","summary":"  Existing statistical methods can estimate a policy, or a mapping from\ncovariates to decisions, which can then instruct decision makers (e.g., whether\nto administer hypotension treatment based on covariates blood pressure and\nheart rate). There is great interest in using such data-driven policies in\nhealthcare. However, it is often important to explain to the healthcare\nprovider, and to the patient, how a new policy differs from the current\nstandard of care. This end is facilitated if one can pinpoint the aspects of\nthe policy (i.e., the parameters for blood pressure and heart rate) that change\nwhen moving from the standard of care to the new, suggested policy. To this\nend, we adapt ideas from Trust Region Policy Optimization (TRPO). In our work,\nhowever, unlike in TRPO, the difference between the suggested policy and\nstandard of care is required to be sparse, aiding with interpretability. This\nyields ``relative sparsity,\" where, as a function of a tuning parameter,\n$\\lambda$, we can approximately control the number of parameters in our\nsuggested policy that differ from their counterparts in the standard of care\n(e.g., heart rate only). We propose a criterion for selecting $\\lambda$,\nperform simulations, and illustrate our method with a real, observational\nhealthcare dataset, deriving a policy that is easy to explain in the context of\nthe current standard of care. Our work promotes the adoption of data-driven\ndecision aids, which have great potential to improve health outcomes.\n","authors":["Samuel J. Weisenthal","Sally W. Thurston","Ashkan Ertefaie"],"pdf_url":"https://arxiv.org/pdf/2211.16566v3.pdf","comment":"55 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.18047v1","updated":"2023-03-31T13:29:27Z","published":"2023-03-31T13:29:27Z","title":"Differentially Private Stochastic Convex Optimization in (Non)-Euclidean\n  Space Revisited","summary":"  In this paper, we revisit the problem of Differentially Private Stochastic\nConvex Optimization (DP-SCO) in Euclidean and general $\\ell_p^d$ spaces.\nSpecifically, we focus on three settings that are still far from well\nunderstood: (1) DP-SCO over a constrained and bounded (convex) set in Euclidean\nspace; (2) unconstrained DP-SCO in $\\ell_p^d$ space; (3) DP-SCO with\nheavy-tailed data over a constrained and bounded set in $\\ell_p^d$ space. For\nproblem (1), for both convex and strongly convex loss functions, we propose\nmethods whose outputs could achieve (expected) excess population risks that are\nonly dependent on the Gaussian width of the constraint set rather than the\ndimension of the space. Moreover, we also show the bound for strongly convex\nfunctions is optimal up to a logarithmic factor. For problems (2) and (3), we\npropose several novel algorithms and provide the first theoretical results for\nboth cases when $1<p<2$ and $2\\leq p\\leq \\infty$.\n","authors":["Jinyan Su","Changhong Zhao","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2303.18047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18042v1","updated":"2023-03-31T13:22:28Z","published":"2023-03-31T13:22:28Z","title":"Scardina: Scalable Join Cardinality Estimation by Multiple Density\n  Estimators","summary":"  In recent years, machine learning-based cardinality estimation methods are\nreplacing traditional methods. This change is expected to contribute to one of\nthe most important applications of cardinality estimation, the query optimizer,\nto speed up query processing. However, none of the existing methods do not\nprecisely estimate cardinalities when relational schemas consist of many tables\nwith strong correlations between tables/attributes. This paper describes that\nmultiple density estimators can be combined to effectively target the\ncardinality estimation of data with large and complex schemas having strong\ncorrelations. We propose Scardina, a new join cardinality estimation method\nusing multiple partitioned models based on the schema structure.\n","authors":["Ryuichi Ito","Yuya Sasaki","Chuan Xiao","Makoto Onizuka"],"pdf_url":"https://arxiv.org/pdf/2303.18042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.06484v4","updated":"2023-03-31T13:20:56Z","published":"2022-06-13T21:30:29Z","title":"On Image Segmentation With Noisy Labels: Characterization and Volume\n  Properties of the Optimal Solutions to Accuracy and Dice","summary":"  We study two of the most popular performance metrics in medical image\nsegmentation, Accuracy and Dice, when the target labels are noisy. For both\nmetrics, several statements related to characterization and volume properties\nof the set of optimal segmentations are proved, and associated experiments are\nprovided. Our main insights are: (i) the volume of the solutions to both\nmetrics may deviate significantly from the expected volume of the target, (ii)\nthe volume of a solution to Accuracy is always less than or equal to the volume\nof a solution to Dice and (iii) the optimal solutions to both of these metrics\ncoincide when the set of feasible segmentations is constrained to the set of\nsegmentations with the volume equal to the expected volume of the target.\n","authors":["Marcus Nordström","Henrik Hult","Jonas Söderberg","Fredrik Löfman"],"pdf_url":"https://arxiv.org/pdf/2206.06484v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18037v1","updated":"2023-03-31T13:14:36Z","published":"2023-03-31T13:14:36Z","title":"Traffic Sign Recognition Dataset and Data Augmentation","summary":"  Although there are many datasets for traffic sign classification, there are\nfew datasets collected for traffic sign recognition and few of them obtain\nenough instances especially for training a model with the deep learning method.\nThe deep learning method is almost the only way to train a model for real-world\nusage that covers various highly similar classes compared with the traditional\nway such as through color, shape, etc. Also, for some certain sign classes,\ntheir sign meanings were destined to can't get enough instances in the dataset.\nTo solve this problem, we purpose a unique data augmentation method for the\ntraffic sign recognition dataset that takes advantage of the standard of the\ntraffic sign. We called it TSR dataset augmentation. We based on the benchmark\nTsinghua-Tencent 100K (TT100K) dataset to verify the unique data augmentation\nmethod. we performed the method on four main iteration version datasets based\non the TT100K dataset and the experimental results showed our method is\nefficacious. The iteration version datasets based on TT100K, data augmentation\nmethod source code and the training results introduced in this paper are\npublicly available.\n","authors":["Jingzhan Ge"],"pdf_url":"https://arxiv.org/pdf/2303.18037v1.pdf","comment":"14pages, 11 figures"},{"id":"http://arxiv.org/abs/2204.05184v3","updated":"2023-03-31T13:10:05Z","published":"2022-04-06T08:06:27Z","title":"Domain Adversarial Graph Convolutional Network Based on RSSI and\n  Crowdsensing for Indoor Localization","summary":"  In recent years, the use of WiFi fingerprints for indoor positioning has\ngrown in popularity, largely due to the widespread availability of WiFi and the\nproliferation of mobile communication devices. However, many existing methods\nfor constructing fingerprint datasets rely on labor-intensive and\ntime-consuming processes of collecting large amounts of data. Additionally,\nthese methods often focus on ideal laboratory environments, rather than\nconsidering the practical challenges of large multi-floor buildings. To address\nthese issues, we present a novel WiDAGCN model that can be trained using a\nsmall number of labeled site survey data and large amounts of unlabeled\ncrowdsensed WiFi fingerprints. By constructing heterogeneous graphs based on\nreceived signal strength indicators (RSSIs) between waypoints and WiFi access\npoints (APs), our model is able to effectively capture the topological\nstructure of the data. We also incorporate graph convolutional networks (GCNs)\nto extract graph-level embeddings, a feature that has been largely overlooked\nin previous WiFi indoor localization studies. To deal with the challenges of\nlarge amounts of unlabeled data and multiple data domains, we employ a\nsemi-supervised domain adversarial training scheme to effectively utilize\nunlabeled data and align the data distributions across domains. Our system is\nevaluated using a public indoor localization dataset that includes multiple\nbuildings, and the results show that it performs competitively in terms of\nlocalization accuracy in large buildings.\n","authors":["Mingxin Zhang","Zipei Fan","Ryosuke Shibasaki","Xuan Song"],"pdf_url":"https://arxiv.org/pdf/2204.05184v3.pdf","comment":"IEEE Internet of Things Journal"},{"id":"http://arxiv.org/abs/2303.18031v1","updated":"2023-03-31T13:08:31Z","published":"2023-03-31T13:08:31Z","title":"Simple Domain Generalization Methods are Strong Baselines for Open\n  Domain Generalization","summary":"  In real-world applications, a machine learning model is required to handle an\nopen-set recognition (OSR), where unknown classes appear during the inference,\nin addition to a domain shift, where the distribution of data differs between\nthe training and inference phases. Domain generalization (DG) aims to handle\nthe domain shift situation where the target domain of the inference phase is\ninaccessible during model training. Open domain generalization (ODG) takes into\naccount both DG and OSR. Domain-Augmented Meta-Learning (DAML) is a method\ntargeting ODG but has a complicated learning process. On the other hand,\nalthough various DG methods have been proposed, they have not been evaluated in\nODG situations. This work comprehensively evaluates existing DG methods in ODG\nand shows that two simple DG methods, CORrelation ALignment (CORAL) and Maximum\nMean Discrepancy (MMD), are competitive with DAML in several cases. In\naddition, we propose simple extensions of CORAL and MMD by introducing the\ntechniques used in DAML, such as ensemble learning and Dirichlet mixup data\naugmentation. The experimental evaluation demonstrates that the extended CORAL\nand MMD can perform comparably to DAML with lower computational costs. This\nsuggests that the simple DG methods and their simple extensions are strong\nbaselines for ODG. The code used in the experiments is available at\nhttps://github.com/shiralab/OpenDG-Eval.\n","authors":["Masashi Noguchi","Shinichi Shirakawa"],"pdf_url":"https://arxiv.org/pdf/2303.18031v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.18017v1","updated":"2023-03-31T12:49:37Z","published":"2023-03-31T12:49:37Z","title":"Rapid prediction of lab-grown tissue properties using deep learning","summary":"  The interactions between cells and the extracellular matrix are vital for the\nself-organisation of tissues. In this paper we present proof-of-concept to use\nmachine learning tools to predict the role of this mechanobiology in the\nself-organisation of cell-laden hydrogels grown in tethered moulds. We develop\na process for the automated generation of mould designs with and without key\nsymmetries. We create a large training set with $N=6500$ cases by running\ndetailed biophysical simulations of cell-matrix interactions using the\ncontractile network dipole orientation (CONDOR) model for the self-organisation\nof cellular hydrogels within these moulds. These are used to train an\nimplementation of the \\texttt{pix2pix} deep learning model, reserving $740$\ncases that were unseen in the training of the neural network for training and\nvalidation. Comparison between the predictions of the machine learning\ntechnique and the reserved predictions from the biophysical algorithm show that\nthe machine learning algorithm makes excellent predictions. The machine\nlearning algorithm is significantly faster than the biophysical method, opening\nthe possibility of very high throughput rational design of moulds for\npharmaceutical testing, regenerative medicine and fundamental studies of\nbiology. Future extensions for scaffolds and 3D bioprinting will open\nadditional applications.\n","authors":["Allison E. Andrews","Hugh Dickinson","James P. Hague"],"pdf_url":"https://arxiv.org/pdf/2303.18017v1.pdf","comment":"26 Pages, 11 Figures"},{"id":"http://arxiv.org/abs/2203.00936v3","updated":"2023-03-31T12:41:19Z","published":"2022-03-02T08:36:22Z","title":"Continual Learning of Multi-modal Dynamics with External Memory","summary":"  We study the problem of fitting a model to a dynamical environment when new\nmodes of behavior emerge sequentially. The learning model is aware when a new\nmode appears, but it does not have access to the true modes of individual\ntraining sequences. The state-of-the-art continual learning approaches cannot\nhandle this setup, because parameter transfer suffers from catastrophic\ninterference and episodic memory design requires the knowledge of the\nground-truth modes of sequences. We devise a novel continual learning method\nthat overcomes both limitations by maintaining a descriptor of the mode of an\nencountered sequence in a neural episodic memory. We employ a Dirichlet Process\nprior on the attention weights of the memory to foster efficient storage of the\nmode descriptors. Our method performs continual learning by transferring\nknowledge across tasks by retrieving the descriptors of similar modes of past\ntasks to the mode of a current sequence and feeding this descriptor into its\ntransition kernel as control input. We observe the continual learning\nperformance of our method to compare favorably to the mainstream parameter\ntransfer approach.\n","authors":["Abdullah Akgül","Gozde Unal","Melih Kandemir"],"pdf_url":"https://arxiv.org/pdf/2203.00936v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.08859v3","updated":"2023-03-31T12:39:27Z","published":"2023-01-21T02:34:06Z","title":"Logical Message Passing Networks with One-hop Inference on Atomic\n  Formulas","summary":"  Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot\nof attention to potentially support many applications. Given that KGs are\nusually incomplete, neural models are proposed to answer the logical queries by\nparameterizing set operators with complex neural networks. However, such\nmethods usually train neural set operators with a large number of entity and\nrelation embeddings from the zero, where whether and how the embeddings or the\nneural set operators contribute to the performance remains not clear. In this\npaper, we propose a simple framework for complex query answering that\ndecomposes the KG embeddings from neural set operators. We propose to represent\nthe complex queries into the query graph. On top of the query graph, we propose\nthe Logical Message Passing Neural Network (LMPNN) that connects the local\none-hop inferences on atomic formulas to the global logical reasoning for\ncomplex query answering. We leverage existing effective KG embeddings to\nconduct one-hop inferences on atomic formulas, the results of which are\nregarded as the messages passed in LMPNN. The reasoning process over the\noverall logical formulas is turned into the forward pass of LMPNN that\nincrementally aggregates local information to finally predict the answers'\nembeddings. The complex logical inference across different types of queries\nwill then be learned from training examples based on the LMPNN architecture.\nTheoretically, our query-graph represenation is more general than the\nprevailing operator-tree formulation, so our approach applies to a broader\nrange of complex KG queries. Empirically, our approach yields the new\nstate-of-the-art neural CQA model. Our research bridges the gap between complex\nKG query answering tasks and the long-standing achievements of knowledge graph\nrepresentation learning.\n","authors":["Zihao Wang","Yangqiu Song","Ginny Y. Wong","Simon See"],"pdf_url":"https://arxiv.org/pdf/2301.08859v3.pdf","comment":"Accepted by ICLR 2023. 20 pages, 4 figures, and 9 tables. Our\n  implementation can be found at https://github.com/HKUST-KnowComp/LMPNN update\n  v3: typo fix. update v2: add code repository"},{"id":"http://arxiv.org/abs/2303.18005v1","updated":"2023-03-31T12:26:29Z","published":"2023-03-31T12:26:29Z","title":"Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic\n  Review","summary":"  Purpose - To characterise and assess the quality of published research\nevaluating artificial intelligence (AI) methods for ovarian cancer diagnosis or\nprognosis using histopathology data. Methods - A search of 5 sources was\nconducted up to 01/12/2022. The inclusion criteria required that research\nevaluated AI on histopathology images for diagnostic or prognostic inferences\nin ovarian cancer, including tubo-ovarian and peritoneal tumours. Reviews and\nnon-English language articles were excluded. The risk of bias was assessed for\nevery included model using PROBAST. Results - A total of 1434 research articles\nwere identified, of which 36 were eligible for inclusion. These studies\nreported 62 models of interest, including 35 classifiers, 14 survival\nprediction models, 7 segmentation models, and 6 regression models. Models were\ndeveloped using 1-1375 slides from 1-664 ovarian cancer patients. A wide array\nof outcomes were predicted, including overall survival (9/62), histological\nsubtypes (7/62), stain quantity (6/62) and malignancy (5/62). Older studies\nused traditional machine learning (ML) models with hand-crafted features, while\nnewer studies typically employed deep learning (DL) to automatically learn\nfeatures and predict the outcome(s) of interest. All models were found to be at\nhigh or unclear risk of bias overall. Research was frequently limited by\ninsufficient reporting, small sample sizes, and insufficient validation.\nConclusion - Limited research has been conducted and none of the associated\nmodels have been demonstrated to be ready for real-world implementation.\nRecommendations are provided addressing underlying biases and flaws in study\ndesign, which should help inform higher-quality reproducible future research.\nKey aspects include more transparent and comprehensive reporting, and improved\nperformance evaluation using cross-validation and external validations.\n","authors":["Jack Breen","Katie Allen","Kieran Zucker","Pratik Adusumilli","Andy Scarsbrook","Geoff Hall","Nicolas M. Orsi","Nishant Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2303.18005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17995v1","updated":"2023-03-31T12:11:21Z","published":"2023-03-31T12:11:21Z","title":"Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series\n  Separation by Entropy Features, Python Package for NNetEn Calculation","summary":"  Entropy measures are effective features for time series classification\nproblems. Traditional entropy measures, such as Shannon entropy, use\nprobability distribution function. However, for the effective separation of\ntime series, new entropy estimation methods are required to characterize the\nchaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn)\nis based on the classification of special datasets (MNIST-10 and\nSARS-CoV-2-RBV1) in relation to the entropy of the time series recorded in the\nreservoir of the LogNNet neural network. NNetEn estimates the chaotic dynamics\nof time series in an original way. Based on the NNetEn algorithm, we propose\ntwo new classification metrics: R2 Efficiency and Pearson Efficiency. The\nefficiency of NNetEn is verified on separation of two chaotic time series of\nsine mapping using dispersion analysis (ANOVA). For two close dynamic time\nseries (r = 1.1918 and r = 1.2243), the F-ratio has reached the value of 124\nand reflects high efficiency of the introduced method in classification\nproblems. The EEG signal classification for healthy persons and patients with\nAlzheimer disease illustrates the practical application of the NNetEn features.\nOur computations demonstrate the synergistic effect of increasing\nclassification accuracy when applying traditional entropy measures and the\nNNetEn concept conjointly. An implementation of the algorithms in Python is\npresented.\n","authors":["Andrei Velichko","Maksim Belyaev","Yuriy Izotov","Murugappan Murugappan","Hanif Heidari"],"pdf_url":"https://arxiv.org/pdf/2303.17995v1.pdf","comment":"24 pages, 18 figures, 2 tables"},{"id":"http://arxiv.org/abs/2303.17992v1","updated":"2023-03-31T12:09:36Z","published":"2023-03-31T12:09:36Z","title":"A fast Multiplicative Updates algorithm for Non-negative Matrix\n  Factorization","summary":"  Nonnegative Matrix Factorization is an important tool in unsupervised machine\nlearning to decompose a data matrix into a product of parts that are often\ninterpretable. Many algorithms have been proposed during the last three\ndecades. A well-known method is the Multiplicative Updates algorithm proposed\nby Lee and Seung in 2002. Multiplicative updates have many interesting\nfeatures: they are simple to implement and can be adapted to popular variants\nsuch as sparse Nonnegative Matrix Factorization, and, according to recent\nbenchmarks, is state-of-the-art for many problems where the loss function is\nnot the Frobenius norm. In this manuscript, we propose to improve the\nMultiplicative Updates algorithm seen as an alternating majorization\nminimization algorithm by crafting a tighter upper bound of the Hessian matrix\nfor each alternate subproblem. Convergence is still ensured and we observe in\npractice on both synthetic and real world dataset that the proposed fastMU\nalgorithm is often several orders of magnitude faster than the regular\nMultiplicative Updates algorithm, and can even be competitive with\nstate-of-the-art methods for the Frobenius loss.\n","authors":["Mai-Quyen Pham","Jérémy Cohen","Thierry Chonavel"],"pdf_url":"https://arxiv.org/pdf/2303.17992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11674v2","updated":"2023-03-31T11:55:55Z","published":"2023-03-21T08:36:34Z","title":"ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency\n  Transform for Domain Generalization","summary":"  Domain generalization (DG) aims to learn a model that generalizes well to\nunseen target domains utilizing multiple source domains without re-training.\nMost existing DG works are based on convolutional neural networks (CNNs).\nHowever, the local operation of the convolution kernel makes the model focus\ntoo much on local representations (e.g., texture), which inherently causes the\nmodel more prone to overfit to the source domains and hampers its\ngeneralization ability. Recently, several MLP-based methods have achieved\npromising results in supervised learning tasks by learning global interactions\namong different patches of the image. Inspired by this, in this paper, we first\nanalyze the difference between CNN and MLP methods in DG and find that MLP\nmethods exhibit a better generalization ability because they can better capture\nthe global representations (e.g., structure) than CNN methods. Then, based on a\nrecent lightweight MLP method, we obtain a strong baseline that outperforms\nmost state-of-the-art CNN-based methods. The baseline can learn global\nstructure representations with a filter to suppress structure irrelevant\ninformation in the frequency space. Moreover, we propose a dynAmic\nLOw-Frequency spectrum Transform (ALOFT) that can perturb local texture\nfeatures while preserving global structure features, thus enabling the filter\nto remove structure-irrelevant information sufficiently. Extensive experiments\non four benchmarks have demonstrated that our method can achieve great\nperformance improvement with a small number of parameters compared to SOTA\nCNN-based DG methods. Our code is available at\nhttps://github.com/lingeringlight/ALOFT/.\n","authors":["Jintao Guo","Na Wang","Lei Qi","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2303.11674v2.pdf","comment":"Accepted by CVPR2023. The code is available at\n  https://github.com/lingeringlight/ALOFT/"},{"id":"http://arxiv.org/abs/2301.10587v2","updated":"2023-03-31T11:37:26Z","published":"2023-01-25T13:45:02Z","title":"On Batching Variable Size Inputs for Training End-to-End Speech\n  Enhancement Systems","summary":"  The performance of neural network-based speech enhancement systems is\nprimarily influenced by the model architecture, whereas training times and\ncomputational resource utilization are primarily affected by training\nparameters such as the batch size. Since noisy and reverberant speech mixtures\ncan have different duration, a batching strategy is required to handle variable\nsize inputs during training, in particular for state-of-the-art end-to-end\nsystems. Such strategies usually strive for a compromise between zero-padding\nand data randomization, and can be combined with a dynamic batch size for a\nmore consistent amount of data in each batch. However, the effect of these\nstrategies on resource utilization and more importantly network performance is\nnot well documented. This paper systematically investigates the effect of\ndifferent batching strategies and batch sizes on the training statistics and\nspeech enhancement performance of a Conv-TasNet, evaluated in both matched and\nmismatched conditions. We find that using a small batch size during training\nimproves performance in both conditions for all batching strategies. Moreover,\nusing sorted or bucket batching with a dynamic batch size allows for reduced\ntraining time and GPU memory usage while achieving similar performance compared\nto random batching with a fixed batch size.\n","authors":["Philippe Gonzalez","Tommy Sonne Alstrøm","Tobias May"],"pdf_url":"https://arxiv.org/pdf/2301.10587v2.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.17971v1","updated":"2023-03-31T11:14:59Z","published":"2023-03-31T11:14:59Z","title":"Promoting Non-Cooperation Through Ordering","summary":"  In many real world situations, like minor traffic offenses in big cities, a\ncentral authority is tasked with periodic administering punishments to a large\nnumber of individuals. Common practice is to give each individual a chance to\nsuffer a smaller fine and be guaranteed to avoid the legal process with\nprobable considerably larger punishment. However, thanks to the large number of\noffenders and a limited capacity of the central authority, the individual risk\nis typically small and a rational individual will not choose to pay the fine.\nHere we show that if the central authority processes the offenders in a\npublicly known order, it properly incentives the offenders to pay the fine. We\nshow analytically and on realistic experiments that our mechanism promotes\nnon-cooperation and incentives individuals to pay. Moreover, the same holds for\nan arbitrary coalition. We quantify the expected total payment the central\nauthority receives, and show it increases considerably.\n","authors":["David Sychrovsky","Sameer Desai","Martin Loebl"],"pdf_url":"https://arxiv.org/pdf/2303.17971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17966v1","updated":"2023-03-31T11:12:25Z","published":"2023-03-31T11:12:25Z","title":"HD-GCN:A Hybrid Diffusion Graph Convolutional Network","summary":"  The information diffusion performance of GCN and its variant models is\nlimited by the adjacency matrix, which can lower their performance. Therefore,\nwe introduce a new framework for graph convolutional networks called Hybrid\nDiffusion-based Graph Convolutional Network (HD-GCN) to address the limitations\nof information diffusion caused by the adjacency matrix. In the HD-GCN\nframework, we initially utilize diffusion maps to facilitate the diffusion of\ninformation among nodes that are adjacent to each other in the feature space.\nThis allows for the diffusion of information between similar points that may\nnot have an adjacent relationship. Next, we utilize graph convolution to\nfurther propagate information among adjacent nodes after the diffusion maps,\nthereby enabling the spread of information among similar nodes that are\nadjacent in the graph. Finally, we employ the diffusion distances obtained\nthrough the use of diffusion maps to regularize and constrain the predicted\nlabels of training nodes. This regularization method is then applied to the\nHD-GCN training, resulting in a smoother classification surface. The model\nproposed in this paper effectively overcomes the limitations of information\ndiffusion imposed only by the adjacency matrix. HD-GCN utilizes hybrid\ndiffusion by combining information diffusion between neighborhood nodes in the\nfeature space and adjacent nodes in the adjacency matrix. This method allows\nfor more comprehensive information propagation among nodes, resulting in\nimproved model performance. We evaluated the performance of DM-GCN on three\nwell-known citation network datasets and the results showed that the proposed\nframework is more effective than several graph-based semi-supervised learning\nmethods.\n","authors":["Zhi Yang","Kang Li","Haitao Gan","Zhongwei Huang","Ming Shi"],"pdf_url":"https://arxiv.org/pdf/2303.17966v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.17963v1","updated":"2023-03-31T11:06:09Z","published":"2023-03-31T11:06:09Z","title":"Learning-Based Optimal Control with Performance Guarantees for Unknown\n  Systems with Latent States","summary":"  As control engineering methods are applied to increasingly complex systems,\ndata-driven approaches for system identification appear as a promising\nalternative to physics-based modeling. While many of these approaches rely on\nthe availability of state measurements, the states of a complex system are\noften not directly measurable. It may then be necessary to jointly estimate the\ndynamics and a latent state, making it considerably more challenging to design\ncontrollers with performance guarantees. This paper proposes a novel method for\nthe computation of an optimal input trajectory for unknown nonlinear systems\nwith latent states. Probabilistic performance guarantees are derived for the\nresulting input trajectory, and an approach to validate the performance of\narbitrary control laws is presented. The effectiveness of the proposed method\nis demonstrated in a numerical simulation.\n","authors":["Robert Lefringhausen","Supitsana Srithasan","Armin Lederer","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2303.17963v1.pdf","comment":"Submitted to the 62nd IEEE Conference on Decision and Control"},{"id":"http://arxiv.org/abs/2303.17956v1","updated":"2023-03-31T10:37:19Z","published":"2023-03-31T10:37:19Z","title":"Ensemble Methods for Multi-Organ Segmentation in CT Series","summary":"  In the medical images field, semantic segmentation is one of the most\nimportant, yet difficult and time-consuming tasks to be performed by\nphysicians. Thanks to the recent advancement in the Deep Learning models\nregarding Computer Vision, the promise to automate this kind of task is getting\nmore and more realistic. However, many problems are still to be solved, like\nthe scarce availability of data and the difficulty to extend the efficiency of\nhighly specialised models to general scenarios. Organs at risk segmentation for\nradiotherapy treatment planning falls in this category, as the limited data\navailable negatively affects the possibility to develop general-purpose models;\nin this work, we focus on the possibility to solve this problem by presenting\nthree types of ensembles of single-organ models able to produce multi-organ\nmasks exploiting the different specialisations of their components. The results\nobtained are promising and prove that this is a possible solution to finding\nefficient multi-organ segmentation methods.\n","authors":["Leonardo Crespi","Paolo Roncaglioni","Damiano Dei","Ciro Franzese","Nicola Lambri","Daniele Loiacono","Pietro Mancosu","Marta Scorsetti"],"pdf_url":"https://arxiv.org/pdf/2303.17956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17951v1","updated":"2023-03-31T10:29:17Z","published":"2023-03-31T10:29:17Z","title":"FP8 versus INT8 for efficient deep learning inference","summary":"  Recently, the idea of using FP8 as a number format for neural network\ntraining has been floating around the deep learning world. Given that most\ntraining is currently conducted with entire networks in FP32, or sometimes FP16\nwith mixed-precision, the step to having some parts of a network run in FP8\nwith 8-bit weights is an appealing potential speed-up for the generally costly\nand time-intensive training procedures in deep learning. A natural question\narises regarding what this development means for efficient inference on edge\ndevices. In the efficient inference device world, workloads are frequently\nexecuted in INT8. Sometimes going even as low as INT4 when efficiency calls for\nit. In this whitepaper, we compare the performance for both the FP8 and INT\nformats for efficient on-device inference. We theoretically show the difference\nbetween the INT and FP formats for neural networks and present a plethora of\npost-training quantization and quantization-aware-training results to show how\nthis theory translates to practice. We also provide a hardware analysis showing\nthat the FP formats are somewhere between 50-180% less efficient in terms of\ncompute in dedicated hardware than the INT format. Based on our research and a\nread of the research field, we conclude that although the proposed FP8 format\ncould be good for training, the results for inference do not warrant a\ndedicated implementation of FP8 in favor of INT8 for efficient inference. We\nshow that our results are mostly consistent with previous findings but that\nimportant comparisons between the formats have thus far been lacking. Finally,\nwe discuss what happens when FP8-trained networks are converted to INT8 and\nconclude with a brief discussion on the most efficient way for on-device\ndeployment and an extensive suite of INT8 results for many models.\n","authors":["Mart van Baalen","Andrey Kuzmin","Suparna S Nair","Yuwei Ren","Eric Mahurin","Chirag Patel","Sundar Subramanian","Sanghyuk Lee","Markus Nagel","Joseph Soriaga","Tijmen Blankevoort"],"pdf_url":"https://arxiv.org/pdf/2303.17951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17949v1","updated":"2023-03-31T10:27:36Z","published":"2023-03-31T10:27:36Z","title":"Unsupervised Anomaly Detection and Localization of Machine Audio: A\n  GAN-based Approach","summary":"  Automatic detection of machine anomaly remains challenging for machine\nlearning. We believe the capability of generative adversarial network (GAN)\nsuits the need of machine audio anomaly detection, yet rarely has this been\ninvestigated by previous work. In this paper, we propose AEGAN-AD, a totally\nunsupervised approach in which the generator (also an autoencoder) is trained\nto reconstruct input spectrograms. It is pointed out that the denoising nature\nof reconstruction deprecates its capacity. Thus, the discriminator is\nredesigned to aid the generator during both training stage and detection stage.\nThe performance of AEGAN-AD on the dataset of DCASE 2022 Challenge TASK 2\ndemonstrates the state-of-the-art result on five machine types. A novel anomaly\nlocalization method is also investigated. Source code available at:\nwww.github.com/jianganbai/AEGAN-AD\n","authors":["Anbai Jiang","Wei-Qiang Zhang","Yufeng Deng","Pingyi Fan","Jia Liu"],"pdf_url":"https://arxiv.org/pdf/2303.17949v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2303.17942v1","updated":"2023-03-31T10:13:01Z","published":"2023-03-31T10:13:01Z","title":"Benchmarking FedAvg and FedCurv for Image Classification Tasks","summary":"  Classic Machine Learning techniques require training on data available in a\nsingle data lake. However, aggregating data from different owners is not always\nconvenient for different reasons, including security, privacy and secrecy. Data\ncarry a value that might vanish when shared with others; the ability to avoid\nsharing the data enables industrial applications where security and privacy are\nof paramount importance, making it possible to train global models by\nimplementing only local policies which can be run independently and even on\nair-gapped data centres. Federated Learning (FL) is a distributed machine\nlearning approach which has emerged as an effective way to address privacy\nconcerns by only sharing local AI models while keeping the data decentralized.\nTwo critical challenges of Federated Learning are managing the heterogeneous\nsystems in the same federated network and dealing with real data, which are\noften not independently and identically distributed (non-IID) among the\nclients. In this paper, we focus on the second problem, i.e., the problem of\nstatistical heterogeneity of the data in the same federated network. In this\nsetting, local models might be strayed far from the local optimum of the\ncomplete dataset, thus possibly hindering the convergence of the federated\nmodel. Several Federated Learning algorithms, such as FedAvg, FedProx and\nFederated Curvature (FedCurv), aiming at tackling the non-IID setting, have\nalready been proposed. This work provides an empirical assessment of the\nbehaviour of FedAvg and FedCurv in common non-IID scenarios. Results show that\nthe number of epochs per round is an important hyper-parameter that, when tuned\nappropriately, can lead to significant performance gains while reducing the\ncommunication cost. As a side product of this work, we release the non-IID\nversion of the datasets we used so to facilitate further comparisons from the\nFL community.\n","authors":["Bruno Casella","Roberto Esposito","Carlo Cavazzoni","Marco Aldinucci"],"pdf_url":"https://arxiv.org/pdf/2303.17942v1.pdf","comment":"12 pages, Proceedings of ITADATA22, The 1st Italian Conference on Big\n  Data and Data Science; Published on CEUR Workshop Proceedings (CEUR-WS.org,\n  ISSN 1613-0073), Vol. 3340, pp. 99-110, 2022"},{"id":"http://arxiv.org/abs/2303.17941v1","updated":"2023-03-31T10:10:05Z","published":"2023-03-31T10:10:05Z","title":"Comparing Adversarial and Supervised Learning for Organs at Risk\n  Segmentation in CT images","summary":"  Organ at Risk (OAR) segmentation from CT scans is a key component of the\nradiotherapy treatment workflow. In recent years, deep learning techniques have\nshown remarkable potential in automating this process. In this paper, we\ninvestigate the performance of Generative Adversarial Networks (GANs) compared\nto supervised learning approaches for segmenting OARs from CT images. We\npropose three GAN-based models with identical generator architectures but\ndifferent discriminator networks. These models are compared with\nwell-established CNN models, such as SE-ResUnet and DeepLabV3, using the\nStructSeg dataset, which consists of 50 annotated CT scans containing contours\nof six OARs. Our work aims to provide insight into the advantages and\ndisadvantages of adversarial training in the context of OAR segmentation. The\nresults are very promising and show that the proposed GAN-based approaches are\nsimilar or superior to their CNN-based counterparts, particularly when\nsegmenting more challenging target organs.\n","authors":["Leonardo Crespi","Mattia Portanti","Daniele Loiacono"],"pdf_url":"https://arxiv.org/pdf/2303.17941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17940v1","updated":"2023-03-31T10:08:23Z","published":"2023-03-31T10:08:23Z","title":"Per-Example Gradient Regularization Improves Learning Signals from Noisy\n  Data","summary":"  Gradient regularization, as described in \\citet{barrett2021implicit}, is a\nhighly effective technique for promoting flat minima during gradient descent.\nEmpirical evidence suggests that this regularization technique can\nsignificantly enhance the robustness of deep learning models against noisy\nperturbations, while also reducing test error. In this paper, we explore the\nper-example gradient regularization (PEGR) and present a theoretical analysis\nthat demonstrates its effectiveness in improving both test error and robustness\nagainst noise perturbations. Specifically, we adopt a signal-noise data model\nfrom \\citet{cao2022benign} and show that PEGR can learn signals effectively\nwhile suppressing noise. In contrast, standard gradient descent struggles to\ndistinguish the signal from the noise, leading to suboptimal generalization\nperformance. Our analysis reveals that PEGR penalizes the variance of pattern\nlearning, thus effectively suppressing the memorization of noises from the\ntraining data. These findings underscore the importance of variance control in\ndeep learning training and offer useful insights for developing more effective\ntraining approaches.\n","authors":["Xuran Meng","Yuan Cao","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2303.17940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17934v1","updated":"2023-03-31T10:00:27Z","published":"2023-03-31T10:00:27Z","title":"Conflict-Averse Gradient Optimization of Ensembles for Effective Offline\n  Model-Based Optimization","summary":"  Data-driven offline model-based optimization (MBO) is an established\npractical approach to black-box computational design problems for which the\ntrue objective function is unknown and expensive to query. However, the\nstandard approach which optimizes designs against a learned proxy model of the\nground truth objective can suffer from distributional shift. Specifically, in\nhigh-dimensional design spaces where valid designs lie on a narrow manifold,\nthe standard approach is susceptible to producing out-of-distribution, invalid\ndesigns that \"fool\" the learned proxy model into outputting a high value. Using\nan ensemble rather than a single model as the learned proxy can help mitigate\ndistribution shift, but naive formulations for combining gradient information\nfrom the ensemble, such as minimum or mean gradient, are still suboptimal and\noften hampered by non-convergent behavior.\n  In this work, we explore alternate approaches for combining gradient\ninformation from the ensemble that are robust to distribution shift without\ncompromising optimality of the produced designs. More specifically, we explore\ntwo functions, formulated as convex optimization problems, for combining\ngradient information: multiple gradient descent algorithm (MGDA) and\nconflict-averse gradient descent (CAGrad). We evaluate these algorithms on a\ndiverse set of five computational design tasks. We compare performance of\nensemble MBO with MGDA and ensemble MBO with CAGrad with three naive baseline\nalgorithms: (a) standard single-model MBO, (b) ensemble MBO with mean gradient,\nand (c) ensemble MBO with minimum gradient.\n  Our results suggest that MGDA and CAGrad strike a desirable balance between\nconservatism and optimality and can help robustify data-driven offline MBO\nwithout compromising optimality of designs.\n","authors":["Sathvik Kolli"],"pdf_url":"https://arxiv.org/pdf/2303.17934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17933v1","updated":"2023-03-31T09:58:02Z","published":"2023-03-31T09:58:02Z","title":"Learning-based Observer Evaluated on the Kinematic Bicycle Model","summary":"  The knowledge of the states of a vehicle is a necessity to perform proper\nplanning and control. These quantities are usually accessible through\nmeasurements. Control theory brings extremely useful methods -- observers -- to\ndeal with quantities that cannot be directly measured or with noisy\nmeasurements. Classical observers are mathematically derived from models. In\nspite of their success, such as the Kalman filter, they show their limits when\nsystems display high non-linearities, modeling errors, high uncertainties or\ndifficult interactions with the environment (e.g. road contact). In this work,\nwe present a method to build a learning-based observer able to outperform\nclassical observing methods. We compare several neural network architectures\nand define the data generation procedure used to train them. The method is\nevaluated on a kinematic bicycle model which allows to easily generate data for\ntraining and testing. This model is also used in an Extended Kalman Filter\n(EKF) for comparison of the learning-based observer with a state of the art\nmodel-based observer. The results prove the interest of our approach and pave\nthe way for future improvements of the technique.\n","authors":["Agapius Bou Ghosn","Philip Polack","Arnaud de La Fortelle"],"pdf_url":"https://arxiv.org/pdf/2303.17933v1.pdf","comment":"ICSC 2022"},{"id":"http://arxiv.org/abs/2303.16066v2","updated":"2023-03-31T09:54:13Z","published":"2023-03-27T05:29:53Z","title":"Neural Collapse Inspired Federated Learning with Non-iid Data","summary":"  One of the challenges in federated learning is the non-independent and\nidentically distributed (non-iid) characteristics between heterogeneous\ndevices, which cause significant differences in local updates and affect the\nperformance of the central server. Although many studies have been proposed to\naddress this challenge, they only focus on local training and aggregation\nprocesses to smooth the changes and fail to achieve high performance with deep\nlearning models. Inspired by the phenomenon of neural collapse, we force each\nclient to be optimized toward an optimal global structure for classification.\nSpecifically, we initialize it as a random simplex Equiangular Tight Frame\n(ETF) and fix it as the unit optimization target of all clients during the\nlocal updating. After guaranteeing all clients are learning to converge to the\nglobal optimum, we propose to add a global memory vector for each category to\nremedy the parameter fluctuation caused by the bias of the intra-class\ncondition distribution among clients. Our experimental results show that our\nmethod can improve the performance with faster convergence speed on\ndifferent-size datasets.\n","authors":["Chenxi Huang","Liang Xie","Yibo Yang","Wenxiao Wang","Binbin Lin","Deng Cai"],"pdf_url":"https://arxiv.org/pdf/2303.16066v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17925v1","updated":"2023-03-31T09:48:16Z","published":"2023-03-31T09:48:16Z","title":"Beyond Multilayer Perceptrons: Investigating Complex Topologies in\n  Neural Networks","summary":"  In this study, we explore the impact of network topology on the approximation\ncapabilities of artificial neural networks (ANNs), with a particular focus on\ncomplex topologies. We propose a novel methodology for constructing complex\nANNs based on various topologies, including Barab\\'asi-Albert,\nErd\\H{o}s-R\\'enyi, Watts-Strogatz, and multilayer perceptrons (MLPs). The\nconstructed networks are evaluated on synthetic datasets generated from\nmanifold learning generators, with varying levels of task difficulty and noise.\nOur findings reveal that complex topologies lead to superior performance in\nhigh-difficulty regimes compared to traditional MLPs. This performance\nadvantage is attributed to the ability of complex networks to exploit the\ncompositionality of the underlying target function. However, this benefit comes\nat the cost of increased forward-pass computation time and reduced robustness\nto graph damage. Additionally, we investigate the relationship between various\ntopological attributes and model performance. Our analysis shows that no single\nattribute can account for the observed performance differences, suggesting that\nthe influence of network topology on approximation capabilities may be more\nintricate than a simple correlation with individual topological attributes. Our\nstudy sheds light on the potential of complex topologies for enhancing the\nperformance of ANNs and provides a foundation for future research exploring the\ninterplay between multiple topological attributes and their impact on model\nperformance.\n","authors":["Tommaso Boccato","Matteo Ferrante","Andrea Duggento","Nicola Toschi"],"pdf_url":"https://arxiv.org/pdf/2303.17925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17907v1","updated":"2023-03-31T09:09:17Z","published":"2023-03-31T09:09:17Z","title":"Predictive Context-Awareness for Full-Immersive Multiuser Virtual\n  Reality with Redirected Walking","summary":"  Virtual Reality (VR) technology is being advanced along the lines of\nenhancing its immersiveness, enabling multiuser Virtual Experiences (VEs), and\nsupporting unconstrained mobility of the users in their VEs, while constraining\nthem within specialized VR setups through Redirected Walking (RDW). For meeting\nthe extreme data-rate and latency requirements of future VR systems, supporting\nwireless networking infrastructures will operate in millimeter Wave (mmWave)\nfrequencies and leverage highly directional communication in both transmission\nand reception through beamforming and beamsteering. We propose to leverage\npredictive context-awareness for optimizing transmitter and receiver-side\nbeamforming and beamsteering. In particular, we argue that short-term\nprediction of users' lateral movements in multiuser VR setups with RDW can be\nutilized for optimizing transmitter-side beamforming and beamsteering through\nLine-of-Sight (LoS) \"tracking\" in the users' directions. At the same time,\nshort-term prediction of orientational movements can be used for receiver-side\nbeamforming for coverage flexibility enhancements. We target two open problems\nin predicting these two context information instances: i) lateral movement\nprediction in multiuser VR settings with RDW and ii) generation of synthetic\nhead rotation datasets to be utilized in the training of existing orientational\nmovements predictors. We follow by experimentally showing that Long Short-Term\nMemory (LSTM) networks feature promising accuracy in predicting lateral\nmovements, as well as that context-awareness stemming from VEs further benefits\nthis accuracy. Second, we show that a TimeGAN-based approach for orientational\ndata generation can generate synthetic samples closely matching the\nexperimentally obtained ones.\n","authors":["Filip Lemic","Jakob Struye","Thomas Van Onsem","Jeroen Famaey","Xavier Costa Perez"],"pdf_url":"https://arxiv.org/pdf/2303.17907v1.pdf","comment":"7 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2207.07520"},{"id":"http://arxiv.org/abs/2210.03516v2","updated":"2023-03-31T08:55:33Z","published":"2022-10-06T11:06:39Z","title":"Neuroevolution is a Competitive Alternative to Reinforcement Learning\n  for Skill Discovery","summary":"  Deep Reinforcement Learning (RL) has emerged as a powerful paradigm for\ntraining neural policies to solve complex control tasks. However, these\npolicies tend to be overfit to the exact specifications of the task and\nenvironment they were trained on, and thus do not perform well when conditions\ndeviate slightly or when composed hierarchically to solve even more complex\ntasks. Recent work has shown that training a mixture of policies, as opposed to\na single one, that are driven to explore different regions of the state-action\nspace can address this shortcoming by generating a diverse set of behaviors,\nreferred to as skills, that can be collectively used to great effect in\nadaptation tasks or for hierarchical planning. This is typically realized by\nincluding a diversity term - often derived from information theory - in the\nobjective function optimized by RL. However these approaches often require\ncareful hyperparameter tuning to be effective. In this work, we demonstrate\nthat less widely-used neuroevolution methods, specifically Quality Diversity\n(QD), are a competitive alternative to information-theory-augmented RL for\nskill discovery. Through an extensive empirical evaluation comparing eight\nstate-of-the-art algorithms (four flagship algorithms from each line of work)\non the basis of (i) metrics directly evaluating the skills' diversity, (ii) the\nskills' performance on adaptation tasks, and (iii) the skills' performance when\nused as primitives for hierarchical planning; QD methods are found to provide\nequal, and sometimes improved, performance whilst being less sensitive to\nhyperparameters and more scalable. As no single method is found to provide\nnear-optimal performance across all environments, there is a rich scope for\nfurther research which we support by proposing future directions and providing\noptimized open-source implementations.\n","authors":["Felix Chalumeau","Raphael Boige","Bryan Lim","Valentin Macé","Maxime Allard","Arthur Flajolet","Antoine Cully","Thomas Pierrot"],"pdf_url":"https://arxiv.org/pdf/2210.03516v2.pdf","comment":"Camera ready version for ICLR2023 (spotlight)"},{"id":"http://arxiv.org/abs/2303.17885v1","updated":"2023-03-31T08:41:42Z","published":"2023-03-31T08:41:42Z","title":"Accelerating Wireless Federated Learning via Nesterov's Momentum and\n  Distributed Principle Component Analysis","summary":"  A wireless federated learning system is investigated by allowing a server and\nworkers to exchange uncoded information via orthogonal wireless channels. Since\nthe workers frequently upload local gradients to the server via\nbandwidth-limited channels, the uplink transmission from the workers to the\nserver becomes a communication bottleneck. Therefore, a one-shot distributed\nprinciple component analysis (PCA) is leveraged to reduce the dimension of\nuploaded gradients such that the communication bottleneck is relieved. A\nPCA-based wireless federated learning (PCA-WFL) algorithm and its accelerated\nversion (i.e., PCA-AWFL) are proposed based on the low-dimensional gradients\nand the Nesterov's momentum. For the non-convex loss functions, a finite-time\nanalysis is performed to quantify the impacts of system hyper-parameters on the\nconvergence of the PCA-WFL and PCA-AWFL algorithms. The PCA-AWFL algorithm is\ntheoretically certified to converge faster than the PCA-WFL algorithm. Besides,\nthe convergence rates of PCA-WFL and PCA-AWFL algorithms quantitatively reveal\nthe linear speedup with respect to the number of workers over the vanilla\ngradient descent algorithm. Numerical results are used to demonstrate the\nimproved convergence rates of the proposed PCA-WFL and PCA-AWFL algorithms over\nthe benchmarks.\n","authors":["Yanjie Dong","Luya Wang","Yuanfang Chi","Jia Wang","Haijun Zhang","Fei Richard Yu","Victor C. M. Leung","Xiping Hu"],"pdf_url":"https://arxiv.org/pdf/2303.17885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17879v1","updated":"2023-03-31T08:26:18Z","published":"2023-03-31T08:26:18Z","title":"CoSMo: a Framework for Implementing Conditioned Process Simulation\n  Models","summary":"  Process simulation is an analysis tool in process mining that allows users to\nmeasure the impact of changes, prevent losses, and update the process without\nrisks or costs. In the literature, several process simulation techniques are\navailable and they are usually built upon process models discovered from a\ngiven event log or learned via deep learning. Each group of approaches has its\nown strengths and limitations. The former is usually restricted to the\ncontrol-flow but it is more interpretable, whereas the latter is not\ninterpretable by nature but has a greater generalization capability on large\nevent logs. Despite the great performance achieved by deep learning approaches,\nthey are still not suitable to be applied to real scenarios and generate value\nfor users. This issue is mainly due to fact their stochasticity is hard to\ncontrol. To address this problem, we propose the CoSMo framework for\nimplementing process simulation models fully based on deep learning. This\nframework enables simulating event logs that satisfy a constraint by\nconditioning the learning phase of a deep neural network. Throughout\nexperiments, the simulation is validated from both control-flow and data-flow\nperspectives, demonstrating the proposed framework's capability of simulating\ncases while satisfying imposed conditions.\n","authors":["Rafael S. Oyamada","Gabriel M. Tavares","Paolo Ceravolo"],"pdf_url":"https://arxiv.org/pdf/2303.17879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17878v1","updated":"2023-03-31T08:26:17Z","published":"2023-03-31T08:26:17Z","title":"Fused Depthwise Tiling for Memory Optimization in TinyML Deep Neural\n  Network Inference","summary":"  Memory optimization for deep neural network (DNN) inference gains high\nrelevance with the emergence of TinyML, which refers to the deployment of DNN\ninference tasks on tiny, low-power microcontrollers. Applications such as audio\nkeyword detection or radar-based gesture recognition are heavily constrained by\nthe limited memory on such tiny devices because DNN inference requires large\nintermediate run-time buffers to store activations and other intermediate data,\nwhich leads to high memory usage. In this paper, we propose a new Fused\nDepthwise Tiling (FDT) method for the memory optimization of DNNs, which,\ncompared to existing tiling methods, reduces memory usage without inducing any\nrun time overhead. FDT applies to a larger variety of network layers than\nexisting tiling methods that focus on convolutions. It improves TinyML memory\noptimization significantly by reducing memory of models where this was not\npossible before and additionally providing alternative design points for models\nthat show high run time overhead with existing methods. In order to identify\nthe best tiling configuration, an end-to-end flow with a new path discovery\nmethod is proposed, which applies FDT and existing tiling methods in a fully\nautomated way, including the scheduling of the operations and planning of the\nlayout of buffers in memory. Out of seven evaluated models, FDT achieved\nsignificant memory reduction for two models by 76.2% and 18.1% where existing\ntiling methods could not be applied. Two other models showed a significant run\ntime overhead with existing methods and FDT provided alternative design points\nwith no overhead but reduced memory savings.\n","authors":["Rafael Stahl","Daniel Mueller-Gritschneder","Ulf Schlichtmann"],"pdf_url":"https://arxiv.org/pdf/2303.17878v1.pdf","comment":"Accepted as a full paper by the TinyML Research Symposium 2023"},{"id":"http://arxiv.org/abs/2206.14262v2","updated":"2023-03-31T08:12:59Z","published":"2022-06-28T19:34:44Z","title":"Supervised Training of Conditional Monge Maps","summary":"  Optimal transport (OT) theory describes general principles to define and\nselect, among many possible choices, the most efficient way to map a\nprobability measure onto another. That theory has been mostly used to estimate,\ngiven a pair of source and target probability measures $(\\mu, \\nu)$, a\nparameterized map $T_\\theta$ that can efficiently map $\\mu$ onto $\\nu$. In many\napplications, such as predicting cell responses to treatments, pairs of\ninput/output data measures $(\\mu, \\nu)$ that define optimal transport problems\ndo not arise in isolation but are associated with a context $c$, as for\ninstance a treatment when comparing populations of untreated and treated cells.\nTo account for that context in OT estimation, we introduce CondOT, a multi-task\napproach to estimate a family of OT maps conditioned on a context variable,\nusing several pairs of measures $\\left(\\mu_i, \\nu_i\\right)$ tagged with a\ncontext label $c_i$. CondOT learns a global map $\\mathcal{T}_\\theta$\nconditioned on context that is not only expected to fit all labeled pairs in\nthe dataset $\\left\\{\\left(c_i,\\left(\\mu_i, \\nu_i\\right)\\right)\\right\\}$, i.e.,\n$\\mathcal{T}_\\theta\\left(c_i\\right) \\sharp \\mu_i \\approx \\nu_i$, but should\nalso generalize to produce meaningful maps $\\mathcal{T}_\\theta\\left(c_{\\text\n{new }}\\right)$ when conditioned on unseen contexts $c_{\\text {new }}$. Our\napproach harnesses and provides a novel usage for partially input convex neural\nnetworks, for which we introduce a robust and efficient initialization strategy\ninspired by Gaussian approximations. We demonstrate the ability of CondOT to\ninfer the effect of an arbitrary combination of genetic or therapeutic\nperturbations on single cells, using only observations of the effects of said\nperturbations separately.\n","authors":["Charlotte Bunne","Andreas Krause","Marco Cuturi"],"pdf_url":"https://arxiv.org/pdf/2206.14262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16539v2","updated":"2023-03-31T08:02:01Z","published":"2022-10-29T09:18:41Z","title":"Exploiting prompt learning with pre-trained language models for\n  Alzheimer's Disease detection","summary":"  Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating\npreventive care and to delay further progression. Speech based automatic AD\nscreening systems provide a non-intrusive and more scalable alternative to\nother clinical screening techniques. Textual embedding features produced by\npre-trained language models (PLMs) such as BERT are widely used in such\nsystems. However, PLM domain fine-tuning is commonly based on the masked word\nor sentence prediction costs that are inconsistent with the back-end AD\ndetection task. To this end, this paper investigates the use of prompt-based\nfine-tuning of PLMs that consistently uses AD classification errors as the\ntraining objective function. Disfluency features based on hesitation or pause\nfiller token frequencies are further incorporated into prompt phrases during\nPLM fine-tuning. The decision voting based combination among systems using\ndifferent PLMs (BERT and RoBERTa) or systems with different fine-tuning\nparadigms (conventional masked-language modelling fine-tuning and prompt-based\nfine-tuning) is further applied. Mean, standard deviation and the maximum among\naccuracy scores over 15 experiment runs are adopted as performance measurements\nfor the AD detection system. Mean detection accuracy of 84.20% (with std 2.09%,\nbest 87.5%) and 82.64% (with std 4.0%, best 89.58%) were obtained using manual\nand ASR speech transcripts respectively on the ADReSS20 test set consisting of\n48 elderly speakers.\n","authors":["Yi Wang","Jiajun Deng","Tianzi Wang","Bo Zheng","Shoukang Hu","Xunying Liu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2210.16539v2.pdf","comment":"Accepted ICASSP 2023 (will update with IEEE vision later)"},{"id":"http://arxiv.org/abs/2303.17867v1","updated":"2023-03-31T08:01:21Z","published":"2023-03-31T08:01:21Z","title":"CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer","summary":"  Content affinity loss including feature and pixel affinity is a main problem\nwhich leads to artifacts in photorealistic and video style transfer. This paper\nproposes a new framework named CAP-VSTNet, which consists of a new reversible\nresidual network and an unbiased linear transform module, for versatile style\ntransfer. This reversible residual network can not only preserve content\naffinity but not introduce redundant information as traditional reversible\nnetworks, and hence facilitate better stylization. Empowered by Matting\nLaplacian training loss which can address the pixel affinity loss problem led\nby the linear transform, the proposed framework is applicable and effective on\nversatile style transfer. Extensive experiments show that CAP-VSTNet can\nproduce better qualitative and quantitative results in comparison with the\nstate-of-the-art methods.\n","authors":["Linfeng Wen","Chengying Gao","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2303.17867v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2209.01701v2","updated":"2023-03-31T07:56:10Z","published":"2022-09-04T22:12:13Z","title":"Concatenated Classic and Neural (CCN) Codes: ConcatenatedAE","summary":"  Small neural networks (NNs) used for error correction were shown to improve\non classic channel codes and to address channel model changes. We extend the\ncode dimension of any such structure by using the same NN under one-hot\nencoding multiple times, then serially-concatenated with an outer classic code.\nWe design NNs with the same network parameters, where each Reed-Solomon\ncodeword symbol is an input to a different NN. Significant improvements in\nblock error probabilities for an additive Gaussian noise channel as compared to\nthe small neural code are illustrated, as well as robustness to channel model\nchanges.\n","authors":["Onur Günlü","Rick Fritschek","Rafael F. Schaefer"],"pdf_url":"https://arxiv.org/pdf/2209.01701v2.pdf","comment":"6 pages, IEEE WCNC 2023"},{"id":"http://arxiv.org/abs/2303.15999v3","updated":"2023-03-31T07:54:40Z","published":"2023-03-28T14:15:13Z","title":"Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised\n  Regression Deep Learning Models","summary":"  In this work, the authors develop regression approaches based on deep\nlearning to perform thread density estimation for plain weave canvas analysis.\nPrevious approaches were based on Fourier analysis, which is quite robust for\nsome scenarios but fails in some others, in machine learning tools, that\ninvolve pre-labeling of the painting at hand, or the segmentation of thread\ncrossing points, that provides good estimations in all scenarios with no need\nof pre-labeling. The segmentation approach is time-consuming as the estimation\nof the densities is performed after locating the crossing points. In this novel\nproposal, we avoid this step by computing the density of threads directly from\nthe image with a regression deep learning model. We also incorporate some\nimprovements in the initial preprocessing of the input image with an impact on\nthe final error. Several models are proposed and analyzed to retain the best\none. Furthermore, we further reduce the density estimation error by introducing\na semi-supervised approach. The performance of our novel algorithm is analyzed\nwith works by Ribera, Vel\\'azquez, and Poussin where we compare our results to\nthe ones of previous approaches. Finally, the method is put into practice to\nsupport the change of authorship or a masterpiece at the Museo del Prado.\n","authors":["A. D. Bejarano","Juan J. Murillo-Fuentes","Laura Alba-Carcelen"],"pdf_url":"https://arxiv.org/pdf/2303.15999v3.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2202.05722v2","updated":"2023-03-31T07:46:26Z","published":"2022-02-11T15:59:01Z","title":"The Schrödinger Bridge between Gaussian Measures has a Closed Form","summary":"  The static optimal transport $(\\mathrm{OT})$ problem between Gaussians seeks\nto recover an optimal map, or more generally a coupling, to morph a Gaussian\ninto another. It has been well studied and applied to a wide variety of tasks.\nHere we focus on the dynamic formulation of OT, also known as the Schr\\\"odinger\nbridge (SB) problem, which has recently seen a surge of interest in machine\nlearning due to its connections with diffusion-based generative models. In\ncontrast to the static setting, much less is known about the dynamic setting,\neven for Gaussian distributions. In this paper, we provide closed-form\nexpressions for SBs between Gaussian measures. In contrast to the static\nGaussian OT problem, which can be simply reduced to studying convex programs,\nour framework for solving SBs requires significantly more involved tools such\nas Riemannian geometry and generator theory. Notably, we establish that the\nsolutions of SBs between Gaussian measures are themselves Gaussian processes\nwith explicit mean and covariance kernels, and thus are readily amenable for\nmany downstream applications such as generative modeling or interpolation. To\ndemonstrate the utility, we devise a new method for modeling the evolution of\nsingle-cell genomics data and report significantly improved numerical stability\ncompared to existing SB-based approaches.\n","authors":["Charlotte Bunne","Ya-Ping Hsieh","Marco Cuturi","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2202.05722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17852v1","updated":"2023-03-31T07:29:36Z","published":"2023-03-31T07:29:36Z","title":"Maximum Covariance Unfolding Regression: A Novel Covariate-based\n  Manifold Learning Approach for Point Cloud Data","summary":"  Point cloud data are widely used in manufacturing applications for process\ninspection, modeling, monitoring and optimization. The state-of-art tensor\nregression techniques have effectively been used for analysis of structured\npoint cloud data, where the measurements on a uniform grid can be formed into a\ntensor. However, these techniques are not capable of handling unstructured\npoint cloud data that are often in the form of manifolds. In this paper, we\npropose a nonlinear dimension reduction approach named Maximum Covariance\nUnfolding Regression that is able to learn the low-dimensional (LD) manifold of\npoint clouds with the highest correlation with explanatory covariates. This LD\nmanifold is then used for regression modeling and process optimization based on\nprocess variables. The performance of the proposed method is subsequently\nevaluated and compared with benchmark methods through simulations and a case\nstudy of steel bracket manufacturing.\n","authors":["Qian Wang","Kamran Paynabar"],"pdf_url":"https://arxiv.org/pdf/2303.17852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08546v2","updated":"2023-03-31T07:29:03Z","published":"2022-12-16T15:55:16Z","title":"Estimating truncation effects of quantum bosonic systems using sampling\n  algorithms","summary":"  To simulate bosons on a qubit- or qudit-based quantum computer, one has to\nregularize the theory by truncating infinite-dimensional local Hilbert spaces\nto finite dimensions. In the search for practical quantum applications, it is\nimportant to know how big the truncation errors can be. In general, it is not\neasy to estimate errors unless we have a good quantum computer. In this paper\nwe show that traditional sampling methods on classical devices, specifically\nMarkov Chain Monte Carlo, can address this issue with a reasonable amount of\ncomputational resources available today. As a demonstration, we apply this idea\nto the scalar field theory on a two-dimensional lattice, with a size that goes\nbeyond what is achievable using exact diagonalization methods. This method can\nbe used to estimate the resources needed for realistic quantum simulations of\nbosonic theories, and also, to check the validity of the results of the\ncorresponding quantum simulations.\n","authors":["Masanori Hanada","Junyu Liu","Enrico Rinaldi","Masaki Tezuka"],"pdf_url":"https://arxiv.org/pdf/2212.08546v2.pdf","comment":"20 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.17001v2","updated":"2023-03-31T07:06:26Z","published":"2023-03-29T20:07:07Z","title":"The G-invariant graph Laplacian","summary":"  Graph Laplacian based algorithms for data lying on a manifold have been\nproven effective for tasks such as dimensionality reduction, clustering, and\ndenoising. In this work, we consider data sets whose data point not only lie on\na manifold, but are also closed under the action of a continuous group. An\nexample of such data set is volumes that line on a low dimensional manifold,\nwhere each volume may be rotated in three-dimensional space. We introduce the\nG-invariant graph Laplacian that generalizes the graph Laplacian by accounting\nfor the action of the group on the data set. We show that like the standard\ngraph Laplacian, the G-invariant graph Laplacian converges to the\nLaplace-Beltrami operator on the data manifold, but with a significantly\nimproved convergence rate. Furthermore, we show that the eigenfunctions of the\nG-invariant graph Laplacian admit the form of tensor products between the group\nelements and eigenvectors of certain matrices, which can be computed\nefficiently using FFT-type algorithms. We demonstrate our construction and its\nadvantages on the problem of filtering data on a noisy manifold closed under\nthe action of the special unitary group SU(2).\n","authors":["Eitan Rosen","Xiuyuan Cheng","Yoel Shkolnisky"],"pdf_url":"https://arxiv.org/pdf/2303.17001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17841v1","updated":"2023-03-31T07:06:24Z","published":"2023-03-31T07:06:24Z","title":"A Benchmark Generative Probabilistic Model for Weak Supervised Learning","summary":"  Finding relevant and high-quality datasets to train machine learning models\nis a major bottleneck for practitioners. Furthermore, to address ambitious\nreal-world use-cases there is usually the requirement that the data come\nlabelled with high-quality annotations that can facilitate the training of a\nsupervised model. Manually labelling data with high-quality labels is generally\na time-consuming and challenging task and often this turns out to be the\nbottleneck in a machine learning project. Weak Supervised Learning (WSL)\napproaches have been developed to alleviate the annotation burden by offering\nan automatic way of assigning approximate labels (pseudo-labels) to unlabelled\ndata based on heuristics, distant supervision and knowledge bases. We apply\nprobabilistic generative latent variable models (PLVMs), trained on heuristic\nlabelling representations of the original dataset, as an accurate, fast and\ncost-effective way to generate pseudo-labels. We show that the PLVMs achieve\nstate-of-the-art performance across four datasets. For example, they achieve\n22% points higher F1 score than Snorkel in the class-imbalanced Spouse dataset.\nPLVMs are plug-and-playable and are a drop-in replacement to existing WSL\nframeworks (e.g. Snorkel) or they can be used as benchmark models for more\ncomplicated algorithms, giving practitioners a compelling accuracy boost.\n","authors":["Georgios Papadopoulos","Fran Silavong","Sean Moran"],"pdf_url":"https://arxiv.org/pdf/2303.17841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17839v1","updated":"2023-03-31T07:02:26Z","published":"2023-03-31T07:02:26Z","title":"Learning Procedure-aware Video Representation from Instructional Videos\n  and Their Narrations","summary":"  The abundance of instructional videos and their narrations over the Internet\noffers an exciting avenue for understanding procedural activities. In this\nwork, we propose to learn video representation that encodes both action steps\nand their temporal ordering, based on a large-scale dataset of web\ninstructional videos and their narrations, without using human annotations. Our\nmethod jointly learns a video representation to encode individual step\nconcepts, and a deep probabilistic model to capture both temporal dependencies\nand immense individual variations in the step ordering. We empirically\ndemonstrate that learning temporal ordering not only enables new capabilities\nfor procedure reasoning, but also reinforces the recognition of individual\nsteps. Our model significantly advances the state-of-the-art results on step\nclassification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting\n(+7.4% on COIN). Moreover, our model attains promising results in zero-shot\ninference for step classification and forecasting, as well as in predicting\ndiverse and plausible steps for incomplete procedures. Our code is available at\nhttps://github.com/facebookresearch/ProcedureVRL.\n","authors":["Yiwu Zhong","Licheng Yu","Yang Bai","Shangwen Li","Xueting Yan","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2303.17839v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2209.12638v2","updated":"2023-03-31T06:59:28Z","published":"2022-09-26T12:37:37Z","title":"Bounded Simplex-Structured Matrix Factorization: Algorithms,\n  Identifiability and Applications","summary":"  In this paper, we propose a new low-rank matrix factorization model dubbed\nbounded simplex-structured matrix factorization (BSSMF). Given an input matrix\n$X$ and a factorization rank $r$, BSSMF looks for a matrix $W$ with $r$ columns\nand a matrix $H$ with $r$ rows such that $X \\approx WH$ where the entries in\neach column of $W$ are bounded, that is, they belong to given intervals, and\nthe columns of $H$ belong to the probability simplex, that is, $H$ is column\nstochastic. BSSMF generalizes nonnegative matrix factorization (NMF), and\nsimplex-structured matrix factorization (SSMF). BSSMF is particularly well\nsuited when the entries of the input matrix $X$ belong to a given interval; for\nexample when the rows of $X$ represent images, or $X$ is a rating matrix such\nas in the Netflix and MovieLens datasets where the entries of $X$ belong to the\ninterval $[1,5]$. The simplex-structured matrix $H$ not only leads to an easily\nunderstandable decomposition providing a soft clustering of the columns of $X$,\nbut implies that the entries of each column of $WH$ belong to the same\nintervals as the columns of $W$. In this paper, we first propose a fast\nalgorithm for BSSMF, even in the presence of missing data in $X$. Then we\nprovide identifiability conditions for BSSMF, that is, we provide conditions\nunder which BSSMF admits a unique decomposition, up to trivial ambiguities.\nFinally, we illustrate the effectiveness of BSSMF on two applications:\nextraction of features in a set of images, and the matrix completion problem\nfor recommender systems.\n","authors":["Olivier Vu Thanh","Nicolas Gillis","Fabian Lecron"],"pdf_url":"https://arxiv.org/pdf/2209.12638v2.pdf","comment":"14 pages, new title, new numerical experiments on synthetic data,\n  clarifications of several parts of the paper, run times added"},{"id":"http://arxiv.org/abs/2303.17836v1","updated":"2023-03-31T06:58:45Z","published":"2023-03-31T06:58:45Z","title":"Rethinking interpretation: Input-agnostic saliency mapping of deep\n  visual classifiers","summary":"  Saliency methods provide post-hoc model interpretation by attributing input\nfeatures to the model outputs. Current methods mainly achieve this using a\nsingle input sample, thereby failing to answer input-independent inquiries\nabout the model. We also show that input-specific saliency mapping is\nintrinsically susceptible to misleading feature attribution. Current attempts\nto use 'general' input features for model interpretation assume access to a\ndataset containing those features, which biases the interpretation. Addressing\nthe gap, we introduce a new perspective of input-agnostic saliency mapping that\ncomputationally estimates the high-level features attributed by the model to\nits outputs. These features are geometrically correlated, and are computed by\naccumulating model's gradient information with respect to an unrestricted data\ndistribution. To compute these features, we nudge independent data points over\nthe model loss surface towards the local minima associated by a\nhuman-understandable concept, e.g., class label for classifiers. With a\nsystematic projection, scaling and refinement process, this information is\ntransformed into an interpretable visualization without compromising its\nmodel-fidelity. The visualization serves as a stand-alone qualitative\ninterpretation. With an extensive evaluation, we not only demonstrate\nsuccessful visualizations for a variety of concepts for large-scale models, but\nalso showcase an interesting utility of this new form of saliency mapping by\nidentifying backdoor signatures in compromised classifiers.\n","authors":["Naveed Akhtar","Mohammad A. A. K. Jalwana"],"pdf_url":"https://arxiv.org/pdf/2303.17836v1.pdf","comment":"Accepted for publication in AAAI 2023"},{"id":"http://arxiv.org/abs/2212.10229v2","updated":"2023-03-31T06:55:37Z","published":"2022-12-20T13:07:20Z","title":"StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for\n  One-shot and Few-shot Domain Adaptation","summary":"  Domain adaptation of GANs is a problem of fine-tuning the state-of-the-art\nGAN models (e.g. StyleGAN) pretrained on a large dataset to a specific domain\nwith few samples (e.g. painting faces, sketches, etc.). While there are a great\nnumber of methods that tackle this problem in different ways, there are still\nmany important questions that remain unanswered.\n  In this paper, we provide a systematic and in-depth analysis of the domain\nadaptation problem of GANs, focusing on the StyleGAN model. First, we perform a\ndetailed exploration of the most important parts of StyleGAN that are\nresponsible for adapting the generator to a new domain depending on the\nsimilarity between the source and target domains. As a result of this in-depth\nstudy, we propose new efficient and lightweight parameterizations of StyleGAN\nfor domain adaptation. Particularly, we show there exist directions in\nStyleSpace (StyleDomain directions) that are sufficient for adapting to similar\ndomains and they can be reduced further. For dissimilar domains, we propose\nAffine$+$ and AffineLight$+$ parameterizations that allows us to outperform\nexisting baselines in few-shot adaptation with low data regime. Finally, we\nexamine StyleDomain directions and discover their many surprising properties\nthat we apply for domain mixing and cross-domain image morphing.\n","authors":["Aibek Alanov","Vadim Titov","Maksim Nakhodnov","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2212.10229v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2303.17824v1","updated":"2023-03-31T06:47:02Z","published":"2023-03-31T06:47:02Z","title":"Implementation and (Inverse Modified) Error Analysis for\n  implicitly-templated ODE-nets","summary":"  We focus on learning hidden dynamics from data using ODE-nets templated on\nimplicit numerical initial value problem solvers. First, we perform Inverse\nModified error analysis of the ODE-nets using unrolled implicit schemes for\nease of interpretation. It is shown that training an ODE-net using an unrolled\nimplicit scheme returns a close approximation of an Inverse Modified\nDifferential Equation (IMDE). In addition, we establish a theoretical basis for\nhyper-parameter selection when training such ODE-nets, whereas current\nstrategies usually treat numerical integration of ODE-nets as a black box. We\nthus formulate an adaptive algorithm which monitors the level of error and\nadapts the number of (unrolled) implicit solution iterations during the\ntraining process, so that the error of the unrolled approximation is less than\nthe current learning loss. This helps accelerate training, while maintaining\naccuracy. Several numerical experiments are performed to demonstrate the\nadvantages of the proposed algorithm compared to nonadaptive unrollings, and\nvalidate the theoretical analysis. We also note that this approach naturally\nallows for incorporating partially known physical terms in the equations,\ngiving rise to what is termed ``gray box\" identification.\n","authors":["Aiqing Zhu","Tom Bertalan","Beibei Zhu","Yifa Tang","Ioannis G. Kevrekidis"],"pdf_url":"https://arxiv.org/pdf/2303.17824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.10324v3","updated":"2023-03-31T06:41:29Z","published":"2022-02-17T09:51:32Z","title":"VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning","summary":"  We propose VRL3, a powerful data-driven framework with a simple design for\nsolving challenging visual deep reinforcement learning (DRL) tasks. We analyze\na number of major obstacles in taking a data-driven approach, and present a\nsuite of design principles, novel findings, and critical insights about\ndata-driven visual DRL. Our framework has three stages: in stage 1, we leverage\nnon-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations;\nin stage 2, we use offline RL data (e.g. a limited number of expert\ndemonstrations) to convert the task-agnostic representations into more powerful\ntask-specific representations; in stage 3, we fine-tune the agent with online\nRL. On a set of challenging hand manipulation tasks with sparse reward and\nrealistic visual inputs, compared to the previous SOTA, VRL3 achieves an\naverage of 780% better sample efficiency. And on the hardest task, VRL3 is\n1220% more sample efficient (2440% when using a wider encoder) and solves the\ntask with only 10% of the computation. These significant results clearly\ndemonstrate the great potential of data-driven deep reinforcement learning.\n","authors":["Che Wang","Xufang Luo","Keith Ross","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2202.10324v3.pdf","comment":"41 pages, camera-ready final version, accepted to NeurIPS 2022"},{"id":"http://arxiv.org/abs/2303.17823v1","updated":"2023-03-31T06:40:27Z","published":"2023-03-31T06:40:27Z","title":"An interpretable neural network-based non-proportional odds model for\n  ordinal regression with continuous response","summary":"  This paper proposes an interpretable neural network-based non-proportional\nodds model (N$^3$POM) for ordinal regression, where the response variable can\ntake not only discrete but also continuous values, and the regression\ncoefficients vary depending on the predicting ordinal response. In contrast to\nconventional approaches estimating the linear coefficients of regression\ndirectly from the discrete response, we train a non-linear neural network that\noutputs the linear coefficients by taking the response as its input. By virtue\nof the neural network, N$^3$POM may have flexibility while preserving the\ninterpretability of the conventional ordinal regression. We show a sufficient\ncondition so that the predicted conditional cumulative probability~(CCP)\nsatisfies the monotonicity constraint locally over a user-specified region in\nthe covariate space; we also provide a monotonicity-preserving stochastic (MPS)\nalgorithm for training the neural network adequately.\n","authors":["Akifumi Okuno","Kazuharu Harada"],"pdf_url":"https://arxiv.org/pdf/2303.17823v1.pdf","comment":"30 pages, 13 figures"},{"id":"http://arxiv.org/abs/2303.17819v1","updated":"2023-03-31T06:30:23Z","published":"2023-03-31T06:30:23Z","title":"An Efficient Off-Policy Reinforcement Learning Algorithm for the\n  Continuous-Time LQR Problem","summary":"  In this paper, an off-policy reinforcement learning algorithm is designed to\nsolve the continuous-time LQR problem using only input-state data measured from\nthe system. Different from other algorithms in the literature, we propose the\nuse of a specific persistently exciting input as the exploration signal during\nthe data collection step. We then show that, using this persistently excited\ndata, the solution of the matrix equation in our algorithm is guaranteed to\nexist and to be unique at every iteration. Convergence of the algorithm to the\noptimal control input is also proven. Moreover, we formulate the policy\nevaluation step as the solution of a Sylvester-transpose equation, which\nincreases the efficiency of its solution. Finally, a method to determine a\nstabilizing policy to initialize the algorithm using only measured data is\nproposed.\n","authors":["Victor G. Lopez","Matthias A. Müller"],"pdf_url":"https://arxiv.org/pdf/2303.17819v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2303.17809v1","updated":"2023-03-31T05:55:54Z","published":"2023-03-31T05:55:54Z","title":"Never a Dull Moment: Distributional Properties as a Baseline for\n  Time-Series Classification","summary":"  The variety of complex algorithmic approaches for tackling time-series\nclassification problems has grown considerably over the past decades, including\nthe development of sophisticated but challenging-to-interpret\ndeep-learning-based methods. But without comparison to simpler methods it can\nbe difficult to determine when such complexity is required to obtain strong\nperformance on a given problem. Here we evaluate the performance of an\nextremely simple classification approach -- a linear classifier in the space of\ntwo simple features that ignore the sequential ordering of the data: the mean\nand standard deviation of time-series values. Across a large repository of 128\nunivariate time-series classification problems, this simple distributional\nmoment-based approach outperformed chance on 69 problems, and reached 100%\naccuracy on two problems. With a neuroimaging time-series case study, we find\nthat a simple linear model based on the mean and standard deviation performs\nbetter at classifying individuals with schizophrenia than a model that\nadditionally includes features of the time-series dynamics. Comparing the\nperformance of simple distributional features of a time series provides\nimportant context for interpreting the performance of complex time-series\nclassification models, which may not always be required to obtain high\naccuracy.\n","authors":["Trent Henderson","Annie G. Bryant","Ben D. Fulcher"],"pdf_url":"https://arxiv.org/pdf/2303.17809v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.17807v1","updated":"2023-03-31T05:43:21Z","published":"2023-03-31T05:43:21Z","title":"Exploring the Potential of Large Language models in Traditional Korean\n  Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare","summary":"  Introduction: Traditional Korean medicine (TKM) emphasizes individualized\ndiagnosis and treatment, making AI modeling difficult due to limited data and\nimplicit processes. GPT-3.5 and GPT-4, large language models, have shown\nimpressive medical knowledge despite lacking medicine-specific training. This\nstudy aimed to assess the capabilities of GPT-3.5 and GPT-4 for TKM using the\nKorean National Licensing Examination for Korean Medicine Doctors. Methods:\nGPT-3.5 (February 2023) and GPT-4 (March 2023) models answered 340 questions\nfrom the 2022 examination across 12 subjects. Each question was independently\nevaluated five times in an initialized session. Results: GPT-3.5 and GPT-4\nachieved 42.06% and 57.29% accuracy, respectively, with GPT-4 nearing passing\nperformance. There were significant differences in accuracy by subjects, with\n83.75% accuracy for neuropsychiatry compared to 28.75% for internal medicine\n(2). Both models showed high accuracy in recall-based and diagnosis-based\nquestions but struggled with intervention-based ones. The accuracy for\nquestions that require TKM-specialized knowledge was relatively lower than the\naccuracy for questions that do not GPT-4 showed high accuracy for table-based\nquestions, and both models demonstrated consistent responses. A positive\ncorrelation between consistency and accuracy was observed. Conclusion: Models\nin this study showed near-passing performance in decision-making for TKM\nwithout domain-specific training. However, limits were also observed that were\nbelieved to be caused by culturally-biased learning. Our study suggests that\nfoundation models have potential in culturally-adapted medicine, specifically\nTKM, for clinical assistance, medical education, and medical research.\n","authors":["Dongyeop Jang","Chang-Eop Kim"],"pdf_url":"https://arxiv.org/pdf/2303.17807v1.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.17805v1","updated":"2023-03-31T05:32:11Z","published":"2023-03-31T05:32:11Z","title":"On the Effect of Initialization: The Scaling Path of 2-Layer Neural\n  Networks","summary":"  In supervised learning, the regularization path is sometimes used as a\nconvenient theoretical proxy for the optimization path of gradient descent\ninitialized with zero. In this paper, we study a modification of the\nregularization path for infinite-width 2-layer ReLU neural networks with\nnon-zero initial distribution of the weights at different scales. By exploiting\na link with unbalanced optimal transport theory, we show that, despite the\nnon-convexity of the 2-layer network training, this problem admits an infinite\ndimensional convex counterpart. We formulate the corresponding functional\noptimization problem and investigate its main properties. In particular, we\nshow that as the scale of the initialization ranges between $0$ and $+\\infty$,\nthe associated path interpolates continuously between the so-called kernel and\nrich regimes. The numerical experiments confirm that, in our setting, the\nscaling path and the final states of the optimization path behave similarly\neven beyond these extreme points.\n","authors":["Sebastian Neumayer","Lénaïc Chizat","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2303.17805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17802v1","updated":"2023-03-31T05:22:56Z","published":"2023-03-31T05:22:56Z","title":"Time-series Anomaly Detection based on Difference Subspace between\n  Signal Subspaces","summary":"  This paper proposes a new method for anomaly detection in time-series data by\nincorporating the concept of difference subspace into the singular spectrum\nanalysis (SSA). The key idea is to monitor slight temporal variations of the\ndifference subspace between two signal subspaces corresponding to the past and\npresent time-series data, as anomaly score. It is a natural generalization of\nthe conventional SSA-based method which measures the minimum angle between the\ntwo signal subspaces as the degree of changes. By replacing the minimum angle\nwith the difference subspace, our method boosts the performance while using the\nSSA-based framework as it can capture the whole structural difference between\nthe two subspaces in its magnitude and direction. We demonstrate our method's\neffectiveness through performance evaluations on public time-series datasets.\n","authors":["Takumi Kanai","Naoya Sogi","Atsuto Maki","Kazuhiro Fukui"],"pdf_url":"https://arxiv.org/pdf/2303.17802v1.pdf","comment":"8pages"},{"id":"http://arxiv.org/abs/2303.17786v1","updated":"2023-03-31T03:17:23Z","published":"2023-03-31T03:17:23Z","title":"Attention is Not Always What You Need: Towards Efficient Classification\n  of Domain-Specific Text","summary":"  For large-scale IT corpora with hundreds of classes organized in a hierarchy,\nthe task of accurate classification of classes at the higher level in the\nhierarchies is crucial to avoid errors propagating to the lower levels. In the\nbusiness world, an efficient and explainable ML model is preferred over an\nexpensive black-box model, especially if the performance increase is marginal.\nA current trend in the Natural Language Processing (NLP) community is towards\nemploying huge pre-trained language models (PLMs) or what is known as\nself-attention models (e.g., BERT) for almost any kind of NLP task (e.g.,\nquestion-answering, sentiment analysis, text classification). Despite the\nwidespread use of PLMs and the impressive performance in a broad range of NLP\ntasks, there is a lack of a clear and well-justified need to as why these\nmodels are being employed for domain-specific text classification (TC) tasks,\ngiven the monosemic nature of specialized words (i.e., jargon) found in\ndomain-specific text which renders the purpose of contextualized embeddings\n(e.g., PLMs) futile. In this paper, we compare the accuracies of some\nstate-of-the-art (SOTA) models reported in the literature against a Linear SVM\nclassifier and TFIDF vectorization model on three TC datasets. Results show a\ncomparable performance for the LinearSVM. The findings of this study show that\nfor domain-specific TC tasks, a linear model can provide a comparable, cheap,\nreproducible, and interpretable alternative to attention-based models.\n","authors":["Yasmen Wahba","Nazim Madhavji","John Steinbacher"],"pdf_url":"https://arxiv.org/pdf/2303.17786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17782v1","updated":"2023-03-31T03:07:53Z","published":"2023-03-31T03:07:53Z","title":"A Slow-Shifting Concerned Machine Learning Method for Short-term Traffic\n  Flow Forecasting","summary":"  The ability to predict traffic flow over time for crowded areas during rush\nhours is increasingly important as it can help authorities make informed\ndecisions for congestion mitigation or scheduling of infrastructure development\nin an area. However, a crucial challenge in traffic flow forecasting is the\nslow shifting in temporal peaks between daily and weekly cycles, resulting in\nthe nonstationarity of the traffic flow signal and leading to difficulty in\naccurate forecasting. To address this challenge, we propose a slow shifting\nconcerned machine learning method for traffic flow forecasting, which includes\ntwo parts. First, we take advantage of Empirical Mode Decomposition as the\nfeature engineering to alleviate the nonstationarity of traffic flow data,\nyielding a series of stationary components. Second, due to the superiority of\nLong-Short-Term-Memory networks in capturing temporal features, an advanced\ntraffic flow forecasting model is developed by taking the stationary components\nas inputs. Finally, we apply this method on a benchmark of real-world data and\nprovide a comparison with other existing methods. Our proposed method\noutperforms the state-of-art results by 14.55% and 62.56% using the metrics of\nroot mean squared error and mean absolute percentage error, respectively.\n","authors":["Zann Koh","Yan Qin","Yong Liang Guan","Chau Yuen"],"pdf_url":"https://arxiv.org/pdf/2303.17782v1.pdf","comment":"6 pages, 4 figures. Accepted for IEEE International Conference on\n  Smart Mobility 2023 (IEEE SM'23)"},{"id":"http://arxiv.org/abs/2301.08843v2","updated":"2023-03-31T03:00:34Z","published":"2023-01-21T01:26:26Z","title":"Towards Flexibility and Interpretability of Gaussian Process State-Space\n  Model","summary":"  The Gaussian process state-space model (GPSSM) has attracted much attention\nover the past decade. However, the model representation power of the GPSSM is\nfar from satisfactory. Most GPSSM studies rely on the standard Gaussian process\n(GP) with a preliminary kernel, such as the squared exponential (SE) kernel or\nMat\\'{e}rn kernel, which limits the model representation power and its\napplication in complex scenarios. To address this issue, this paper proposes a\nnovel class of probabilistic state-space models, called TGPSSMs. By leveraging\na parametric normalizing flow, the TGPSSMs enrich the GP priors in the standard\nGPSSM, rendering the state-space model more flexible and expressive.\nAdditionally, we present a scalable variational inference algorithm for\nlearning and inference in TGPSSMs, which provides a flexible and optimal\nstructure for the variational distribution of latent states. The algorithm is\ninterpretable and computationally efficient owing to the sparse representation\nof GP and the bijective nature of normalizing flow. To further improve the\nlearning and inference performance of the proposed algorithm, we integrate a\nconstrained optimization framework to enhance the state-space representation\ncapabilities and optimize the hyperparameters. The experimental results based\non various synthetic and real datasets corroborate that the proposed TGPSSM\nyields superior learning and inference performance compared to several\nstate-of-the-art methods. The accompanying source code is available at\n\\url{https://github.com/zhidilin/TGPSSM}.\n","authors":["Zhid Lin","Feng Yin","Juan Maroñas"],"pdf_url":"https://arxiv.org/pdf/2301.08843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17779v1","updated":"2023-03-31T02:56:23Z","published":"2023-03-31T02:56:23Z","title":"Decentralized Weakly Convex Optimization Over the Stiefel Manifold","summary":"  We focus on a class of non-smooth optimization problems over the Stiefel\nmanifold in the decentralized setting, where a connected network of $n$ agents\ncooperatively minimize a finite-sum objective function with each component\nbeing weakly convex in the ambient Euclidean space. Such optimization problems,\nalbeit frequently encountered in applications, are quite challenging due to\ntheir non-smoothness and non-convexity. To tackle them, we propose an iterative\nmethod called the decentralized Riemannian subgradient method (DRSM). The\nglobal convergence and an iteration complexity of $\\mathcal{O}(\\varepsilon^{-2}\n\\log^2(\\varepsilon^{-1}))$ for forcing a natural stationarity measure below\n$\\varepsilon$ are established via the powerful tool of proximal smoothness from\nvariational analysis, which could be of independent interest. Besides, we show\nthe local linear convergence of the DRSM using geometrically diminishing\nstepsizes when the problem at hand further possesses a sharpness property.\nNumerical experiments are conducted to corroborate our theoretical findings.\n","authors":["Jinxin Wang","Jiang Hu","Shixiang Chen","Zengde Deng","Anthony Man-Cho So"],"pdf_url":"https://arxiv.org/pdf/2303.17779v1.pdf","comment":"27 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2208.01700v2","updated":"2023-03-31T02:55:04Z","published":"2022-08-02T19:23:48Z","title":"Differentially Private Vertical Federated Clustering","summary":"  In many applications, multiple parties have private data regarding the same\nset of users but on disjoint sets of attributes, and a server wants to leverage\nthe data to train a model. To enable model learning while protecting the\nprivacy of the data subjects, we need vertical federated learning (VFL)\ntechniques, where the data parties share only information for training the\nmodel, instead of the private data. However, it is challenging to ensure that\nthe shared information maintains privacy while learning accurate models. To the\nbest of our knowledge, the algorithm proposed in this paper is the first\npractical solution for differentially private vertical federated k-means\nclustering, where the server can obtain a set of global centers with a provable\ndifferential privacy guarantee. Our algorithm assumes an untrusted central\nserver that aggregates differentially private local centers and membership\nencodings from local data parties. It builds a weighted grid as the synopsis of\nthe global dataset based on the received information. Final centers are\ngenerated by running any k-means algorithm on the weighted grid. Our approach\nfor grid weight estimation uses a novel, light-weight, and differentially\nprivate set intersection cardinality estimation algorithm based on the\nFlajolet-Martin sketch. To improve the estimation accuracy in the setting with\nmore than two data parties, we further propose a refined version of the weights\nestimation algorithm and a parameter tuning strategy to reduce the final\nk-means utility to be close to that in the central private setting. We provide\ntheoretical utility analysis and experimental evaluation results for the\ncluster centers computed by our algorithm and show that our approach performs\nbetter both theoretically and empirically than the two baselines based on\nexisting techniques.\n","authors":["Zitao Li","Tianhao Wang","Ninghui Li"],"pdf_url":"https://arxiv.org/pdf/2208.01700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.04570v2","updated":"2023-03-31T02:43:39Z","published":"2023-02-09T11:17:34Z","title":"NeuKron: Constant-Size Lossy Compression of Sparse Reorderable Matrices\n  and Tensors","summary":"  Many real-world data are naturally represented as a sparse reorderable\nmatrix, whose rows and columns can be arbitrarily ordered (e.g., the adjacency\nmatrix of a bipartite graph). Storing a sparse matrix in conventional ways\nrequires an amount of space linear in the number of non-zeros, and lossy\ncompression of sparse matrices (e.g., Truncated SVD) typically requires an\namount of space linear in the number of rows and columns. In this work, we\npropose NeuKron for compressing a sparse reorderable matrix into a\nconstant-size space. NeuKron generalizes Kronecker products using a recurrent\nneural network with a constant number of parameters. NeuKron updates the\nparameters so that a given matrix is approximated by the product and reorders\nthe rows and columns of the matrix to facilitate the approximation. The updates\ntake time linear in the number of non-zeros in the input matrix, and the\napproximation of each entry can be retrieved in logarithmic time. We also\nextend NeuKron to compress sparse reorderable tensors (e.g. multi-layer\ngraphs), which generalize matrices. Through experiments on ten real-world\ndatasets, we show that NeuKron is (a) Compact: requiring up to five orders of\nmagnitude less space than its best competitor with similar approximation\nerrors, (b) Accurate: giving up to 10x smaller approximation error than its\nbest competitors with similar size outputs, and (c) Scalable: successfully\ncompressing a matrix with over 230 million non-zero entries.\n","authors":["Taehyung Kwon","Jihoon Ko","Jinhong Jung","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2302.04570v2.pdf","comment":"Accepted to WWW 2023 - The Web Conference 2023"},{"id":"http://arxiv.org/abs/2303.17776v1","updated":"2023-03-31T02:43:01Z","published":"2023-03-31T02:43:01Z","title":"Learning Internal Representations of 3D Transformations from 2D\n  Projected Inputs","summary":"  When interacting in a three dimensional world, humans must estimate 3D\nstructure from visual inputs projected down to two dimensional retinal images.\nIt has been shown that humans use the persistence of object shape over\nmotion-induced transformations as a cue to resolve depth ambiguity when solving\nthis underconstrained problem. With the aim of understanding how biological\nvision systems may internally represent 3D transformations, we propose a\ncomputational model, based on a generative manifold model, which can be used to\ninfer 3D structure from the motion of 2D points. Our model can also learn\nrepresentations of the transformations with minimal supervision, providing a\nproof of concept for how humans may develop internal representations on a\ndevelopmental or evolutionary time scale. Focused on rotational motion, we show\nhow our model infers depth from moving 2D projected points, learns 3D\nrotational transformations from 2D training stimuli, and compares to human\nperformance on psychophysical structure-from-motion experiments.\n","authors":["Marissa Connor","Bruno Olshausen","Christopher Rozell"],"pdf_url":"https://arxiv.org/pdf/2303.17776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04445v2","updated":"2023-03-31T02:31:16Z","published":"2023-03-08T08:48:02Z","title":"An ADMM Solver for the MKL-$L_{0/1}$-SVM","summary":"  We formulate the Multiple Kernel Learning (abbreviated as MKL) problem for\nthe support vector machine with the infamous $(0,1)$-loss function. Some\nfirst-order optimality conditions are given and then exploited to develop a\nfast ADMM solver for the nonconvex and nonsmooth optimization problem. A simple\nnumerical experiment on synthetic planar data shows that our MKL-$L_{0/1}$-SVM\nframework could be promising.\n","authors":["Yijie Shi","Bin Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.04445v2.pdf","comment":"8 pages, 3 figures, 2 tables. Submitted to the 62nd IEEE Conference\n  on Decision and Control as a Regular paper, with a shortened version (arXiv\n  version 1) submitted to the 3rd Chinese Conference on Predictive Control and\n  Intelligent Decision (CPCID) as an Extended Abstract"},{"id":"http://arxiv.org/abs/2303.17769v1","updated":"2023-03-31T02:13:32Z","published":"2023-03-31T02:13:32Z","title":"Domain Knowledge integrated for Blast Furnace Classifier Design","summary":"  Blast furnace modeling and control is one of the important problems in the\nindustrial field, and the black-box model is an effective mean to describe the\ncomplex blast furnace system. In practice, there are often different learning\ntargets, such as safety and energy saving in industrial applications, depending\non the application. For this reason, this paper proposes a framework to design\na domain knowledge integrated classification model that yields a classifier for\nindustrial application. Our knowledge incorporated learning scheme allows the\nusers to create a classifier that identifies \"important samples\" (whose\nmisclassifications can lead to severe consequences) more correctly, while\nkeeping the proper precision of classifying the remaining samples. The\neffectiveness of the proposed method has been verified by two real blast\nfurnace datasets, which guides the operators to utilize their prior experience\nfor controlling the blast furnace systems better.\n","authors":["Shaohan Chen","Di Fan","Chuanhou Gao"],"pdf_url":"https://arxiv.org/pdf/2303.17769v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.17768v1","updated":"2023-03-31T02:10:30Z","published":"2023-03-31T02:10:30Z","title":"Scalable Bayesian Meta-Learning through Generalized Implicit Gradients","summary":"  Meta-learning owns unique effectiveness and swiftness in tackling emerging\ntasks with limited data. Its broad applicability is revealed by viewing it as a\nbi-level optimization problem. The resultant algorithmic viewpoint however,\nfaces scalability issues when the inner-level optimization relies on\ngradient-based iterations. Implicit differentiation has been considered to\nalleviate this challenge, but it is restricted to an isotropic Gaussian prior,\nand only favors deterministic meta-learning approaches. This work markedly\nmitigates the scalability bottleneck by cross-fertilizing the benefits of\nimplicit differentiation to probabilistic Bayesian meta-learning. The novel\nimplicit Bayesian meta-learning (iBaML) method not only broadens the scope of\nlearnable priors, but also quantifies the associated uncertainty. Furthermore,\nthe ultimate complexity is well controlled regardless of the inner-level\noptimization trajectory. Analytical error bounds are established to demonstrate\nthe precision and efficiency of the generalized implicit gradient over the\nexplicit one. Extensive numerical tests are also carried out to empirically\nvalidate the performance of the proposed method.\n","authors":["Yilang Zhang","Bingcong Li","Shijian Gao","Georgios B. Giannakis"],"pdf_url":"https://arxiv.org/pdf/2303.17768v1.pdf","comment":"Accepted as a poster paper in the main track of Proceedings of the\n  37th AAAI Conference on Artificial Intelligence (AAAI-23)"},{"id":"http://arxiv.org/abs/2303.17765v1","updated":"2023-03-31T01:56:13Z","published":"2023-03-31T01:56:13Z","title":"Learning from Similar Linear Representations: Adaptivity, Minimaxity,\n  and Robustness","summary":"  Representation multi-task learning (MTL) and transfer learning (TL) have\nachieved tremendous success in practice. However, the theoretical understanding\nof these methods is still lacking. Most existing theoretical works focus on\ncases where all tasks share the same representation, and claim that MTL and TL\nalmost always improve performance. However, as the number of tasks grow,\nassuming all tasks share the same representation is unrealistic. Also, this\ndoes not always match empirical findings, which suggest that a shared\nrepresentation may not necessarily improve single-task or target-only learning\nperformance. In this paper, we aim to understand how to learn from tasks with\n\\textit{similar but not exactly the same} linear representations, while dealing\nwith outlier tasks. We propose two algorithms that are \\textit{adaptive} to the\nsimilarity structure and \\textit{robust} to outlier tasks under both MTL and TL\nsettings. Our algorithms outperform single-task or target-only learning when\nrepresentations across tasks are sufficiently similar and the fraction of\noutlier tasks is small. Furthermore, they always perform no worse than\nsingle-task learning or target-only learning, even when the representations are\ndissimilar. We provide information-theoretic lower bounds to show that our\nalgorithms are nearly \\textit{minimax} optimal in a large regime.\n","authors":["Ye Tian","Yuqi Gu","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2303.17765v1.pdf","comment":"60 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17764v1","updated":"2023-03-31T01:55:52Z","published":"2023-03-31T01:55:52Z","title":"Towards Adversarially Robust Continual Learning","summary":"  Recent studies show that models trained by continual learning can achieve the\ncomparable performances as the standard supervised learning and the learning\nflexibility of continual learning models enables their wide applications in the\nreal world. Deep learning models, however, are shown to be vulnerable to\nadversarial attacks. Though there are many studies on the model robustness in\nthe context of standard supervised learning, protecting continual learning from\nadversarial attacks has not yet been investigated. To fill in this research\ngap, we are the first to study adversarial robustness in continual learning and\npropose a novel method called \\textbf{T}ask-\\textbf{A}ware \\textbf{B}oundary\n\\textbf{A}ugmentation (TABA) to boost the robustness of continual learning\nmodels. With extensive experiments on CIFAR-10 and CIFAR-100, we show the\nefficacy of adversarial training and TABA in defending adversarial attacks.\n","authors":["Tao Bai","Chen Chen","Lingjuan Lyu","Jun Zhao","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2303.17764v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2108.03084v3","updated":"2023-03-31T01:43:33Z","published":"2021-08-06T12:38:42Z","title":"Transferring Knowledge Distillation for Multilingual Social Event\n  Detection","summary":"  Recently published graph neural networks (GNNs) show promising performance at\nsocial event detection tasks. However, most studies are oriented toward\nmonolingual data in languages with abundant training samples. This has left the\nmore common multilingual settings and lesser-spoken languages relatively\nunexplored. Thus, we present a GNN that incorporates cross-lingual word\nembeddings for detecting events in multilingual data streams. The first exploit\nis to make the GNN work with multilingual data. For this, we outline a\nconstruction strategy that aligns messages in different languages at both the\nnode and semantic levels. Relationships between messages are established by\nmerging entities that are the same but are referred to in different languages.\nNon-English message representations are converted into English semantic space\nvia the cross-lingual word embeddings. The resulting message graph is then\nuniformly encoded by a GNN model. In special cases where a lesser-spoken\nlanguage needs to be detected, a novel cross-lingual knowledge distillation\nframework, called CLKD, exploits prior knowledge learned from similar threads\nin English to make up for the paucity of annotated data. Experiments on both\nsynthetic and real-world datasets show the framework to be highly effective at\ndetection in both multilingual data and in languages where training samples are\nscarce.\n","authors":["Jiaqian Ren","Hao Peng","Lei Jiang","Jia Wu","Yongxin Tong","Lihong Wang","Xu Bai","Bo Wang","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2108.03084v3.pdf","comment":"We withdrew this paper from TPAMI due to the long review cycle.\n  Recently we have restudied our methods and some new great results are\n  discovered"},{"id":"http://arxiv.org/abs/2303.17762v1","updated":"2023-03-31T01:38:26Z","published":"2023-03-31T01:38:26Z","title":"Generalized Information Bottleneck for Gaussian Variables","summary":"  The information bottleneck (IB) method offers an attractive framework for\nunderstanding representation learning, however its applications are often\nlimited by its computational intractability. Analytical characterization of the\nIB method is not only of practical interest, but it can also lead to new\ninsights into learning phenomena. Here we consider a generalized IB problem, in\nwhich the mutual information in the original IB method is replaced by\ncorrelation measures based on Renyi and Jeffreys divergences. We derive an\nexact analytical IB solution for the case of Gaussian correlated variables. Our\nanalysis reveals a series of structural transitions, similar to those\npreviously observed in the original IB case. We find further that although\nsolving the original, Renyi and Jeffreys IB problems yields different\nrepresentations in general, the structural transitions occur at the same\ncritical tradeoff parameters, and the Renyi and Jeffreys IB solutions perform\nwell under the original IB objective. Our results suggest that formulating the\nIB method with alternative correlation measures could offer a strategy for\nobtaining an approximate solution to the original IB problem.\n","authors":["Vudtiwat Ngampruetikorn","David J. Schwab"],"pdf_url":"https://arxiv.org/pdf/2303.17762v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2209.06015v2","updated":"2023-03-31T01:11:50Z","published":"2022-08-04T05:32:20Z","title":"Black-box Dataset Ownership Verification via Backdoor Watermarking","summary":"  Deep learning, especially deep neural networks (DNNs), has been widely and\nsuccessfully adopted in many critical applications for its high effectiveness\nand efficiency. The rapid development of DNNs has benefited from the existence\nof some high-quality datasets ($e.g.$, ImageNet), which allow researchers and\ndevelopers to easily verify the performance of their methods. Currently, almost\nall existing released datasets require that they can only be adopted for\nacademic or educational purposes rather than commercial purposes without\npermission. However, there is still no good way to ensure that. In this paper,\nwe formulate the protection of released datasets as verifying whether they are\nadopted for training a (suspicious) third-party model, where defenders can only\nquery the model while having no information about its parameters and training\ndetails. Based on this formulation, we propose to embed external patterns via\nbackdoor watermarking for the ownership verification to protect them. Our\nmethod contains two main parts, including dataset watermarking and dataset\nverification. Specifically, we exploit poison-only backdoor attacks ($e.g.$,\nBadNets) for dataset watermarking and design a hypothesis-test-guided method\nfor dataset verification. We also provide some theoretical analyses of our\nmethods. Experiments on multiple benchmark datasets of different tasks are\nconducted, which verify the effectiveness of our method. The code for\nreproducing main experiments is available at\n\\url{https://github.com/THUYimingLi/DVBW}.\n","authors":["Yiming Li","Mingyan Zhu","Xue Yang","Yong Jiang","Tao Wei","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2209.06015v2.pdf","comment":"This paper is accepted by IEEE TIFS. 15 pages. The preliminary short\n  version of this paper was posted on arXiv (arXiv:2010.05821) and presented in\n  a non-archival NeurIPS Workshop (2020)"},{"id":"http://arxiv.org/abs/2303.17760v1","updated":"2023-03-31T01:09:00Z","published":"2023-03-31T01:09:00Z","title":"CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale\n  Language Model Society","summary":"  The rapid advancement of conversational and chat-based language models has\nled to remarkable progress in complex task-solving. However, their success\nheavily relies on human input to guide the conversation, which can be\nchallenging and time-consuming. This paper explores the potential of building\nscalable techniques to facilitate autonomous cooperation among communicative\nagents and provide insight into their \"cognitive\" processes. To address the\nchallenges of achieving autonomous cooperation, we propose a novel\ncommunicative agent framework named role-playing. Our approach involves using\ninception prompting to guide chat agents toward task completion while\nmaintaining consistency with human intentions. We showcase how role-playing can\nbe used to generate conversational data for studying the behaviors and\ncapabilities of chat agents, providing a valuable resource for investigating\nconversational language models. Our contributions include introducing a novel\ncommunicative agent framework, offering a scalable approach for studying the\ncooperative behaviors and capabilities of multi-agent systems, and\nopen-sourcing our library to support research on communicative agents and\nbeyond. The GitHub repository of this project is made publicly available on:\nhttps://github.com/lightaime/camel.\n","authors":["Guohao Li","Hasan Abed Al Kader Hammoud","Hani Itani","Dmitrii Khizbullin","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2303.17760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06441v2","updated":"2023-03-31T00:08:46Z","published":"2022-10-12T17:42:01Z","title":"How Much Data Are Augmentations Worth? An Investigation into Scaling\n  Laws, Invariance, and Implicit Regularization","summary":"  Despite the clear performance benefits of data augmentations, little is known\nabout why they are so effective. In this paper, we disentangle several key\nmechanisms through which data augmentations operate. Establishing an exchange\nrate between augmented and additional real data, we find that in\nout-of-distribution testing scenarios, augmentations which yield samples that\nare diverse, but inconsistent with the data distribution can be even more\nvaluable than additional training data. Moreover, we find that data\naugmentations which encourage invariances can be more valuable than invariance\nalone, especially on small and medium sized training sets. Following this\nobservation, we show that augmentations induce additional stochasticity during\ntraining, effectively flattening the loss landscape.\n","authors":["Jonas Geiping","Micah Goldblum","Gowthami Somepalli","Ravid Shwartz-Ziv","Tom Goldstein","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2210.06441v2.pdf","comment":"31 pages, 29 figures. To be presented at ICLR 2023. Code at\n  https://github.com/JonasGeiping/dataaugs"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.06859v2","updated":"2023-03-31T08:02:01Z","published":"2023-03-13T05:04:18Z","title":"Learning Distortion Invariant Representation for Image Restoration from\n  A Causality Perspective","summary":"  In recent years, we have witnessed the great advancement of Deep neural\nnetworks (DNNs) in image restoration. However, a critical limitation is that\nthey cannot generalize well to real-world degradations with different degrees\nor types. In this paper, we are the first to propose a novel training strategy\nfor image restoration from the causality perspective, to improve the\ngeneralization ability of DNNs for unknown degradations. Our method, termed\nDistortion Invariant representation Learning (DIL), treats each distortion type\nand degree as one specific confounder, and learns the distortion-invariant\nrepresentation by eliminating the harmful confounding effect of each\ndegradation. We derive our DIL with the back-door criterion in causality by\nmodeling the interventions of different distortions from the optimization\nperspective. Particularly, we introduce counterfactual distortion augmentation\nto simulate the virtual distortion types and degrees as the confounders. Then,\nwe instantiate the intervention of each distortion with a virtual model\nupdating based on corresponding distorted images, and eliminate them from the\nmeta-learning perspective. Extensive experiments demonstrate the effectiveness\nof our DIL on the generalization capability for unseen distortion types and\ndegrees. Our code will be available at\nhttps://github.com/lixinustc/Causal-IR-DIL.\n","authors":["Xin Li","Bingchen Li","Xin Jin","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2303.06859v2.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2303.17839v1","updated":"2023-03-31T07:02:26Z","published":"2023-03-31T07:02:26Z","title":"Learning Procedure-aware Video Representation from Instructional Videos\n  and Their Narrations","summary":"  The abundance of instructional videos and their narrations over the Internet\noffers an exciting avenue for understanding procedural activities. In this\nwork, we propose to learn video representation that encodes both action steps\nand their temporal ordering, based on a large-scale dataset of web\ninstructional videos and their narrations, without using human annotations. Our\nmethod jointly learns a video representation to encode individual step\nconcepts, and a deep probabilistic model to capture both temporal dependencies\nand immense individual variations in the step ordering. We empirically\ndemonstrate that learning temporal ordering not only enables new capabilities\nfor procedure reasoning, but also reinforces the recognition of individual\nsteps. Our model significantly advances the state-of-the-art results on step\nclassification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting\n(+7.4% on COIN). Moreover, our model attains promising results in zero-shot\ninference for step classification and forecasting, as well as in predicting\ndiverse and plausible steps for incomplete procedures. Our code is available at\nhttps://github.com/facebookresearch/ProcedureVRL.\n","authors":["Yiwu Zhong","Licheng Yu","Yang Bai","Shangwen Li","Xueting Yan","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2303.17839v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01347v1","updated":"2023-03-31T02:54:01Z","published":"2023-03-31T02:54:01Z","title":"Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia\n  Diagnosis and Lateralization Analysis","summary":"  Available evidence suggests that dynamic functional connectivity (dFC) can\ncapture time-varying abnormalities in brain activity in rs-fMRI data and has a\nnatural advantage in uncovering mechanisms of abnormal brain activity in\nschizophrenia(SZ) patients. Hence, an advanced dynamic brain network analysis\nmodel called the temporal brain category graph convolutional network\n(temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysis\nmodule, DSF-BrainNet, was designed to construct dynamic synchronization\nfeatures. Subsequently, a revolutionary graph convolution method, TemporalConv,\nwas proposed, based on the synchronous temporal properties of feature. Finally,\nthe first modular abnormal hemispherical lateralization test tool in deep\nlearning based on rs-fMRI data, named CategoryPool, was proposed. This study\nwas validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% average\naccuracy, respectively, outperforming the baseline model and other\nState-of-the-Art methods. The ablation results also demonstrate the advantages\nof TemporalConv over the traditional edge feature convolution approach of graph\nconvolutional neural network (GCN) and the improvement of CategoryPool over the\nclassical graph pooling approach. Interestingly, this study showed that the\nlower order perceptual system and higher order network regions in the left\nhemisphere are more severely dysfunctional than in the right hemisphere of SZ,\nand reaffirms the importance of the left medial superior frontal gyrus in SZ.\nOur core code is available at: https://github.com/swfen/Temporal-BCGCN.\n","authors":["Cheng Zhu","Ying Tan","Shuqi Yang","Jiaqing Miao","Jiayi Zhu","Huan Huang","Dezhong Yao","Cheng Luo"],"pdf_url":"https://arxiv.org/pdf/2304.01347v1.pdf","comment":"10 pages,8 fig,Journal paper"}]},"2023-04-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2304.01196v1","updated":"2023-04-03T17:59:09Z","published":"2023-04-03T17:59:09Z","title":"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on\n  Self-Chat Data","summary":"  Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks.\n","authors":["Canwen Xu","Daya Guo","Nan Duan","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2304.01196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01179v1","updated":"2023-04-03T17:49:04Z","published":"2023-04-03T17:49:04Z","title":"Hate Speech Targets Detection in Parler using BERT","summary":"  Online social networks have become a fundamental component of our everyday\nlife. Unfortunately, these platforms are also a stage for hate speech. Popular\nsocial networks have regularized rules against hate speech. Consequently,\nsocial networks like Parler and Gab advocating and claiming to be free speech\nplatforms have evolved. These platforms have become a district for hate speech\nagainst diverse targets. We present in our paper a pipeline for detecting hate\nspeech and its targets and use it for creating Parler hate targets'\ndistribution. The pipeline consists of two models; one for hate speech\ndetection and the second for target classification, both based on BERT with\nBack-Translation and data pre-processing for improved results. The source code\nused in this work, as well as other relevant sources, are available at:\nhttps://github.com/NadavSc/HateRecognition.git\n","authors":["Nadav Schneider","Shimon Shouei","Saleem Ghantous","Elad Feldman"],"pdf_url":"https://arxiv.org/pdf/2304.01179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17649v2","updated":"2023-04-03T17:33:16Z","published":"2023-03-30T18:27:15Z","title":"Aligning a medium-size GPT model in English to a small closed domain in\n  Spanish using reinforcement learning","summary":"  In this paper, we propose a methodology to align a medium-sized GPT model,\noriginally trained in English for an open domain, to a small closed domain in\nSpanish. The application for which the model is finely tuned is the question\nanswering task. To achieve this we also needed to train and implement another\nneural network (which we called the reward model) that could score and\ndetermine whether an answer is appropriate for a given question. This component\nserved to improve the decoding and generation of the answers of the system.\nNumerical metrics such as BLEU and perplexity were used to evaluate the model,\nand human judgment was also used to compare the decoding technique with others.\nFinally, the results favored the proposed method, and it was determined that it\nis feasible to use a reward model to align the generation of responses.\n","authors":["Oscar R. Navarrete-Parra","Victor Uc-Cetina","Jorge Reyes-Magana"],"pdf_url":"https://arxiv.org/pdf/2303.17649v2.pdf","comment":"Under review in the journal Procesamiento del Lenguaje Natural"},{"id":"http://arxiv.org/abs/2208.07316v2","updated":"2023-04-03T16:15:04Z","published":"2022-08-15T16:30:14Z","title":"MENLI: Robust Evaluation Metrics from Natural Language Inference","summary":"  Recently proposed BERT-based evaluation metrics for text generation perform\nwell on standard benchmarks but are vulnerable to adversarial attacks, e.g.,\nrelating to information correctness. We argue that this stems (in part) from\nthe fact that they are models of semantic similarity. In contrast, we develop\nevaluation metrics based on Natural Language Inference (NLI), which we deem a\nmore appropriate modeling. We design a preference-based adversarial attack\nframework and show that our NLI based metrics are much more robust to the\nattacks than the recent BERT-based metrics. On standard benchmarks, our NLI\nbased metrics outperform existing summarization metrics, but perform below SOTA\nMT metrics. However, when combining existing metrics with our NLI metrics, we\nobtain both higher adversarial robustness (15%-30%) and higher quality metrics\nas measured on standard benchmarks (+5% to 30%).\n","authors":["Yanran Chen","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2208.07316v2.pdf","comment":"TACL 2023 camera-ready"},{"id":"http://arxiv.org/abs/2304.01106v1","updated":"2023-04-03T16:04:06Z","published":"2023-04-03T16:04:06Z","title":"Crossword: A Semantic Approach to Data Compression via Masking","summary":"  The traditional methods for data compression are typically based on the\nsymbol-level statistics, with the information source modeled as a long sequence\nof i.i.d. random variables or a stochastic process, thus establishing the\nfundamental limit as entropy for lossless compression and as mutual information\nfor lossy compression. However, the source (including text, music, and speech)\nin the real world is often statistically ill-defined because of its close\nconnection to human perception, and thus the model-driven approach can be quite\nsuboptimal. This study places careful emphasis on English text and exploits its\nsemantic aspect to enhance the compression efficiency further. The main idea\nstems from the puzzle crossword, observing that the hidden words can still be\nprecisely reconstructed so long as some key letters are provided. The proposed\nmasking-based strategy resembles the above game. In a nutshell, the encoder\nevaluates the semantic importance of each word according to the semantic loss\nand then masks the minor ones, while the decoder aims to recover the masked\nwords from the semantic context by means of the Transformer. Our experiments\nshow that the proposed semantic approach can achieve much higher compression\nefficiency than the traditional methods such as Huffman code and UTF-8 code,\nwhile preserving the meaning in the target text to a great extent.\n","authors":["Mingxiao Li","Rui Jin","Liyao Xiang","Kaiming Shen","Shuguang Cui"],"pdf_url":"https://arxiv.org/pdf/2304.01106v1.pdf","comment":"6 pages, 8 figures"},{"id":"http://arxiv.org/abs/2304.01097v1","updated":"2023-04-03T15:57:51Z","published":"2023-04-03T15:57:51Z","title":"DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task","summary":"  The recent progress of large language models (LLMs), including ChatGPT and\nGPT-4, in comprehending and responding to human instructions has been\nremarkable. Nevertheless, these models typically perform better in English and\nhave not been explicitly trained for the medical domain, resulting in\nsuboptimal precision in diagnoses, drug recommendations, and other medical\nadvice. Additionally, training and deploying a dialogue model is still believed\nto be impossible for hospitals, hindering the promotion of LLMs. To tackle\nthese challenges, we have collected databases of medical dialogues in Chinese\nwith ChatGPT's help and adopted several techniques to train an easy-deploy LLM.\nRemarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13\nhours, which means having a healthcare-purpose LLM can be very affordable.\nDoctorGLM is currently an early-stage engineering attempt and contain various\nmistakes. We are sharing it with the broader community to invite feedback and\nsuggestions to improve its healthcare-focused capabilities:\nhttps://github.com/xionghonglin/DoctorGLM.\n","authors":["Honglin Xiong","Sheng Wang","Yitao Zhu","Zihao Zhao","Yuxiao Liu","Qian Wang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2304.01097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01089v1","updated":"2023-04-03T15:46:15Z","published":"2023-04-03T15:46:15Z","title":"RPTQ: Reorder-based Post-training Quantization for Large Language Models","summary":"  Large-scale language models (LLMs) have demonstrated outstanding performance\non various tasks, but their deployment poses challenges due to their enormous\nmodel size. In this paper, we identify that the main challenge in quantizing\nLLMs stems from the different activation ranges between the channels, rather\nthan just the issue of outliers.We propose a novel reorder-based quantization\napproach, RPTQ, that addresses the issue of quantizing the activations of LLMs.\nRPTQ rearranges the channels in the activations and then quantizing them in\nclusters, thereby reducing the impact of range difference of channels. In\naddition, we reduce the storage and computation overhead by avoiding explicit\nreordering. By implementing this approach, we achieved a significant\nbreakthrough by pushing LLM models to 3 bit activation for the first time.\n","authors":["Zhihang Yuan","Lin Niu","Jiawei Liu","Wenyu Liu","Xinggang Wang","Yuzhang Shang","Guangyu Sun","Qiang Wu","Jiaxiang Wu","Bingzhe Wu"],"pdf_url":"https://arxiv.org/pdf/2304.01089v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2304.01083v1","updated":"2023-04-03T15:39:35Z","published":"2023-04-03T15:39:35Z","title":"Can the Inference Logic of Large Language Models be Disentangled into\n  Symbolic Concepts?","summary":"  In this paper, we explain the inference logic of large language models (LLMs)\nas a set of symbolic concepts. Many recent studies have discovered that\ntraditional DNNs usually encode sparse symbolic concepts. However, because an\nLLM has much more parameters than traditional DNNs, whether the LLM also\nencodes sparse symbolic concepts is still an open problem. Therefore, in this\npaper, we propose to disentangle the inference score of LLMs for dialogue tasks\ninto a small number of symbolic concepts. We verify that we can use those\nsparse concepts to well estimate all inference scores of the LLM on all\narbitrarily masking states of the input sentence. We also evaluate the\ntransferability of concepts encoded by an LLM and verify that symbolic concepts\nusually exhibit high transferability across similar input sentences. More\ncrucially, those symbolic concepts can be used to explain the exact reasons\naccountable for the LLM's prediction errors.\n","authors":["Wen Shen","Lei Cheng","Yuxiao Yang","Mingjie Li","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.13312v2","updated":"2023-04-03T15:26:20Z","published":"2022-10-24T14:58:58Z","title":"Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs","summary":"  Social intelligence and Theory of Mind (ToM), i.e., the ability to reason\nabout the different mental states, intents, and reactions of all people\ninvolved, allow humans to effectively navigate and understand everyday social\ninteractions. As NLP systems are used in increasingly complex social\nsituations, their ability to grasp social dynamics becomes crucial. In this\nwork, we examine the open question of social intelligence and Theory of Mind in\nmodern NLP systems from an empirical and theory-based perspective. We show that\none of today's largest language models (GPT-3; Brown et al., 2020) lacks this\nkind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et\nal., 2019), which measures models' ability to understand intents and reactions\nof participants of social interactions, and ToMi (Le et al., 2019), which\nmeasures whether models can infer mental states and realities of participants\nof situations. Our results show that models struggle substantially at these\nTheory of Mind tasks, with well-below-human accuracies of 55% and 60% on\nSocialIQa and ToMi, respectively. To conclude, we draw on theories from\npragmatics to contextualize this shortcoming of large language models, by\nexamining the limitations stemming from their data, neural architecture, and\ntraining paradigms. Challenging the prevalent narrative that only scale is\nneeded, we posit that person-centric NLP approaches might be more effective\ntowards neural Theory of Mind.\n  In our updated version, we also analyze newer instruction tuned and RLFH\nmodels for neural ToM. We find that even ChatGPT and GPT-4 do not display\nemergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on\nthe ToMi questions related to mental states and realities.\n","authors":["Maarten Sap","Ronan LeBras","Daniel Fried","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2210.13312v2.pdf","comment":"Originally published at EMNLP 2022, extended to include ChatGPT and\n  GPT-4 models on March 30th 2023 (extension not peer reviewed)"},{"id":"http://arxiv.org/abs/2304.01046v1","updated":"2023-04-03T14:48:34Z","published":"2023-04-03T14:48:34Z","title":"Polytuplet Loss: A Reverse Approach to Training Reading Comprehension\n  and Logical Reasoning Models","summary":"  Throughout schooling, students are tested on reading comprehension and\nlogical reasoning. Students have developed various strategies for completing\nsuch exams, some of which are generally thought to outperform others. One such\nstrategy involves emphasizing relative accuracy over absolute accuracy and can\ntheoretically produce the correct answer without full knowledge of the\ninformation required to solve the question. This paper examines the\neffectiveness of applying such a strategy to train transfer learning models to\nsolve reading comprehension and logical reasoning questions. The models were\nevaluated on the ReClor dataset, a challenging reading comprehension and\nlogical reasoning benchmark. While previous studies targeted logical reasoning\nskills, we focus on a general training method and model architecture. We\npropose the polytuplet loss function, an extension of the triplet loss\nfunction, to ensure prioritization of learning the relative correctness of\nanswer choices over learning the true accuracy of each choice. Our results\nindicate that models employing polytuplet loss outperform existing baseline\nmodels. Although polytuplet loss is a promising alternative to other\ncontrastive loss functions, further research is required to quantify the\nbenefits it may present.\n","authors":["Jeffrey Lu","Ivan Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2304.01046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01019v1","updated":"2023-04-03T14:17:00Z","published":"2023-04-03T14:17:00Z","title":"Simple Yet Effective Neural Ranking and Reranking Baselines for\n  Cross-Lingual Information Retrieval","summary":"  The advent of multilingual language models has generated a resurgence of\ninterest in cross-lingual information retrieval (CLIR), which is the task of\nsearching documents in one language with queries from another. However, the\nrapid pace of progress has led to a confusing panoply of methods and\nreproducibility has lagged behind the state of the art. In this context, our\nwork makes two important contributions: First, we provide a conceptual\nframework for organizing different approaches to cross-lingual retrieval using\nmulti-stage architectures for mono-lingual retrieval as a scaffold. Second, we\nimplement simple yet effective reproducible baselines in the Anserini and\nPyserini IR toolkits for test collections from the TREC 2022 NeuCLIR Track, in\nPersian, Russian, and Chinese. Our efforts are built on a collaboration of the\ntwo teams that submitted the most effective runs to the TREC evaluation. These\ncontributions provide a firm foundation for future advances.\n","authors":["Jimmy Lin","David Alfonso-Hermelo","Vitor Jeronymo","Ehsan Kamalloo","Carlos Lassance","Rodrigo Nogueira","Odunayo Ogundepo","Mehdi Rezagholizadeh","Nandan Thakur","Jheng-Hong Yang","Xinyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04310v2","updated":"2023-04-03T14:11:43Z","published":"2022-10-10T18:43:16Z","title":"Montague semantics and modifier consistency measurement in neural\n  language models","summary":"  In recent years, distributional language representation models have\ndemonstrated great practical success. At the same time, the need for\ninterpretability has elicited questions on their intrinsic properties and\ncapabilities. Crucially, distributional models are often inconsistent when\ndealing with compositional phenomena in natural language, which has significant\nimplications for their safety and fairness. Despite this, most current research\non compositionality is directed towards improving their performance on\nsimilarity tasks only. This work takes a different approach, and proposes a\nmethodology for measuring compositional behavior in contemporary language\nmodels. Specifically, we focus on adjectival modifier phenomena in\nadjective-noun phrases. We introduce three novel tests of compositional\nbehavior inspired by Montague semantics. Our experimental results indicate that\ncurrent neural language models behave according to the expected linguistic\ntheories to a limited extent only. This raises the question of whether these\nlanguage models are not able to capture the semantic properties we evaluated,\nor whether linguistic theories from Montagovian tradition would not match the\nexpected capabilities of distributional models.\n","authors":["Danilo S. Carvalho","Edoardo Manino","Julia Rozanova","Lucas Cordeiro","André Freitas"],"pdf_url":"https://arxiv.org/pdf/2212.04310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01002v1","updated":"2023-04-03T14:06:47Z","published":"2023-04-03T14:06:47Z","title":"Understanding Individual and Team-based Human Factors in Detecting\n  Deepfake Texts","summary":"  In recent years, Natural Language Generation (NLG) techniques in AI (e.g.,\nT5, GPT-3, ChatGPT) have shown a massive improvement and are now capable of\ngenerating human-like long coherent texts at scale, yielding so-called deepfake\ntexts. This advancement, despite their benefits, can also cause security and\nprivacy issues (e.g., plagiarism, identity obfuscation, disinformation attack).\nAs such, it has become critically important to develop effective, practical,\nand scalable solutions to differentiate deepfake texts from human-written\ntexts. Toward this challenge, in this work, we investigate how factors such as\nskill levels and collaborations impact how humans identify deepfake texts,\nstudying three research questions: (1) do collaborative teams detect deepfake\ntexts better than individuals? (2) do expert humans detect deepfake texts\nbetter than non-expert humans? (3) what are the factors that maximize the\ndetection performance of humans? We implement these questions on two platforms:\n(1) non-expert humans or asynchronous teams on Amazon Mechanical Turk (AMT) and\n(2) expert humans or synchronous teams on the Upwork. By analyzing the\ndetection performance and the factors that affected performance, some of our\nkey findings are: (1) expert humans detect deepfake texts significantly better\nthan non-expert humans, (2) synchronous teams on the Upwork detect deepfake\ntexts significantly better than individuals, while asynchronous teams on the\nAMT detect deepfake texts weakly better than individuals, and (3) among various\nerror categories, examining coherence and consistency in texts is useful in\ndetecting deepfake texts. In conclusion, our work could inform the design of\nfuture tools/framework to improve collaborative human detection of deepfake\ntexts.\n","authors":["Adaku Uchendu","Jooyoung Lee","Hua Shen","Thai Le","Ting-Hao 'Kenneth' Huang","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2304.01002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00964v1","updated":"2023-04-03T13:30:48Z","published":"2023-04-03T13:30:48Z","title":"Robust Text-driven Image Editing Method that Adaptively Explores\n  Directions in Latent Spaces of StyleGAN and CLIP","summary":"  Automatic image editing has great demands because of its numerous\napplications, and the use of natural language instructions is essential to\nachieving flexible and intuitive editing as the user imagines. A pioneering\nwork in text-driven image editing, StyleCLIP, finds an edit direction in the\nCLIP space and then edits the image by mapping the direction to the StyleGAN\nspace. At the same time, it is difficult to tune appropriate inputs other than\nthe original image and text instructions for image editing. In this study, we\npropose a method to construct the edit direction adaptively in the StyleGAN and\nCLIP spaces with SVM. Our model represents the edit direction as a normal\nvector in the CLIP space obtained by training a SVM to classify positive and\nnegative images. The images are retrieved from a large-scale image corpus,\noriginally used for pre-training StyleGAN, according to the CLIP similarity\nbetween the images and the text instruction. We confirmed that our model\nperformed as well as the StyleCLIP baseline, whereas it allows simple inputs\nwithout increasing the computational time.\n","authors":["Tsuyoshi Baba","Kosuke Nishida","Kyosuke Nishida"],"pdf_url":"https://arxiv.org/pdf/2304.00964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00958v1","updated":"2023-04-03T13:25:53Z","published":"2023-04-03T13:25:53Z","title":"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical\n  domains","summary":"  In recent years, pre-trained language models (PLMs) achieve the best\nperformance on a wide range of natural language processing (NLP) tasks. While\nthe first models were trained on general domain data, specialized ones have\nemerged to more effectively treat specific domains. In this paper, we propose\nan original study of PLMs in the medical domain on French language. We compare,\nfor the first time, the performance of PLMs trained on both public data from\nthe web and private data from healthcare establishments. We also evaluate\ndifferent learning strategies on a set of biomedical tasks. In particular, we\nshow that we can take advantage of already existing biomedical PLMs in a\nforeign language by further pre-train it on our targeted data. Finally, we\nrelease the first specialized PLMs for the biomedical field in French, called\nDrBERT, as well as the largest corpus of medical data under free license on\nwhich these models are trained.\n","authors":["Yanis Labrak","Adrien Bazoge","Richard Dufour","Mickael Rouvier","Emmanuel Morin","Béatrice Daille","Pierre-Antoine Gourraud"],"pdf_url":"https://arxiv.org/pdf/2304.00958v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2304.00913v1","updated":"2023-04-03T12:03:45Z","published":"2023-04-03T12:03:45Z","title":"LAHM : Large Annotated Dataset for Multi-Domain and Multilingual Hate\n  Speech Identification","summary":"  Current research on hate speech analysis is typically oriented towards\nmonolingual and single classification tasks. In this paper, we present a new\nmultilingual hate speech analysis dataset for English, Hindi, Arabic, French,\nGerman and Spanish languages for multiple domains across hate speech - Abuse,\nRacism, Sexism, Religious Hate and Extremism. To the best of our knowledge,\nthis paper is the first to address the problem of identifying various types of\nhate speech in these five wide domains in these six languages. In this work, we\ndescribe how we created the dataset, created annotations at high level and low\nlevel for different domains and how we use it to test the current\nstate-of-the-art multilingual and multitask learning approaches. We evaluate\nour dataset in various monolingual, cross-lingual and machine translation\nclassification settings and compare it against open source English datasets\nthat we aggregated and merged for this task. Then we discuss how this approach\ncan be used to create large scale hate-speech datasets and how to leverage our\nannotations in order to improve hate speech detection and classification in\ngeneral.\n","authors":["Ankit Yadav","Shubham Chandel","Sushant Chatufale","Anil Bandhakavi"],"pdf_url":"https://arxiv.org/pdf/2304.00913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00906v1","updated":"2023-04-03T11:51:46Z","published":"2023-04-03T11:51:46Z","title":"ScandEval: A Benchmark for Scandinavian Natural Language Processing","summary":"  This paper introduces a Scandinavian benchmarking platform, ScandEval, which\ncan benchmark any pretrained model on four different tasks in the Scandinavian\nlanguages. The datasets used in two of the tasks, linguistic acceptability and\nquestion answering, are new. We develop and release a Python package and\ncommand-line interface, scandeval, which can benchmark any model that has been\nuploaded to the Hugging Face Hub, with reproducible results. Using this\npackage, we benchmark more than 100 Scandinavian or multilingual models and\npresent the results of these in an interactive online leaderboard, as well as\nprovide an analysis of the results. The analysis shows that there is\nsubstantial cross-lingual transfer among the Mainland Scandinavian languages\n(Danish, Swedish and Norwegian), with limited cross-lingual transfer between\nthe group of Mainland Scandinavian languages and the group of Insular\nScandinavian languages (Icelandic and Faroese). The benchmarking results also\nshow that the investment in language technology in Norway, Sweden and Denmark\nhas led to language models that outperform massively multilingual models such\nas XLM-RoBERTa and mDeBERTaV3. We release the source code for both the package\nand leaderboard.\n","authors":["Dan Saattrup Nielsen"],"pdf_url":"https://arxiv.org/pdf/2304.00906v1.pdf","comment":"17 pages, 11 figures, camera-ready NoDaLiDa 2023 submission"},{"id":"http://arxiv.org/abs/2304.00884v1","updated":"2023-04-03T11:09:20Z","published":"2023-04-03T11:09:20Z","title":"Dialog-to-Actions: Building Task-Oriented Dialogue System via\n  Action-Level Generation","summary":"  End-to-end generation-based approaches have been investigated and applied in\ntask-oriented dialogue systems. However, in industrial scenarios, existing\nmethods face the bottlenecks of controllability (e.g., domain-inconsistent\nresponses, repetition problem, etc) and efficiency (e.g., long computation\ntime, etc). In this paper, we propose a task-oriented dialogue system via\naction-level generation. Specifically, we first construct dialogue actions from\nlarge-scale dialogues and represent each natural language (NL) response as a\nsequence of dialogue actions. Further, we train a Sequence-to-Sequence model\nwhich takes the dialogue history as input and outputs sequence of dialogue\nactions. The generated dialogue actions are transformed into verbal responses.\nExperimental results show that our light-weighted method achieves competitive\nperformance, and has the advantage of controllability and efficiency.\n","authors":["Yuncheng Hua","Xiangyu Xi","Zheng Jiang","Guanwei Zhang","Chaobo Sun","Guanglu Wan","Wei Ye"],"pdf_url":"https://arxiv.org/pdf/2304.00884v1.pdf","comment":"Accepted at SIGIR 2023 Industry Track"},{"id":"http://arxiv.org/abs/2304.00869v1","updated":"2023-04-03T10:48:51Z","published":"2023-04-03T10:48:51Z","title":"GreekBART: The First Pretrained Greek Sequence-to-Sequence Model","summary":"  The era of transfer learning has revolutionized the fields of Computer Vision\nand Natural Language Processing, bringing powerful pretrained models with\nexceptional performance across a variety of tasks. Specifically, Natural\nLanguage Processing tasks have been dominated by transformer-based language\nmodels. In Natural Language Inference and Natural Language Generation tasks,\nthe BERT model and its variants, as well as the GPT model and its successors,\ndemonstrated exemplary performance. However, the majority of these models are\npretrained and assessed primarily for the English language or on a multilingual\ncorpus. In this paper, we introduce GreekBART, the first Seq2Seq model based on\nBART-base architecture and pretrained on a large-scale Greek corpus. We\nevaluate and compare GreekBART against BART-random, Greek-BERT, and XLM-R on a\nvariety of discriminative tasks. In addition, we examine its performance on two\nNLG tasks from GreekSUM, a newly introduced summarization dataset for the Greek\nlanguage. The model, the code, and the new summarization dataset will be\npublicly available.\n","authors":["Iakovos Evdaimon","Hadi Abdine","Christos Xypolopoulos","Stamatis Outsios","Michalis Vazirgiannis","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2304.00869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.02228v3","updated":"2023-04-03T09:57:51Z","published":"2023-01-05T18:55:09Z","title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in\n  Radiology","summary":"  In this paper, we consider enhancing medical visual-language pre-training\n(VLP) with domain-specific knowledge, by exploiting the paired image-text\nreports from the radiological daily practice. In particular, we make the\nfollowing contributions: First, unlike existing works that directly process the\nraw reports, we adopt a novel triplet extraction module to extract the\nmedical-related information, avoiding unnecessary complexity from language\ngrammar and enhancing the supervision signals; Second, we propose a novel\ntriplet encoding module with entity translation by querying a knowledge base,\nto exploit the rich domain knowledge in medical field, and implicitly build\nrelationships between medical entities in the language embedding space; Third,\nwe propose to use a Transformer-based fusion model for spatially aligning the\nentity description with visual signals at the image patch level, enabling the\nability for medical diagnosis; Fourth, we conduct thorough experiments to\nvalidate the effectiveness of our architecture, and benchmark on numerous\npublic benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax,\nCOVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning\nsettings, our model has demonstrated strong performance compared with the\nformer methods on disease classification and grounding.\n","authors":["Chaoyi Wu","Xiaoman Zhang","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2301.02228v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00830v1","updated":"2023-04-03T09:15:51Z","published":"2023-04-03T09:15:51Z","title":"AUDIT: Audio Editing by Following Instructions with Latent Diffusion\n  Models","summary":"  Audio editing is applicable for various purposes, such as adding background\nsound effects, replacing a musical instrument, and repairing damaged audio.\nRecently, some diffusion-based methods achieved zero-shot audio editing by\nusing a diffusion and denoising process conditioned on the text description of\nthe output audio. However, these methods still have some problems: 1) they have\nnot been trained on editing tasks and cannot ensure good editing effects; 2)\nthey can erroneously modify audio segments that do not require editing; 3) they\nneed a complete description of the output audio, which is not always available\nor necessary in practical scenarios. In this work, we propose AUDIT, an\ninstruction-guided audio editing model based on latent diffusion models.\nSpecifically, AUDIT has three main design features: 1) we construct triplet\ntraining data (instruction, input audio, output audio) for different audio\nediting tasks and train a diffusion model using instruction and input (to be\nedited) audio as conditions and generating output (edited) audio; 2) it can\nautomatically learn to only modify segments that need to be edited by comparing\nthe difference between the input and output audio; 3) it only needs edit\ninstructions instead of full target audio descriptions as text input. AUDIT\nachieves state-of-the-art results in both objective and subjective metrics for\nseveral audio editing tasks (e.g., adding, dropping, replacement, inpainting,\nsuper-resolution). Demo samples are available at https://audit-demo.github.io/.\n","authors":["Yuancheng Wang","Zeqian Ju","Xu Tan","Lei He","Zhizheng Wu","Jiang Bian","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.00830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00824v1","updated":"2023-04-03T09:11:18Z","published":"2023-04-03T09:11:18Z","title":"Towards Integration of Discriminability and Robustness for\n  Document-Level Relation Extraction","summary":"  Document-level relation extraction (DocRE) predicts relations for entity\npairs that rely on long-range context-dependent reasoning in a document. As a\ntypical multi-label classification problem, DocRE faces the challenge of\neffectively distinguishing a small set of positive relations from the majority\nof negative ones. This challenge becomes even more difficult to overcome when\nthere exists a significant number of annotation errors in the dataset. In this\nwork, we aim to achieve better integration of both the discriminability and\nrobustness for the DocRE problem. Specifically, we first design an effective\nloss function to endow high discriminability to both probabilistic outputs and\ninternal representations. We innovatively customize entropy minimization and\nsupervised contrastive learning for the challenging multi-label and long-tailed\nlearning problems. To ameliorate the impact of label errors, we equipped our\nmethod with a novel negative label sampling strategy to strengthen the model\nrobustness. In addition, we introduce two new data regimes to mimic more\nrealistic scenarios with annotation errors and evaluate our sampling strategy.\nExperimental results verify the effectiveness of each component and show that\nour method achieves new state-of-the-art results on the DocRED dataset, its\nrecently cleaned version, Re-DocRED, and the proposed data regimes.\n","authors":["Jia Guo","Stanley Kok","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2304.00824v1.pdf","comment":"EACL 2023 (Main conference, Long paper)"},{"id":"http://arxiv.org/abs/2304.00815v1","updated":"2023-04-03T09:04:18Z","published":"2023-04-03T09:04:18Z","title":"Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing\n  the Biases Introduced by Task Design","summary":"  Disagreement in natural language annotation has mostly been studied from a\nperspective of biases introduced by the annotators and the annotation\nframeworks. Here, we propose to analyze another source of bias: task design\nbias, which has a particularly strong impact on crowdsourced linguistic\nannotations where natural language is used to elicit the interpretation of\nlaymen annotators. For this purpose we look at implicit discourse relation\nannotation, a task that has repeatedly been shown to be difficult due to the\nrelations' ambiguity. We compare the annotations of 1,200 discourse relations\nobtained using two distinct annotation tasks and quantify the biases of both\nmethods across four different domains. Both methods are natural language\nannotation tasks designed for crowdsourcing. We show that the task design can\npush annotators towards certain relations and that some discourse relations\nsenses can be better elicited with one or the other annotation approach. We\nalso conclude that this type of bias should be taken into account when training\nand testing models.\n","authors":["Valentina Pyatkin","Frances Yung","Merel C. J. Scholman","Reut Tsarfaty","Ido Dagan","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2304.00815v1.pdf","comment":"Accepted to TACL, pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2303.17811v2","updated":"2023-04-03T08:58:36Z","published":"2023-03-31T06:00:50Z","title":"Zero-shot Referring Image Segmentation with Global-Local Context\n  Features","summary":"  Referring image segmentation (RIS) aims to find a segmentation mask given a\nreferring expression grounded to a region of the input image. Collecting\nlabelled datasets for this task, however, is notoriously costly and\nlabor-intensive. To overcome this issue, we propose a simple yet effective\nzero-shot referring image segmentation method by leveraging the pre-trained\ncross-modal knowledge from CLIP. In order to obtain segmentation masks grounded\nto the input text, we propose a mask-guided visual encoder that captures global\nand local contextual information of an input image. By utilizing instance masks\nobtained from off-the-shelf mask proposal techniques, our method is able to\nsegment fine-detailed Istance-level groundings. We also introduce a\nglobal-local text encoder where the global feature captures complex\nsentence-level semantics of the entire input expression while the local feature\nfocuses on the target noun phrase extracted by a dependency parser. In our\nexperiments, the proposed method outperforms several zero-shot baselines of the\ntask and even the weakly supervised referring expression segmentation method\nwith substantial margins. Our code is available at\nhttps://github.com/Seonghoon-Yu/Zero-shot-RIS.\n","authors":["Seonghoon Yu","Paul Hongsuck Seo","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2303.17811v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.12570v2","updated":"2023-04-03T08:07:16Z","published":"2023-03-22T13:54:46Z","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval\n  and Generation","summary":"  The task of repository-level code completion is to continue writing the\nunfinished code based on a broader context of the repository. While for\nautomated code completion tools, it is difficult to utilize the useful\ninformation scattered in different files. We propose RepoCoder, a simple,\ngeneric, and effective framework to address the challenge. It streamlines the\nrepository-level code completion process by incorporating a similarity-based\nretriever and a pre-trained code language model, which allows for the effective\nutilization of repository-level information for code completion and grants the\nability to generate code at various levels of granularity. Furthermore,\nRepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges\nthe gap between retrieval context and the intended completion target. We also\npropose a new benchmark RepoEval, which consists of the latest and high-quality\nreal-world repositories covering line, API invocation, and function body\ncompletion scenarios. We test the performance of RepoCoder by using various\ncombinations of code retrievers and generators. Experimental results indicate\nthat RepoCoder significantly improves the zero-shot code completion baseline by\nover 10% in all settings and consistently outperforms the vanilla\nretrieval-augmented code completion approach. Furthermore, we validate the\neffectiveness of RepoCoder through comprehensive analysis, providing valuable\ninsights for future research.\n","authors":["Fengji Zhang","Bei Chen","Yue Zhang","Jin Liu","Daoguang Zan","Yi Mao","Jian-Guang Lou","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12570v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00588v2","updated":"2023-04-03T06:51:35Z","published":"2022-03-01T16:19:14Z","title":"Structural invariants and semantic fingerprints in the \"ego network\" of\n  words","summary":"  Well-established cognitive models coming from anthropology have shown that,\ndue to the cognitive constraints that limit our \"bandwidth\" for social\ninteractions, humans organize their social relations according to a regular\nstructure. In this work, we postulate that similar regularities can be found in\nother cognitive processes, such as those involving language production. In\norder to investigate this claim, we analyse a dataset containing tweets of a\nheterogeneous group of Twitter users (regular users and professional writers).\nLeveraging a methodology similar to the one used to uncover the\nwell-established social cognitive constraints, we find regularities at both the\nstructural and semantic level. At the former, we find that a concentric layered\nstructure (which we call ego network of words, in analogy to the ego network of\nsocial relationships) very well captures how individuals organise the words\nthey use. The size of the layers in this structure regularly grows\n(approximately 2-3 times with respect to the previous one) when moving\noutwards, and the two penultimate external layers consistently account for\napproximately 60% and 30% of the used words, irrespective of the number of the\ntotal number of layers of the user. For the semantic analysis, each ring of\neach ego network is described by a semantic profile, which captures the topics\nassociated with the words in the ring. We find that ring #1 has a special role\nin the model. It is semantically the most dissimilar and the most diverse among\nthe rings. We also show that the topics that are important in the innermost\nring also have the characteristic of being predominant in each of the other\nrings, as well as in the entire ego network. In this respect, ring #1 can be\nseen as the semantic fingerprint of the ego network of words.\n","authors":["Kilian Ollivier","Chiara Boldrini","Andrea Passarella","Marco Conti"],"pdf_url":"https://arxiv.org/pdf/2203.00588v2.pdf","comment":"This work was partially funded by the H2020 SoBigData++ (Grant No\n  871042), H2020 HumaneAI-Net (Grant No 952026), and CHIST-ERA SAI (Grant No\n  not yet available) projects. arXiv admin note: text overlap with\n  arXiv:2110.06015"},{"id":"http://arxiv.org/abs/2210.10341v3","updated":"2023-04-03T06:49:33Z","published":"2022-10-19T07:17:39Z","title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text\n  Generation and Mining","summary":"  Pre-trained language models have attracted increasing attention in the\nbiomedical domain, inspired by their great success in the general natural\nlanguage domain. Among the two main branches of pre-trained language models in\nthe general language domain, i.e., BERT (and its variants) and GPT (and its\nvariants), the first one has been extensively studied in the biomedical domain,\nsuch as BioBERT and PubMedBERT. While they have achieved great success on a\nvariety of discriminative downstream biomedical tasks, the lack of generation\nability constrains their application scope. In this paper, we propose BioGPT, a\ndomain-specific generative Transformer language model pre-trained on large\nscale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and\ndemonstrate that our model outperforms previous models on most tasks.\nEspecially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI\nend-to-end relation extraction tasks respectively, and 78.2% accuracy on\nPubMedQA, creating a new record. Our case study on text generation further\ndemonstrates the advantage of BioGPT on biomedical literature to generate\nfluent descriptions for biomedical terms. Code is available at\nhttps://github.com/microsoft/BioGPT.\n","authors":["Renqian Luo","Liai Sun","Yingce Xia","Tao Qin","Sheng Zhang","Hoifung Poon","Tie-Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2210.10341v3.pdf","comment":"Published at Briefings in Bioinformatics. Code is available at\n  https://github.com/microsoft/BioGPT"},{"id":"http://arxiv.org/abs/2211.11152v2","updated":"2023-04-03T06:41:13Z","published":"2022-11-21T02:32:25Z","title":"You Need Multiple Exiting: Dynamic Early Exiting for Accelerating\n  Unified Vision Language Model","summary":"  Large-scale Transformer models bring significant improvements for various\ndownstream vision language tasks with a unified architecture. The performance\nimprovements come with increasing model size, resulting in slow inference speed\nand increased cost for severing. While some certain predictions benefit from\nthe full complexity of the large-scale model, not all of inputs need the same\namount of computation to conduct, potentially leading to computation resource\nwaste. To handle this challenge, early exiting is proposed to adaptively\nallocate computational power in term of input complexity to improve inference\nefficiency. The existing early exiting strategies usually adopt output\nconfidence based on intermediate layers as a proxy of input complexity to incur\nthe decision of skipping following layers. However, such strategies cannot\napply to encoder in the widely-used unified architecture with both encoder and\ndecoder due to difficulty of output confidence estimation in the encoder. It is\nsuboptimal in term of saving computation power to ignore the early exiting in\nencoder component. To handle this challenge, we propose a novel early exiting\nstrategy for unified visual language models, which allows dynamically skip the\nlayers in encoder and decoder simultaneously in term of input layer-wise\nsimilarities with multiple times of early exiting, namely \\textbf{MuE}. By\ndecomposing the image and text modalities in the encoder, MuE is flexible and\ncan skip different layers in term of modalities, advancing the inference\nefficiency while minimizing performance drop. Experiments on the SNLI-VE and MS\nCOCO datasets show that the proposed approach MuE can reduce expected inference\ntime by up to 50\\% and 40\\% while maintaining 99\\% and 96\\% performance\nrespectively.\n","authors":["Shengkun Tang","Yaqing Wang","Zhenglun Kong","Tianchi Zhang","Yao Li","Caiwen Ding","Yanzhi Wang","Yi Liang","Dongkuan Xu"],"pdf_url":"https://arxiv.org/pdf/2211.11152v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00740v1","updated":"2023-04-03T06:24:10Z","published":"2023-04-03T06:24:10Z","title":"Measuring and Manipulating Knowledge Representations in Language Models","summary":"  Neural language models (LMs) represent facts about the world described by\ntext. Sometimes these facts derive from training data (in most LMs, a\nrepresentation of the word banana encodes the fact that bananas are fruits).\nSometimes facts derive from input text itself (a representation of the sentence\n\"I poured out the bottle\" encodes the fact that the bottle became empty). Tools\nfor inspecting and modifying LM fact representations would be useful almost\neverywhere LMs are used: making it possible to update them when the world\nchanges, to localize and remove sources of bias, and to identify errors in\ngenerated text. We describe REMEDI, an approach for querying and modifying\nfactual knowledge in LMs. REMEDI learns a map from textual queries to fact\nencodings in an LM's internal representation system. These encodings can be\nused as knowledge editors: by adding them to LM hidden representations, we can\nmodify downstream generation to be consistent with new facts. REMEDI encodings\ncan also be used as model probes: by comparing them to LM representations, we\ncan ascertain what properties LMs attribute to mentioned entities, and predict\nwhen they will generate outputs that conflict with background knowledge or\ninput text. REMEDI thus links work on probing, prompting, and model editing,\nand offers steps toward general tools for fine-grained inspection and control\nof knowledge in LMs.\n","authors":["Evan Hernandez","Belinda Z. Li","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2304.00740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12528v2","updated":"2023-04-03T05:57:24Z","published":"2023-03-22T13:03:10Z","title":"MEGA: Multilingual Evaluation of Generative AI","summary":"  Generative AI models have impressive performance on many Natural Language\nProcessing tasks such as language understanding, reasoning and language\ngeneration. One of the most important questions that is being asked by the AI\ncommunity today is about the capabilities and limits of these models, and it is\nclear that evaluating generative AI is very challenging. Most studies on\ngenerative Large Language Models (LLMs) are restricted to English and it is\nunclear how capable these models are at understanding and generating other\nlanguages. We present the first comprehensive benchmarking of generative LLMs -\nMEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse\ntasks and 33 typologically diverse languages. We also compare the performance\nof generative LLMs to State of the Art (SOTA) non-autoregressive models on\nthese tasks to determine how well generative models perform compared to the\nprevious generation of LLMs. We present a thorough analysis of the performance\nof models across languages and discuss some of the reasons why generative LLMs\nare currently not optimal for all languages. We create a framework for\nevaluating generative LLMs in the multilingual setting and provide directions\nfor future progress in the field.\n","authors":["Kabir Ahuja","Rishav Hada","Millicent Ochieng","Prachi Jain","Harshita Diddee","Samuel Maina","Tanuja Ganu","Sameer Segal","Maxamed Axmed","Kalika Bali","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2303.12528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00723v1","updated":"2023-04-03T05:29:58Z","published":"2023-04-03T05:29:58Z","title":"Exploring the Use of Large Language Models for Reference-Free Text\n  Quality Evaluation: A Preliminary Empirical Study","summary":"  Evaluating the quality of generated text is a challenging task in natural\nlanguage processing. This difficulty arises from the inherent complexity and\ndiversity of text. Recently, OpenAI's ChatGPT, a powerful large language model\n(LLM), has garnered significant attention due to its impressive performance in\nvarious tasks. Therefore, we present this report to investigate the\neffectiveness of LLMs, especially ChatGPT, and explore ways to optimize their\nuse in assessing text quality. We compared three kinds of reference-free\nevaluation methods based on ChatGPT or similar LLMs. The experimental results\nprove that ChatGPT is capable to evaluate text quality effectively from various\nperspectives without reference and demonstrates superior performance than most\nexisting automatic metrics. In particular, the Explicit Score, which utilizes\nChatGPT to generate a numeric score measuring text quality, is the most\neffective and reliable method among the three exploited approaches. However,\ndirectly comparing the quality of two texts using ChatGPT may lead to\nsuboptimal results. We hope this report will provide valuable insights into\nselecting appropriate methods for evaluating text quality with LLMs such as\nChatGPT.\n","authors":["Yi Chen","Rui Wang","Haiyun Jiang","Shuming Shi","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2304.00723v1.pdf","comment":"Technical Report, 13 pages"},{"id":"http://arxiv.org/abs/2304.00717v1","updated":"2023-04-03T04:45:57Z","published":"2023-04-03T04:45:57Z","title":"MiniRBT: A Two-stage Distilled Small Chinese Pre-trained Model","summary":"  In natural language processing, pre-trained language models have become\nessential infrastructures. However, these models often suffer from issues such\nas large size, long inference time, and challenging deployment. Moreover, most\nmainstream pre-trained models focus on English, and there are insufficient\nstudies on small Chinese pre-trained models. In this paper, we introduce\nMiniRBT, a small Chinese pre-trained model that aims to advance research in\nChinese natural language processing. MiniRBT employs a narrow and deep student\nmodel and incorporates whole word masking and two-stage distillation during\npre-training to make it well-suited for most downstream tasks. Our experiments\non machine reading comprehension and text classification tasks reveal that\nMiniRBT achieves 94% performance relative to RoBERTa, while providing a 6.8x\nspeedup, demonstrating its effectiveness and efficiency.\n","authors":["Xin Yao","Ziqing Yang","Yiming Cui","Shijin Wang"],"pdf_url":"https://arxiv.org/pdf/2304.00717v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2203.05711v2","updated":"2023-04-03T03:52:14Z","published":"2022-03-11T01:45:33Z","title":"Synopses of Movie Narratives: a Video-Language Dataset for Story\n  Understanding","summary":"  Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SYMON), containing\n5,193 video summaries of popular movies and TV series. SYMON captures\nnaturalistic story-telling videos for human audience made by human creators. As\na prototypical and naturalistic story dataset, SYMON features high coverage of\nmultimodal story events, abundant mental-state descriptions, and large semantic\ngaps between the visual and the textual modalities. We establish benchmarks on\nvideo-text retrieval and zero-shot alignment on movie summary videos, which\nshowcase the importance of in-domain data in story understanding. With SYMON,\nwe hope to lay the groundwork for progress in multimodal story understanding.\n","authors":["Yidan Sun","Qin Chao","Yangfeng Ji","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2203.05711v2.pdf","comment":"25 pages, 17 figures"},{"id":"http://arxiv.org/abs/2303.18149v2","updated":"2023-04-03T02:50:06Z","published":"2023-03-31T15:37:17Z","title":"Can AI Chatbots Pass the Fundamentals of Engineering (FE) and Principles\n  and Practice of Engineering (PE) Structural Exams?","summary":"  The engineering community has recently witnessed the emergence of chatbot\ntechnology with the release of OpenAI ChatGPT-4 and Google Bard. While these\nchatbots have been reported to perform well and even pass various standardized\ntests, including medical and law exams, this forum paper explores whether these\nchatbots can also pass the Fundamentals of Engineering (FE) and Principles and\nPractice of Engineering (PE) exams. A diverse range of civil and environmental\nengineering questions and scenarios are used to evaluate the chatbots'\nperformance, as commonly present in the FE and PE exams. The chatbots'\nresponses were analyzed based on their relevance, accuracy, and clarity and\nthen compared against the recommendations of the National Council of Examiners\nfor Engineering and Surveying (NCEES). Our report shows that ChatGPT-4 and\nBard, respectively scored 70.9% and 39.2% in the FE exam and 46.2% and 41% in\nthe PE exam. It is evident that the current version of ChatGPT-4 could\npotentially pass the FE exam. While future editions are much more likely to\npass both exams, this study also highlights the potential of using chatbots as\nteaching assistants and guiding engineers.\n","authors":["M. Z. Naser","Brandon Ross","Jennier Ogle","Venkatesh Kodur","Rami Hawileh","Jamal Abdalla","Huu-Tai Thai"],"pdf_url":"https://arxiv.org/pdf/2303.18149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10658v4","updated":"2023-04-03T00:28:34Z","published":"2022-06-21T18:16:31Z","title":"Questions Are All You Need to Train a Dense Passage Retriever","summary":"  We introduce ART, a new corpus-level autoencoding approach for training dense\nretrieval models that does not require any labeled training data. Dense\nretrieval is a central challenge for open-domain tasks, such as Open QA, where\nstate-of-the-art methods typically require large supervised datasets with\ncustom hard-negative mining and denoising of positive examples. ART, in\ncontrast, only requires access to unpaired inputs and outputs (e.g. questions\nand potential answer documents). It uses a new document-retrieval autoencoding\nscheme, where (1) an input question is used to retrieve a set of evidence\ndocuments, and (2) the documents are then used to compute the probability of\nreconstructing the original question. Training for retrieval based on question\nreconstruction enables effective unsupervised learning of both document and\nquestion encoders, which can be later incorporated into complete Open QA\nsystems without any further finetuning. Extensive experiments demonstrate that\nART obtains state-of-the-art results on multiple QA retrieval benchmarks with\nonly generic initialization from a pre-trained language model, removing the\nneed for labeled data and task-specific losses.\n","authors":["Devendra Singh Sachan","Mike Lewis","Dani Yogatama","Luke Zettlemoyer","Joelle Pineau","Manzil Zaheer"],"pdf_url":"https://arxiv.org/pdf/2206.10658v4.pdf","comment":"Accepted to TACL, pre MIT Press publication version"},{"id":"http://arxiv.org/abs/2204.07496v4","updated":"2023-04-03T00:07:58Z","published":"2022-04-15T14:51:41Z","title":"Improving Passage Retrieval with Zero-Shot Question Generation","summary":"  We propose a simple and effective re-ranking method for improving passage\nretrieval in open question answering. The re-ranker re-scores retrieved\npassages with a zero-shot question generation model, which uses a pre-trained\nlanguage model to compute the probability of the input question conditioned on\na retrieved passage. This approach can be applied on top of any retrieval\nmethod (e.g. neural or keyword-based), does not require any domain- or\ntask-specific training (and therefore is expected to generalize better to data\ndistribution shifts), and provides rich cross-attention between query and\npassage (i.e. it must explain every token in the question). When evaluated on a\nnumber of open-domain retrieval datasets, our re-ranker improves strong\nunsupervised retrieval models by 6%-18% absolute and strong supervised models\nby up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new\nstate-of-the-art results on full open-domain question answering by simply\nadding the new re-ranker to existing models with no further changes.\n","authors":["Devendra Singh Sachan","Mike Lewis","Mandar Joshi","Armen Aghajanyan","Wen-tau Yih","Joelle Pineau","Luke Zettlemoyer"],"pdf_url":"https://arxiv.org/pdf/2204.07496v4.pdf","comment":"EMNLP 2022 camera-ready version. Code is available at:\n  https://github.com/DevSinghSachan/unsupervised-passage-reranking"},{"id":"http://arxiv.org/abs/2209.02821v4","updated":"2023-04-03T23:20:17Z","published":"2022-09-06T21:20:41Z","title":"Multilingual Bidirectional Unsupervised Translation Through Multilingual\n  Finetuning and Back-Translation","summary":"  We propose a two-stage approach for training a single NMT model to translate\nunseen languages both to and from English. For the first stage, we initialize\nan encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform\nmultilingual fine-tuning on parallel data in 40 languages to English. We find\nthis model can generalize to zero-shot translations on unseen languages. For\nthe second stage, we leverage this generalization ability to generate synthetic\nparallel data from monolingual datasets, then bidirectionally train with\nsuccessive rounds of back-translation.\n  Our approach, which we EcXTra (English-centric Crosslingual (X) Transfer), is\nconceptually simple, only using a standard cross-entropy objective throughout.\nIt is also data-driven, sequentially leveraging auxiliary parallel data and\nmonolingual data. We evaluate unsupervised NMT results for 7 low-resource\nlanguages, and find that each round of back-translation training further\nrefines bidirectional performance. Our final single EcXTra-trained model\nachieves competitive translation performance in all translation directions,\nnotably establishing a new state-of-the-art for English-to-Kazakh (22.9 > 10.4\nBLEU). Our code is available at https://github.com/manestay/EcXTra .\n","authors":["Bryan Li","Mohammad Sadegh Rasooli","Ajay Patel","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2209.02821v4.pdf","comment":"LoResMT @ EACL 2023"},{"id":"http://arxiv.org/abs/2304.01412v1","updated":"2023-04-03T23:18:30Z","published":"2023-04-03T23:18:30Z","title":"The StatCan Dialogue Dataset: Retrieving Data Tables through\n  Conversations with Genuine Intents","summary":"  We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation\nturns between agents working at Statistics Canada and online users looking for\npublished data tables. The conversations stem from genuine intents, are held in\nEnglish or French, and lead to agents retrieving one of over 5000 complex data\ntables. Based on this dataset, we propose two tasks: (1) automatic retrieval of\nrelevant tables based on a on-going conversation, and (2) automatic generation\nof appropriate agent responses at each turn. We investigate the difficulty of\neach task by establishing strong baselines. Our experiments on a temporal data\nsplit reveal that all models struggle to generalize to future conversations, as\nwe observe a significant drop in performance across both tasks when we move\nfrom the validation to the test set. In addition, we find that response\ngeneration models struggle to decide when to return a table. Considering that\nthe tasks pose significant challenges to existing models, we encourage the\ncommunity to develop models for our task, which can be directly used to help\nknowledge workers find relevant tables for live chat users.\n","authors":["Xing Han Lu","Siva Reddy","Harm de Vries"],"pdf_url":"https://arxiv.org/pdf/2304.01412v1.pdf","comment":"Accepted at EACL 2023"},{"id":"http://arxiv.org/abs/2303.17710v2","updated":"2023-04-03T21:14:54Z","published":"2023-03-30T21:05:22Z","title":"What Types of Questions Require Conversation to Answer? A Case Study of\n  AskReddit Questions","summary":"  The proliferation of automated conversational systems such as chatbots,\nspoken-dialogue systems, and smart speakers, has significantly impacted modern\ndigital life. However, these systems are primarily designed to provide answers\nto well-defined questions rather than to support users in exploring complex,\nill-defined questions. In this paper, we aim to push the boundaries of\nconversational systems by examining the types of nebulous, open-ended questions\nthat can best be answered through conversation. We first sampled 500 questions\nfrom one million open-ended requests posted on AskReddit, and then recruited\nonline crowd workers to answer eight inquiries about these questions. We also\nperformed open coding to categorize the questions into 27 different domains. We\nfound that the issues people believe require conversation to resolve\nsatisfactorily are highly social and personal. Our work provides insights into\nhow future research could be geared to align with users' needs.\n","authors":["Shih-Hong Huang","Chieh-Yang Huang","Ya-Fang Lin","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2303.17710v2.pdf","comment":"To appear in CHI 2023 Late-Breaking Work"},{"id":"http://arxiv.org/abs/2202.00396v3","updated":"2023-04-03T21:12:06Z","published":"2022-02-01T13:25:19Z","title":"Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis\n  on the Role of Sentiment in Political Communication","summary":"  Social media has become extremely influential when it comes to policy making\nin modern societies, especially in the western world, where platforms such as\nTwitter allow users to follow politicians, thus making citizens more involved\nin political discussion. In the same vein, politicians use Twitter to express\ntheir opinions, debate among others on current topics and promote their\npolitical agendas aiming to influence voter behaviour. In this paper, we\nattempt to analyse tweets of politicians from three European countries and\nexplore the virality of their tweets. Previous studies have shown that tweets\nconveying negative sentiment are likely to be retweeted more frequently. By\nutilising state-of-the-art pre-trained language models, we performed sentiment\nanalysis on hundreds of thousands of tweets collected from members of\nparliament in Greece, Spain and the United Kingdom, including devolved\nadministrations. We achieved this by systematically exploring and analysing the\ndifferences between influential and less popular tweets. Our analysis indicates\nthat politicians' negatively charged tweets spread more widely, especially in\nmore recent times, and highlights interesting differences between political\nparties as well as between politicians and the general population.\n","authors":["Dimosthenis Antypas","Alun Preece","Jose Camacho-Collados"],"pdf_url":"https://arxiv.org/pdf/2202.00396v3.pdf","comment":"Accepted at \"Online Social Networks and Media, Volume 33\"; for code\n  and data used see https://github.com/cardiffnlp/politics-and-virality-twitter"},{"id":"http://arxiv.org/abs/2304.01373v1","updated":"2023-04-03T20:58:15Z","published":"2023-04-03T20:58:15Z","title":"Pythia: A Suite for Analyzing Large Language Models Across Training and\n  Scaling","summary":"  How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\nhttps://github.com/EleutherAI/pythia.\n","authors":["Stella Biderman","Hailey Schoelkopf","Quentin Anthony","Herbie Bradley","Kyle O'Brien","Eric Hallahan","Mohammad Aflah Khan","Shivanshu Purohit","USVSN Sai Prashanth","Edward Raff","Aviya Skowron","Lintang Sutawika","Oskar van der Wal"],"pdf_url":"https://arxiv.org/pdf/2304.01373v1.pdf","comment":"Code at https://github.com/EleutherAI/pythia"},{"id":"http://arxiv.org/abs/2304.01352v1","updated":"2023-04-03T20:27:10Z","published":"2023-04-03T20:27:10Z","title":"A Simple and Effective Method of Cross-Lingual Plagiarism Detection","summary":"  We present a simple cross-lingual plagiarism detection method applicable to a\nlarge number of languages. The presented approach leverages open multilingual\nthesauri for candidate retrieval task and pre-trained multilingual BERT-based\nlanguage models for detailed analysis. The method does not rely on machine\ntranslation and word sense disambiguation when in use, and therefore is\nsuitable for a large number of languages, including under-resourced languages.\nThe effectiveness of the proposed approach is demonstrated for several existing\nand new benchmarks, achieving state-of-the-art results for French, Russian, and\nArmenian languages.\n","authors":["Karen Avetisyan","Arthur Malajyan","Tsolak Ghukasyan"],"pdf_url":"https://arxiv.org/pdf/2304.01352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01344v1","updated":"2023-04-03T20:20:22Z","published":"2023-04-03T20:20:22Z","title":"End-to-End Models for Chemical-Protein Interaction Extraction: Better\n  Tokenization and Span-Based Pipeline Strategies","summary":"  End-to-end relation extraction (E2ERE) is an important task in information\nextraction, more so for biomedicine as scientific literature continues to grow\nexponentially. E2ERE typically involves identifying entities (or named entity\nrecognition (NER)) and associated relations, while most RE tasks simply assume\nthat the entities are provided upfront and end up performing relation\nclassification. E2ERE is inherently more difficult than RE alone given the\npotential snowball effect of errors from NER leading to more errors in RE. A\ncomplex dataset in biomedical E2ERE is the ChemProt dataset (BioCreative VI,\n2017) that identifies relations between chemical compounds and genes/proteins\nin scientific literature. ChemProt is included in all recent biomedical natural\nlanguage processing benchmarks including BLUE, BLURB, and BigBio. However, its\ntreatment in these benchmarks and in other separate efforts is typically not\nend-to-end, with few exceptions. In this effort, we employ a span-based\npipeline approach to produce a new state-of-the-art E2ERE performance on the\nChemProt dataset, resulting in $> 4\\%$ improvement in F1-score over the prior\nbest effort. Our results indicate that a straightforward fine-grained\ntokenization scheme helps span-based approaches excel in E2ERE, especially with\nregards to handling complex named entities. Our error analysis also identifies\na few key failure modes in E2ERE for ChemProt.\n","authors":["Xuguang Ai","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2304.01344v1.pdf","comment":"Accepted to appear in IEEE ICHI 2023 (HealthNLP workshop). Tokenized\n  dataset and code: https://github.com/bionlproc/end-to-end-ChemProt"},{"id":"http://arxiv.org/abs/2302.03494v8","updated":"2023-04-03T20:02:26Z","published":"2023-02-06T04:21:59Z","title":"A Categorical Archive of ChatGPT Failures","summary":"  Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.\n","authors":["Ali Borji"],"pdf_url":"https://arxiv.org/pdf/2302.03494v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01331v1","updated":"2023-04-03T19:51:00Z","published":"2023-04-03T19:51:00Z","title":"Creating Custom Event Data Without Dictionaries: A Bag-of-Tricks","summary":"  Event data, or structured records of ``who did what to whom'' that are\nautomatically extracted from text, is an important source of data for scholars\nof international politics. The high cost of developing new event datasets,\nespecially using automated systems that rely on hand-built dictionaries, means\nthat most researchers draw on large, pre-existing datasets such as ICEWS rather\nthan developing tailor-made event datasets optimized for their specific\nresearch question. This paper describes a ``bag of tricks'' for efficient,\ncustom event data production, drawing on recent advances in natural language\nprocessing (NLP) that allow researchers to rapidly produce customized event\ndatasets. The paper introduces techniques for training an event category\nclassifier with active learning, identifying actors and the recipients of\nactions in text using large language models and standard machine learning\nclassifiers and pretrained ``question-answering'' models from NLP, and\nresolving mentions of actors to their Wikipedia article to categorize them. We\ndescribe how these techniques produced the new POLECAT global event dataset\nthat is intended to replace ICEWS, along with examples of how scholars can\nquickly produce smaller, custom event datasets. We publish example code and\nmodels to implement our new techniques.\n","authors":["Andrew Halterman","Philip A. Schrodt","Andreas Beger","Benjamin E. Bagozzi","Grace I. Scarborough"],"pdf_url":"https://arxiv.org/pdf/2304.01331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01330v1","updated":"2023-04-03T19:50:55Z","published":"2023-04-03T19:50:55Z","title":"A Comparison of Document Similarity Algorithms","summary":"  Document similarity is an important part of Natural Language Processing and\nis most commonly used for plagiarism-detection and text summarization. Thus,\nfinding the overall most effective document similarity algorithm could have a\nmajor positive impact on the field of Natural Language Processing. This report\nsets out to examine the numerous document similarity algorithms, and determine\nwhich ones are the most useful. It addresses the most effective document\nsimilarity algorithm by categorizing them into 3 types of document similarity\nalgorithms: statistical algorithms, neural networks, and corpus/knowledge-based\nalgorithms. The most effective algorithms in each category are also compared in\nour work using a series of benchmark datasets and evaluations that test every\npossible area that each algorithm could be used in.\n","authors":["Nicholas Gahman","Vinayak Elangovan"],"pdf_url":"https://arxiv.org/pdf/2304.01330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01328v1","updated":"2023-04-03T19:50:26Z","published":"2023-04-03T19:50:26Z","title":"Grand Challenge On Detecting Cheapfakes","summary":"  Cheapfake is a recently coined term that encompasses non-AI (\"cheap\")\nmanipulations of multimedia content. Cheapfakes are known to be more prevalent\nthan deepfakes. Cheapfake media can be created using editing software for\nimage/video manipulations, or even without using any software, by simply\naltering the context of an image/video by sharing the media alongside\nmisleading claims. This alteration of context is referred to as out-of-context\n(OOC) misuse of media. OOC media is much harder to detect than fake media,\nsince the images and videos are not tampered. In this challenge, we focus on\ndetecting OOC images, and more specifically the misuse of real photographs with\nconflicting image captions in news items. The aim of this challenge is to\ndevelop and benchmark models that can be used to detect whether given samples\n(news image and associated captions) are OOC, based on the recently compiled\nCOSMOS dataset.\n","authors":["Duc-Tien Dang-Nguyen","Sohail Ahmed Khan","Cise Midoglu","Michael Riegler","Pål Halvorsen","Minh-Son Dao"],"pdf_url":"https://arxiv.org/pdf/2304.01328v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2207.14534"},{"id":"http://arxiv.org/abs/2304.01322v1","updated":"2023-04-03T19:40:14Z","published":"2023-04-03T19:40:14Z","title":"PALI: A Language Identification Benchmark for Perso-Arabic Scripts","summary":"  The Perso-Arabic scripts are a family of scripts that are widely adopted and\nused by various linguistic communities around the globe. Identifying various\nlanguages using such scripts is crucial to language technologies and\nchallenging in low-resource setups. As such, this paper sheds light on the\nchallenges of detecting languages using Perso-Arabic scripts, especially in\nbilingual communities where ``unconventional'' writing is practiced. To address\nthis, we use a set of supervised techniques to classify sentences into their\nlanguages. Building on these, we also propose a hierarchical model that targets\nclusters of languages that are more often confused by the classifiers. Our\nexperiment results indicate the effectiveness of our solutions.\n","authors":["Sina Ahmadi","Milind Agarwal","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2304.01322v1.pdf","comment":"13 pages - accepted at VarDial at EACL 2023"},{"id":"http://arxiv.org/abs/2304.01319v1","updated":"2023-04-03T19:36:32Z","published":"2023-04-03T19:36:32Z","title":"Approaches to Corpus Creation for Low-Resource Language Technology: the\n  Case of Southern Kurdish and Laki","summary":"  One of the major challenges that under-represented and endangered language\ncommunities face in language technology is the lack or paucity of language\ndata. This is also the case of the Southern varieties of the Kurdish and Laki\nlanguages for which very limited resources are available with insubstantial\nprogress in tools. To tackle this, we provide a few approaches that rely on the\ncontent of local news websites, a local radio station that broadcasts content\nin Southern Kurdish and fieldwork for Laki. In this paper, we describe some of\nthe challenges of such under-represented languages, particularly in writing and\nstandardization, and also, in retrieving sources of data and retro-digitizing\nhandwritten content to create a corpus for Southern Kurdish and Laki. In\naddition, we study the task of language identification in light of the other\nvariants of Kurdish and Zaza-Gorani languages.\n","authors":["Sina Ahmadi","Zahra Azin","Sara Belelli","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2304.01319v1.pdf","comment":"12 pages, accepted at FieldMatters at EACL 2023"},{"id":"http://arxiv.org/abs/2304.01295v1","updated":"2023-04-03T18:46:01Z","published":"2023-04-03T18:46:01Z","title":"Efficiently Aligned Cross-Lingual Transfer Learning for Conversational\n  Tasks using Prompt-Tuning","summary":"  Cross-lingual transfer of language models trained on high-resource languages\nlike English has been widely studied for many NLP tasks, but focus on\nconversational tasks has been rather limited. This is partly due to the high\ncost of obtaining non-English conversational data, which results in limited\ncoverage. In this work, we introduce XSGD, a parallel and large-scale\nmultilingual conversation dataset that we created by translating the\nEnglish-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into\n105 other languages. XSGD contains approximately 330k utterances per language.\nTo facilitate aligned cross-lingual representations, we develop an efficient\nprompt-tuning-based method for learning alignment prompts. We also investigate\ntwo different classifiers: NLI-based and vanilla classifiers, and test\ncross-lingual capability enabled by the aligned prompts. We evaluate our\nmodel's cross-lingual generalization capabilities on two conversation tasks:\nslot-filling and intent classification. Our results demonstrate the strong and\nefficient modeling ability of NLI-based classifiers and the large cross-lingual\ntransfer improvements achieved by our aligned prompts, particularly in few-shot\nsettings.\n","authors":["Lifu Tu","Jin Qu","Semih Yavuz","Shafiq Joty","Wenhao Liu","Caiming Xiong","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.01295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01282v1","updated":"2023-04-03T18:19:26Z","published":"2023-04-03T18:19:26Z","title":"PEACH: Pre-Training Sequence-to-Sequence Multilingual Models for\n  Translation with Semi-Supervised Pseudo-Parallel Document Generation","summary":"  Multilingual pre-training significantly improves many multilingual NLP tasks,\nincluding machine translation. Most existing methods are based on some variants\nof masked language modeling and text-denoising objectives on monolingual data.\nMultilingual pre-training on monolingual data ignores the availability of\nparallel data in many language pairs. Also, some other works integrate the\navailable human-generated parallel translation data in their pre-training. This\nkind of parallel data is definitely helpful, but it is limited even in\nhigh-resource language pairs. This paper introduces a novel semi-supervised\nmethod, SPDG, that generates high-quality pseudo-parallel data for multilingual\npre-training. First, a denoising model is pre-trained on monolingual data to\nreorder, add, remove, and substitute words, enhancing the pre-training\ndocuments' quality. Then, we generate different pseudo-translations for each\npre-training document using dictionaries for word-by-word translation and\napplying the pre-trained denoising model. The resulting pseudo-parallel data is\nthen used to pre-train our multilingual sequence-to-sequence model, PEACH. Our\nexperiments show that PEACH outperforms existing approaches used in training\nmT5 and mBART on various translation tasks, including supervised, zero- and\nfew-shot scenarios. Moreover, PEACH's ability to transfer knowledge between\nsimilar languages makes it particularly useful for low-resource languages. Our\nresults demonstrate that with high-quality dictionaries for generating accurate\npseudo-parallel, PEACH can be valuable for low-resource languages.\n","authors":["Alireza Salemi","Amirhossein Abaskohi","Sara Tavakoli","Yadollah Yaghoobzadeh","Azadeh Shakery"],"pdf_url":"https://arxiv.org/pdf/2304.01282v1.pdf","comment":"15 pages, 5 figures, 16 tables, 1 algorithm, LoResMT@EACL 2023"},{"id":"http://arxiv.org/abs/2304.01246v1","updated":"2023-04-03T16:46:49Z","published":"2023-04-03T16:46:49Z","title":"Safety Analysis in the Era of Large Language Models: A Case Study of\n  STPA using ChatGPT","summary":"  Large Language Models (LLMs), such as ChatGPT and BERT, are leading a new AI\nheatwave due to its human-like conversations with detailed and articulate\nanswers across many domains of knowledge. While LLMs are being quickly applied\nto many AI application domains, we are interested in the following question:\nCan safety analysis for safety-critical systems make use of LLMs? To answer, we\nconduct a case study of Systems Theoretic Process Analysis (STPA) on Automatic\nEmergency Brake (AEB) systems using ChatGPT. STPA, one of the most prevalent\ntechniques for hazard analysis, is known to have limitations such as high\ncomplexity and subjectivity, which this paper aims to explore the use of\nChatGPT to address. Specifically, three ways of incorporating ChatGPT into STPA\nare investigated by considering its interaction with human experts: one-off\nsimplex interaction, recurring simplex interaction, and recurring duplex\ninteraction. Comparative results reveal that: (i) using ChatGPT without human\nexperts' intervention can be inadequate due to reliability and accuracy issues\nof LLMs; (ii) more interactions between ChatGPT and human experts may yield\nbetter results; and (iii) using ChatGPT in STPA with extra care can outperform\nhuman safety experts alone, as demonstrated by reusing an existing comparison\nmethod with baselines. In addition to making the first attempt to apply LLMs in\nsafety analysis, this paper also identifies key challenges (e.g.,\ntrustworthiness concern of LLMs, the need of standardisation) for future\nresearch in this direction.\n","authors":["Yi Qi","Xingyu Zhao","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2304.01246v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2304.01242v1","updated":"2023-04-03T12:15:53Z","published":"2023-04-03T12:15:53Z","title":"Enhancing Clinical Evidence Recommendation with Multi-Channel\n  Heterogeneous Learning on Evidence Graphs","summary":"  Clinical evidence encompasses the associations and impacts between patients,\ninterventions (such as drugs or physiotherapy), problems, and outcomes. The\ngoal of recommending clinical evidence is to provide medical practitioners with\nrelevant information to support their decision-making processes and to generate\nnew evidence. Our specific task focuses on recommending evidence based on\nclinical problems. However, the direct connections between certain clinical\nproblems and related evidence are often sparse, creating a challenge of link\nsparsity. Additionally, to recommend appropriate evidence, it is essential to\njointly exploit both topological relationships among evidence and textual\ninformation describing them. To address these challenges, we define two\nknowledge graphs: an Evidence Co-reference Graph and an Evidence Text Graph, to\nrepresent the topological and linguistic relations among evidential elements,\nrespectively. We also introduce a multi-channel heterogeneous learning model\nand a fusional attention mechanism to handle the co-reference-text\nheterogeneity in evidence recommendation. Our experiments demonstrate that our\nmodel outperforms state-of-the-art methods on open data.\n","authors":["Maolin Luo","Xiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01241v1","updated":"2023-04-03T12:15:27Z","published":"2023-04-03T12:15:27Z","title":"Detection of Homophobia & Transphobia in Dravidian Languages: Exploring\n  Deep Learning Methods","summary":"  The increase in abusive content on online social media platforms is impacting\nthe social life of online users. Use of offensive and hate speech has been\nmaking so-cial media toxic. Homophobia and transphobia constitute offensive\ncomments against LGBT+ community. It becomes imperative to detect and handle\nthese comments, to timely flag or issue a warning to users indulging in such\nbehaviour. However, automated detection of such content is a challenging task,\nmore so in Dravidian languages which are identified as low resource languages.\nMotivated by this, the paper attempts to explore applicability of different\ndeep learning mod-els for classification of the social media comments in\nMalayalam and Tamil lan-guages as homophobic, transphobic and\nnon-anti-LGBT+content. The popularly used deep learning models- Convolutional\nNeural Network (CNN), Long Short Term Memory (LSTM) using GloVe embedding and\ntransformer-based learning models (Multilingual BERT and IndicBERT) are applied\nto the classification problem. Results obtained show that IndicBERT outperforms\nthe other imple-mented models, with obtained weighted average F1-score of 0.86\nand 0.77 for Malayalam and Tamil, respectively. Therefore, the present work\nconfirms higher performance of IndicBERT on the given task in selected\nDravidian languages.\n","authors":["Deepawali Sharma","Vedika Gupta","Vivek Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2304.01241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01240v1","updated":"2023-04-03T11:56:11Z","published":"2023-04-03T11:56:11Z","title":"Identifying Mentions of Pain in Mental Health Records Text: A Natural\n  Language Processing Approach","summary":"  Pain is a common reason for accessing healthcare resources and is a growing\narea of research, especially in its overlap with mental health. Mental health\nelectronic health records are a good data source to study this overlap.\nHowever, much information on pain is held in the free text of these records,\nwhere mentions of pain present a unique natural language processing problem due\nto its ambiguous nature. This project uses data from an anonymised mental\nhealth electronic health records database. The data are used to train a machine\nlearning based classification algorithm to classify sentences as discussing\npatient pain or not. This will facilitate the extraction of relevant pain\ninformation from large databases, and the use of such outputs for further\nstudies on pain and mental health. 1,985 documents were manually\ntriple-annotated for creation of gold standard training data, which was used to\ntrain three commonly used classification algorithms. The best performing model\nachieved an F1-score of 0.98 (95% CI 0.98-0.99).\n","authors":["Jaya Chaturvedi","Sumithra Velupillai","Robert Stewart","Angus Roberts"],"pdf_url":"https://arxiv.org/pdf/2304.01240v1.pdf","comment":"5 pages, 2 tables, submitted to MEDINFO 2023 conference"},{"id":"http://arxiv.org/abs/2304.01238v1","updated":"2023-04-03T10:27:53Z","published":"2023-04-03T10:27:53Z","title":"Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam\n  Detection","summary":"  This paper investigates the effectiveness of large language models (LLMs) in\nemail spam detection by comparing prominent models from three distinct\nfamilies: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we\nexamine well-established machine learning techniques for spam detection, such\nas Na\\\"ive Bayes and LightGBM, as baseline methods. We assess the performance\nof these models across four public datasets, utilizing different numbers of\ntraining samples (full training set and few-shot settings). Our findings reveal\nthat, in the majority of cases, LLMs surpass the performance of the popular\nbaseline techniques, particularly in few-shot scenarios. This adaptability\nrenders LLMs uniquely suited to spam detection tasks, where labeled samples are\nlimited in number and models require frequent updates. Additionally, we\nintroduce Spam-T5, a Flan-T5 model that has been specifically adapted and\nfine-tuned for the purpose of detecting email spam. Our results demonstrate\nthat Spam-T5 surpasses baseline models and other LLMs in the majority of\nscenarios, particularly when there are a limited number of training samples\navailable. Our code is publicly available at\nhttps://github.com/jpmorganchase/emailspamdetection.\n","authors":["Maxime Labonne","Sean Moran"],"pdf_url":"https://arxiv.org/pdf/2304.01238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01233v1","updated":"2023-04-03T06:32:00Z","published":"2023-04-03T06:32:00Z","title":"Multi-Modal Perceiver Language Model for Outcome Prediction in Emergency\n  Department","summary":"  Language modeling have shown impressive progress in generating compelling\ntext with good accuracy and high semantic coherence. An interesting research\ndirection is to augment these powerful models for specific applications using\ncontextual information. In this work, we explore multi-modal language modeling\nfor healthcare applications. We are interested in outcome prediction and\npatient triage in hospital emergency department based on text information in\nchief complaints and vital signs recorded at triage. We adapt Perceiver - a\nmodality-agnostic transformer-based model that has shown promising results in\nseveral applications. Since vital-sign modality is represented in tabular\nformat, we modified Perceiver position encoding to ensure permutation\ninvariance. We evaluated the multi-modal language model for the task of\ndiagnosis code prediction using MIMIC-IV ED dataset on 120K visits. In the\nexperimental analysis, we show that mutli-modality improves the prediction\nperformance compared with models trained solely on text or vital signs. We\nidentified disease categories for which multi-modality leads to performance\nimprovement and show that for these categories, vital signs have added\npredictive power. By analyzing the cross-attention layer, we show how\nmulti-modality contributes to model predictions. This work gives interesting\ninsights on the development of multi-modal language models for healthcare\napplications.\n","authors":["Sabri Boughorbel","Fethi Jarray","Abdulaziz Al Homaid","Rashid Niaz","Khalid Alyafei"],"pdf_url":"https://arxiv.org/pdf/2304.01233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01905v1","updated":"2023-04-03T01:19:39Z","published":"2023-04-03T01:19:39Z","title":"Dual-Attention Neural Transducers for Efficient Wake Word Spotting in\n  Speech Recognition","summary":"  We present dual-attention neural biasing, an architecture designed to boost\nWake Words (WW) recognition and improve inference time latency on speech\nrecognition tasks. This architecture enables a dynamic switch for its runtime\ncompute paths by exploiting WW spotting to select which branch of its attention\nnetworks to execute for an input audio frame. With this approach, we\neffectively improve WW spotting accuracy while saving runtime compute cost as\ndefined by floating point operations (FLOPs). Using an in-house de-identified\ndataset, we demonstrate that the proposed dual-attention network can reduce the\ncompute cost by $90\\%$ for WW audio frames, with only $1\\%$ increase in the\nnumber of parameters. This architecture improves WW F1 score by $16\\%$ relative\nand improves generic rare word error rate by $3\\%$ relative compared to the\nbaselines.\n","authors":["Saumya Y. Sahai","Jing Liu","Thejaswi Muniyappa","Kanthashree M. Sathyendra","Anastasios Alexandridis","Grant P. Strimel","Ross McGowan","Ariya Rastrow","Feng-Ju Chang","Athanasios Mouchtaris","Siegfried Kunzmann"],"pdf_url":"https://arxiv.org/pdf/2304.01905v1.pdf","comment":"Accepted in ICASSP 2023"},{"id":"http://arxiv.org/abs/2304.02020v1","updated":"2023-04-03T21:46:41Z","published":"2023-04-03T21:46:41Z","title":"A Bibliometric Review of Large Language Models Research from 2017 to\n  2023","summary":"  Large language models (LLMs) are a class of language models that have\ndemonstrated outstanding performance across a range of natural language\nprocessing (NLP) tasks and have become a highly sought-after research area,\nbecause of their ability to generate human-like language and their potential to\nrevolutionize science and technology. In this study, we conduct bibliometric\nand discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000\npublications, this paper serves as a roadmap for researchers, practitioners,\nand policymakers to navigate the current landscape of LLMs research. We present\nthe research trends from 2017 to early 2023, identifying patterns in research\nparadigms and collaborations. We start with analyzing the core algorithm\ndevelopments and NLP tasks that are fundamental in LLMs research. We then\ninvestigate the applications of LLMs in various fields and domains including\nmedicine, engineering, social science, and humanities. Our review also reveals\nthe dynamic, fast-paced evolution of LLMs research. Overall, this paper offers\nvaluable insights into the current state, impact, and potential of LLMs\nresearch and its applications.\n","authors":["Lizhou Fan","Lingyao Li","Zihui Ma","Sanggyu Lee","Huizi Yu","Libby Hemphill"],"pdf_url":"https://arxiv.org/pdf/2304.02020v1.pdf","comment":"36 pages, 9 figures, and 4 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2304.01201v1","updated":"2023-04-03T17:59:56Z","published":"2023-04-03T17:59:56Z","title":"Neural Volumetric Memory for Visual Locomotion Control","summary":"  Legged robots have the potential to expand the reach of autonomy beyond paved\nroads. In this work, we consider the difficult problem of locomotion on\nchallenging terrains using a single forward-facing depth camera. Due to the\npartial observability of the problem, the robot has to rely on past\nobservations to infer the terrain currently beneath it. To solve this problem,\nwe follow the paradigm in computer vision that explicitly models the 3D\ngeometry of the scene and propose Neural Volumetric Memory (NVM), a geometric\nmemory architecture that explicitly accounts for the SE(3) equivariance of the\n3D world. NVM aggregates feature volumes from multiple camera views by first\nbringing them back to the ego-centric frame of the robot. We test the learned\nvisual-locomotion policy on a physical robot and show that our approach, which\nexplicitly introduces geometric priors during training, offers superior\nperformance than more na\\\"ive methods. We also include ablation studies and\nshow that the representations stored in the neural volumetric memory capture\nsufficient geometric information to reconstruct the scene. Our project page\nwith videos is https://rchalyang.github.io/NVM .\n","authors":["Ruihan Yang","Ge Yang","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2304.01201v1.pdf","comment":"CVPR 2023 Highlight. Our project page with videos is\n  https://rchalyang.github.io/NVM"},{"id":"http://arxiv.org/abs/2304.01200v1","updated":"2023-04-03T17:59:52Z","published":"2023-04-03T17:59:52Z","title":"Video Instance Segmentation in an Open-World","summary":"  Existing video instance segmentation (VIS) approaches generally follow a\nclosed-world assumption, where only seen category instances are identified and\nspatio-temporally segmented at inference. Open-world formulation relaxes the\nclose-world static-learning assumption as follows: (a) first, it distinguishes\na set of known categories as well as labels an unknown object as `unknown' and\nthen (b) it incrementally learns the class of an unknown as and when the\ncorresponding semantic labels become available. We propose the first open-world\nVIS approach, named OW-VISFormer, that introduces a novel feature enrichment\nmechanism and a spatio-temporal objectness (STO) module. The feature enrichment\nmechanism based on a light-weight auxiliary network aims at accurate\npixel-level (unknown) object delineation from the background as well as\ndistinguishing category-specific known semantic classes. The STO module strives\nto generate instance-level pseudo-labels by enhancing the foreground\nactivations through a contrastive loss. Moreover, we also introduce an\nextensive experimental protocol to measure the characteristics of OW-VIS. Our\nOW-VISFormer performs favorably against a solid baseline in OW-VIS setting.\nFurther, we evaluate our contributions in the standard fully-supervised VIS\nsetting by integrating them into the recent SeqFormer, achieving an absolute\ngain of 1.6\\% AP on Youtube-VIS 2019 val. set. Lastly, we show the\ngeneralizability of our contributions for the open-world detection (OWOD)\nsetting, outperforming the best existing OWOD method in the literature. Code,\nmodels along with OW-VIS splits are available at\n\\url{https://github.com/OmkarThawakar/OWVISFormer}.\n","authors":["Omkar Thawakar","Sanath Narayan","Hisham Cholakkal","Rao Muhammad Anwer","Salman Khan","Jorma Laaksonen","Mubarak Shah","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2304.01200v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2304.01199v1","updated":"2023-04-03T17:59:49Z","published":"2023-04-03T17:59:49Z","title":"On the Benefits of 3D Pose and Tracking for Human Action Recognition","summary":"  In this work we study the benefits of using tracking and 3D poses for action\nrecognition. To achieve this, we take the Lagrangian view on analysing actions\nover a trajectory of human motion rather than at a fixed point in space. Taking\nthis stand allows us to use the tracklets of people to predict their actions.\nIn this spirit, first we show the benefits of using 3D pose to infer actions,\nand study person-person interactions. Subsequently, we propose a Lagrangian\nAction Recognition model by fusing 3D pose and contextualized appearance over\ntracklets. To this end, our method achieves state-of-the-art performance on the\nAVA v2.2 dataset on both pose only settings and on standard benchmark settings.\nWhen reasoning about the action using only pose cues, our pose model achieves\n+10.0 mAP gain over the corresponding state-of-the-art while our fused model\nhas a gain of +2.8 mAP over the best state-of-the-art model. Code and results\nare available at: https://brjathu.github.io/LART\n","authors":["Jathushan Rajasegaran","Georgios Pavlakos","Angjoo Kanazawa","Christoph Feichtenhofer","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2304.01199v1.pdf","comment":"CVPR2023 (project page: https://brjathu.github.io/LART)"},{"id":"http://arxiv.org/abs/2304.01198v1","updated":"2023-04-03T17:59:21Z","published":"2023-04-03T17:59:21Z","title":"Zero-Shot Semantic Segmentation with Decoupled One-Pass Network","summary":"  Recently, the zero-shot semantic segmentation problem has attracted\nincreasing attention, and the best performing methods are based on two-stream\nnetworks: one stream for proposal mask generation and the other for segment\nclassification using a pre-trained visual-language model. However, existing\ntwo-stream methods require passing a great number of (up to a hundred) image\ncrops into the visuallanguage model, which is highly inefficient. To address\nthe problem, we propose a network that only needs a single pass through the\nvisual-language model for each input image. Specifically, we first propose a\nnovel network adaptation approach, termed patch severance, to restrict the\nharmful interference between the patch embeddings in the pre-trained visual\nencoder. We then propose classification anchor learning to encourage the\nnetwork to spatially focus on more discriminative features for classification.\nExtensive experiments demonstrate that the proposed method achieves outstanding\nperformance, surpassing state-of-theart methods while being 4 to 7 times faster\nat inference. We release our code at https://github.com/CongHan0808/DeOP.git.\n","authors":["Cong Han","Yujie Zhong","Dengjie Li","Kai Han","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2304.01198v1.pdf","comment":"13pages, 9 figures"},{"id":"http://arxiv.org/abs/2304.01197v1","updated":"2023-04-03T17:59:16Z","published":"2023-04-03T17:59:16Z","title":"Bringing Telepresence to Every Desk","summary":"  In this paper, we work to bring telepresence to every desktop. Unlike\ncommercial systems, personal 3D video conferencing systems must render\nhigh-quality videos while remaining financially and computationally viable for\nthe average consumer. To this end, we introduce a capturing and rendering\nsystem that only requires 4 consumer-grade RGBD cameras and synthesizes\nhigh-quality free-viewpoint videos of users as well as their environments.\n  Experimental results show that our system renders high-quality free-viewpoint\nvideos without using object templates or heavy pre-processing. While not\nreal-time, our system is fast and does not require per-video optimizations.\nMoreover, our system is robust to complex hand gestures and clothing, and it\ncan generalize to new users. This work provides a strong basis for further\noptimization, and it will help bring telepresence to every desk in the near\nfuture. The code and dataset will be made available on our website\nhttps://mcmvmc.github.io/PersonalTelepresence/.\n","authors":["Shengze Wang","Ziheng Wang","Ryan Schmelzle","Liujie Zheng","YoungJoong Kwon","Soumyadip Sengupta","Henry Fuchs"],"pdf_url":"https://arxiv.org/pdf/2304.01197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01195v1","updated":"2023-04-03T17:58:54Z","published":"2023-04-03T17:58:54Z","title":"Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior\n  Refinement","summary":"  The popularity of Contrastive Language-Image Pre-training (CLIP) has\npropelled its application to diverse downstream vision tasks. To improve its\ncapacity on downstream tasks, few-shot learning has become a widely-adopted\ntechnique. However, existing methods either exhibit limited performance or\nsuffer from excessive learnable parameters. In this paper, we propose APE, an\nAdaptive Prior rEfinement method for CLIP's pre-trained knowledge, which\nachieves superior accuracy with high computational efficiency. Via a prior\nrefinement module, we analyze the inter-class disparity in the downstream data\nand decouple the domain-specific knowledge from the CLIP-extracted cache model.\nOn top of that, we introduce two model variants, a training-free APE and a\ntraining-required APE-T. We explore the trilateral affinities between the test\nimage, prior cache model, and textual representations, and only enable a\nlightweight category-residual module to be trained. For the average accuracy\nover 11 benchmarks, both APE and APE-T attain state-of-the-art and respectively\noutperform the second-best by +1.59% and +1.99% under 16 shots with x30 less\nlearnable parameters.\n","authors":["Xiangyang Zhu","Renrui Zhang","Bowei He","Aojun Zhou","Dong Wang","Bin Zhao","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2304.01195v1.pdf","comment":"Code is available at https://github.com/yangyangyang127/APE"},{"id":"http://arxiv.org/abs/2304.01194v1","updated":"2023-04-03T17:58:44Z","published":"2023-04-03T17:58:44Z","title":"Burstormer: Burst Image Restoration and Enhancement Transformer","summary":"  On a shutter press, modern handheld cameras capture multiple images in rapid\nsuccession and merge them to generate a single image. However, individual\nframes in a burst are misaligned due to inevitable motions and contain multiple\ndegradations. The challenge is to properly align the successive image shots and\nmerge their complimentary information to achieve high-quality outputs. Towards\nthis direction, we propose Burstormer: a novel transformer-based architecture\nfor burst image restoration and enhancement. In comparison to existing works,\nour approach exploits multi-scale local and non-local features to achieve\nimproved alignment and feature fusion. Our key idea is to enable inter-frame\ncommunication in the burst neighborhoods for information aggregation and\nprogressive fusion while modeling the burst-wide context. However, the input\nburst frames need to be properly aligned before fusing their information.\nTherefore, we propose an enhanced deformable alignment module for aligning\nburst features with regards to the reference frame. Unlike existing methods,\nthe proposed alignment module not only aligns burst features but also exchanges\nfeature information and maintains focused communication with the reference\nframe through the proposed reference-based feature enrichment mechanism, which\nfacilitates handling complex motions. After multi-level alignment and\nenrichment, we re-emphasize on inter-frame communication within burst using a\ncyclic burst sampling module. Finally, the inter-frame information is\naggregated using the proposed burst feature fusion module followed by\nprogressive upsampling. Our Burstormer outperforms state-of-the-art methods on\nburst super-resolution, burst denoising and burst low-light enhancement. Our\ncodes and pretrained models are available at https://\ngithub.com/akshaydudhane16/Burstormer\n","authors":["Akshay Dudhane","Syed Waqas Zamir","Salman Khan","Fahad Shahbaz Khan","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2304.01194v1.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2211.16488v2","updated":"2023-04-03T17:58:21Z","published":"2022-11-29T18:56:04Z","title":"Taming Normalizing Flows","summary":"  We propose an algorithm for taming Normalizing Flow models - changing the\nprobability that the model will produce a specific image or image category. We\nfocus on Normalizing Flows because they can calculate the exact generation\nprobability likelihood for a given image. We demonstrate taming using models\nthat generate human faces, a subdomain with many interesting privacy and bias\nconsiderations. Our method can be used in the context of privacy, e.g.,\nremoving a specific person from the output of a model, and also in the context\nof debiasing by forcing a model to output specific image categories according\nto a given target distribution. Taming is achieved with a fast fine-tuning\nprocess without retraining the model from scratch, achieving the goal in a\nmatter of minutes. We evaluate our method qualitatively and quantitatively,\nshowing that the generation quality remains intact, while the desired changes\nare applied.\n","authors":["Shimon Malnick","Shai Avidan","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2211.16488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01192v1","updated":"2023-04-03T17:58:00Z","published":"2023-04-03T17:58:00Z","title":"Navigating to Objects Specified by Images","summary":"  Images are a convenient way to specify which particular object instance an\nembodied agent should navigate to. Solving this task requires semantic visual\nreasoning and exploration of unknown environments. We present a system that can\nperform this task in both simulation and the real world. Our modular method\nsolves sub-tasks of exploration, goal instance re-identification, goal\nlocalization, and local navigation. We re-identify the goal instance in\negocentric vision using feature-matching and localize the goal instance by\nprojecting matched features to a map. Each sub-task is solved using\noff-the-shelf components requiring zero fine-tuning. On the HM3D\nInstanceImageNav benchmark, this system outperforms a baseline end-to-end RL\npolicy 7x and a state-of-the-art ImageNav model 2.3x (56% vs 25% success). We\ndeploy this system to a mobile robot platform and demonstrate effective\nreal-world performance, achieving an 88% success rate across a home and an\noffice environment.\n","authors":["Jacob Krantz","Theophile Gervet","Karmesh Yadav","Austin Wang","Chris Paxton","Roozbeh Mottaghi","Dhruv Batra","Jitendra Malik","Stefan Lee","Devendra Singh Chaplot"],"pdf_url":"https://arxiv.org/pdf/2304.01192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01186v1","updated":"2023-04-03T17:55:14Z","published":"2023-04-03T17:55:14Z","title":"Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free\n  Videos","summary":"  Generating text-editable and pose-controllable character videos have an\nimperious demand in creating various digital human. Nevertheless, this task has\nbeen restricted by the absence of a comprehensive dataset featuring paired\nvideo-pose captions and the generative prior models for videos. In this work,\nwe design a novel two-stage training scheme that can utilize easily obtained\ndatasets (i.e.,image pose pair and pose-free video) and the pre-trained\ntext-to-image (T2I) model to obtain the pose-controllable character videos.\nSpecifically, in the first stage, only the keypoint-image pairs are used only\nfor a controllable text-to-image generation. We learn a zero-initialized\nconvolu- tional encoder to encode the pose information. In the second stage, we\nfinetune the motion of the above network via a pose-free video dataset by\nadding the learnable temporal self-attention and reformed cross-frame\nself-attention blocks. Powered by our new designs, our method successfully\ngenerates continuously pose-controllable character videos while keeps the\nediting and concept composition ability of the pre-trained T2I model. The code\nand models will be made publicly available.\n","authors":["Yue Ma","Yingqing He","Xiaodong Cun","Xintao Wang","Ying Shan","Xiu Li","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2304.01186v1.pdf","comment":"Project page: https://follow-your-pose.github.io/; Github repository:\n  https://github.com/mayuelala/FollowYourPose"},{"id":"http://arxiv.org/abs/2304.01184v1","updated":"2023-04-03T17:54:10Z","published":"2023-04-03T17:54:10Z","title":"WeakTr: Exploring Plain Vision Transformer for Weakly-supervised\n  Semantic Segmentation","summary":"  This paper explores the properties of the plain Vision Transformer (ViT) for\nWeakly-supervised Semantic Segmentation (WSSS). The class activation map (CAM)\nis of critical importance for understanding a classification network and\nlaunching WSSS. We observe that different attention heads of ViT focus on\ndifferent image areas. Thus a novel weight-based method is proposed to\nend-to-end estimate the importance of attention heads, while the self-attention\nmaps are adaptively fused for high-quality CAM results that tend to have more\ncomplete objects. Besides, we propose a ViT-based gradient clipping decoder for\nonline retraining with the CAM results to complete the WSSS task. We name this\nplain Transformer-based Weakly-supervised learning framework WeakTr. It\nachieves the state-of-the-art WSSS performance on standard benchmarks, i.e.,\n78.4% mIoU on the val set of PASCAL VOC 2012 and 50.3% mIoU on the val set of\nCOCO 2014. Code is available at https://github.com/hustvl/WeakTr.\n","authors":["Lianghui Zhu","Yingyue Li","Jieming Fang","Yan Liu","Hao Xin","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2304.01184v1.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2304.01172v1","updated":"2023-04-03T17:41:20Z","published":"2023-04-03T17:41:20Z","title":"Generative Multiplane Neural Radiance for 3D-Aware Image Generation","summary":"  We present a method to efficiently generate 3D-aware high-resolution images\nthat are view-consistent across multiple target views. The proposed multiplane\nneural radiance model, named GMNR, consists of a novel {\\alpha}-guided\nview-dependent representation ({\\alpha}-VdR) module for learning view-dependent\ninformation. The {\\alpha}-VdR module, faciliated by an {\\alpha}-guided pixel\nsampling technique, computes the view-dependent representation efficiently by\nlearning viewing direction and position coefficients. Moreover, we propose a\nview-consistency loss to enforce photometric similarity across multiple views.\nThe GMNR model can generate 3D-aware high-resolution images that are\nviewconsistent across multiple camera poses, while maintaining the\ncomputational efficiency in terms of both training and inference time.\nExperiments on three datasets demonstrate the effectiveness of the proposed\nmodules, leading to favorable results in terms of both generation quality and\ninference time, compared to existing approaches. Our GMNR model generates\n3D-aware images of 1024 X 1024 pixels with 17.6 FPS on a single V100. Code :\nhttps://github.com/VIROBO-15/GMNR\n","authors":["Amandeep Kumar","Ankan Kumar Bhunia","Sanath Narayan","Hisham Cholakkal","Rao Muhammad Anwer","Salman Khan","Ming-Hsuan Yang","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2304.01172v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2304.01171v1","updated":"2023-04-03T17:40:30Z","published":"2023-04-03T17:40:30Z","title":"Rethinking Context Aggregation in Natural Image Matting","summary":"  For natural image matting, context information plays a crucial role in\nestimating alpha mattes especially when it is challenging to distinguish\nforeground from its background. Exiting deep learning-based methods exploit\nspecifically designed context aggregation modules to refine encoder features.\nHowever, the effectiveness of these modules has not been thoroughly explored.\nIn this paper, we conduct extensive experiments to reveal that the context\naggregation modules are actually not as effective as expected. We also\ndemonstrate that when learned on large image patches, basic encoder-decoder\nnetworks with a larger receptive field can effectively aggregate context to\nachieve better performance.Upon the above findings, we propose a simple yet\neffective matting network, named AEMatter, which enlarges the receptive field\nby incorporating an appearance-enhanced axis-wise learning block into the\nencoder and adopting a hybrid-transformer decoder. Experimental results on four\ndatasets demonstrate that our AEMatter significantly outperforms\nstate-of-the-art matting methods (e.g., on the Adobe Composition-1K dataset,\n\\textbf{25\\%} and \\textbf{40\\%} reduction in terms of SAD and MSE,\nrespectively, compared against MatteFormer). The code and model are available\nat \\url{https://github.com/QLYoo/AEMatter}.\n","authors":["Qinglin Liu","Shengping Zhang","Quanling Meng","Ru Li","Bineng Zhong","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2304.01171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01168v1","updated":"2023-04-03T17:37:00Z","published":"2023-04-03T17:37:00Z","title":"DeepAccident: A Motion and Accident Prediction Benchmark for V2X\n  Autonomous Driving","summary":"  Safety is the primary priority of autonomous driving. Nevertheless, no\npublished dataset currently supports the direct and explainable safety\nevaluation for autonomous driving. In this work, we propose DeepAccident, a\nlarge-scale dataset generated via a realistic simulator containing diverse\naccident scenarios that frequently occur in real-world driving. The proposed\nDeepAccident dataset contains 57K annotated frames and 285K annotated samples,\napproximately 7 times more than the large-scale nuScenes dataset with 40k\nannotated samples. In addition, we propose a new task, end-to-end motion and\naccident prediction, based on the proposed dataset, which can be used to\ndirectly evaluate the accident prediction ability for different autonomous\ndriving algorithms. Furthermore, for each scenario, we set four vehicles along\nwith one infrastructure to record data, thus providing diverse viewpoints for\naccident scenarios and enabling V2X (vehicle-to-everything) research on\nperception and prediction tasks. Finally, we present a baseline V2X model named\nV2XFormer that demonstrates superior performance for motion and accident\nprediction and 3D object detection compared to the single-vehicle model.\n","authors":["Tianqi Wang","Sukmin Kim","Wenxuan Ji","Enze Xie","Chongjian Ge","Junsong Chen","Zhenguo Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2304.01168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10526v2","updated":"2023-04-03T17:20:17Z","published":"2022-11-18T22:49:04Z","title":"Castling-ViT: Compressing Self-Attention via Switching Towards\n  Linear-Angular Attention During Vision Transformer Inference","summary":"  Vision Transformers (ViTs) have shown impressive performance but still\nrequire a high computation cost as compared to convolutional neural networks\n(CNNs), one reason is that ViTs' attention measures global similarities and\nthus has a quadratic complexity with the number of input tokens. Existing\nefficient ViTs adopt local attention (e.g., Swin) or linear attention (e.g.,\nPerformer), which sacrifice ViTs' capabilities of capturing either global or\nlocal context. In this work, we ask an important research question: Can ViTs\nlearn both global and local context while being more efficient during\ninference? To this end, we propose a framework called Castling-ViT, which\ntrains ViTs using both linear-angular attention and masked softmax-based\nquadratic attention, but then switches to having only linear angular attention\nduring ViT inference. Our Castling-ViT leverages angular kernels to measure the\nsimilarities between queries and keys via spectral angles. And we further\nsimplify it with two techniques: (1) a novel linear-angular attention\nmechanism: we decompose the angular kernels into linear terms and high-order\nresiduals, and only keep the linear terms; and (2) we adopt two parameterized\nmodules to approximate high-order residuals: a depthwise convolution and an\nauxiliary masked softmax attention to help learn both global and local\ninformation, where the masks for softmax attention are regularized to gradually\nbecome zeros and thus incur no overhead during ViT inference. Extensive\nexperiments and ablation studies on three tasks consistently validate the\neffectiveness of the proposed Castling-ViT, e.g., achieving up to a 1.8% higher\naccuracy or 40% MACs reduction on ImageNet classification and 1.2 higher mAP on\nCOCO detection under comparable FLOPs, as compared to ViTs with vanilla\nsoftmax-based attentions.\n","authors":["Haoran You","Yunyang Xiong","Xiaoliang Dai","Bichen Wu","Peizhao Zhang","Haoqi Fan","Peter Vajda","Yingyan Lin"],"pdf_url":"https://arxiv.org/pdf/2211.10526v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2301.05169v2","updated":"2023-04-03T17:19:51Z","published":"2023-01-12T17:43:38Z","title":"Causal Triplet: An Open Challenge for Intervention-centric Causal\n  Representation Learning","summary":"  Recent years have seen a surge of interest in learning high-level causal\nrepresentations from low-level image pairs under interventions. Yet, existing\nefforts are largely limited to simple synthetic settings that are far away from\nreal-world problems. In this paper, we present Causal Triplet, a causal\nrepresentation learning benchmark featuring not only visually more complex\nscenes, but also two crucial desiderata commonly overlooked in previous works:\n(i) an actionable counterfactual setting, where only certain object-level\nvariables allow for counterfactual observations whereas others do not; (ii) an\ninterventional downstream task with an emphasis on out-of-distribution\nrobustness from the independent causal mechanisms principle. Through extensive\nexperiments, we find that models built with the knowledge of disentangled or\nobject-centric representations significantly outperform their distributed\ncounterparts. However, recent causal representation learning methods still\nstruggle to identify such latent structures, indicating substantial challenges\nand opportunities for future work. Our code and datasets will be available at\nhttps://sites.google.com/view/causaltriplet.\n","authors":["Yuejiang Liu","Alexandre Alahi","Chris Russell","Max Horn","Dominik Zietlow","Bernhard Schölkopf","Francesco Locatello"],"pdf_url":"https://arxiv.org/pdf/2301.05169v2.pdf","comment":"Conference on Causal Learning and Reasoning (CLeaR) 2023"},{"id":"http://arxiv.org/abs/2304.01143v1","updated":"2023-04-03T17:09:47Z","published":"2023-04-03T17:09:47Z","title":"Use Your Head: Improving Long-Tail Video Recognition","summary":"  This paper presents an investigation into long-tail video recognition. We\ndemonstrate that, unlike naturally-collected video datasets and existing\nlong-tail image benchmarks, current video benchmarks fall short on multiple\nlong-tailed properties. Most critically, they lack few-shot classes in their\ntails. In response, we propose new video benchmarks that better assess\nlong-tail recognition, by sampling subsets from two datasets: SSv2 and VideoLT.\n  We then propose a method, Long-Tail Mixed Reconstruction, which reduces\noverfitting to instances from few-shot classes by reconstructing them as\nweighted combinations of samples from head classes. LMR then employs label\nmixing to learn robust decision boundaries. It achieves state-of-the-art\naverage class accuracy on EPIC-KITCHENS and the proposed SSv2-LT and\nVideoLT-LT. Benchmarks and code at: tobyperrett.github.io/lmr\n","authors":["Toby Perrett","Saptarshi Sinha","Tilo Burghardt","Majid Mirmehdi","Dima Damen"],"pdf_url":"https://arxiv.org/pdf/2304.01143v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2212.05706v2","updated":"2023-04-03T16:40:22Z","published":"2022-12-12T05:15:18Z","title":"Detection Selection Algorithm: A Likelihood based Optimization Method to\n  Perform Post Processing for Object Detection","summary":"  In object detection, post-processing methods like Non-maximum Suppression\n(NMS) are widely used. NMS can substantially reduce the number of false\npositive detections but may still keep some detections with low objectness\nscores. In order to find the exact number of objects and their labels in the\nimage, we propose a post processing method called Detection Selection Algorithm\n(DSA) which is used after NMS or related methods. DSA greedily selects a subset\nof detected bounding boxes, together with full object reconstructions that give\nthe interpretation of the whole image with highest likelihood, taking into\naccount object occlusions. The algorithm consists of four components. First, we\nadd an occlusion branch to Faster R-CNN to obtain occlusion relationships\nbetween objects. Second, we develop a single reconstruction algorithm which can\nreconstruct the whole appearance of an object given its visible part, based on\nthe optimization of latent variables of a trained generative network which we\ncall the decoder. Third, we propose a whole reconstruction algorithm which\ngenerates the joint reconstruction of all objects in a hypothesized\ninterpretation, taking into account occlusion ordering. Finally we propose a\ngreedy algorithm that incrementally adds or removes detections from a list to\nmaximize the likelihood of the corresponding interpretation. DSA with NMS or\nSoft-NMS can achieve better results than NMS or Soft-NMS themselves, as is\nillustrated in our experiments on synthetic images with mutiple 3d objects.\n","authors":["Angzhi Fan","Benjamin Ticknor","Yali Amit"],"pdf_url":"https://arxiv.org/pdf/2212.05706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05598v2","updated":"2023-04-03T16:38:14Z","published":"2022-12-11T20:28:59Z","title":"Recurrent Vision Transformers for Object Detection with Event Cameras","summary":"  We present Recurrent Vision Transformers (RVTs), a novel backbone for object\ndetection with event cameras. Event cameras provide visual information with\nsub-millisecond latency at a high-dynamic range and with strong robustness\nagainst motion blur. These unique properties offer great potential for\nlow-latency object detection and tracking in time-critical scenarios. Prior\nwork in event-based vision has achieved outstanding detection performance but\nat the cost of substantial inference time, typically beyond 40 milliseconds. By\nrevisiting the high-level design of recurrent vision backbones, we reduce\ninference time by a factor of 6 while retaining similar performance. To achieve\nthis, we explore a multi-stage design that utilizes three key concepts in each\nstage: First, a convolutional prior that can be regarded as a conditional\npositional embedding. Second, local and dilated global self-attention for\nspatial feature interaction. Third, recurrent temporal feature aggregation to\nminimize latency while retaining temporal information. RVTs can be trained from\nscratch to reach state-of-the-art performance on event-based object detection -\nachieving an mAP of 47.2% on the Gen1 automotive dataset. At the same time,\nRVTs offer fast inference (<12 ms on a T4 GPU) and favorable parameter\nefficiency (5 times fewer than prior art). Our study brings new insights into\neffective design choices that can be fruitful for research beyond event-based\nvision.\n","authors":["Mathias Gehrig","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2212.05598v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2207.00400v2","updated":"2023-04-03T16:35:49Z","published":"2022-07-01T13:17:01Z","title":"WNet: A data-driven dual-domain denoising model for sparse-view computed\n  tomography with a trainable reconstruction layer","summary":"  Deep learning based solutions are being succesfully implemented for a wide\nvariety of applications. Most notably, clinical use-cases have gained an\nincreased interest and have been the main driver behind some of the\ncutting-edge data-driven algorithms proposed in the last years. For\napplications like sparse-view tomographic reconstructions, where the amount of\nmeasurement data is small in order to keep acquisition time short and radiation\ndose low, reduction of the streaking artifacts has prompted the development of\ndata-driven denoising algorithms with the main goal of obtaining diagnostically\nviable images with only a subset of a full-scan data. We propose WNet, a\ndata-driven dual-domain denoising model which contains a trainable\nreconstruction layer for sparse-view artifact denoising. Two encoder-decoder\nnetworks perform denoising in both sinogram- and reconstruction-domain\nsimultaneously, while a third layer implementing the Filtered Backprojection\nalgorithm is sandwiched between the first two and takes care of the\nreconstruction operation. We investigate the performance of the network on\nsparse-view chest CT scans, and we highlight the added benefit of having a\ntrainable reconstruction layer over the more conventional fixed ones. We train\nand test our network on two clinically relevant datasets and we compare the\nobtained results with three different types of sparse-view CT denoising and\nreconstruction algorithms.\n","authors":["Theodor Cheslerean-Boghiu","Felix C. Hofmann","Manuel Schultheiß","Franz Pfeiffer","Daniela Pfeiffer","Tobias Lasser"],"pdf_url":"https://arxiv.org/pdf/2207.00400v2.pdf","comment":"Publisehd at IEEE TCI in January 2023. Supplementary materials are\n  available @IEEE"},{"id":"http://arxiv.org/abs/2304.01116v1","updated":"2023-04-03T16:29:00Z","published":"2023-04-03T16:29:00Z","title":"ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model","summary":"  3D human motion generation is crucial for creative industry. Recent advances\nrely on generative models with domain knowledge for text-driven motion\ngeneration, leading to substantial progress in capturing common motions.\nHowever, the performance on more diverse motions remains unsatisfactory. In\nthis work, we propose ReMoDiffuse, a diffusion-model-based motion generation\nframework that integrates a retrieval mechanism to refine the denoising\nprocess. ReMoDiffuse enhances the generalizability and diversity of text-driven\nmotion generation with three key designs: 1) Hybrid Retrieval finds appropriate\nreferences from the database in terms of both semantic and kinematic\nsimilarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval\nknowledge, adapting to the difference between retrieved samples and the target\nmotion sequence. 3) Condition Mixture better utilizes the retrieval database\nduring inference, overcoming the scale sensitivity in classifier-free guidance.\nExtensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art\nmethods by balancing both text-motion consistency and motion quality,\nespecially for more diverse motion generation.\n","authors":["Mingyuan Zhang","Xinying Guo","Liang Pan","Zhongang Cai","Fangzhou Hong","Huirong Li","Lei Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2304.01116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01114v1","updated":"2023-04-03T16:24:39Z","published":"2023-04-03T16:24:39Z","title":"Associating Spatially-Consistent Grouping with Text-supervised Semantic\n  Segmentation","summary":"  In this work, we investigate performing semantic segmentation solely through\nthe training on image-sentence pairs. Due to the lack of dense annotations,\nexisting text-supervised methods can only learn to group an image into semantic\nregions via pixel-insensitive feedback. As a result, their grouped results are\ncoarse and often contain small spurious regions, limiting the upper-bound\nperformance of segmentation. On the other hand, we observe that grouped results\nfrom self-supervised models are more semantically consistent and break the\nbottleneck of existing methods. Motivated by this, we introduce associate\nself-supervised spatially-consistent grouping with text-supervised semantic\nsegmentation. Considering the part-like grouped results, we further adapt a\ntext-supervised model from image-level to region-level recognition with two\ncore designs. First, we encourage fine-grained alignment with a one-way\nnoun-to-region contrastive loss, which reduces the mismatched noun-region\npairs. Second, we adopt a contextually aware masking strategy to enable\nsimultaneous recognition of all grouped regions. Coupled with\nspatially-consistent grouping and region-adapted recognition, our method\nachieves 59.2% mIoU and 32.4% mIoU on Pascal VOC and Pascal Context benchmarks,\nsignificantly surpassing the state-of-the-art methods.\n","authors":["Yabo Zhang","Zihao Wang","Jun Hao Liew","Jingjia Huang","Manyu Zhu","Jiashi Feng","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2304.01114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01110v1","updated":"2023-04-03T16:13:41Z","published":"2023-04-03T16:13:41Z","title":"AutoLabel: CLIP-based framework for Open-set Video Domain Adaptation","summary":"  Open-set Unsupervised Video Domain Adaptation (OUVDA) deals with the task of\nadapting an action recognition model from a labelled source domain to an\nunlabelled target domain that contains \"target-private\" categories, which are\npresent in the target but absent in the source. In this work we deviate from\nthe prior work of training a specialized open-set classifier or weighted\nadversarial learning by proposing to use pre-trained Language and Vision Models\n(CLIP). The CLIP is well suited for OUVDA due to its rich representation and\nthe zero-shot recognition capabilities. However, rejecting target-private\ninstances with the CLIP's zero-shot protocol requires oracle knowledge about\nthe target-private label names. To circumvent the impossibility of the\nknowledge of label names, we propose AutoLabel that automatically discovers and\ngenerates object-centric compositional candidate target-private class names.\nDespite its simplicity, we show that CLIP when equipped with AutoLabel can\nsatisfactorily reject the target-private instances, thereby facilitating better\nalignment between the shared classes of the two domains. The code is available.\n","authors":["Giacomo Zara","Subhankar Roy","Paolo Rota","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2304.01110v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01108v1","updated":"2023-04-03T16:08:22Z","published":"2023-04-03T16:08:22Z","title":"Coincidental Generation","summary":"  Generative AI models are emerging as a versatile tool across diverse\nindustries with applications in synthetic data generation computational art\npersonalization of products and services and immersive entertainment Here we\nintroduce a new privacy concern in the adoption and use of generative AI models\nthat of coincidental generation Coincidental generation occurs when a models\noutput inadvertently bears a likeness to a realworld entity Consider for\nexample synthetic portrait generators which are today deployed in commercial\napplications such as virtual modeling agencies and synthetic stock photography\nWe argue that the low intrinsic dimensionality of human face perception implies\nthat every synthetically generated face will coincidentally resemble an actual\nperson all but guaranteeing a privacy violation in the form of a\nmisappropriation of likeness.\n","authors":["Jordan W. Suchow","Necdet Gürkan"],"pdf_url":"https://arxiv.org/pdf/2304.01108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12067v2","updated":"2023-04-03T16:05:50Z","published":"2023-01-28T02:48:14Z","title":"Learning Optimal Features via Partial Invariance","summary":"  Learning models that are robust to distribution shifts is a key concern in\nthe context of their real-life applicability. Invariant Risk Minimization (IRM)\nis a popular framework that aims to learn robust models from multiple\nenvironments. The success of IRM requires an important assumption: the\nunderlying causal mechanisms/features remain invariant across environments.\nWhen not satisfied, we show that IRM can over-constrain the predictor and to\nremedy this, we propose a relaxation via $\\textit{partial invariance}$. In this\nwork, we theoretically highlight the sub-optimality of IRM and then demonstrate\nhow learning from a partition of training domains can help improve invariant\nmodels. Several experiments, conducted both in linear settings as well as with\ndeep neural networks on tasks over both language and image data, allow us to\nverify our conclusions.\n","authors":["Moulik Choraria","Ibtihal Ferwana","Ankur Mani","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2301.12067v2.pdf","comment":"Presented at the 37th AAAI Conference on Artificial Intelligence,\n  2023"},{"id":"http://arxiv.org/abs/2304.01101v1","updated":"2023-04-03T16:01:03Z","published":"2023-04-03T16:01:03Z","title":"Dsfer-Net: A Deep Supervision and Feature Retrieval Network for\n  Bitemporal Change Detection Using Modern Hopfield Networks","summary":"  Change detection, as an important application for high-resolution remote\nsensing images, aims to monitor and analyze changes in the land surface over\ntime. With the rapid growth in the quantity of high-resolution remote sensing\ndata and the complexity of texture features, a number of quantitative deep\nlearning-based methods have been proposed. Although these methods outperform\ntraditional change detection methods by extracting deep features and combining\nspatial-temporal information, reasonable explanations about how deep features\nwork on improving the detection performance are still lacking. In our\ninvestigations, we find that modern Hopfield network layers achieve\nconsiderable performance in semantic understandings. In this paper, we propose\na Deep Supervision and FEature Retrieval network (Dsfer-Net) for bitemporal\nchange detection. Specifically, the highly representative deep features of\nbitemporal images are jointly extracted through a fully convolutional Siamese\nnetwork. Based on the sequential geo-information of the bitemporal images, we\nthen design a feature retrieval module to retrieve the difference feature and\nleverage discriminative information in a deeply supervised manner. We also note\nthat the deeply supervised feature retrieval module gives explainable proofs\nabout the semantic understandings of the proposed network in its deep layers.\nFinally, this end-to-end network achieves a novel framework by aggregating the\nretrieved features and feature pairs from different layers. Experiments\nconducted on three public datasets (LEVIR-CD, WHU-CD, and CDD) confirm the\nsuperiority of the proposed Dsfer-Net over other state-of-the-art methods. Code\nwill be available online (https://github.com/ShizhenChang/Dsfer-Net).\n","authors":["Shizhen Chang","Michael Kopp","Pedram Ghamisi"],"pdf_url":"https://arxiv.org/pdf/2304.01101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.02379v2","updated":"2023-04-03T15:58:43Z","published":"2023-01-06T05:04:32Z","title":"CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior","summary":"  Speech-driven 3D facial animation has been widely studied, yet there is still\na gap to achieving realism and vividness due to the highly ill-posed nature and\nscarcity of audio-visual data. Existing works typically formulate the\ncross-modal mapping into a regression task, which suffers from the\nregression-to-mean problem leading to over-smoothed facial motions. In this\npaper, we propose to cast speech-driven facial animation as a code query task\nin a finite proxy space of the learned codebook, which effectively promotes the\nvividness of the generated motions by reducing the cross-modal mapping\nuncertainty. The codebook is learned by self-reconstruction over real facial\nmotions and thus embedded with realistic facial motion priors. Over the\ndiscrete motion space, a temporal autoregressive model is employed to\nsequentially synthesize facial motions from the input speech signal, which\nguarantees lip-sync as well as plausible facial expressions. We demonstrate\nthat our approach outperforms current state-of-the-art methods both\nqualitatively and quantitatively. Also, a user study further justifies our\nsuperiority in perceptual quality.\n","authors":["Jinbo Xing","Menghan Xia","Yuechen Zhang","Xiaodong Cun","Jue Wang","Tien-Tsin Wong"],"pdf_url":"https://arxiv.org/pdf/2301.02379v2.pdf","comment":"CVPR2023 Camera-Ready. Project Page:\n  https://doubiiu.github.io/projects/codetalker/, Code:\n  https://github.com/Doubiiu/CodeTalker"},{"id":"http://arxiv.org/abs/2212.14704v2","updated":"2023-04-03T15:55:40Z","published":"2022-12-28T18:23:47Z","title":"Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and\n  Text-to-Image Diffusion Models","summary":"  Recent CLIP-guided 3D optimization methods, such as DreamFields and\nPureCLIPNeRF, have achieved impressive results in zero-shot text-to-3D\nsynthesis. However, due to scratch training and random initialization without\nprior knowledge, these methods often fail to generate accurate and faithful 3D\nstructures that conform to the input text. In this paper, we make the first\nattempt to introduce explicit 3D shape priors into the CLIP-guided 3D\noptimization process. Specifically, we first generate a high-quality 3D shape\nfrom the input text in the text-to-shape stage as a 3D shape prior. We then use\nit as the initialization of a neural radiance field and optimize it with the\nfull prompt. To address the challenging text-to-shape generation task, we\npresent a simple yet effective approach that directly bridges the text and\nimage modalities with a powerful text-to-image diffusion model. To narrow the\nstyle domain gap between the images synthesized by the text-to-image diffusion\nmodel and shape renderings used to train the image-to-shape generator, we\nfurther propose to jointly optimize a learnable text prompt and fine-tune the\ntext-to-image diffusion model for rendering-style image generation. Our method,\nDream3D, is capable of generating imaginative 3D content with superior visual\nquality and shape accuracy compared to state-of-the-art methods.\n","authors":["Jiale Xu","Xintao Wang","Weihao Cheng","Yan-Pei Cao","Ying Shan","Xiaohu Qie","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2212.14704v2.pdf","comment":"Accepted by CVPR 2023. Project page:\n  https://bluestyle97.github.io/dream3d/"},{"id":"http://arxiv.org/abs/2304.01091v1","updated":"2023-04-03T15:51:42Z","published":"2023-04-03T15:51:42Z","title":"Changes to Captions: An Attentive Network for Remote Sensing Change\n  Captioning","summary":"  In recent years, advanced research has focused on the direct learning and\nanalysis of remote sensing images using natural language processing (NLP)\ntechniques. The ability to accurately describe changes occurring in\nmulti-temporal remote sensing images is becoming increasingly important for\ngeospatial understanding and land planning. Unlike natural image change\ncaptioning tasks, remote sensing change captioning aims to capture the most\nsignificant changes, irrespective of various influential factors such as\nillumination, seasonal effects, and complex land covers. In this study, we\nhighlight the significance of accurately describing changes in remote sensing\nimages and present a comparison of the change captioning task for natural and\nsynthetic images and remote sensing images. To address the challenge of\ngenerating accurate captions, we propose an attentive changes-to-captions\nnetwork, called Chg2Cap for short, for bi-temporal remote sensing images. The\nnetwork comprises three main components: 1) a Siamese CNN-based feature\nextractor to collect high-level representations for each image pair; 2) an\nattentive decoder that includes a hierarchical self-attention block to locate\nchange-related features and a residual block to generate the image embedding;\nand 3) a transformer-based caption generator to decode the relationship between\nthe image embedding and the word embedding into a description. The proposed\nChg2Cap network is evaluated on two representative remote sensing datasets, and\na comprehensive experimental analysis is provided. The code and pre-trained\nmodels will be available online at https://github.com/ShizhenChang/Chg2Cap.\n","authors":["Shizhen Chang","Pedram Ghamisi"],"pdf_url":"https://arxiv.org/pdf/2304.01091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01090v1","updated":"2023-04-03T15:48:24Z","published":"2023-04-03T15:48:24Z","title":"LIGHT: Joint Individual Building Extraction and Height Estimation from\n  Satellite Images through a Unified Multitask Learning Network","summary":"  Building extraction and height estimation are two important basic tasks in\nremote sensing image interpretation, which are widely used in urban planning,\nreal-world 3D construction, and other fields. Most of the existing research\nregards the two tasks as independent studies. Therefore the height information\ncannot be fully used to improve the accuracy of building extraction and vice\nversa. In this work, we combine the individuaL buIlding extraction and heiGHt\nestimation through a unified multiTask learning network (LIGHT) for the first\ntime, which simultaneously outputs a height map, bounding boxes, and a\nsegmentation mask map of buildings. Specifically, LIGHT consists of an instance\nsegmentation branch and a height estimation branch. In particular, so as to\neffectively unify multi-scale feature branches and alleviate feature spans\nbetween branches, we propose a Gated Cross Task Interaction (GCTI) module that\ncan efficiently perform feature interaction between branches. Experiments on\nthe DFC2023 dataset show that our LIGHT can achieve superior performance, and\nour GCTI module with ResNet101 as the backbone can significantly improve the\nperformance of multitask learning by 2.8% AP50 and 6.5% delta1, respectively.\n","authors":["Yongqiang Mao","Xian Sun","Xingliang Huang","Kaiqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2304.01090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01085v1","updated":"2023-04-03T15:42:27Z","published":"2023-04-03T15:42:27Z","title":"Unsupervised Cross-domain Pulmonary Nodule Detection without Source Data","summary":"  Cross domain pulmonary nodule detection suffers from performance degradation\ndue to large shift of data distributions between the source and target domain.\nBesides, considering the high cost of medical data annotation, it is often\nassumed that the target images are unlabeled. Existing approaches have made\nmuch progress for this unsupervised domain adaptation setting. However, this\nsetting is still rarely plausible in the medical application since the source\nmedical data are often not accessible due to the privacy concerns. This\nmotivates us to propose a Source-free Unsupervised cross-domain method for\nPulmonary nodule detection (SUP). It first adapts the source model to the\ntarget domain by utilizing instance-level contrastive learning. Then the\nadapted model is trained in a teacher-student interaction manner, and a\nweighted entropy loss is incorporated to further improve the accuracy.\nExtensive experiments by adapting a pre-trained source model to three popular\npulmonary nodule datasets demonstrate the effectiveness of our method.\n","authors":["Rui Xu","Yong Luo","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2304.01085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01083v1","updated":"2023-04-03T15:39:35Z","published":"2023-04-03T15:39:35Z","title":"Can the Inference Logic of Large Language Models be Disentangled into\n  Symbolic Concepts?","summary":"  In this paper, we explain the inference logic of large language models (LLMs)\nas a set of symbolic concepts. Many recent studies have discovered that\ntraditional DNNs usually encode sparse symbolic concepts. However, because an\nLLM has much more parameters than traditional DNNs, whether the LLM also\nencodes sparse symbolic concepts is still an open problem. Therefore, in this\npaper, we propose to disentangle the inference score of LLMs for dialogue tasks\ninto a small number of symbolic concepts. We verify that we can use those\nsparse concepts to well estimate all inference scores of the LLM on all\narbitrarily masking states of the input sentence. We also evaluate the\ntransferability of concepts encoded by an LLM and verify that symbolic concepts\nusually exhibit high transferability across similar input sentences. More\ncrucially, those symbolic concepts can be used to explain the exact reasons\naccountable for the LLM's prediction errors.\n","authors":["Wen Shen","Lei Cheng","Yuxiao Yang","Mingjie Li","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16247v2","updated":"2023-04-03T15:29:57Z","published":"2023-03-28T18:51:22Z","title":"Data Efficient Contrastive Learning in Histopathology using Active\n  Sampling","summary":"  Deep Learning based diagnostics systems can provide accurate and robust\nquantitative analysis in digital pathology. These algorithms require large\namounts of annotated training data which is impractical in pathology due to the\nhigh resolution of histopathological images. Hence, self-supervised methods\nhave been proposed to learn features using ad-hoc pretext tasks. The\nself-supervised training process is time consuming and often leads to subpar\nfeature representation due to a lack of constrain on the learnt feature space,\nparticularly prominent under data imbalance. In this work, we propose to\nactively sample the training set using a handful of labels and a small proxy\nnetwork, decreasing sample requirement by 93% and training time by 99%.\n","authors":["Tahsin Reasat","David S. Smith"],"pdf_url":"https://arxiv.org/pdf/2303.16247v2.pdf","comment":"fixed typos"},{"id":"http://arxiv.org/abs/2304.01064v1","updated":"2023-04-03T15:21:56Z","published":"2023-04-03T15:21:56Z","title":"HyperThumbnail: Real-time 6K Image Rescaling with Rate-distortion\n  Optimization","summary":"  Contemporary image rescaling aims at embedding a high-resolution (HR) image\ninto a low-resolution (LR) thumbnail image that contains embedded information\nfor HR image reconstruction. Unlike traditional image super-resolution, this\nenables high-fidelity HR image restoration faithful to the original one, given\nthe embedded information in the LR thumbnail. However, state-of-the-art image\nrescaling methods do not optimize the LR image file size for efficient sharing\nand fall short of real-time performance for ultra-high-resolution (e.g., 6K)\nimage reconstruction. To address these two challenges, we propose a novel\nframework (HyperThumbnail) for real-time 6K rate-distortion-aware image\nrescaling. Our framework first embeds an HR image into a JPEG LR thumbnail by\nan encoder with our proposed quantization prediction module, which minimizes\nthe file size of the embedding LR JPEG thumbnail while maximizing HR\nreconstruction quality. Then, an efficient frequency-aware decoder reconstructs\na high-fidelity HR image from the LR one in real time. Extensive experiments\ndemonstrate that our framework outperforms previous image rescaling baselines\nin rate-distortion performance and can perform 6K image reconstruction in real\ntime.\n","authors":["Chenyang Qi","Xin Yang","Ka Leong Cheng","Ying-Cong Chen","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2304.01064v1.pdf","comment":"Accepted by CVPR 2023; Github Repository:\n  https://github.com/AbnerVictor/HyperThumbnail"},{"id":"http://arxiv.org/abs/2211.09445v2","updated":"2023-04-03T15:06:19Z","published":"2022-11-17T10:19:59Z","title":"aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving\n  with Long-Range Perception","summary":"  Autonomous driving is a popular research area within the computer vision\nresearch community. Since autonomous vehicles are highly safety-critical,\nensuring robustness is essential for real-world deployment. While several\npublic multimodal datasets are accessible, they mainly comprise two sensor\nmodalities (camera, LiDAR) which are not well suited for adverse weather. In\naddition, they lack far-range annotations, making it harder to train neural\nnetworks that are the base of a highway assistant function of an autonomous\nvehicle. Therefore, we introduce a multimodal dataset for robust autonomous\ndriving with long-range perception. The dataset consists of 176 scenes with\nsynchronized and calibrated LiDAR, camera, and radar sensors covering a\n360-degree field of view. The collected data was captured in highway, urban,\nand suburban areas during daytime, night, and rain and is annotated with 3D\nbounding boxes with consistent identifiers across frames. Furthermore, we\ntrained unimodal and multimodal baseline models for 3D object detection. Data\nare available at \\url{https://github.com/aimotive/aimotive_dataset}.\n","authors":["Tamás Matuszka","Iván Barton","Ádám Butykai","Péter Hajas","Dávid Kiss","Domonkos Kovács","Sándor Kunsági-Máté","Péter Lengyel","Gábor Németh","Levente Pető","Dezső Ribli","Dávid Szeghy","Szabolcs Vajna","Bálint Varga"],"pdf_url":"https://arxiv.org/pdf/2211.09445v2.pdf","comment":"The paper was accepted to ICLR 2023 Workshop Scene Representations\n  for Autonomous Driving"},{"id":"http://arxiv.org/abs/2304.01054v1","updated":"2023-04-03T15:00:36Z","published":"2023-04-03T15:00:36Z","title":"VoxelFormer: Bird's-Eye-View Feature Generation based on Dual-view\n  Attention for Multi-view 3D Object Detection","summary":"  In recent years, transformer-based detectors have demonstrated remarkable\nperformance in 2D visual perception tasks. However, their performance in\nmulti-view 3D object detection remains inferior to the state-of-the-art (SOTA)\nof convolutional neural network based detectors. In this work, we investigate\nthis issue from the perspective of bird's-eye-view (BEV) feature generation.\nSpecifically, we examine the BEV feature generation method employed by the\ntransformer-based SOTA, BEVFormer, and identify its two limitations: (i) it\nonly generates attention weights from BEV, which precludes the use of lidar\npoints for supervision, and (ii) it aggregates camera view features to the BEV\nthrough deformable sampling, which only selects a small subset of features and\nfails to exploit all information. To overcome these limitations, we propose a\nnovel BEV feature generation method, dual-view attention, which generates\nattention weights from both the BEV and camera view. This method encodes all\ncamera features into the BEV feature. By combining dual-view attention with the\nBEVFormer architecture, we build a new detector named VoxelFormer. Extensive\nexperiments are conducted on the nuScenes benchmark to verify the superiority\nof dual-view attention and VoxelForer. We observe that even only adopting 3\nencoders and 1 historical frame during training, VoxelFormer still outperforms\nBEVFormer significantly. When trained in the same setting, VoxelFormer can\nsurpass BEVFormer by 4.9% NDS point. Code is available at:\nhttps://github.com/Lizhuoling/VoxelFormer-public.git.\n","authors":["Zhuoling Li","Chuanrui Zhang","Wei-Chiu Ma","Yipin Zhou","Linyan Huang","Haoqian Wang","SerNam Lim","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.01054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01053v1","updated":"2023-04-03T15:00:06Z","published":"2023-04-03T15:00:06Z","title":"ViT-DAE: Transformer-driven Diffusion Autoencoder for Histopathology\n  Image Analysis","summary":"  Generative AI has received substantial attention in recent years due to its\nability to synthesize data that closely resembles the original data source.\nWhile Generative Adversarial Networks (GANs) have provided innovative\napproaches for histopathological image analysis, they suffer from limitations\nsuch as mode collapse and overfitting in discriminator. Recently, Denoising\nDiffusion models have demonstrated promising results in computer vision. These\nmodels exhibit superior stability during training, better distribution\ncoverage, and produce high-quality diverse images. Additionally, they display a\nhigh degree of resilience to noise and perturbations, making them well-suited\nfor use in digital pathology, where images commonly contain artifacts and\nexhibit significant variations in staining. In this paper, we present a novel\napproach, namely ViT-DAE, which integrates vision transformers (ViT) and\ndiffusion autoencoders for high-quality histopathology image synthesis. This\nmarks the first time that ViT has been introduced to diffusion autoencoders in\ncomputational pathology, allowing the model to better capture the complex and\nintricate details of histopathology images. We demonstrate the effectiveness of\nViT-DAE on three publicly available datasets. Our approach outperforms recent\nGAN-based and vanilla DAE methods in generating realistic images.\n","authors":["Xuan Xu","Saarthak Kapse","Rajarsi Gupta","Prateek Prasanna"],"pdf_url":"https://arxiv.org/pdf/2304.01053v1.pdf","comment":"Submitted to MICCAI 2023"},{"id":"http://arxiv.org/abs/2302.03802v2","updated":"2023-04-03T14:49:49Z","published":"2023-02-07T23:46:34Z","title":"Standing Between Past and Future: Spatio-Temporal Modeling for\n  Multi-Camera 3D Multi-Object Tracking","summary":"  This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT)\nframework. It emphasizes spatio-temporal continuity and integrates both past\nand future reasoning for tracked objects. Thus, we name it \"Past-and-Future\nreasoning for Tracking\" (PF-Track). Specifically, our method adapts the\n\"tracking by attention\" framework and represents tracked instances coherently\nover time with object queries. To explicitly use historical cues, our \"Past\nReasoning\" module learns to refine the tracks and enhance the object features\nby cross-attending to queries from previous frames and other objects. The\n\"Future Reasoning\" module digests historical information and predicts robust\nfuture trajectories. In the case of long-term occlusions, our method maintains\nthe object positions and enables re-association by integrating motion\npredictions. On the nuScenes dataset, our method improves AMOTA by a large\nmargin and remarkably reduces ID-Switches by 90% compared to prior approaches,\nwhich is an order of magnitude less. The code and models are made available at\nhttps://github.com/TRI-ML/PF-Track.\n","authors":["Ziqi Pang","Jie Li","Pavel Tokmakov","Dian Chen","Sergey Zagoruyko","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2302.03802v2.pdf","comment":"CVPR 2023 Camera Ready, 15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.09642v2","updated":"2023-04-03T14:47:11Z","published":"2023-03-16T20:42:24Z","title":"SUD$^2$: Supervision by Denoising Diffusion Models for Image\n  Reconstruction","summary":"  Many imaging inverse problems$\\unicode{x2014}$such as image-dependent\nin-painting and dehazing$\\unicode{x2014}$are challenging because their forward\nmodels are unknown or depend on unknown latent parameters. While one can solve\nsuch problems by training a neural network with vast quantities of paired\ntraining data, such paired training data is often unavailable. In this paper,\nwe propose a generalized framework for training image reconstruction networks\nwhen paired training data is scarce. In particular, we demonstrate the ability\nof image denoising algorithms and, by extension, denoising diffusion models to\nsupervise network training in the absence of paired training data.\n","authors":["Matthew A. Chan","Sean I. Young","Christopher A. Metzler"],"pdf_url":"https://arxiv.org/pdf/2303.09642v2.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2304.01042v1","updated":"2023-04-03T14:45:43Z","published":"2023-04-03T14:45:43Z","title":"DivClust: Controlling Diversity in Deep Clustering","summary":"  Clustering has been a major research topic in the field of machine learning,\none to which Deep Learning has recently been applied with significant success.\nHowever, an aspect of clustering that is not addressed by existing deep\nclustering methods, is that of efficiently producing multiple, diverse\npartitionings for a given dataset. This is particularly important, as a diverse\nset of base clusterings are necessary for consensus clustering, which has been\nfound to produce better and more robust results than relying on a single\nclustering. To address this gap, we propose DivClust, a diversity controlling\nloss that can be incorporated into existing deep clustering frameworks to\nproduce multiple clusterings with the desired degree of diversity. We conduct\nexperiments with multiple datasets and deep clustering frameworks and show\nthat: a) our method effectively controls diversity across frameworks and\ndatasets with very small additional computational cost, b) the sets of\nclusterings learned by DivClust include solutions that significantly outperform\nsingle-clustering baselines, and c) using an off-the-shelf consensus clustering\nalgorithm, DivClust produces consensus clustering solutions that consistently\noutperform single-clustering baselines, effectively improving the performance\nof the base deep clustering framework.\n","authors":["Ioannis Maniadis Metaxas","Georgios Tzimiropoulos","Ioannis Patras"],"pdf_url":"https://arxiv.org/pdf/2304.01042v1.pdf","comment":"Accepted for publication in CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01029v1","updated":"2023-04-03T14:28:29Z","published":"2023-04-03T14:28:29Z","title":"Domain Generalization for Crop Segmentation with Knowledge Distillation","summary":"  In recent years, precision agriculture has gradually oriented farming closer\nto automation processes to support all the activities related to field\nmanagement. Service robotics plays a predominant role in this evolution by\ndeploying autonomous agents that can navigate fields while performing tasks\nwithout human intervention, such as monitoring, spraying, and harvesting. To\nexecute these precise actions, mobile robots need a real-time perception system\nthat understands their surroundings and identifies their targets in the wild.\nGeneralizing to new crops and environmental conditions is critical for\npractical applications, as labeled samples are rarely available. In this paper,\nwe investigate the problem of crop segmentation and propose a novel approach to\nenhance domain generalization using knowledge distillation. In the proposed\nframework, we transfer knowledge from an ensemble of models individually\ntrained on source domains to a student model that can adapt to unseen target\ndomains. To evaluate the proposed method, we present a synthetic multi-domain\ndataset for crop segmentation containing plants of variegate shapes and\ncovering different terrain styles, weather conditions, and light scenarios for\nmore than 50,000 samples. We demonstrate significant improvements in\nperformance over state-of-the-art methods. Our approach provides a promising\nsolution for domain generalization in crop segmentation and has the potential\nto enhance precision agriculture applications.\n","authors":["Simone Angarano","Mauro Martini","Alessandro Navone","Marcello Chiaberge"],"pdf_url":"https://arxiv.org/pdf/2304.01029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11867v2","updated":"2023-04-03T14:22:52Z","published":"2023-02-23T09:12:58Z","title":"Transformers in Single Object Tracking: An Experimental Survey","summary":"  Single object tracking is a well-known and challenging research topic in\ncomputer vision. Over the last two decades, numerous researchers have proposed\nvarious algorithms to solve this problem and achieved promising results.\nRecently, Transformer-based tracking approaches have ushered in a new era in\nsingle object tracking due to their superior tracking robustness. Although\nseveral survey studies have been conducted to analyze the performance of\ntrackers, there is a need for another survey study after the introduction of\nTransformers in single object tracking. In this survey, we aim to analyze the\nliterature and performances of Transformer tracking approaches. Therefore, we\nconduct an in-depth literature analysis of Transformer tracking approaches and\nevaluate their tracking robustness and computational efficiency on challenging\nbenchmark datasets. In addition, we have measured their performances on\ndifferent tracking scenarios to find their strength and weaknesses. Our survey\nprovides insights into the underlying principles of Transformer tracking\napproaches, the challenges they face, and their future directions.\n","authors":["Janani Thangavel","Thanikasalam Kokul","Amirthalingam Ramanan","Subha Fernando"],"pdf_url":"https://arxiv.org/pdf/2302.11867v2.pdf","comment":"32 pages, 19 figures, review paper, submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2210.15943v2","updated":"2023-04-03T14:16:14Z","published":"2022-10-28T07:07:13Z","title":"Grafting Vision Transformers","summary":"  Vision Transformers (ViTs) have recently become the state-of-the-art across\nmany computer vision tasks. In contrast to convolutional networks (CNNs), ViTs\nenable global information sharing even within shallow layers of a network,\ni.e., among high-resolution features. However, this perk was later overlooked\nwith the success of pyramid architectures such as Swin Transformer, which show\nbetter performance-complexity trade-offs. In this paper, we present a simple\nand efficient add-on component (termed GrafT) that considers global\ndependencies and multi-scale information throughout the network, in both high-\nand low-resolution features alike. It has the flexibility of branching out at\narbitrary depths and shares most of the parameters and computations of the\nbackbone. GrafT shows consistent gains over various well-known models which\nincludes both hybrid and pure Transformer types, both homogeneous and pyramid\nstructures, and various self-attention methods. In particular, it largely\nbenefits mobile-size models by providing high-level semantics. On the\nImageNet-1k dataset, GrafT delivers +3.9%, +1.4%, and +1.9% top-1 accuracy\nimprovement to DeiT-T, Swin-T, and MobileViT-XXS, respectively. Our code and\nmodels will be made available.\n","authors":["Jongwoo Park","Kumara Kahatapitiya","Donghyun Kim","Shivchander Sudalairaj","Quanfu Fan","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2210.15943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11431v3","updated":"2023-04-03T14:13:21Z","published":"2023-01-26T21:31:32Z","title":"Semidefinite Relaxations for Robust Multiview Triangulation","summary":"  We propose an approach based on convex relaxations for certifiably optimal\nrobust multiview triangulation. To this end, we extend existing relaxation\napproaches to non-robust multiview triangulation by incorporating a least\nsquares cost function. We propose two formulations, one based on epipolar\nconstraints and one based on fractional reprojection constraints. The first is\nlower dimensional and remains tight under moderate noise and outlier levels,\nwhile the second is higher dimensional and therefore slower but remains tight\neven under extreme noise and outlier levels. We demonstrate through extensive\nexperiments that the proposed approaches allow us to compute provably optimal\nreconstructions even under significant noise and a large percentage of\noutliers.\n","authors":["Linus Härenstam-Nielsen","Niclas Zeller","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2301.11431v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.12761v3","updated":"2023-04-03T13:57:31Z","published":"2021-12-23T18:30:31Z","title":"BANMo: Building Animatable 3D Neural Models from Many Casual Videos","summary":"  Prior work for articulated 3D shape reconstruction often relies on\nspecialized sensors (e.g., synchronized multi-camera systems), or pre-built 3D\ndeformable models (e.g., SMAL or SMPL). Such methods are not able to scale to\ndiverse sets of objects in the wild. We present BANMo, a method that requires\nneither a specialized sensor nor a pre-defined template shape. BANMo builds\nhigh-fidelity, articulated 3D models (including shape and animatable skinning\nweights) from many monocular casual videos in a differentiable rendering\nframework. While the use of many videos provides more coverage of camera views\nand object articulations, they introduce significant challenges in establishing\ncorrespondence across scenes with different backgrounds, illumination\nconditions, etc. Our key insight is to merge three schools of thought; (1)\nclassic deformable shape models that make use of articulated bones and blend\nskinning, (2) volumetric neural radiance fields (NeRFs) that are amenable to\ngradient-based optimization, and (3) canonical embeddings that generate\ncorrespondences between pixels and an articulated model. We introduce neural\nblend skinning models that allow for differentiable and invertible articulated\ndeformations. When combined with canonical embeddings, such models allow us to\nestablish dense correspondences across videos that can be self-supervised with\ncycle consistency. On real and synthetic datasets, BANMo shows higher-fidelity\n3D reconstructions than prior works for humans and animals, with the ability to\nrender realistic images from novel viewpoints and poses. Project webpage:\nbanmo-www.github.io .\n","authors":["Gengshan Yang","Minh Vo","Natalia Neverova","Deva Ramanan","Andrea Vedaldi","Hanbyul Joo"],"pdf_url":"https://arxiv.org/pdf/2112.12761v3.pdf","comment":"CVPR 2022 camera-ready version (last update: May 2022)"},{"id":"http://arxiv.org/abs/2304.00990v1","updated":"2023-04-03T13:56:01Z","published":"2023-04-03T13:56:01Z","title":"Efficient human-in-loop deep learning model training with iterative\n  refinement and statistical result validation","summary":"  Annotation and labeling of images are some of the biggest challenges in\napplying deep learning to medical data. Current processes are time and\ncost-intensive and, therefore, a limiting factor for the wide adoption of the\ntechnology. Additionally validating that measured performance improvements are\nsignificant is important to select the best model. In this paper, we\ndemonstrate a method for creating segmentations, a necessary part of a data\ncleaning for ultrasound imaging machine learning pipelines. We propose a\nfour-step method to leverage automatically generated training data and fast\nhuman visual checks to improve model accuracy while keeping the time/effort and\ncost low. We also showcase running experiments multiple times to allow the\nusage of statistical analysis. Poor quality automated ground truth data and\nquick visual inspections efficiently train an initial base model, which is\nrefined using a small set of more expensive human-generated ground truth data.\nThe method is demonstrated on a cardiac ultrasound segmentation task, removing\nbackground data, including static PHI. Significance is shown by running the\nexperiments multiple times and using the student's t-test on the performance\ndistributions. The initial segmentation accuracy of a simple thresholding\nalgorithm of 92% was improved to 98%. The performance of models trained on\ncomplicated algorithms can be matched or beaten by pre-training with the poorer\nperforming algorithms and a small quantity of high-quality data. The\nintroduction of statistic significance analysis for deep learning models helps\nto validate the performance improvements measured. The method offers a\ncost-effective and fast approach to achieving high-accuracy models while\nminimizing the cost and effort of acquiring high-quality training data.\n","authors":["Manuel Zahn","Douglas P. Perrin"],"pdf_url":"https://arxiv.org/pdf/2304.00990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18171v2","updated":"2023-04-03T13:53:33Z","published":"2023-03-29T18:52:10Z","title":"How Efficient Are Today's Continual Learning Algorithms?","summary":"  Supervised Continual learning involves updating a deep neural network (DNN)\nfrom an ever-growing stream of labeled data. While most work has focused on\novercoming catastrophic forgetting, one of the major motivations behind\ncontinual learning is being able to efficiently update a network with new\ninformation, rather than retraining from scratch on the training dataset as it\ngrows over time. Despite recent continual learning methods largely solving the\ncatastrophic forgetting problem, there has been little attention paid to the\nefficiency of these algorithms. Here, we study recent methods for incremental\nclass learning and illustrate that many are highly inefficient in terms of\ncompute, memory, and storage. Some methods even require more compute than\ntraining from scratch! We argue that for continual learning to have real-world\napplicability, the research community cannot ignore the resources used by these\nalgorithms. There is more to continual learning than mitigating catastrophic\nforgetting.\n","authors":["Md Yousuf Harun","Jhair Gallardo","Tyler L. Hayes","Christopher Kanan"],"pdf_url":"https://arxiv.org/pdf/2303.18171v2.pdf","comment":"To appear in the IEEE Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision\n  (CLVision) 2023"},{"id":"http://arxiv.org/abs/2304.00979v1","updated":"2023-04-03T13:47:38Z","published":"2023-04-03T13:47:38Z","title":"A Latent Fingerprint in the Wild Database","summary":"  Latent fingerprints are among the most important and widely used evidence in\ncrime scenes, digital forensics and law enforcement worldwide. Despite the\nnumber of advancements reported in recent works, we note that significant open\nissues such as independent benchmarking and lack of large-scale evaluation\ndatabases for improving the algorithms are inadequately addressed. The\navailable databases are mostly of semi-public nature, lack of acquisition in\nthe wild environment, and post-processing pipelines. Moreover, they do not\nrepresent a realistic capture scenario similar to real crime scenes, to\nbenchmark the robustness of the algorithms. Further, existing databases for\nlatent fingerprint recognition do not have a large number of unique\nsubjects/fingerprint instances or do not provide ground truth/reference\nfingerprint images to conduct a cross-comparison against the latent. In this\npaper, we introduce a new wild large-scale latent fingerprint database that\nincludes five different acquisition scenarios: reference fingerprints from (1)\noptical and (2) capacitive sensors, (3) smartphone fingerprints, latent\nfingerprints captured from (4) wall surface, (5) Ipad surface, and (6)\naluminium foil surface. The new database consists of 1,318 unique fingerprint\ninstances captured in all above mentioned settings. A total of 2,636 reference\nfingerprints from optical and capacitive sensors, 1,318 fingerphotos from\nsmartphones, and 9,224 latent fingerprints from each of the 132 subjects were\nprovided in this work. The dataset is constructed considering various age\ngroups, equal representations of genders and backgrounds. In addition, we\nprovide an extensive set of analysis of various subset evaluations to highlight\nopen challenges for future directions in latent fingerprint recognition\nresearch.\n","authors":["Xinwei Liu","Kiran Raja","Renfang Wang","Hong Qiu","Hucheng Wu","Dechao Sun","Qiguang Zheng","Nian Liu","Xiaoxia Wang","Gehang Huang","Raghavendra Ramachandra","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2304.00979v1.pdf","comment":"Submitted to IEEE Transactions on Information Forensics and Security\n  (under review)"},{"id":"http://arxiv.org/abs/2304.00971v1","updated":"2023-04-03T13:41:35Z","published":"2023-04-03T13:41:35Z","title":"Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection,\n  Segmentation, and Depth Estimation","summary":"  This report serves as a supplementary document for TaskPrompter, detailing\nits implementation on a new joint 2D-3D multi-task learning benchmark based on\nCityscapes-3D. TaskPrompter presents an innovative multi-task prompting\nframework that unifies the learning of (i) task-generic representations, (ii)\ntask-specific representations, and (iii) cross-task interactions, as opposed to\nprevious approaches that separate these learning objectives into different\nnetwork modules. This unified approach not only reduces the need for meticulous\nempirical structure design but also significantly enhances the multi-task\nnetwork's representation learning capability, as the entire model capacity is\ndevoted to optimizing the three objectives simultaneously. TaskPrompter\nintroduces a new multi-task benchmark based on Cityscapes-3D dataset, which\nrequires the multi-task model to concurrently generate predictions for\nmonocular 3D vehicle detection, semantic segmentation, and monocular depth\nestimation. These tasks are essential for achieving a joint 2D-3D understanding\nof visual scenes, particularly in the development of autonomous driving\nsystems. On this challenging benchmark, our multi-task model demonstrates\nstrong performance compared to single-task state-of-the-art methods and\nestablishes new state-of-the-art results on the challenging 3D detection and\ndepth estimation tasks.\n","authors":["Hanrong Ye"],"pdf_url":"https://arxiv.org/pdf/2304.00971v1.pdf","comment":"A supplementary document for TaskPromtper accepted by ICLR 2023.\n  Project page:\n  https://github.com/prismformore/Multi-Task-Transformer/tree/main/TaskPrompter"},{"id":"http://arxiv.org/abs/2304.00967v1","updated":"2023-04-03T13:35:29Z","published":"2023-04-03T13:35:29Z","title":"Temporal Enhanced Training of Multi-view 3D Object Detector via\n  Historical Object Prediction","summary":"  In this paper, we propose a new paradigm, named Historical Object Prediction\n(HoP) for multi-view 3D detection to leverage temporal information more\neffectively. The HoP approach is straightforward: given the current timestamp\nt, we generate a pseudo Bird's-Eye View (BEV) feature of timestamp t-k from its\nadjacent frames and utilize this feature to predict the object set at timestamp\nt-k. Our approach is motivated by the observation that enforcing the detector\nto capture both the spatial location and temporal motion of objects occurring\nat historical timestamps can lead to more accurate BEV feature learning. First,\nwe elaborately design short-term and long-term temporal decoders, which can\ngenerate the pseudo BEV feature for timestamp t-k without the involvement of\nits corresponding camera images. Second, an additional object decoder is\nflexibly attached to predict the object targets using the generated pseudo BEV\nfeature. Note that we only perform HoP during training, thus the proposed\nmethod does not introduce extra overheads during inference. As a plug-and-play\napproach, HoP can be easily incorporated into state-of-the-art BEV detection\nframeworks, including BEVFormer and BEVDet series. Furthermore, the auxiliary\nHoP approach is complementary to prevalent temporal modeling methods, leading\nto significant performance gains. Extensive experiments are conducted to\nevaluate the effectiveness of the proposed HoP on the nuScenes dataset. We\nchoose the representative methods, including BEVFormer and BEVDet4D-Depth to\nevaluate our method. Surprisingly, HoP achieves 68.5% NDS and 62.4% mAP with\nViT-L on nuScenes test, outperforming all the 3D object detectors on the\nleaderboard. Codes will be available at https://github.com/Sense-X/HoP.\n","authors":["Zhuofan Zong","Dongzhi Jiang","Guanglu Song","Zeyue Xue","Jingyong Su","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2304.00967v1.pdf","comment":"Tech report. Codes will be available at\n  https://github.com/Sense-X/HoP"},{"id":"http://arxiv.org/abs/2304.00964v1","updated":"2023-04-03T13:30:48Z","published":"2023-04-03T13:30:48Z","title":"Robust Text-driven Image Editing Method that Adaptively Explores\n  Directions in Latent Spaces of StyleGAN and CLIP","summary":"  Automatic image editing has great demands because of its numerous\napplications, and the use of natural language instructions is essential to\nachieving flexible and intuitive editing as the user imagines. A pioneering\nwork in text-driven image editing, StyleCLIP, finds an edit direction in the\nCLIP space and then edits the image by mapping the direction to the StyleGAN\nspace. At the same time, it is difficult to tune appropriate inputs other than\nthe original image and text instructions for image editing. In this study, we\npropose a method to construct the edit direction adaptively in the StyleGAN and\nCLIP spaces with SVM. Our model represents the edit direction as a normal\nvector in the CLIP space obtained by training a SVM to classify positive and\nnegative images. The images are retrieved from a large-scale image corpus,\noriginally used for pre-training StyleGAN, according to the CLIP similarity\nbetween the images and the text instruction. We confirmed that our model\nperformed as well as the StyleCLIP baseline, whereas it allows simple inputs\nwithout increasing the computational time.\n","authors":["Tsuyoshi Baba","Kosuke Nishida","Kyosuke Nishida"],"pdf_url":"https://arxiv.org/pdf/2304.00964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00962v1","updated":"2023-04-03T13:30:04Z","published":"2023-04-03T13:30:04Z","title":"RegionPLC: Regional Point-Language Contrastive Learning for Open-World\n  3D Scene Understanding","summary":"  Existing 3D scene understanding tasks have achieved high performance on\nclose-set benchmarks but fail to handle novel categories in real-world\napplications. To this end, we propose a Regional Point-Language Contrastive\nlearning framework, namely RegionPLC, for open-world 3D scene understanding,\nwhich equips models trained on closed-set datasets with open-vocabulary\nrecognition capabilities. We propose dense visual prompts to elicit\nregion-level visual-language knowledge from 2D foundation models via\ncaptioning, which further allows us to build dense regional point-language\nassociations. Then, we design a point-discriminative contrastive learning\nobjective to enable point-independent learning from captions for dense scene\nunderstanding. We conduct extensive experiments on ScanNet, ScanNet200, and\nnuScenes datasets. Our RegionPLC significantly outperforms previous\nbase-annotated 3D open-world scene understanding approaches by an average of\n11.6\\% and 6.6\\% for semantic and instance segmentation, respectively. It also\nshows promising open-world results in absence of any human annotation with low\ntraining and inference costs. Code will be released.\n","authors":["Jihan Yang","Runyu Ding","Zhe Wang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2304.00962v1.pdf","comment":"project page: https://jihanyang.github.io/projects/RegionPLC"},{"id":"http://arxiv.org/abs/2304.00961v1","updated":"2023-04-03T13:26:52Z","published":"2023-04-03T13:26:52Z","title":"Self-Ordering Point Clouds","summary":"  In this paper we address the task of finding representative subsets of points\nin a 3D point cloud by means of a point-wise ordering. Only a few works have\ntried to address this challenging vision problem, all with the help of hard to\nobtain point and cloud labels. Different from these works, we introduce the\ntask of point-wise ordering in 3D point clouds through self-supervision, which\nwe call self-ordering. We further contribute the first end-to-end trainable\nnetwork that learns a point-wise ordering in a self-supervised fashion. It\nutilizes a novel differentiable point scoring-sorting strategy and it\nconstructs an hierarchical contrastive scheme to obtain self-supervision\nsignals. We extensively ablate the method and show its scalability and superior\nperformance even compared to supervised ordering methods on multiple datasets\nand tasks including zero-shot ordering of point clouds from unseen categories.\n","authors":["Pengwan Yang","Yuki M. Asano","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2304.00961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00954v1","updated":"2023-04-03T13:16:47Z","published":"2023-04-03T13:16:47Z","title":"AirLoc: Object-based Indoor Relocalization","summary":"  Indoor relocalization is vital for both robotic tasks like autonomous\nexploration and civil applications such as navigation with a cell phone in a\nshopping mall. Some previous approaches adopt geometrical information such as\nkey-point features or local textures to carry out indoor relocalization, but\nthey either easily fail in an environment with visually similar scenes or\nrequire many database images. Inspired by the fact that humans often remember\nplaces by recognizing unique landmarks, we resort to objects, which are more\ninformative than geometry elements. In this work, we propose a simple yet\neffective object-based indoor relocalization approach, dubbed AirLoc. To\novercome the critical challenges of object reidentification and remembering\nobject relationships, we extract object-wise appearance embedding and\ninter-object geometric relationships. The geometry and appearance features are\nintegrated to generate cumulative scene features. This results in a robust,\naccurate, and portable indoor relocalization system, which outperforms the\nstate-of-the-art methods in room-level relocalization by 9.5% of PR-AUC and 7%\nof accuracy. In addition to exhaustive evaluation, we also carry out real-world\ntests, where AirLoc shows robustness in challenges like severe occlusion,\nperceptual aliasing, viewpoint shift, and deformation.\n","authors":[" Aryan","Bowen Li","Sebastian Scherer","Yun-Jou Lin","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2304.00954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00950v1","updated":"2023-04-03T13:15:20Z","published":"2023-04-03T13:15:20Z","title":"Semi-Automated Computer Vision based Tracking of Multiple Industrial\n  Entities -- A Framework and Dataset Creation Approach","summary":"  This contribution presents the TOMIE framework (Tracking Of Multiple\nIndustrial Entities), a framework for the continuous tracking of industrial\nentities (e.g., pallets, crates, barrels) over a network of, in this example,\nsix RGB cameras. This framework, makes use of multiple sensors, data pipelines\nand data annotation procedures, and is described in detail in this\ncontribution. With the vision of a fully automated tracking system for\nindustrial entities in mind, it enables researchers to efficiently capture high\nquality data in an industrial setting. Using this framework, an image dataset,\nthe TOMIE dataset, is created, which at the same time is used to gauge the\nframework's validity. This dataset contains annotation files for 112,860 frames\nand 640,936 entity instances that are captured from a set of six cameras that\nperceive a large indoor space. This dataset out-scales comparable datasets by a\nfactor of four and is made up of scenarios, drawn from industrial applications\nfrom the sector of warehousing. Three tracking algorithms, namely ByteTrack,\nBot-Sort and SiamMOT are applied to this dataset, serving as a proof-of-concept\nand providing tracking results that are comparable to the state of the art.\n","authors":["Jérôme Rutinowski","Hazem Youssef","Sven Franke","Irfan Fachrudin Priyanta","Frederik Polachowski","Moritz Roidl","Christopher Reining"],"pdf_url":"https://arxiv.org/pdf/2304.00950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00948v1","updated":"2023-04-03T13:13:19Z","published":"2023-04-03T13:13:19Z","title":"VTAE: Variational Transformer Autoencoder with Manifolds Learning","summary":"  Deep generative models have demonstrated successful applications in learning\nnon-linear data distributions through a number of latent variables and these\nmodels use a nonlinear function (generator) to map latent samples into the data\nspace. On the other hand, the nonlinearity of the generator implies that the\nlatent space shows an unsatisfactory projection of the data space, which\nresults in poor representation learning. This weak projection, however, can be\naddressed by a Riemannian metric, and we show that geodesics computation and\naccurate interpolations between data samples on the Riemannian manifold can\nsubstantially improve the performance of deep generative models. In this paper,\na Variational spatial-Transformer AutoEncoder (VTAE) is proposed to minimize\ngeodesics on a Riemannian manifold and improve representation learning. In\nparticular, we carefully design the variational autoencoder with an encoded\nspatial-Transformer to explicitly expand the latent variable model to data on a\nRiemannian manifold, and obtain global context modelling. Moreover, to have\nsmooth and plausible interpolations while traversing between two different\nobjects' latent representations, we propose a geodesic interpolation network\ndifferent from the existing models that use linear interpolation with inferior\nperformance. Experiments on benchmarks show that our proposed model can improve\npredictive accuracy and versatility over a range of computer vision tasks,\nincluding image interpolations, and reconstructions.\n","authors":["Pourya Shamsolmoali","Masoumeh Zareapoor","Huiyu Zhou","Dacheng Tao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2304.00948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00947v1","updated":"2023-04-03T13:13:12Z","published":"2023-04-03T13:13:12Z","title":"RePAST: Relative Pose Attention Scene Representation Transformer","summary":"  The Scene Representation Transformer (SRT) is a recent method to render novel\nviews at interactive rates. Since SRT uses camera poses with respect to an\narbitrarily chosen reference camera, it is not invariant to the order of the\ninput views. As a result, SRT is not directly applicable to large-scale scenes\nwhere the reference frame would need to be changed regularly. In this work, we\npropose Relative Pose Attention SRT (RePAST): Instead of fixing a reference\nframe at the input, we inject pairwise relative camera pose information\ndirectly into the attention mechanism of the Transformers. This leads to a\nmodel that is by definition invariant to the choice of any global reference\nframe, while still retaining the full capabilities of the original method.\nEmpirical results show that adding this invariance to the model does not lead\nto a loss in quality. We believe that this is a step towards applying fully\nlatent transformer-based rendering methods to large-scale scenes.\n","authors":["Aleksandr Safin","Daniel Durckworth","Mehdi S. M. Sajjadi"],"pdf_url":"https://arxiv.org/pdf/2304.00947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00946v1","updated":"2023-04-03T13:09:39Z","published":"2023-04-03T13:09:39Z","title":"MoLo: Motion-augmented Long-short Contrastive Learning for Few-shot\n  Action Recognition","summary":"  Current state-of-the-art approaches for few-shot action recognition achieve\npromising performance by conducting frame-level matching on learned visual\nfeatures. However, they generally suffer from two limitations: i) the matching\nprocedure between local frames tends to be inaccurate due to the lack of\nguidance to force long-range temporal perception; ii) explicit motion learning\nis usually ignored, leading to partial information loss. To address these\nissues, we develop a Motion-augmented Long-short Contrastive Learning (MoLo)\nmethod that contains two crucial components, including a long-short contrastive\nobjective and a motion autodecoder. Specifically, the long-short contrastive\nobjective is to endow local frame features with long-form temporal awareness by\nmaximizing their agreement with the global token of videos belonging to the\nsame class. The motion autodecoder is a lightweight architecture to reconstruct\npixel motions from the differential features, which explicitly embeds the\nnetwork with motion dynamics. By this means, MoLo can simultaneously learn\nlong-range temporal context and motion cues for comprehensive few-shot\nmatching. To demonstrate the effectiveness, we evaluate MoLo on five standard\nbenchmarks, and the results show that MoLo favorably outperforms recent\nadvanced methods. The source code is available at\nhttps://github.com/alibaba-mmai-research/MoLo.\n","authors":["Xiang Wang","Shiwei Zhang","Zhiwu Qing","Changxin Gao","Yingya Zhang","Deli Zhao","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2304.00946v1.pdf","comment":"Accepted by CVPR-2023. Code:\n  https://github.com/alibaba-mmai-research/MoLo"},{"id":"http://arxiv.org/abs/2209.09616v7","updated":"2023-04-03T12:53:52Z","published":"2022-09-19T09:16:07Z","title":"Provably Uncertainty-Guided Universal Domain Adaptation","summary":"  Universal domain adaptation (UniDA) aims to transfer the knowledge from a\nlabeled source domain to an unlabeled target domain without any assumptions of\nthe label sets, which requires distinguishing the unknown samples from the\nknown ones in the target domain. A main challenge of UniDA is that the\nnonidentical label sets cause the misalignment between the two domains.\nMoreover, the domain discrepancy and the supervised objectives in the source\ndomain easily lead the whole model to be biased towards the common classes and\nproduce overconfident predictions for unknown samples. To address the above\nchallenging problems, we propose a new uncertainty-guided UniDA framework.\nFirstly, we introduce an empirical estimation of the probability of a target\nsample belonging to the unknown class which fully exploits the distribution of\nthe target samples in the latent space. Then, based on the estimation, we\npropose a novel neighbors searching scheme in a linear subspace with a\n$\\delta$-filter to estimate the uncertainty score of a target sample and\ndiscover unknown samples. It fully utilizes the relationship between a target\nsample and its neighbors in the source domain to avoid the influence of domain\nmisalignment. Secondly, this paper well balances the confidences of predictions\nfor both known and unknown samples through an uncertainty-guided margin loss\nbased on the confidences of discovered unknown samples, which can reduce the\ngap between the intra-class variances of known classes with respect to the\nunknown class. Finally, experiments on three public datasets demonstrate that\nour method significantly outperforms existing state-of-the-art methods.\n","authors":["Yifan Wang","Lin Zhang","Ran Song","Paul L. Rosin","Yibin Li","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2209.09616v7.pdf","comment":"13 pages. arXiv admin note: text overlap with arXiv:2207.09280"},{"id":"http://arxiv.org/abs/2304.00933v1","updated":"2023-04-03T12:45:52Z","published":"2023-04-03T12:45:52Z","title":"Knowledge Accumulation in Continually Learned Representations and the\n  Issue of Feature Forgetting","summary":"  By default, neural networks learn on all training data at once. When such a\nmodel is trained on sequential chunks of new data, it tends to catastrophically\nforget how to handle old data. In this work we investigate how continual\nlearners learn and forget representations. We observe two phenomena: knowledge\naccumulation, i.e. the improvement of a representation over time, and feature\nforgetting, i.e. the loss of task-specific representations. To better\nunderstand both phenomena, we introduce a new analysis technique called task\nexclusion comparison. If a model has seen a task and it has not forgotten all\nthe task-specific features, then its representation for that task should be\nbetter than that of a model that was trained on similar tasks, but not that\nexact one. Our image classification experiments show that most task-specific\nfeatures are quickly forgotten, in contrast to what has been suggested in the\npast. Further, we demonstrate how some continual learning methods, like replay,\nand ideas from representation learning affect a continually learned\nrepresentation. We conclude by observing that representation quality is tightly\ncorrelated with continual learning performance.\n","authors":["Timm Hess","Eli Verwimp","Gido M. van de Ven","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2304.00933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00932v1","updated":"2023-04-03T12:43:34Z","published":"2023-04-03T12:43:34Z","title":"HypLiLoc: Towards Effective LiDAR Pose Regression with Hyperbolic Fusion","summary":"  LiDAR relocalization plays a crucial role in many fields, including robotics,\nautonomous driving, and computer vision. LiDAR-based retrieval from a database\ntypically incurs high computation storage costs and can lead to globally\ninaccurate pose estimations if the database is too sparse. On the other hand,\npose regression methods take images or point clouds as inputs and directly\nregress global poses in an end-to-end manner. They do not perform database\nmatching and are more computationally efficient than retrieval techniques. We\npropose HypLiLoc, a new model for LiDAR pose regression. We use two branched\nbackbones to extract 3D features and 2D projection features, respectively. We\nconsider multi-modal feature fusion in both Euclidean and hyperbolic spaces to\nobtain more effective feature representations. Experimental results indicate\nthat HypLiLoc achieves state-of-the-art performance in both outdoor and indoor\ndatasets. We also conduct extensive ablation studies on the framework design,\nwhich demonstrate the effectiveness of multi-modal feature extraction and\nmulti-space embedding. Our code is released at:\nhttps://github.com/sijieaaa/HypLiLoc\n","authors":["Sijie Wang","Qiyu Kang","Rui She","Wei Wang","Kai Zhao","Yang Song","Wee Peng Tay"],"pdf_url":"https://arxiv.org/pdf/2304.00932v1.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2212.14042v2","updated":"2023-04-03T12:40:57Z","published":"2022-12-20T16:37:20Z","title":"FunkNN: Neural Interpolation for Functional Generation","summary":"  Can we build continuous generative models which generalize across scales, can\nbe evaluated at any coordinate, admit calculation of exact derivatives, and are\nconceptually simple? Existing MLP-based architectures generate worse samples\nthan the grid-based generators with favorable convolutional inductive biases.\nModels that focus on generating images at different scales do better, but\nemploy complex architectures not designed for continuous evaluation of images\nand derivatives. We take a signal-processing perspective and treat continuous\nimage generation as interpolation from samples. Indeed, correctly sampled\ndiscrete images contain all information about the low spatial frequencies. The\nquestion is then how to extrapolate the spectrum in a data-driven way while\nmeeting the above design criteria. Our answer is FunkNN -- a new convolutional\nnetwork which learns how to reconstruct continuous images at arbitrary\ncoordinates and can be applied to any image dataset. Combined with a discrete\ngenerative model it becomes a functional generator which can act as a prior in\ncontinuous ill-posed inverse problems. We show that FunkNN generates\nhigh-quality continuous images and exhibits strong out-of-distribution\nperformance thanks to its patch-based design. We further showcase its\nperformance in several stylized inverse problems with exact spatial\nderivatives.\n","authors":["AmirEhsan Khorashadizadeh","Anadi Chaman","Valentin Debarnot","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2212.14042v2.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2211.11238v3","updated":"2023-04-03T12:36:50Z","published":"2022-11-21T08:02:39Z","title":"RobustLoc: Robust Camera Pose Regression in Challenging Driving\n  Environments","summary":"  Camera relocalization has various applications in autonomous driving.\nPrevious camera pose regression models consider only ideal scenarios where\nthere is little environmental perturbation. To deal with challenging driving\nenvironments that may have changing seasons, weather, illumination, and the\npresence of unstable objects, we propose RobustLoc, which derives its\nrobustness against perturbations from neural differential equations. Our model\nuses a convolutional neural network to extract feature maps from multi-view\nimages, a robust neural differential equation diffusion block module to diffuse\ninformation interactively, and a branched pose decoder with multi-layer\ntraining to estimate the vehicle poses. Experiments demonstrate that RobustLoc\nsurpasses current state-of-the-art camera pose regression models and achieves\nrobust performance in various environments. Our code is released at:\nhttps://github.com/sijieaaa/RobustLoc\n","authors":["Sijie Wang","Qiyu Kang","Rui She","Wee Peng Tay","Andreas Hartmannsgruber","Diego Navarro Navarro"],"pdf_url":"https://arxiv.org/pdf/2211.11238v3.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2304.00930v1","updated":"2023-04-03T12:36:39Z","published":"2023-04-03T12:36:39Z","title":"Online Lane Graph Extraction from Onboard Video","summary":"  Autonomous driving requires a structured understanding of the surrounding\nroad network to navigate. One of the most common and useful representation of\nsuch an understanding is done in the form of BEV lane graphs. In this work, we\nuse the video stream from an onboard camera for online extraction of the\nsurrounding's lane graph. Using video, instead of a single image, as input\nposes both benefits and challenges in terms of combining the information from\ndifferent timesteps. We study the emerged challenges using three different\napproaches. The first approach is a post-processing step that is capable of\nmerging single frame lane graph estimates into a unified lane graph. The second\napproach uses the spatialtemporal embeddings in the transformer to enable the\nnetwork to discover the best temporal aggregation strategy. Finally, the third,\nand the proposed method, is an early temporal aggregation through explicit BEV\nprojection and alignment of framewise features. A single model of this proposed\nsimple, yet effective, method can process any number of images, including one,\nto produce accurate lane graphs. The experiments on the Nuscenes and Argoverse\ndatasets show the validity of all the approaches while highlighting the\nsuperiority of the proposed method. The code will be made public.\n","authors":["Yigit Baran Can","Alexander Liniger","Danda Pani Paudel","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2304.00930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12826v2","updated":"2023-04-03T12:18:16Z","published":"2022-11-23T10:20:11Z","title":"Data-driven Feature Tracking for Event Cameras","summary":"  Because of their high temporal resolution, increased resilience to motion\nblur, and very sparse output, event cameras have been shown to be ideal for\nlow-latency and low-bandwidth feature tracking, even in challenging scenarios.\nExisting feature tracking methods for event cameras are either handcrafted or\nderived from first principles but require extensive parameter tuning, are\nsensitive to noise, and do not generalize to different scenarios due to\nunmodeled effects. To tackle these deficiencies, we introduce the first\ndata-driven feature tracker for event cameras, which leverages low-latency\nevents to track features detected in a grayscale frame. We achieve robust\nperformance via a novel frame attention module, which shares information across\nfeature tracks. By directly transferring zero-shot from synthetic to real data,\nour data-driven tracker outperforms existing approaches in relative feature age\nby up to 120% while also achieving the lowest latency. This performance gap is\nfurther increased to 130% by adapting our tracker to real data with a novel\nself-supervision strategy.\n","authors":["Nico Messikommer","Carter Fang","Mathias Gehrig","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2211.12826v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00916v1","updated":"2023-04-03T12:11:51Z","published":"2023-04-03T12:11:51Z","title":"DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via\n  Diffusion Models","summary":"  We present DreamAvatar, a text-and-shape guided framework for generating\nhigh-quality 3D human avatars with controllable poses. While encouraging\nresults have been produced by recent methods on text-guided 3D common object\ngeneration, generating high-quality human avatars remains an open challenge due\nto the complexity of the human body's shape, pose, and appearance. We propose\nDreamAvatar to tackle this challenge, which utilizes a trainable NeRF for\npredicting density and color features for 3D points and a pre-trained\ntext-to-image diffusion model for providing 2D self-supervision. Specifically,\nwe leverage SMPL models to provide rough pose and shape guidance for the\ngeneration. We introduce a dual space design that comprises a canonical space\nand an observation space, which are related by a learnable deformation field\nthrough the NeRF, allowing for the transfer of well-optimized texture and\ngeometry from the canonical space to the target posed avatar. Additionally, we\nexploit a normal-consistency regularization to allow for more vivid generation\nwith detailed geometry and texture. Through extensive evaluations, we\ndemonstrate that DreamAvatar significantly outperforms existing methods,\nestablishing a new state-of-the-art for text-and-shape guided 3D human\ngeneration.\n","authors":["Yukang Cao","Yan-Pei Cao","Kai Han","Ying Shan","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2304.00916v1.pdf","comment":"19 pages, 19 figures"},{"id":"http://arxiv.org/abs/2111.06206v6","updated":"2023-04-03T12:02:02Z","published":"2021-11-11T13:48:20Z","title":"Defining and Quantifying the Emergence of Sparse Concepts in DNNs","summary":"  This paper aims to illustrate the concept-emerging phenomenon in a trained\nDNN. Specifically, we find that the inference score of a DNN can be\ndisentangled into the effects of a few interactive concepts. These concepts can\nbe understood as causal patterns in a sparse, symbolic causal graph, which\nexplains the DNN. The faithfulness of using such a causal graph to explain the\nDNN is theoretically guaranteed, because we prove that the causal graph can\nwell mimic the DNN's outputs on an exponential number of different masked\nsamples. Besides, such a causal graph can be further simplified and re-written\nas an And-Or graph (AOG), without losing much explanation accuracy.\n","authors":["Jie Ren","Mingjie Li","Qirui Chen","Huiqi Deng","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2111.06206v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17803v2","updated":"2023-04-03T11:53:31Z","published":"2023-03-31T05:25:32Z","title":"Rethinking Local Perception in Lightweight Vision Transformer","summary":"  Vision Transformers (ViTs) have been shown to be effective in various vision\ntasks. However, resizing them to a mobile-friendly size leads to significant\nperformance degradation. Therefore, developing lightweight vision transformers\nhas become a crucial area of research. This paper introduces CloFormer, a\nlightweight vision transformer that leverages context-aware local enhancement.\nCloFormer explores the relationship between globally shared weights often used\nin vanilla convolutional operators and token-specific context-aware weights\nappearing in attention, then proposes an effective and straightforward module\nto capture high-frequency local information. In CloFormer, we introduce\nAttnConv, a convolution operator in attention's style. The proposed AttnConv\nuses shared weights to aggregate local information and deploys carefully\ndesigned context-aware weights to enhance local features. The combination of\nthe AttnConv and vanilla attention which uses pooling to reduce FLOPs in\nCloFormer enables the model to perceive high-frequency and low-frequency\ninformation. Extensive experiments were conducted in image classification,\nobject detection, and semantic segmentation, demonstrating the superiority of\nCloFormer.\n","authors":["Qihang Fan","Huaibo Huang","Jiyang Guan","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.17803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00935v4","updated":"2023-04-03T11:40:16Z","published":"2022-10-03T13:47:47Z","title":"Analysis of (sub-)Riemannian PDE-G-CNNs","summary":"  Group equivariant convolutional neural networks (G-CNNs) have been\nsuccessfully applied in geometric deep learning. Typically, G-CNNs have the\nadvantage over CNNs that they do not waste network capacity on training\nsymmetries that should have been hard-coded in the network. The recently\nintroduced framework of PDE-based G-CNNs (PDE-G-CNNs) generalises G-CNNs.\nPDE-G-CNNs have the core advantages that they simultaneously 1) reduce network\ncomplexity, 2) increase classification performance, and 3) provide geometric\ninterpretability. Their implementations primarily consist of linear and\nmorphological convolutions with kernels.\n  In this paper we show that the previously suggested approximative\nmorphological kernels do not always accurately approximate the exact kernels\naccurately. More specifically, depending on the spatial anisotropy of the\nRiemannian metric, we argue that one must resort to sub-Riemannian\napproximations. We solve this problem by providing a new approximative kernel\nthat works regardless of the anisotropy. We provide new theorems with better\nerror estimates of the approximative kernels, and prove that they all carry the\nsame reflectional symmetries as the exact ones.\n  We test the effectiveness of multiple approximative kernels within the\nPDE-G-CNN framework on two datasets, and observe an improvement with the new\napproximative kernels. We report that the PDE-G-CNNs again allow for a\nconsiderable reduction of network complexity while having comparable or better\nperformance than G-CNNs and CNNs on the two datasets. Moreover, PDE-G-CNNs have\nthe advantage of better geometric interpretability over G-CNNs, as the\nmorphological kernels are related to association fields from neurogeometry.\n","authors":["Gijs Bellaard","Daan L. J. Bon","Gautam Pai","Bart M. N. Smets","Remco Duits"],"pdf_url":"https://arxiv.org/pdf/2210.00935v4.pdf","comment":"29 pages, 21 figures"},{"id":"http://arxiv.org/abs/2304.00898v1","updated":"2023-04-03T11:36:10Z","published":"2023-04-03T11:36:10Z","title":"Tunable Convolutions with Parametric Multi-Loss Optimization","summary":"  Behavior of neural networks is irremediably determined by the specific loss\nand data used during training. However it is often desirable to tune the model\nat inference time based on external factors such as preferences of the user or\ndynamic characteristics of the data. This is especially important to balance\nthe perception-distortion trade-off of ill-posed image-to-image translation\ntasks. In this work, we propose to optimize a parametric tunable convolutional\nlayer, which includes a number of different kernels, using a parametric\nmulti-loss, which includes an equal number of objectives. Our key insight is to\nuse a shared set of parameters to dynamically interpolate both the objectives\nand the kernels. During training, these parameters are sampled at random to\nexplicitly optimize all possible combinations of objectives and consequently\ndisentangle their effect into the corresponding kernels. During inference,\nthese parameters become interactive inputs of the model hence enabling reliable\nand consistent control over the model behavior. Extensive experimental results\ndemonstrate that our tunable convolutions effectively work as a drop-in\nreplacement for traditional convolutions in existing neural networks at\nvirtually no extra computational cost, outperforming state-of-the-art control\nstrategies in a wide range of applications; including image denoising,\ndeblurring, super-resolution, and style transfer.\n","authors":["Matteo Maggioni","Thomas Tanay","Francesca Babiloni","Steven McDonagh","Aleš Leonardis"],"pdf_url":"https://arxiv.org/pdf/2304.00898v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2304.00891v1","updated":"2023-04-03T11:26:56Z","published":"2023-04-03T11:26:56Z","title":"Online Algorithms for Hierarchical Inference in Deep Learning\n  applications at the Edge","summary":"  We consider a resource-constrained Edge Device (ED) embedded with a\nsmall-size ML model (S-ML) for a generic classification application, and an\nEdge Server (ES) that hosts a large-size ML model (L-ML). Since the inference\naccuracy of S-ML is lower than that of the L-ML, offloading all the data\nsamples to the ES results in high inference accuracy, but it defeats the\npurpose of embedding S-ML on the ED and deprives the benefits of reduced\nlatency, bandwidth savings, and energy efficiency of doing local inference. To\nget the best out of both worlds, i.e., the benefits of doing inference on the\nED and the benefits of doing inference on ES, we explore the idea of\nHierarchical Inference (HI), wherein S-ML inference is only accepted when it is\ncorrect, otherwise the data sample is offloaded for L-ML inference. However,\nthe ideal implementation of HI is infeasible as the correctness of the S-ML\ninference is not known to the ED. We thus propose an online meta-learning\nframework to predict the correctness of the S-ML inference. The resulting\nonline learning problem turns out to be a Prediction with Expert Advice (PEA)\nproblem with continuous expert space. We consider the full feedback scenario,\nwhere the ED receives feedback on the correctness of the S-ML once it accepts\nthe inference, and the no-local feedback scenario, where the ED does not\nreceive the ground truth for the classification, and propose the HIL-F and\nHIL-N algorithms and prove a regret bound that is sublinear with the number of\ndata samples. We evaluate and benchmark the performance of the proposed\nalgorithms for image classification applications using four datasets, namely,\nImagenette, Imagewoof, MNIST, and CIFAR-10.\n","authors":["Vishnu Narayanan Moothedath","Jaya Prakash Champati","James Gross"],"pdf_url":"https://arxiv.org/pdf/2304.00891v1.pdf","comment":"This work will be appearing in a journal soon and the 'Journal\n  reference' will be updated as and when the information is available. The\n  submission contains 22 pages, 7 figures including subfigures, 2 tables and 2\n  algorithms"},{"id":"http://arxiv.org/abs/2303.00448v2","updated":"2023-04-03T11:17:11Z","published":"2023-03-01T12:17:33Z","title":"The style transformer with common knowledge optimization for image-text\n  retrieval","summary":"  Image-text retrieval which associates different modalities has drawn broad\nattention due to its excellent research value and broad real-world application.\nHowever, most of the existing methods haven't taken the high-level semantic\nrelationships (\"style embedding\") and common knowledge from multi-modalities\ninto full consideration. To this end, we introduce a novel style transformer\nnetwork with common knowledge optimization (CKSTN) for image-text retrieval.\nThe main module is the common knowledge adaptor (CKA) with both the style\nembedding extractor (SEE) and the common knowledge optimization (CKO) modules.\nSpecifically, the SEE uses the sequential update strategy to effectively\nconnect the features of different stages in SEE. The CKO module is introduced\nto dynamically capture the latent concepts of common knowledge from different\nmodalities. Besides, to get generalized temporal common knowledge, we propose a\nsequential update strategy to effectively integrate the features of different\nlayers in SEE with previous common feature units. CKSTN demonstrates the\nsuperiorities of the state-of-the-art methods in image-text retrieval on MSCOCO\nand Flickr30K datasets. Moreover, CKSTN is constructed based on the lightweight\ntransformer which is more convenient and practical for the application of real\nscenes, due to the better performance and lower parameters.\n","authors":["Wenrui Li","Zhengyu Ma","Jinqiao Shi","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2303.00448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.06275v2","updated":"2023-04-03T10:40:32Z","published":"2022-04-13T09:52:02Z","title":"Assessing cloudiness in nonwovens","summary":"  The homogeneity of filter media is important for material selection and\nquality control, along with the specific weight (nominal grammage) and the\ndistribution of the local weight. Cloudiness or formation is a concept used to\ndescribe deviations from homogeneity in filter media. We suggest to derive the\ncloudiness index from the power spectrum of the relative local areal weight,\nintegrated over a selected frequency range. The power spectrum captures the\nenergy density in a broad spectral range. Moreover, under certain conditions,\nthe structure of a nonwoven is fully characterized by the areal weight, the\nvariance of the local areal weight, and the power spectrum. Consequently, the\npower spectrum is the parameter that exclusively reflects the cloudiness. Here,\nwe address questions arising from practical application. The most prominent is\nthe choice of the spectral band. It certainly depends on the characteristic\n\"size of the clouds\", but is limited by the size and lateral resolution of the\nimages. We show that the cloudiness index based on the power spectrum of the\nrelative local areal weight is theoretically well founded and can be robustly\nmeasured from image data. Choosing the spectral band allows to capture the\ncloudiness either visually perceived or found to be decisive for product\nproperties. It is thus well suited to build a technical standard on it.\n","authors":["Michael Godehardt","Ali Moghiseh","Christine Oetjen","Joachim Ohser","Simon Ringger","Katja Schladitz","Ingo Windschiegel"],"pdf_url":"https://arxiv.org/pdf/2204.06275v2.pdf","comment":"FILTECH 2022 - Filter Media Quality Control / Pore Size Analysis"},{"id":"http://arxiv.org/abs/2302.04419v2","updated":"2023-04-03T10:34:44Z","published":"2023-02-09T03:11:21Z","title":"An Investigation into Pre-Training Object-Centric Representations for\n  Reinforcement Learning","summary":"  Unsupervised object-centric representation (OCR) learning has recently drawn\nattention as a new paradigm of visual representation. This is because of its\npotential of being an effective pre-training technique for various downstream\ntasks in terms of sample efficiency, systematic generalization, and reasoning.\nAlthough image-based reinforcement learning (RL) is one of the most important\nand thus frequently mentioned such downstream tasks, the benefit in RL has\nsurprisingly not been investigated systematically thus far. Instead, most of\nthe evaluations have focused on rather indirect metrics such as segmentation\nquality and object property prediction accuracy. In this paper, we investigate\nthe effectiveness of OCR pre-training for image-based reinforcement learning\nvia empirical experiments. For systematic evaluation, we introduce a simple\nobject-centric visual RL benchmark and conduct experiments to answer questions\nsuch as ``Does OCR pre-training improve performance on object-centric tasks?''\nand ``Can OCR pre-training help with out-of-distribution generalization?''. Our\nresults provide empirical evidence for valuable insights into the effectiveness\nof OCR pre-training for RL and the potential limitations of its use in certain\nscenarios. Additionally, this study also examines the critical aspects of\nincorporating OCR pre-training in RL, including performance in a visually\ncomplex environment and the appropriate pooling layer to aggregate the object\nrepresentations.\n","authors":["Jaesik Yoon","Yi-Fu Wu","Heechul Bae","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2302.04419v2.pdf","comment":"We study unsupervised object-centric representations in reinforcement\n  learning through systematic investigation"},{"id":"http://arxiv.org/abs/2210.13984v2","updated":"2023-04-03T10:28:38Z","published":"2022-10-24T07:43:59Z","title":"Abductive Action Inference","summary":"  Abductive reasoning aims to make the most likely inference for a given set of\nincomplete observations. In this work, we propose a new task called abductive\naction inference, in which given a situation, the model answers the question\n`what actions were executed by the human in order to arrive in the current\nstate?'. Given a state, we investigate three abductive inference problems:\naction set prediction, action sequence prediction, and abductive action\nverification. We benchmark several SOTA models such as Transformers, Graph\nneural networks, CLIP, BLIP, end-to-end trained Slow-Fast, and Resnet50-3D\nmodels. Our newly proposed object-relational BiGED model outperforms all other\nmethods on this challenging task on the Action Genome dataset. Codes will be\nmade available.\n","authors":["Clement Tan","Chai Kiat Yeo","Cheston Tan","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2210.13984v2.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2212.04808v2","updated":"2023-04-03T10:27:21Z","published":"2022-12-09T12:25:58Z","title":"CEPHA29: Automatic Cephalometric Landmark Detection Challenge 2023","summary":"  Quantitative cephalometric analysis is the most widely used clinical and\nresearch tool in modern orthodontics. Accurate localization of cephalometric\nlandmarks enables the quantification and classification of anatomical\nabnormalities, however, the traditional manual way of marking these landmarks\nis a very tedious job. Endeavours have constantly been made to develop\nautomated cephalometric landmark detection systems but they are inadequate for\northodontic applications. The fundamental reason for this is that the amount of\npublicly available datasets as well as the images provided for training in\nthese datasets are insufficient for an AI model to perform well. To facilitate\nthe development of robust AI solutions for morphometric analysis, we organise\nthe CEPHA29 Automatic Cephalometric Landmark Detection Challenge in conjunction\nwith IEEE International Symposium on Biomedical Imaging (ISBI 2023). In this\ncontext, we provide the largest known publicly available dataset, consisting of\n1000 cephalometric X-ray images. We hope that our challenge will not only\nderive forward research and innovation in automatic cephalometric landmark\nidentification but will also signal the beginning of a new era in the\ndiscipline.\n","authors":["Muhammad Anwaar Khalid","Kanwal Zulfiqar","Ulfat Bashir","Areeba Shaheen","Rida Iqbal","Zarnab Rizwan","Ghina Rizwan","Muhammad Moazam Fraz"],"pdf_url":"https://arxiv.org/pdf/2212.04808v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00858v1","updated":"2023-04-03T10:12:30Z","published":"2023-04-03T10:12:30Z","title":"Focalized Contrastive View-invariant Learning for Self-supervised\n  Skeleton-based Action Recognition","summary":"  Learning view-invariant representation is a key to improving feature\ndiscrimination power for skeleton-based action recognition. Existing approaches\ncannot effectively remove the impact of viewpoint due to the implicit\nview-dependent representations. In this work, we propose a self-supervised\nframework called Focalized Contrastive View-invariant Learning (FoCoViL), which\nsignificantly suppresses the view-specific information on the representation\nspace where the viewpoints are coarsely aligned. By maximizing mutual\ninformation with an effective contrastive loss between multi-view sample pairs,\nFoCoViL associates actions with common view-invariant properties and\nsimultaneously separates the dissimilar ones. We further propose an adaptive\nfocalization method based on pairwise similarity to enhance contrastive\nlearning for a clearer cluster boundary in the learned space. Different from\nmany existing self-supervised representation learning work that rely heavily on\nsupervised classifiers, FoCoViL performs well on both unsupervised and\nsupervised classifiers with superior recognition performance. Extensive\nexperiments also show that the proposed contrastive-based focalization\ngenerates a more discriminative latent representation.\n","authors":["Qianhui Men","Edmond S. L. Ho","Hubert P. H. Shum","Howard Leung"],"pdf_url":"https://arxiv.org/pdf/2304.00858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11589v2","updated":"2023-04-03T10:01:09Z","published":"2022-11-21T15:41:28Z","title":"Conjugate Product Graphs for Globally Optimal 2D-3D Shape Matching","summary":"  We consider the problem of finding a continuous and non-rigid matching\nbetween a 2D contour and a 3D mesh. While such problems can be solved to global\noptimality by finding a shortest path in the product graph between both shapes,\nexisting solutions heavily rely on unrealistic prior assumptions to avoid\ndegenerate solutions (e.g. knowledge to which region of the 3D shape each point\nof the 2D contour is matched). To address this, we propose a novel 2D-3D shape\nmatching formalism based on the conjugate product graph between the 2D contour\nand the 3D shape. Doing so allows us for the first time to consider\nhigher-order costs, i.e. defined for edge chains, as opposed to costs defined\nfor single edges. This offers substantially more flexibility, which we utilise\nto incorporate a local rigidity prior. By doing so, we effectively circumvent\ndegenerate solutions and thereby obtain smoother and more realistic matchings,\neven when using only a one-dimensional feature descriptor. Overall, our method\nfinds globally optimal and continuous 2D-3D matchings, has the same asymptotic\ncomplexity as previous solutions, produces state-of-the-art results for shape\nmatching and is even capable of matching partial shapes. Our code is publicly\navailable (https://github.com/paul0noah/sm-2D3D).\n","authors":["Paul Roetzer","Zorah Lähner","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2211.11589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.02228v3","updated":"2023-04-03T09:57:51Z","published":"2023-01-05T18:55:09Z","title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in\n  Radiology","summary":"  In this paper, we consider enhancing medical visual-language pre-training\n(VLP) with domain-specific knowledge, by exploiting the paired image-text\nreports from the radiological daily practice. In particular, we make the\nfollowing contributions: First, unlike existing works that directly process the\nraw reports, we adopt a novel triplet extraction module to extract the\nmedical-related information, avoiding unnecessary complexity from language\ngrammar and enhancing the supervision signals; Second, we propose a novel\ntriplet encoding module with entity translation by querying a knowledge base,\nto exploit the rich domain knowledge in medical field, and implicitly build\nrelationships between medical entities in the language embedding space; Third,\nwe propose to use a Transformer-based fusion model for spatially aligning the\nentity description with visual signals at the image patch level, enabling the\nability for medical diagnosis; Fourth, we conduct thorough experiments to\nvalidate the effectiveness of our architecture, and benchmark on numerous\npublic benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax,\nCOVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning\nsettings, our model has demonstrated strong performance compared with the\nformer methods on disease classification and grounding.\n","authors":["Chaoyi Wu","Xiaoman Zhang","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2301.02228v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00844v1","updated":"2023-04-03T09:42:13Z","published":"2023-04-03T09:42:13Z","title":"Spectral Enhanced Rectangle Transformer for Hyperspectral Image\n  Denoising","summary":"  Denoising is a crucial step for hyperspectral image (HSI) applications.\nThough witnessing the great power of deep learning, existing HSI denoising\nmethods suffer from limitations in capturing the non-local self-similarity.\nTransformers have shown potential in capturing long-range dependencies, but few\nattempts have been made with specifically designed Transformer to model the\nspatial and spectral correlation in HSIs. In this paper, we address these\nissues by proposing a spectral enhanced rectangle Transformer, driving it to\nexplore the non-local spatial similarity and global spectral low-rank property\nof HSIs. For the former, we exploit the rectangle self-attention horizontally\nand vertically to capture the non-local similarity in the spatial domain. For\nthe latter, we design a spectral enhancement module that is capable of\nextracting global underlying low-rank property of spatial-spectral cubes to\nsuppress noise, while enabling the interactions among non-overlapping spatial\nrectangles. Extensive experiments have been conducted on both synthetic noisy\nHSIs and real noisy HSIs, showing the effectiveness of our proposed method in\nterms of both objective metric and subjective visual quality. The code is\navailable at https://github.com/MyuLi/SERT.\n","authors":["Miaoyu Li","Ji Liu","Ying Fu","Yulun Zhang","Dejing Dou"],"pdf_url":"https://arxiv.org/pdf/2304.00844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00838v1","updated":"2023-04-03T09:29:17Z","published":"2023-04-03T09:29:17Z","title":"MetaHead: An Engine to Create Realistic Digital Head","summary":"  Collecting and labeling training data is one important step for\nlearning-based methods because the process is time-consuming and biased. For\nface analysis tasks, although some generative models can be used to generate\nface data, they can only achieve a subset of generation diversity,\nreconstruction accuracy, 3D consistency, high-fidelity visual quality, and easy\neditability. One recent related work is the graphics-based generative method,\nbut it can only render low realism head with high computation cost. In this\npaper, we propose MetaHead, a unified and full-featured controllable digital\nhead engine, which consists of a controllable head radiance field(MetaHead-F)\nto super-realistically generate or reconstruct view-consistent 3D controllable\ndigital heads and a generic top-down image generation framework LabelHead to\ngenerate digital heads consistent with the given customizable feature labels.\nExperiments validate that our controllable digital head engine achieves the\nstate-of-the-art generation visual quality and reconstruction accuracy.\nMoreover, the generated labeled data can assist real training data and\nsignificantly surpass the labeled data generated by graphics-based methods in\nterms of training effect.\n","authors":["Dingyun Zhang","Chenglai Zhong","Yudong Guo","Yang Hong","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.00838v1.pdf","comment":"Project page: https://ustc3dv.github.io/MetaHead/"},{"id":"http://arxiv.org/abs/2304.00837v1","updated":"2023-04-03T09:28:48Z","published":"2023-04-03T09:28:48Z","title":"Disorder-invariant Implicit Neural Representation","summary":"  Implicit neural representation (INR) characterizes the attributes of a signal\nas a function of corresponding coordinates which emerges as a sharp weapon for\nsolving inverse problems. However, the expressive power of INR is limited by\nthe spectral bias in the network training. In this paper, we find that such a\nfrequency-related problem could be greatly solved by re-arranging the\ncoordinates of the input signal, for which we propose the disorder-invariant\nimplicit neural representation (DINER) by augmenting a hash-table to a\ntraditional INR backbone. Given discrete signals sharing the same histogram of\nattributes and different arrangement orders, the hash-table could project the\ncoordinates into the same distribution for which the mapped signal can be\nbetter modeled using the subsequent INR network, leading to significantly\nalleviated spectral bias. Furthermore, the expressive power of the DINER is\ndetermined by the width of the hash-table. Different width corresponds to\ndifferent geometrical elements in the attribute space, \\textit{e.g.}, 1D curve,\n2D curved-plane and 3D curved-volume when the width is set as $1$, $2$ and $3$,\nrespectively. More covered areas of the geometrical elements result in stronger\nexpressive power. Experiments not only reveal the generalization of the DINER\nfor different INR backbones (MLP vs. SIREN) and various tasks (image/video\nrepresentation, phase retrieval, refractive index recovery, and neural radiance\nfield optimization) but also show the superiority over the state-of-the-art\nalgorithms both in quality and speed. \\textit{Project page:}\n\\url{https://ezio77.github.io/DINER-website/}\n","authors":["Hao Zhu","Shaowen Xie","Zhen Liu","Fengyi Liu","Qi Zhang","You Zhou","Yi Lin","Zhan Ma","Xun Cao"],"pdf_url":"https://arxiv.org/pdf/2304.00837v1.pdf","comment":"Journal extension of the CVPR'23 highlight paper \"DINER:\n  Disorder-invariant Implicit Neural Representation\". In the extension, we\n  model the expressive power of the DINER using parametric functions in the\n  attribute space. As a result, better results are achieved than the conference\n  version. arXiv admin note: substantial text overlap with arXiv:2211.07871"},{"id":"http://arxiv.org/abs/2210.00413v2","updated":"2023-04-03T09:18:10Z","published":"2022-10-02T03:12:03Z","title":"Unsupervised Visual Odometry and Action Integration for PointGoal\n  Navigation in Indoor Environment","summary":"  PointGoal navigation in indoor environment is a fundamental task for personal\nrobots to navigate to a specified point. Recent studies solved this PointGoal\nnavigation task with near-perfect success rate in photo-realistically simulated\nenvironments, under the assumptions with noiseless actuation and most\nimportantly, perfect localization with GPS and compass sensors. However,\naccurate GPS signalis difficult to be obtained in real indoor environment. To\nimprove the PointGoal navigation accuracy without GPS signal, we use visual\nodometry (VO) and propose a novel action integration module (AIM) trained in\nunsupervised manner. Sepecifically, unsupervised VO computes the relative pose\nof the agent from the re-projection error of two adjacent frames, and then\nreplaces the accurate GPS signal with the path integration. The pseudo position\nestimated by VO is used to train action integration which assists agent to\nupdate their internal perception of location and helps improve the success rate\nof navigation. The training and inference process only use RGB, depth,\ncollision as well as self-action information. The experiments show that the\nproposed system achieves satisfactory results and outperforms the partially\nsupervised learning algorithms on the popular Gibson dataset.\n","authors":["Yijun Cao","Xianshi Zhang","Fuya Luo","Chuan Lin","Yongjie Li"],"pdf_url":"https://arxiv.org/pdf/2210.00413v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2304.00827v1","updated":"2023-04-03T09:13:59Z","published":"2023-04-03T09:13:59Z","title":"Multi-modal Fake News Detection on Social Media via Multi-grained\n  Information Fusion","summary":"  The easy sharing of multimedia content on social media has caused a rapid\ndissemination of fake news, which threatens society's stability and security.\nTherefore, fake news detection has garnered extensive research interest in the\nfield of social forensics. Current methods primarily concentrate on the\nintegration of textual and visual features but fail to effectively exploit\nmulti-modal information at both fine-grained and coarse-grained levels.\nFurthermore, they suffer from an ambiguity problem due to a lack of correlation\nbetween modalities or a contradiction between the decisions made by each\nmodality. To overcome these challenges, we present a Multi-grained Multi-modal\nFusion Network (MMFN) for fake news detection. Inspired by the multi-grained\nprocess of human assessment of news authenticity, we respectively employ two\nTransformer-based pre-trained models to encode token-level features from text\nand images. The multi-modal module fuses fine-grained features, taking into\naccount coarse-grained features encoded by the CLIP encoder. To address the\nambiguity problem, we design uni-modal branches with similarity-based weighting\nto adaptively adjust the use of multi-modal features. Experimental results\ndemonstrate that the proposed framework outperforms state-of-the-art methods on\nthree prevalent datasets.\n","authors":["Yangming Zhou","Yuzhou Yang","Qichao Ying","Zhenxing Qian","Xinpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.00827v1.pdf","comment":"Accepted by ICMR 2023"},{"id":"http://arxiv.org/abs/2211.01201v4","updated":"2023-04-03T09:02:13Z","published":"2022-11-02T15:23:16Z","title":"Human alignment of neural network representations","summary":"  Today's computer vision models achieve human or near-human level performance\nacross a wide variety of vision tasks. However, their architectures, data, and\nlearning algorithms differ in numerous ways from those that give rise to human\nvision. In this paper, we investigate the factors that affect the alignment\nbetween the representations learned by neural networks and human mental\nrepresentations inferred from behavioral responses. We find that model scale\nand architecture have essentially no effect on the alignment with human\nbehavioral responses, whereas the training dataset and objective function both\nhave a much larger impact. These findings are consistent across three datasets\nof human similarity judgments collected using two different tasks. Linear\ntransformations of neural network representations learned from behavioral\nresponses from one dataset substantially improve alignment with human\nsimilarity judgments on the other two datasets. In addition, we find that some\nhuman concepts such as food and animals are well-represented by neural networks\nwhereas others such as royal or sports-related objects are not. Overall,\nalthough models trained on larger, more diverse datasets achieve better\nalignment with humans than models trained on ImageNet alone, our results\nindicate that scaling alone is unlikely to be sufficient to train neural\nnetworks with conceptual representations that match those used by humans.\n","authors":["Lukas Muttenthaler","Jonas Dippel","Lorenz Linhardt","Robert A. Vandermeulen","Simon Kornblith"],"pdf_url":"https://arxiv.org/pdf/2211.01201v4.pdf","comment":"Accepted for publication at ICLR 2023"},{"id":"http://arxiv.org/abs/2303.17811v2","updated":"2023-04-03T08:58:36Z","published":"2023-03-31T06:00:50Z","title":"Zero-shot Referring Image Segmentation with Global-Local Context\n  Features","summary":"  Referring image segmentation (RIS) aims to find a segmentation mask given a\nreferring expression grounded to a region of the input image. Collecting\nlabelled datasets for this task, however, is notoriously costly and\nlabor-intensive. To overcome this issue, we propose a simple yet effective\nzero-shot referring image segmentation method by leveraging the pre-trained\ncross-modal knowledge from CLIP. In order to obtain segmentation masks grounded\nto the input text, we propose a mask-guided visual encoder that captures global\nand local contextual information of an input image. By utilizing instance masks\nobtained from off-the-shelf mask proposal techniques, our method is able to\nsegment fine-detailed Istance-level groundings. We also introduce a\nglobal-local text encoder where the global feature captures complex\nsentence-level semantics of the entire input expression while the local feature\nfocuses on the target noun phrase extracted by a dependency parser. In our\nexperiments, the proposed method outperforms several zero-shot baselines of the\ntask and even the weakly supervised referring expression segmentation method\nwith substantial margins. Our code is available at\nhttps://github.com/Seonghoon-Yu/Zero-shot-RIS.\n","authors":["Seonghoon Yu","Paul Hongsuck Seo","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2303.17811v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2210.06462v2","updated":"2023-04-03T08:56:41Z","published":"2022-10-12T17:57:58Z","title":"Self-Guided Diffusion Models","summary":"  Diffusion models have demonstrated remarkable progress in image generation\nquality, especially when guidance is used to control the generative process.\nHowever, guidance requires a large amount of image-annotation pairs for\ntraining and is thus dependent on their availability, correctness and\nunbiasedness. In this paper, we eliminate the need for such annotation by\ninstead leveraging the flexibility of self-supervision signals to design a\nframework for self-guided diffusion models. By leveraging a feature extraction\nfunction and a self-annotation function, our method provides guidance signals\nat various image granularities: from the level of holistic images to object\nboxes and even segmentation masks. Our experiments on single-label and\nmulti-label image datasets demonstrate that self-labeled guidance always\noutperforms diffusion models without guidance and may even surpass guidance\nbased on ground-truth labels, especially on unbalanced data. When equipped with\nself-supervised box or mask proposals, our method further generates visually\ndiverse yet semantically consistent images, without the need for any class,\nbox, or segment label annotation. Self-guided diffusion is simple, flexible and\nexpected to profit from deployment at scale.\n","authors":["Vincent Tao Hu","David W Zhang","Yuki M. Asano","Gertjan J. Burghouts","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2210.06462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11704v2","updated":"2023-04-03T08:54:42Z","published":"2022-11-21T18:25:14Z","title":"ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of\n  Signed Distance Fields","summary":"  We present ESLAM, an efficient implicit neural representation method for\nSimultaneous Localization and Mapping (SLAM). ESLAM reads RGB-D frames with\nunknown camera poses in a sequential manner and incrementally reconstructs the\nscene representation while estimating the current camera position in the scene.\nWe incorporate the latest advances in Neural Radiance Fields (NeRF) into a SLAM\nsystem, resulting in an efficient and accurate dense visual SLAM method. Our\nscene representation consists of multi-scale axis-aligned perpendicular feature\nplanes and shallow decoders that, for each point in the continuous space,\ndecode the interpolated features into Truncated Signed Distance Field (TSDF)\nand RGB values. Our extensive experiments on three standard datasets, Replica,\nScanNet, and TUM RGB-D show that ESLAM improves the accuracy of 3D\nreconstruction and camera localization of state-of-the-art dense visual SLAM\nmethods by more than 50%, while it runs up to 10 times faster and does not\nrequire any pre-training.\n","authors":["Mohammad Mahdi Johari","Camilla Carta","François Fleuret"],"pdf_url":"https://arxiv.org/pdf/2211.11704v2.pdf","comment":"CVPR 2023 Highlight. Project page: https://www.idiap.ch/paper/eslam/"},{"id":"http://arxiv.org/abs/2304.00801v1","updated":"2023-04-03T08:46:56Z","published":"2023-04-03T08:46:56Z","title":"Noisy Image Segmentation With Soft-Dice","summary":"  This paper presents a study on the soft-Dice loss, one of the most popular\nloss functions in medical image segmentation, for situations where noise is\npresent in target labels. In particular, the set of optimal solutions are\ncharacterized and sharp bounds on the volume bias of these solutions are\nprovided. It is further shown that a sequence of soft segmentations converging\nto optimal soft-Dice also converges to optimal Dice when converted to hard\nsegmentations using thresholding. This is an important result because soft-Dice\nis often used as a proxy for maximizing the Dice metric. Finally, experiments\nconfirming the theoretical results are provided.\n","authors":["Marcus Nordström","Henrik Hult","Atsuto Maki","Fredrik Löfman"],"pdf_url":"https://arxiv.org/pdf/2304.00801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10396v2","updated":"2023-04-03T08:44:14Z","published":"2023-02-21T02:07:13Z","title":"Assessing Domain Gap for Continual Domain Adaptation in Object Detection","summary":"  To ensure reliable object detection in autonomous systems, the detector must\nbe able to adapt to changes in appearance caused by environmental factors such\nas time of day, weather, and seasons. Continually adapting the detector to\nincorporate these changes is a promising solution, but it can be\ncomputationally costly. Our proposed approach is to selectively adapt the\ndetector only when necessary, using new data that does not have the same\ndistribution as the current training data. To this end, we investigate three\npopular metrics for domain gap evaluation and find that there is a correlation\nbetween the domain gap and detection accuracy. Therefore, we apply the domain\ngap as a criterion to decide when to adapt the detector. Our experiments show\nthat our approach has the potential to improve the efficiency of the detector's\noperation in real-world scenarios, where environmental conditions change in a\ncyclical manner, without sacrificing the overall performance of the detector.\nOur code is publicly available at https://github.com/dadung/DGE-CDA.\n","authors":["Anh-Dzung Doan","Bach Long Nguyen","Surabhi Gupta","Ian Reid","Markus Wagner","Tat-Jun Chin"],"pdf_url":"https://arxiv.org/pdf/2302.10396v2.pdf","comment":"Submitted to CVIU"},{"id":"http://arxiv.org/abs/2212.05221v2","updated":"2023-04-03T08:32:39Z","published":"2022-12-10T06:17:56Z","title":"REVEAL: Retrieval-Augmented Visual-Language Pre-Training with\n  Multi-Source Multimodal Knowledge Memory","summary":"  In this paper, we propose an end-to-end Retrieval-Augmented Visual Language\nModel (REVEAL) that learns to encode world knowledge into a large-scale memory,\nand to retrieve from it to answer knowledge-intensive queries. REVEAL consists\nof four key components: the memory, the encoder, the retriever and the\ngenerator. The large-scale memory encodes various sources of multimodal world\nknowledge (e.g. image-text pairs, question answering pairs, knowledge graph\ntriplets, etc) via a unified encoder. The retriever finds the most relevant\nknowledge entries in the memory, and the generator fuses the retrieved\nknowledge with the input query to produce the output. A key novelty in our\napproach is that the memory, encoder, retriever and generator are all\npre-trained end-to-end on a massive amount of data. Furthermore, our approach\ncan use a diverse set of multimodal knowledge sources, which is shown to result\nin significant gains. We show that REVEAL achieves state-of-the-art results on\nvisual question answering and image captioning.\n","authors":["Ziniu Hu","Ahmet Iscen","Chen Sun","Zirui Wang","Kai-Wei Chang","Yizhou Sun","Cordelia Schmid","David A. Ross","Alireza Fathi"],"pdf_url":"https://arxiv.org/pdf/2212.05221v2.pdf","comment":"Published on CVPR 2023"},{"id":"http://arxiv.org/abs/2304.00793v1","updated":"2023-04-03T08:28:13Z","published":"2023-04-03T08:28:13Z","title":"FinnWoodlands Dataset","summary":"  While the availability of large and diverse datasets has contributed to\nsignificant breakthroughs in autonomous driving and indoor applications,\nforestry applications are still lagging behind and new forest datasets would\nmost certainly contribute to achieving significant progress in the development\nof data-driven methods for forest-like scenarios. This paper introduces a\nforest dataset called \\textit{FinnWoodlands}, which consists of RGB stereo\nimages, point clouds, and sparse depth maps, as well as ground truth manual\nannotations for semantic, instance, and panoptic segmentation.\n\\textit{FinnWoodlands} comprises a total of 4226 objects manually annotated,\nout of which 2562 objects (60.6\\%) correspond to tree trunks classified into\nthree different instance categories, namely \"Spruce Tree\", \"Birch Tree\", and\n\"Pine Tree\". Besides tree trunks, we also annotated \"Obstacles\" objects as\ninstances as well as the semantic stuff classes \"Lake\", \"Ground\", and \"Track\".\nOur dataset can be used in forestry applications where a holistic\nrepresentation of the environment is relevant. We provide an initial benchmark\nusing three models for instance segmentation, panoptic segmentation, and depth\ncompletion, and illustrate the challenges that such unstructured scenarios\nintroduce.\n","authors":["Juan Lagos","Urho Lempiö","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2304.00793v1.pdf","comment":"Scandinavian Conference on Image Analysis 2023"},{"id":"http://arxiv.org/abs/2304.00792v1","updated":"2023-04-03T08:24:40Z","published":"2023-04-03T08:24:40Z","title":"Few-shot Fine-tuning is All You Need for Source-free Domain Adaptation","summary":"  Recently, source-free unsupervised domain adaptation (SFUDA) has emerged as a\nmore practical and feasible approach compared to unsupervised domain adaptation\n(UDA) which assumes that labeled source data are always accessible. However,\nsignificant limitations associated with SFUDA approaches are often overlooked,\nwhich limits their practicality in real-world applications. These limitations\ninclude a lack of principled ways to determine optimal hyperparameters and\nperformance degradation when the unlabeled target data fail to meet certain\nrequirements such as a closed-set and identical label distribution to the\nsource data. All these limitations stem from the fact that SFUDA entirely\nrelies on unlabeled target data. We empirically demonstrate the limitations of\nexisting SFUDA methods in real-world scenarios including out-of-distribution\nand label distribution shifts in target data, and verify that none of these\nmethods can be safely applied to real-world settings. Based on our experimental\nresults, we claim that fine-tuning a source pretrained model with a few labeled\ndata (e.g., 1- or 3-shot) is a practical and reliable solution to circumvent\nthe limitations of SFUDA. Contrary to common belief, we find that carefully\nfine-tuned models do not suffer from overfitting even when trained with only a\nfew labeled data, and also show little change in performance due to sampling\nbias. Our experimental results on various domain adaptation benchmarks\ndemonstrate that the few-shot fine-tuning approach performs comparatively under\nthe standard SFUDA settings, and outperforms comparison methods under realistic\nscenarios. Our code is available at https://github.com/daintlab/fewshot-SFDA .\n","authors":["Suho Lee","Seungwon Seo","Jihyo Kim","Yejin Lee","Sangheum Hwang"],"pdf_url":"https://arxiv.org/pdf/2304.00792v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2304.00788v1","updated":"2023-04-03T08:22:02Z","published":"2023-04-03T08:22:02Z","title":"Open-Vocabulary Point-Cloud Object Detection without 3D Annotation","summary":"  The goal of open-vocabulary detection is to identify novel objects based on\narbitrary textual descriptions. In this paper, we address open-vocabulary 3D\npoint-cloud detection by a dividing-and-conquering strategy, which involves: 1)\ndeveloping a point-cloud detector that can learn a general representation for\nlocalizing various objects, and 2) connecting textual and point-cloud\nrepresentations to enable the detector to classify novel object categories\nbased on text prompting. Specifically, we resort to rich image pre-trained\nmodels, by which the point-cloud detector learns localizing objects under the\nsupervision of predicted 2D bounding boxes from 2D pre-trained detectors.\nMoreover, we propose a novel de-biased triplet cross-modal contrastive learning\nto connect the modalities of image, point-cloud and text, thereby enabling the\npoint-cloud detector to benefit from vision-language pre-trained\nmodels,i.e.,CLIP. The novel use of image and vision-language pre-trained models\nfor point-cloud detectors allows for open-vocabulary 3D object detection\nwithout the need for 3D annotations. Experiments demonstrate that the proposed\nmethod improves at least 3.03 points and 7.47 points over a wide range of\nbaselines on the ScanNet and SUN RGB-D datasets, respectively. Furthermore, we\nprovide a comprehensive analysis to explain why our approach works.\n","authors":["Yuheng Lu","Chenfeng Xu","Xiaobao Wei","Xiaodong Xie","Masayoshi Tomizuka","Kurt Keutzer","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.00788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04780v2","updated":"2023-04-03T08:21:02Z","published":"2022-12-09T11:18:40Z","title":"Genie: Show Me the Data for Quantization","summary":"  Zero-shot quantization is a promising approach for developing lightweight\ndeep neural networks when data is inaccessible owing to various reasons,\nincluding cost and issues related to privacy. By exploiting the learned\nparameters ($\\mu$ and $\\sigma$) of batch normalization layers in an\nFP32-pre-trained model, zero-shot quantization schemes focus on generating\nsynthetic data. Subsequently, they distill knowledge from the pre-trained model\n(teacher) to the quantized model (student) such that the quantized model can be\noptimized with the synthetic dataset. However, thus far, zero-shot quantization\nhas primarily been discussed in the context of quantization-aware training\nmethods, which require task-specific losses and long-term optimization as much\nas retraining. We thus introduce a post-training quantization scheme for\nzero-shot quantization that produces high-quality quantized networks within a\nfew hours. Furthermore, we propose a framework called \\genie~that generates\ndata suited for quantization. With the data synthesized by Genie, we can\nproduce robust quantized models without real datasets, which is comparable to\nfew-shot quantization. We also propose a post-training quantization algorithm\nto enhance the performance of quantized models. By combining them, we can\nbridge the gap between zero-shot and few-shot quantization while significantly\nimproving the quantization performance compared to that of existing approaches.\nIn other words, we can obtain a unique state-of-the-art zero-shot quantization\napproach.\n","authors":["Yongkweon Jeon","Chungman Lee","Ho-young Kim"],"pdf_url":"https://arxiv.org/pdf/2212.04780v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2304.00784v1","updated":"2023-04-03T08:16:02Z","published":"2023-04-03T08:16:02Z","title":"Disentangled Pre-training for Image Matting","summary":"  Image matting requires high-quality pixel-level human annotations to support\nthe training of a deep model in recent literature. Whereas such annotation is\ncostly and hard to scale, significantly holding back the development of the\nresearch. In this work, we make the first attempt towards addressing this\nproblem, by proposing a self-supervised pre-training approach that can leverage\ninfinite numbers of data to boost the matting performance. The pre-training\ntask is designed in a similar manner as image matting, where random trimap and\nalpha matte are generated to achieve an image disentanglement objective. The\npre-trained model is then used as an initialisation of the downstream matting\ntask for fine-tuning. Extensive experimental evaluations show that the proposed\napproach outperforms both the state-of-the-art matting methods and other\nalternative self-supervised initialisation approaches by a large margin. We\nalso show the robustness of the proposed approach over different backbone\narchitectures. The code and models will be publicly available.\n","authors":["Yanda Li","Zilong Huang","Gang Yu","Ling Chen","Yunchao Wei","Jianbo Jiao"],"pdf_url":"https://arxiv.org/pdf/2304.00784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00782v1","updated":"2023-04-03T08:12:18Z","published":"2023-04-03T08:12:18Z","title":"NeMF: Inverse Volume Rendering with Neural Microflake Field","summary":"  Recovering the physical attributes of an object's appearance from its images\ncaptured under an unknown illumination is challenging yet essential for\nphoto-realistic rendering. Recent approaches adopt the emerging implicit scene\nrepresentations and have shown impressive results.However, they unanimously\nadopt a surface-based representation,and hence can not well handle scenes with\nvery complex geometry, translucent object and etc.In this paper, we propose to\nconduct inverse volume rendering, in contrast to surface-based, by representing\na scene using microflake volume, which assumes the space is filled with\ninfinite small flakes and light reflects or scatters at each spatial location\naccording to microflake distributions. We further adopt the coordinate networks\nto implicitly encode the microflake volume, and develop a differentiable\nmicroflake volume renderer to train the network in an end-to-end way in\nprinciple.Our NeMF enables effective recovery of appearance attributes for\nhighly complex geometry and scattering object, enables high-quality relighting,\nmaterial editing, and especially simulates volume rendering effects, such as\nscattering, which is infeasible for surface-based approaches.\n","authors":["Youjia Zhang","Teng Xu","Junqing Yu","Yuteng Ye","Junle Wang","Yanqing Jing","Jingyi Yu","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2304.00782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00779v1","updated":"2023-04-03T08:01:27Z","published":"2023-04-03T08:01:27Z","title":"Probabilistic Prompt Learning for Dense Prediction","summary":"  Recent progress in deterministic prompt learning has become a promising\nalternative to various downstream vision tasks, enabling models to learn\npowerful visual representations with the help of pre-trained vision-language\nmodels. However, this approach results in limited performance for dense\nprediction tasks that require handling more complex and diverse objects, since\na single and deterministic description cannot sufficiently represent the entire\nimage. In this paper, we present a novel probabilistic prompt learning to fully\nexploit the vision-language knowledge in dense prediction tasks. First, we\nintroduce learnable class-agnostic attribute prompts to describe universal\nattributes across the object class. The attributes are combined with class\ninformation and visual-context knowledge to define the class-specific textual\ndistribution. Text representations are sampled and used to guide the dense\nprediction task using the probabilistic pixel-text matching loss, enhancing the\nstability and generalization capability of the proposed method. Extensive\nexperiments on different dense prediction tasks and ablation studies\ndemonstrate the effectiveness of our proposed method.\n","authors":["Hyeongjun Kwon","Taeyong Song","Somi Jeong","Jin Kim","Jinhyun Jang","Kwanghoon Sohn"],"pdf_url":"https://arxiv.org/pdf/2304.00779v1.pdf","comment":"accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2304.00774v1","updated":"2023-04-03T07:55:42Z","published":"2023-04-03T07:55:42Z","title":"MGMT promoter methylation status prediction using MRI scans? An\n  extensive experimental evaluation of deep learning models","summary":"  The number of studies on deep learning for medical diagnosis is expanding,\nand these systems are often claimed to outperform clinicians. However, only a\nfew systems have shown medical efficacy. From this perspective, we examine a\nwide range of deep learning algorithms for the assessment of glioblastoma - a\ncommon brain tumor in older adults that is lethal. Surgery, chemotherapy, and\nradiation are the standard treatments for glioblastoma patients. The\nmethylation status of the MGMT promoter, a specific genetic sequence found in\nthe tumor, affects chemotherapy's effectiveness. MGMT promoter methylation\nimproves chemotherapy response and survival in several cancers. MGMT promoter\nmethylation is determined by a tumor tissue biopsy, which is then genetically\ntested. This lengthy and invasive procedure increases the risk of infection and\nother complications. Thus, researchers have used deep learning models to\nexamine the tumor from brain MRI scans to determine the MGMT promoter's\nmethylation state. We employ deep learning models and one of the largest public\nMRI datasets of 585 participants to predict the methylation status of the MGMT\npromoter in glioblastoma tumors using MRI scans. We test these models using\nGrad-CAM, occlusion sensitivity, feature visualizations, and training loss\nlandscapes. Our results show no correlation between these two, indicating that\nexternal cohort data should be used to verify these models' performance to\nassure the accuracy and reliability of deep learning systems in cancer\ndiagnosis.\n","authors":["Numan Saeed","Muhammad Ridzuan","Hussain Alasmawi","Ikboljon Sobirov","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2304.00774v1.pdf","comment":"12 pages, 10 figures, MedIA"},{"id":"http://arxiv.org/abs/2304.00763v1","updated":"2023-04-03T07:31:30Z","published":"2023-04-03T07:31:30Z","title":"BOLLWM: A real-world dataset for bollworm pest monitoring from cotton\n  fields in India","summary":"  This paper presents a dataset of agricultural pest images captured over five\nyears by thousands of small holder farmers and farming extension workers across\nIndia. The dataset has been used to support a mobile application that relies on\nartificial intelligence to assist farmers with pest management decisions.\nCreation came from a mix of organized data collection, and from mobile\napplication usage that was less controlled. This makes the dataset unique\nwithin the pest detection community, exhibiting a number of characteristics\nthat place it closer to other non-agricultural objected detection datasets.\nThis not only makes the dataset applicable to future pest management\napplications, it opens the door for a wide variety of other research agendas.\n","authors":["Jerome White","Chandan Agrawal","Anmol Ojha","Apoorv Agnihotri","Makkunda Sharma","Jigar Doshi"],"pdf_url":"https://arxiv.org/pdf/2304.00763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00761v1","updated":"2023-04-03T07:27:38Z","published":"2023-04-03T07:27:38Z","title":"Learning Anchor Transformations for 3D Garment Animation","summary":"  This paper proposes an anchor-based deformation model, namely AnchorDEF, to\npredict 3D garment animation from a body motion sequence. It deforms a garment\nmesh template by a mixture of rigid transformations with extra nonlinear\ndisplacements. A set of anchors around the mesh surface is introduced to guide\nthe learning of rigid transformation matrices. Once the anchor transformations\nare found, per-vertex nonlinear displacements of the garment template can be\nregressed in a canonical space, which reduces the complexity of deformation\nspace learning. By explicitly constraining the transformed anchors to satisfy\nthe consistencies of position, normal and direction, the physical meaning of\nlearned anchor transformations in space is guaranteed for better\ngeneralization. Furthermore, an adaptive anchor updating is proposed to\noptimize the anchor position by being aware of local mesh topology for learning\nrepresentative anchor transformations. Qualitative and quantitative experiments\non different types of garments demonstrate that AnchorDEF achieves the\nstate-of-the-art performance on 3D garment deformation prediction in motion,\nespecially for loose-fitting garments.\n","authors":["Fang Zhao","Zekun Li","Shaoli Huang","Junwu Weng","Tianfei Zhou","Guo-Sen Xie","Jue Wang","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2304.00761v1.pdf","comment":"Accepted to CVPR 2023. Project page:\n  https://semanticdh.github.io/AnchorDEF"},{"id":"http://arxiv.org/abs/2303.14184v2","updated":"2023-04-03T07:18:27Z","published":"2023-03-24T17:54:22Z","title":"Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion\n  Prior","summary":"  In this work, we investigate the problem of creating high-fidelity 3D content\nfrom only a single image. This is inherently challenging: it essentially\ninvolves estimating the underlying 3D geometry while simultaneously\nhallucinating unseen textures. To address this challenge, we leverage prior\nknowledge from a well-trained 2D diffusion model to act as 3D-aware supervision\nfor 3D creation. Our approach, Make-It-3D, employs a two-stage optimization\npipeline: the first stage optimizes a neural radiance field by incorporating\nconstraints from the reference image at the frontal view and diffusion prior at\nnovel views; the second stage transforms the coarse model into textured point\nclouds and further elevates the realism with diffusion prior while leveraging\nthe high-quality textures from the reference image. Extensive experiments\ndemonstrate that our method outperforms prior works by a large margin,\nresulting in faithful reconstructions and impressive visual quality. Our method\npresents the first attempt to achieve high-quality 3D creation from a single\nimage for general objects and enables various applications such as text-to-3D\ncreation and texture editing.\n","authors":["Junshu Tang","Tengfei Wang","Bo Zhang","Ting Zhang","Ran Yi","Lizhuang Ma","Dong Chen"],"pdf_url":"https://arxiv.org/pdf/2303.14184v2.pdf","comment":"17 pages, 18 figures, Project page: https://make-it-3d.github.io/"},{"id":"http://arxiv.org/abs/2304.00757v1","updated":"2023-04-03T07:16:14Z","published":"2023-04-03T07:16:14Z","title":"Spot-the-Camel: Computer Vision for Safer Roads","summary":"  As the population grows and more land is being used for urbanization,\necosystems are disrupted by our roads and cars. This expansion of\ninfrastructure cuts through wildlife territories, leading to many instances of\nWildlife-Vehicle Collision (WVC). These instances of WVC are a global issue\nthat is having a global socio-economic impact, resulting in billions of dollars\nin property damage and, at times, fatalities for vehicle occupants. In Saudi\nArabia, this issue is similar, with instances of Camel-Vehicle Collision (CVC)\nbeing particularly deadly due to the large size of camels, which results in a\n25% fatality rate [1]. The focus of this work is to test different object\ndetection models on the task of detecting camels on the road. The Deep Learning\n(DL) object detection models used in the experiments are: Center Net, Efficient\nDet, Faster R-CNN, SSD, and YOLOv8. Results of the experiments show that YOLOv8\nperformed the best in terms of accuracy and was the most efficient in training.\nIn the future, the plan is to expand on this work by developing a system to\nmake countryside roads safer.\n","authors":["Khalid Alnujaidi","Ghada Alhabib","Abdulaziz Alodhieb"],"pdf_url":"https://arxiv.org/pdf/2304.00757v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2301.09339"},{"id":"http://arxiv.org/abs/2304.00749v1","updated":"2023-04-03T06:59:08Z","published":"2023-04-03T06:59:08Z","title":"Small but Mighty: Enhancing 3D Point Clouds Semantic Segmentation with\n  U-Next Framework","summary":"  We study the problem of semantic segmentation of large-scale 3D point clouds.\nIn recent years, significant research efforts have been directed toward local\nfeature aggregation, improved loss functions and sampling strategies. While the\nfundamental framework of point cloud semantic segmentation has been largely\noverlooked, with most existing approaches rely on the U-Net architecture by\ndefault. In this paper, we propose U-Next, a small but mighty framework\ndesigned for point cloud semantic segmentation. The key to this framework is to\nlearn multi-scale hierarchical representations from semantically similar\nfeature maps. Specifically, we build our U-Next by stacking multiple U-Net\n$L^1$ codecs in a nested and densely arranged manner to minimize the semantic\ngap, while simultaneously fusing the feature maps across scales to effectively\nrecover the fine-grained details. We also devised a multi-level deep\nsupervision mechanism to further smooth gradient propagation and facilitate\nnetwork optimization. Extensive experiments conducted on three large-scale\nbenchmarks including S3DIS, Toronto3D, and SensatUrban demonstrate the\nsuperiority and the effectiveness of the proposed U-Next architecture. Our\nU-Next architecture shows consistent and visible performance improvements\nacross different tasks and baseline models, indicating its great potential to\nserve as a general framework for future research.\n","authors":["Ziyin Zeng","Qingyong Hu","Zhong Xie","Jian Zhou","Yongyang Xu"],"pdf_url":"https://arxiv.org/pdf/2304.00749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00776v2","updated":"2023-04-03T06:55:09Z","published":"2022-12-01T18:57:20Z","title":"ResFormer: Scaling ViTs with Multi-Resolution Training","summary":"  Vision Transformers (ViTs) have achieved overwhelming success, yet they\nsuffer from vulnerable resolution scalability, i.e., the performance drops\ndrastically when presented with input resolutions that are unseen during\ntraining. We introduce, ResFormer, a framework that is built upon the seminal\nidea of multi-resolution training for improved performance on a wide spectrum\nof, mostly unseen, testing resolutions. In particular, ResFormer operates on\nreplicated images of different resolutions and enforces a scale consistency\nloss to engage interactive information across different scales. More\nimportantly, to alternate among varying resolutions effectively, especially\nnovel ones in testing, we propose a global-local positional embedding strategy\nthat changes smoothly conditioned on input sizes. We conduct extensive\nexperiments for image classification on ImageNet. The results provide strong\nquantitative evidence that ResFormer has promising scaling abilities towards a\nwide range of resolutions. For instance, ResFormer-B-MR achieves a Top-1\naccuracy of 75.86% and 81.72% when evaluated on relatively low and high\nresolutions respectively (i.e., 96 and 640), which are 48% and 7.49% better\nthan DeiT-B. We also demonstrate, moreover, ResFormer is flexible and can be\neasily extended to semantic segmentation, object detection and video action\nrecognition. Code is available at https://github.com/ruitian12/resformer.\n","authors":["Rui Tian","Zuxuan Wu","Qi Dai","Han Hu","Yu Qiao","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2212.00776v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2212.08067v2","updated":"2023-04-03T06:54:50Z","published":"2022-12-15T18:59:54Z","title":"VolRecon: Volume Rendering of Signed Ray Distance Functions for\n  Generalizable Multi-View Reconstruction","summary":"  The success of the Neural Radiance Fields (NeRF) in novel view synthesis has\ninspired researchers to propose neural implicit scene reconstruction. However,\nmost existing neural implicit reconstruction methods optimize per-scene\nparameters and therefore lack generalizability to new scenes. We introduce\nVolRecon, a novel generalizable implicit reconstruction method with Signed Ray\nDistance Function (SRDF). To reconstruct the scene with fine details and little\nnoise, VolRecon combines projection features aggregated from multi-view\nfeatures, and volume features interpolated from a coarse global feature volume.\nUsing a ray transformer, we compute SRDF values of sampled points on a ray and\nthen render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by\nabout 30% in sparse view reconstruction and achieves comparable accuracy as\nMVSNet in full view reconstruction. Furthermore, our approach exhibits good\ngeneralization performance on the large-scale ETH3D benchmark.\n","authors":["Yufan Ren","Fangjinhua Wang","Tong Zhang","Marc Pollefeys","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2212.08067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.02998v2","updated":"2023-04-03T06:41:41Z","published":"2021-08-06T08:18:28Z","title":"AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases\n  Treatment: Status Quo","summary":"  The aortic vessel tree is composed of the aorta and its branching arteries,\nand plays a key role in supplying the whole body with blood. Aortic diseases,\nlike aneurysms or dissections, can lead to an aortic rupture, whose treatment\nwith open surgery is highly risky. Therefore, patients commonly undergo drug\ntreatment under constant monitoring, which requires regular inspections of the\nvessels through imaging. The standard imaging modality for diagnosis and\nmonitoring is computed tomography (CT), which can provide a detailed picture of\nthe aorta and its branching vessels if completed with a contrast agent, called\nCT angiography (CTA). Optimally, the whole aortic vessel tree geometry from\nconsecutive CTAs is overlaid and compared. This allows not only detection of\nchanges in the aorta, but also of its branches, caused by the primary pathology\nor newly developed. When performed manually, this reconstruction requires slice\nby slice contouring, which could easily take a whole day for a single aortic\nvessel tree, and is therefore not feasible in clinical practice. Automatic or\nsemi-automatic vessel tree segmentation algorithms, however, can complete this\ntask in a fraction of the manual execution time and run in parallel to the\nclinical routine of the clinicians. In this paper, we systematically review\ncomputing techniques for the automatic and semi-automatic segmentation of the\naortic vessel tree. The review concludes with an in-depth discussion on how\nclose these state-of-the-art approaches are to an application in clinical\npractice and how active this research field is, taking into account the number\nof publications, datasets and challenges.\n","authors":["Yuan Jin","Antonio Pepe","Jianning Li","Christina Gsaxner","Fen-hua Zhao","Kelsey L. Pomykala","Jens Kleesiek","Alejandro F. Frangi","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2108.02998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11152v2","updated":"2023-04-03T06:41:13Z","published":"2022-11-21T02:32:25Z","title":"You Need Multiple Exiting: Dynamic Early Exiting for Accelerating\n  Unified Vision Language Model","summary":"  Large-scale Transformer models bring significant improvements for various\ndownstream vision language tasks with a unified architecture. The performance\nimprovements come with increasing model size, resulting in slow inference speed\nand increased cost for severing. While some certain predictions benefit from\nthe full complexity of the large-scale model, not all of inputs need the same\namount of computation to conduct, potentially leading to computation resource\nwaste. To handle this challenge, early exiting is proposed to adaptively\nallocate computational power in term of input complexity to improve inference\nefficiency. The existing early exiting strategies usually adopt output\nconfidence based on intermediate layers as a proxy of input complexity to incur\nthe decision of skipping following layers. However, such strategies cannot\napply to encoder in the widely-used unified architecture with both encoder and\ndecoder due to difficulty of output confidence estimation in the encoder. It is\nsuboptimal in term of saving computation power to ignore the early exiting in\nencoder component. To handle this challenge, we propose a novel early exiting\nstrategy for unified visual language models, which allows dynamically skip the\nlayers in encoder and decoder simultaneously in term of input layer-wise\nsimilarities with multiple times of early exiting, namely \\textbf{MuE}. By\ndecomposing the image and text modalities in the encoder, MuE is flexible and\ncan skip different layers in term of modalities, advancing the inference\nefficiency while minimizing performance drop. Experiments on the SNLI-VE and MS\nCOCO datasets show that the proposed approach MuE can reduce expected inference\ntime by up to 50\\% and 40\\% while maintaining 99\\% and 96\\% performance\nrespectively.\n","authors":["Shengkun Tang","Yaqing Wang","Zhenglun Kong","Tianchi Zhang","Yao Li","Caiwen Ding","Yanzhi Wang","Yi Liang","Dongkuan Xu"],"pdf_url":"https://arxiv.org/pdf/2211.11152v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00746v1","updated":"2023-04-03T06:40:52Z","published":"2023-04-03T06:40:52Z","title":"OTS: A One-shot Learning Approach for Text Spotting in Historical\n  Manuscripts","summary":"  Historical manuscript processing poses challenges like limited annotated\ntraining data and novel class emergence. To address this, we propose a novel\nOne-shot learning-based Text Spotting (OTS) approach that accurately and\nreliably spots novel characters with just one annotated support sample. Drawing\ninspiration from cognitive research, we introduce a spatial alignment module\nthat finds, focuses on, and learns the most discriminative spatial regions in\nthe query image based on one support image. Especially, since the low-resource\nspotting task often faces the problem of example imbalance, we propose a novel\nloss function called torus loss which can make the embedding space of distance\nmetric more discriminative. Our approach is highly efficient and requires only\na few training samples while exhibiting the remarkable ability to handle novel\ncharacters, and symbols. To enhance dataset diversity, a new manuscript dataset\nthat contains the ancient Dongba hieroglyphics (DBH) is created. We conduct\nexperiments on publicly available VML-HD, TKH, NC datasets, and the new\nproposed DBH dataset. The experimental results demonstrate that OTS outperforms\nthe state-of-the-art methods in one-shot text spotting. Overall, our proposed\nmethod offers promising applications in the field of text spotting in\nhistorical manuscripts.\n","authors":["Wen-Bo Hu","Hong-Jian Zhan","Cong Liu","Bing Yin","Yue Lu"],"pdf_url":"https://arxiv.org/pdf/2304.00746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17200v2","updated":"2023-04-03T06:30:19Z","published":"2023-03-30T07:43:27Z","title":"SynthVSR: Scaling Up Visual Speech Recognition With Synthetic\n  Supervision","summary":"  Recently reported state-of-the-art results in visual speech recognition (VSR)\noften rely on increasingly large amounts of video data, while the publicly\navailable transcribed video datasets are limited in size. In this paper, for\nthe first time, we study the potential of leveraging synthetic visual data for\nVSR. Our method, termed SynthVSR, substantially improves the performance of VSR\nsystems with synthetic lip movements. The key idea behind SynthVSR is to\nleverage a speech-driven lip animation model that generates lip movements\nconditioned on the input speech. The speech-driven lip animation model is\ntrained on an unlabeled audio-visual dataset and could be further optimized\ntowards a pre-trained VSR model when labeled videos are available. As plenty of\ntranscribed acoustic data and face images are available, we are able to\ngenerate large-scale synthetic data using the proposed lip animation model for\nsemi-supervised VSR training. We evaluate the performance of our approach on\nthe largest public VSR benchmark - Lip Reading Sentences 3 (LRS3). SynthVSR\nachieves a WER of 43.3% with only 30 hours of real labeled data, outperforming\noff-the-shelf approaches using thousands of hours of video. The WER is further\nreduced to 27.9% when using all 438 hours of labeled data from LRS3, which is\non par with the state-of-the-art self-supervised AV-HuBERT method. Furthermore,\nwhen combined with large-scale pseudo-labeled audio-visual data SynthVSR yields\na new state-of-the-art VSR WER of 16.9% using publicly available data only,\nsurpassing the recent state-of-the-art approaches trained with 29 times more\nnon-public machine-transcribed video data (90,000 hours). Finally, we perform\nextensive ablation studies to understand the effect of each component in our\nproposed method.\n","authors":["Xubo Liu","Egor Lakomkin","Konstantinos Vougioukas","Pingchuan Ma","Honglie Chen","Ruiming Xie","Morrie Doulaty","Niko Moritz","Jáchym Kolář","Stavros Petridis","Maja Pantic","Christian Fuegen"],"pdf_url":"https://arxiv.org/pdf/2303.17200v2.pdf","comment":"IEEE/CVF CVPR 2023"},{"id":"http://arxiv.org/abs/2304.00741v1","updated":"2023-04-03T06:25:45Z","published":"2023-04-03T06:25:45Z","title":"DeGPR: Deep Guided Posterior Regularization for Multi-Class Cell\n  Detection and Counting","summary":"  Multi-class cell detection and counting is an essential task for many\npathological diagnoses. Manual counting is tedious and often leads to\ninter-observer variations among pathologists. While there exist multiple,\ngeneral-purpose, deep learning-based object detection and counting methods,\nthey may not readily transfer to detecting and counting cells in medical\nimages, due to the limited data, presence of tiny overlapping objects, multiple\ncell types, severe class-imbalance, minute differences in size/shape of cells,\netc. In response, we propose guided posterior regularization (DeGPR), which\nassists an object detector by guiding it to exploit discriminative features\namong cells. The features may be pathologist-provided or inferred directly from\nvisual data. We validate our model on two publicly available datasets (CoNSeP\nand MoNuSAC), and on MuCeD, a novel dataset that we contribute. MuCeD consists\nof 55 biopsy images of the human duodenum for predicting celiac disease. We\nperform extensive experimentation with three object detection baselines on\nthree datasets to show that DeGPR is model-agnostic, and consistently improves\nbaselines obtaining up to 9% (absolute) mAP gains.\n","authors":["Aayush Kumar Tyagi","Chirag Mohapatra","Prasenjit Das","Govind Makharia","Lalita Mehra","Prathosh AP"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2304.00741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00738v1","updated":"2023-04-03T06:19:42Z","published":"2023-04-03T06:19:42Z","title":"Device Image-IV Mapping using Variational Autoencoder for Inverse Design\n  and Forward Prediction","summary":"  This paper demonstrates the learning of the underlying device physics by\nmapping device structure images to their corresponding Current-Voltage (IV)\ncharacteristics using a novel framework based on variational autoencoders\n(VAE). Since VAE is used, domain expertise is not required and the framework\ncan be quickly deployed on any new device and measurement. This is expected to\nbe useful in the compact modeling of novel devices when only device\ncross-sectional images and electrical characteristics are available (e.g. novel\nemerging memory). Technology Computer-Aided Design (TCAD) generated and\nhand-drawn Metal-Oxide-Semiconductor (MOS) device images and noisy\ndrain-current-gate-voltage curves (IDVG) are used for the demonstration. The\nframework is formed by stacking two VAEs (one for image manifold learning and\none for IDVG manifold learning) which communicate with each other through the\nlatent variables. Five independent variables with different strengths are used.\nIt is shown that it can perform inverse design (generate a design structure for\na given IDVG) and forward prediction (predict IDVG for a given structure image,\nwhich can be used for compact modeling if the image is treated as device\nparameters) successfully. Since manifold learning is used, the machine is shown\nto be robust against noise in the inputs (i.e. using hand-drawn images and\nnoisy IDVG curves) and not confused by weak and irrelevant independent\nvariables.\n","authors":["Thomas Lu","Albert Lu","Hiu Yung Wong"],"pdf_url":"https://arxiv.org/pdf/2304.00738v1.pdf","comment":"5 pages 6 figures"},{"id":"http://arxiv.org/abs/2304.00733v1","updated":"2023-04-03T06:10:06Z","published":"2023-04-03T06:10:06Z","title":"Unbiased Scene Graph Generation in Videos","summary":"  The task of dynamic scene graph generation (SGG) from videos is complicated\nand challenging due to the inherent dynamics of a scene, temporal fluctuation\nof model predictions, and the long-tailed distribution of the visual\nrelationships in addition to the already existing challenges in image-based\nSGG. Existing methods for dynamic SGG have primarily focused on capturing\nspatio-temporal context using complex architectures without addressing the\nchallenges mentioned above, especially the long-tailed distribution of\nrelationships. This often leads to the generation of biased scene graphs. To\naddress these challenges, we introduce a new framework called TEMPURA: TEmporal\nconsistency and Memory Prototype guided UnceRtainty Attenuation for unbiased\ndynamic SGG. TEMPURA employs object-level temporal consistencies via\ntransformer-based sequence modeling, learns to synthesize unbiased relationship\nrepresentations using memory-guided training, and attenuates the predictive\nuncertainty of visual relations using a Gaussian Mixture Model (GMM). Extensive\nexperiments demonstrate that our method achieves significant (up to 10% in some\ncases) performance gain over existing methods highlighting its superiority in\ngenerating more unbiased scene graphs.\n","authors":["Sayak Nag","Kyle Min","Subarna Tripathi","Amit K. Roy Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2304.00733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00725v1","updated":"2023-04-03T05:39:02Z","published":"2023-04-03T05:39:02Z","title":"CG-3DSRGAN: A classification guided 3D generative adversarial network\n  for image quality recovery from low-dose PET images","summary":"  Positron emission tomography (PET) is the most sensitive molecular imaging\nmodality routinely applied in our modern healthcare. High radioactivity caused\nby the injected tracer dose is a major concern in PET imaging and limits its\nclinical applications. However, reducing the dose leads to inadequate image\nquality for diagnostic practice. Motivated by the need to produce high quality\nimages with minimum low-dose, Convolutional Neural Networks (CNNs) based\nmethods have been developed for high quality PET synthesis from its low-dose\ncounterparts. Previous CNNs-based studies usually directly map low-dose PET\ninto features space without consideration of different dose reduction level. In\nthis study, a novel approach named CG-3DSRGAN (Classification-Guided Generative\nAdversarial Network with Super Resolution Refinement) is presented.\nSpecifically, a multi-tasking coarse generator, guided by a classification\nhead, allows for a more comprehensive understanding of the noise-level features\npresent in the low-dose data, resulting in improved image synthesis. Moreover,\nto recover spatial details of standard PET, an auxiliary super resolution\nnetwork - Contextual-Net - is proposed as a second-stage training to narrow the\ngap between coarse prediction and standard PET. We compared our method to the\nstate-of-the-art methods on whole-body PET with different dose reduction\nfactors (DRFs). Experiments demonstrate our method can outperform others on all\nDRF.\n","authors":["Yuxin Xue","Yige Peng","Lei Bi","Dagan Feng","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2304.00725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07199v3","updated":"2023-04-03T05:35:31Z","published":"2022-10-13T17:19:22Z","title":"Self-Supervised Geometric Correspondence for Category-Level 6D Object\n  Pose Estimation in the Wild","summary":"  While 6D object pose estimation has wide applications across computer vision\nand robotics, it remains far from being solved due to the lack of annotations.\nThe problem becomes even more challenging when moving to category-level 6D\npose, which requires generalization to unseen instances. Current approaches are\nrestricted by leveraging annotations from simulation or collected from humans.\nIn this paper, we overcome this barrier by introducing a self-supervised\nlearning approach trained directly on large-scale real-world object videos for\ncategory-level 6D pose estimation in the wild. Our framework reconstructs the\ncanonical 3D shape of an object category and learns dense correspondences\nbetween input images and the canonical shape via surface embedding. For\ntraining, we propose novel geometrical cycle-consistency losses which construct\ncycles across 2D-3D spaces, across different instances and different time\nsteps. The learned correspondence can be applied for 6D pose estimation and\nother downstream tasks such as keypoint transfer. Surprisingly, our method,\nwithout any human annotations or simulators, can achieve on-par or even better\nperformance than previous supervised or semi-supervised methods on in-the-wild\nimages. Our project page is: https://kywind.github.io/self-pose .\n","authors":["Kaifeng Zhang","Yang Fu","Shubhankar Borse","Hong Cai","Fatih Porikli","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2210.07199v3.pdf","comment":"Project page: https://kywind.github.io/self-pose"},{"id":"http://arxiv.org/abs/2211.16431v2","updated":"2023-04-03T05:15:59Z","published":"2022-11-29T17:59:06Z","title":"NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with\n  360° Views","summary":"  Virtual reality and augmented reality (XR) bring increasing demand for 3D\ncontent. However, creating high-quality 3D content requires tedious work that a\nhuman expert must do. In this work, we study the challenging task of lifting a\nsingle image to a 3D object and, for the first time, demonstrate the ability to\ngenerate a plausible 3D object with 360{\\deg} views that correspond well with\nthe given reference image. By conditioning on the reference image, our model\ncan fulfill the everlasting curiosity for synthesizing novel views of objects\nfrom images. Our technique sheds light on a promising direction of easing the\nworkflows for 3D artists and XR designers. We propose a novel framework, dubbed\nNeuralLift-360, that utilizes a depth-aware neural radiance representation\n(NeRF) and learns to craft the scene guided by denoising diffusion models. By\nintroducing a ranking loss, our NeuralLift-360 can be guided with rough depth\nestimation in the wild. We also adopt a CLIP-guided sampling strategy for the\ndiffusion prior to provide coherent guidance. Extensive experiments demonstrate\nthat our NeuralLift-360 significantly outperforms existing state-of-the-art\nbaselines. Project page: https://vita-group.github.io/NeuralLift-360/\n","authors":["Dejia Xu","Yifan Jiang","Peihao Wang","Zhiwen Fan","Yi Wang","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2211.16431v2.pdf","comment":"Project page: https://vita-group.github.io/NeuralLift-360/"},{"id":"http://arxiv.org/abs/2304.00719v1","updated":"2023-04-03T05:07:49Z","published":"2023-04-03T05:07:49Z","title":"Multi-Modal Representation Learning with Text-Driven Soft Masks","summary":"  We propose a visual-linguistic representation learning approach within a\nself-supervised learning framework by introducing a new operation, loss, and\ndata augmentation strategy. First, we generate diverse features for the\nimage-text matching (ITM) task via soft-masking the regions in an image, which\nare most relevant to a certain word in the corresponding caption, instead of\ncompletely removing them. Since our framework relies only on image-caption\npairs with no fine-grained annotations, we identify the relevant regions to\neach word by computing the word-conditional visual attention using multi-modal\nencoder. Second, we encourage the model to focus more on hard but diverse\nexamples by proposing a focal loss for the image-text contrastive learning\n(ITC) objective, which alleviates the inherent limitations of overfitting and\nbias issues. Last, we perform multi-modal data augmentations for\nself-supervised learning via mining various examples by masking texts and\nrendering distortions on images. We show that the combination of these three\ninnovations is effective for learning a pretrained model, leading to\noutstanding performance on multiple vision-language downstream tasks.\n","authors":["Jaeyoo Park","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2304.00719v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2301.02778v2","updated":"2023-04-03T05:02:47Z","published":"2023-01-07T04:33:51Z","title":"Lightweight Salient Object Detection in Optical Remote-Sensing Images\n  via Semantic Matching and Edge Alignment","summary":"  Recently, relying on convolutional neural networks (CNNs), many methods for\nsalient object detection in optical remote sensing images (ORSI-SOD) are\nproposed. However, most methods ignore the huge parameters and computational\ncost brought by CNNs, and only a few pay attention to the portability and\nmobility. To facilitate practical applications, in this paper, we propose a\nnovel lightweight network for ORSI-SOD based on semantic matching and edge\nalignment, termed SeaNet. Specifically, SeaNet includes a lightweight\nMobileNet-V2 for feature extraction, a dynamic semantic matching module (DSMM)\nfor high-level features, an edge self-alignment module (ESAM) for low-level\nfeatures, and a portable decoder for inference. First, the high-level features\nare compressed into semantic kernels. Then, semantic kernels are used to\nactivate salient object locations in two groups of high-level features through\ndynamic convolution operations in DSMM. Meanwhile, in ESAM, cross-scale edge\ninformation extracted from two groups of low-level features is self-aligned\nthrough L2 loss and used for detail enhancement. Finally, starting from the\nhighest-level features, the decoder infers salient objects based on the\naccurate locations and fine details contained in the outputs of the two\nmodules. Extensive experiments on two public datasets demonstrate that our\nlightweight SeaNet not only outperforms most state-of-the-art lightweight\nmethods but also yields comparable accuracy with state-of-the-art conventional\nmethods, while having only 2.76M parameters and running with 1.7G FLOPs for\n288x288 inputs. Our code and results are available at\nhttps://github.com/MathLee/SeaNet.\n","authors":["Gongyang Li","Zhi Liu","Xinpeng Zhang","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2301.02778v2.pdf","comment":"11 pages, 4 figures, Accepted by IEEE Transactions on Geoscience and\n  Remote Sensing 2023"},{"id":"http://arxiv.org/abs/2203.05711v2","updated":"2023-04-03T03:52:14Z","published":"2022-03-11T01:45:33Z","title":"Synopses of Movie Narratives: a Video-Language Dataset for Story\n  Understanding","summary":"  Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SYMON), containing\n5,193 video summaries of popular movies and TV series. SYMON captures\nnaturalistic story-telling videos for human audience made by human creators. As\na prototypical and naturalistic story dataset, SYMON features high coverage of\nmultimodal story events, abundant mental-state descriptions, and large semantic\ngaps between the visual and the textual modalities. We establish benchmarks on\nvideo-text retrieval and zero-shot alignment on movie summary videos, which\nshowcase the importance of in-domain data in story understanding. With SYMON,\nwe hope to lay the groundwork for progress in multimodal story understanding.\n","authors":["Yidan Sun","Qin Chao","Yangfeng Ji","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2203.05711v2.pdf","comment":"25 pages, 17 figures"},{"id":"http://arxiv.org/abs/2211.14513v2","updated":"2023-04-03T03:38:52Z","published":"2022-11-26T08:43:12Z","title":"Rethinking Alignment and Uniformity in Unsupervised Image Semantic\n  Segmentation","summary":"  Unsupervised image semantic segmentation(UISS) aims to match low-level visual\nfeatures with semantic-level representations without outer supervision. In this\npaper, we address the critical properties from the view of feature alignments\nand feature uniformity for UISS models. We also make a comparison between UISS\nand image-wise representation learning. Based on the analysis, we argue that\nthe existing MI-based methods in UISS suffer from representation collapse. By\nthis, we proposed a robust network called Semantic Attention Network(SAN), in\nwhich a new module Semantic Attention(SEAT) is proposed to generate pixel-wise\nand semantic features dynamically. Experimental results on multiple semantic\nsegmentation benchmarks show that our unsupervised segmentation framework\nspecializes in catching semantic representations, which outperforms all the\nunpretrained and even several pretrained methods.\n","authors":["Daoan Zhang","Chenming Li","Haoquan Li","Wenjian Huang","Lingyun Huang","Jianguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.14513v2.pdf","comment":"AAAI23"},{"id":"http://arxiv.org/abs/2303.03709v4","updated":"2023-04-03T03:25:32Z","published":"2023-03-07T07:47:41Z","title":"Bootstrap The Original Latent: Learning a Private Model from a Black-box\n  Model","summary":"  In this paper, considering the balance of data/model privacy of model owners\nand user needs, we propose a new setting called Back-Propagated Black-Box\nAdaptation (BPBA) for users to better train their private models via the\nguidance of the back-propagated results of a Black-box foundation/source model.\nOur setting can ease the usage of foundation/source models as well as prevent\nthe leakage and misuse of foundation/source models. Moreover, we also propose a\nnew training strategy called Bootstrap The Original Latent (BTOL) to fully\nutilize the foundation/source models. Our strategy consists of a domain adapter\nand a freeze-and-thaw strategy. We apply our BTOL under BPBA and Black-box UDA\nsettings on three different datasets. Experiments show that our strategy is\nefficient and robust in various settings without manual augmentations.\n","authors":["Shuai Wang","Daoan Zhang","Jianguo Zhang","Weiwei Zhang","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2303.03709v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02350v4","updated":"2023-04-03T03:17:42Z","published":"2023-02-05T09:48:57Z","title":"Aggregation of Disentanglement: Reconsidering Domain Variations in\n  Domain Generalization","summary":"  Domain Generalization (DG) is a fundamental challenge for machine learning\nmodels, which aims to improve model generalization on various domains. Previous\nmethods focus on generating domain invariant features from various source\ndomains. However, we argue that the domain variantions also contain useful\ninformation, ie, classification-aware information, for downstream tasks, which\nhas been largely ignored. Different from learning domain invariant features\nfrom source domains, we decouple the input images into Domain Expert Features\nand noise. The proposed domain expert features lie in a learned latent space\nwhere the images in each domain can be classified independently, enabling the\nimplicit use of classification-aware domain variations. Based on the analysis,\nwe proposed a novel paradigm called Domain Disentanglement Network (DDN) to\ndisentangle the domain expert features from the source domain images and\naggregate the source domain expert features for representing the target test\ndomain. We also propound a new contrastive learning method to guide the domain\nexpert features to form a more balanced and separable feature space.\nExperiments on the widely-used benchmarks of PACS, VLCS, OfficeHome, DomainNet,\nand TerraIncognita demonstrate the competitive performance of our method\ncompared to the recently proposed alternatives.\n","authors":["Daoan Zhang","Mingkai Chen","Chenming Li","Lingyun Huang","Jianguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.02350v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00697v1","updated":"2023-04-03T03:13:59Z","published":"2023-04-03T03:13:59Z","title":"D-Score: A White-Box Diagnosis Score for CNNs Based on Mutation\n  Operators","summary":"  Convolutional neural networks (CNNs) have been widely applied in many\nsafety-critical domains, such as autonomous driving and medical diagnosis.\nHowever, concerns have been raised with respect to the trustworthiness of these\nmodels: The standard testing method evaluates the performance of a model on a\ntest set, while low-quality and insufficient test sets can lead to unreliable\nevaluation results, which can have unforeseeable consequences. Therefore, how\nto comprehensively evaluate CNNs and, based on the evaluation results, how to\nenhance their trustworthiness are the key problems to be urgently addressed.\nPrior work has used mutation tests to evaluate the test sets of CNNs. However,\nthe evaluation scores are black boxes and not explicit enough for what is being\ntested. In this paper, we propose a white-box diagnostic approach that uses\nmutation operators and image transformation to calculate the feature and\nattention distribution of the model and further present a diagnosis score,\nnamely D-Score, to reflect the model's robustness and fitness to a dataset. We\nalso propose a D-Score based data augmentation method to enhance the CNN's\nperformance to translations and rescalings. Comprehensive experiments on two\nwidely used datasets and three commonly adopted CNNs demonstrate the\neffectiveness of our approach.\n","authors":["Xin Zhang","Yuqi Song","Xiaofeng Wang","Fei Zuo"],"pdf_url":"https://arxiv.org/pdf/2304.00697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.16782v2","updated":"2023-04-03T03:07:56Z","published":"2022-03-31T04:01:02Z","title":"Weakly Supervised Patch Label Inference Networks for Efficient Pavement\n  Distress Detection and Recognition in the Wild","summary":"  Automatic image-based pavement distress detection and recognition are vital\nfor pavement maintenance and management. However, existing deep learning-based\nmethods largely omit the specific characteristics of pavement images, such as\nhigh image resolution and low distress area ratio, and are not end-to-end\ntrainable. In this paper, we present a series of simple yet effective\nend-to-end deep learning approaches named Weakly Supervised Patch Label\nInference Networks (WSPLIN) for efficiently addressing these tasks under\nvarious application settings. WSPLIN transforms the fully supervised pavement\nimage classification problem into a weakly supervised pavement patch\nclassification problem for solutions. Specifically, WSPLIN first divides the\npavement image under different scales into patches with different collection\nstrategies and then employs a Patch Label Inference Network (PLIN) to infer the\nlabels of these patches to fully exploit the resolution and scale information.\nNotably, we design a patch label sparsity constraint based on the prior\nknowledge of distress distribution and leverage the Comprehensive Decision\nNetwork (CDN) to guide the training of PLIN in a weakly supervised way.\nTherefore, the patch labels produced by PLIN provide interpretable intermediate\ninformation, such as the rough location and the type of distress. We evaluate\nour method on a large-scale bituminous pavement distress dataset named CQU-BPDD\nand the augmented Crack500 (Crack500-PDD) dataset, which is a newly constructed\npavement distress detection dataset augmented from the Crack500. Extensive\nresults demonstrate the superiority of our method over baselines in both\nperformance and efficiency. The source codes of WSPLIN are released on\nhttps://github.com/DearCaat/wsplin.\n","authors":["Sheng Huang","Wenhao Tang","Guixin Huang","Luwen Huangfu","Dan Yang"],"pdf_url":"https://arxiv.org/pdf/2203.16782v2.pdf","comment":"Accepted by IEEE Transactions on Intelligent Transportation Systems"},{"id":"http://arxiv.org/abs/2304.00696v1","updated":"2023-04-03T03:07:26Z","published":"2023-04-03T03:07:26Z","title":"Thermal Spread Functions (TSF): Physics-guided Material Classification","summary":"  Robust and non-destructive material classification is a challenging but\ncrucial first-step in numerous vision applications. We propose a physics-guided\nmaterial classification framework that relies on thermal properties of the\nobject. Our key observation is that the rate of heating and cooling of an\nobject depends on the unique intrinsic properties of the material, namely the\nemissivity and diffusivity. We leverage this observation by gently heating the\nobjects in the scene with a low-power laser for a fixed duration and then\nturning it off, while a thermal camera captures measurements during the heating\nand cooling process. We then take this spatial and temporal \"thermal spread\nfunction\" (TSF) to solve an inverse heat equation using the finite-differences\napproach, resulting in a spatially varying estimate of diffusivity and\nemissivity. These tuples are then used to train a classifier that produces a\nfine-grained material label at each spatial pixel. Our approach is extremely\nsimple requiring only a small light source (low power laser) and a thermal\ncamera, and produces robust classification results with 86% accuracy over 16\nclasses.\n","authors":["Aniket Dashpute","Vishwanath Saragadam","Emma Alexander","Florian Willomitzer","Aggelos Katsaggelos","Ashok Veeraraghavan","Oliver Cossairt"],"pdf_url":"https://arxiv.org/pdf/2304.00696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16378v2","updated":"2023-04-03T03:00:46Z","published":"2023-03-29T01:24:25Z","title":"A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion","summary":"  Despite the record-breaking performance in Text-to-Image (T2I) generation by\nStable Diffusion, less research attention is paid to its adversarial\nrobustness. In this work, we study the problem of adversarial attack generation\nfor Stable Diffusion and ask if an adversarial text prompt can be obtained even\nin the absence of end-to-end model queries. We call the resulting problem\n'query-free attack generation'. To resolve this problem, we show that the\nvulnerability of T2I models is rooted in the lack of robustness of text\nencoders, e.g., the CLIP text encoder used for attacking Stable Diffusion.\nBased on such insight, we propose both untargeted and targeted query-free\nattacks, where the former is built on the most influential dimensions in the\ntext embedding space, which we call steerable key dimensions. By leveraging the\nproposed attacks, we empirically show that only a five-character perturbation\nto the text prompt is able to cause the significant content shift of\nsynthesized images using Stable Diffusion. Moreover, we show that the proposed\ntarget attack can precisely steer the diffusion model to scrub the targeted\nimage content without causing much change in untargeted image content. Our code\nis available at https://github.com/OPTML-Group/QF-Attack.\n","authors":["Haomin Zhuang","Yihua Zhang","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2303.16378v2.pdf","comment":"The 3rd Workshop of Adversarial Machine Learning on Computer Vision:\n  Art of Robustness"},{"id":"http://arxiv.org/abs/2210.16579v2","updated":"2023-04-03T02:58:58Z","published":"2022-10-29T11:54:58Z","title":"INR-V: A Continuous Representation Space for Video-based Generative\n  Tasks","summary":"  Generating videos is a complex task that is accomplished by generating a set\nof temporally coherent images frame-by-frame. This limits the expressivity of\nvideos to only image-based operations on the individual video frames needing\nnetwork designs to obtain temporally coherent trajectories in the underlying\nimage space. We propose INR-V, a video representation network that learns a\ncontinuous space for video-based generative tasks. INR-V parameterizes videos\nusing implicit neural representations (INRs), a multi-layered perceptron that\npredicts an RGB value for each input pixel location of the video. The INR is\npredicted using a meta-network which is a hypernetwork trained on neural\nrepresentations of multiple video instances. Later, the meta-network can be\nsampled to generate diverse novel videos enabling many downstream video-based\ngenerative tasks. Interestingly, we find that conditional regularization and\nprogressive weight initialization play a crucial role in obtaining INR-V. The\nrepresentation space learned by INR-V is more expressive than an image space\nshowcasing many interesting properties not possible with the existing works.\nFor instance, INR-V can smoothly interpolate intermediate videos between known\nvideo instances (such as intermediate identities, expressions, and poses in\nface videos). It can also in-paint missing portions in videos to recover\ntemporally coherent full videos. In this work, we evaluate the space learned by\nINR-V on diverse generative tasks such as video interpolation, novel video\ngeneration, video inversion, and video inpainting against the existing\nbaselines. INR-V significantly outperforms the baselines on several of these\ndemonstrated tasks, clearly showcasing the potential of the proposed\nrepresentation space.\n","authors":["Bipasha Sen","Aditya Agarwal","Vinay P Namboodiri","C. V. Jawahar"],"pdf_url":"https://arxiv.org/pdf/2210.16579v2.pdf","comment":"Published in Transactions on Machine Learning Research (10/2022);\n  https://openreview.net/forum?id=aIoEkwc2oB"},{"id":"http://arxiv.org/abs/2304.00690v1","updated":"2023-04-03T02:39:46Z","published":"2023-04-03T02:39:46Z","title":"3D Semantic Segmentation in the Wild: Learning Generalized Models for\n  Adverse-Condition Point Clouds","summary":"  Robust point cloud parsing under all-weather conditions is crucial to level-5\nautonomy in autonomous driving. However, how to learn a universal 3D semantic\nsegmentation (3DSS) model is largely neglected as most existing benchmarks are\ndominated by point clouds captured under normal weather. We introduce\nSemanticSTF, an adverse-weather point cloud dataset that provides dense\npoint-level annotations and allows to study 3DSS under various adverse weather\nconditions. We study all-weather 3DSS modeling under two setups: 1) domain\nadaptive 3DSS that adapts from normal-weather data to adverse-weather data; 2)\ndomain generalizable 3DSS that learns all-weather 3DSS models from\nnormal-weather data. Our studies reveal the challenge while existing 3DSS\nmethods encounter adverse-weather data, showing the great value of SemanticSTF\nin steering the future endeavor along this very meaningful research direction.\nIn addition, we design a domain randomization technique that alternatively\nrandomizes the geometry styles of point clouds and aggregates their embeddings,\nultimately leading to a generalizable model that can improve 3DSS under various\nadverse weather effectively. The SemanticSTF and related codes are available at\n\\url{https://github.com/xiaoaoran/SemanticSTF}.\n","authors":["Aoran Xiao","Jiaxing Huang","Weihao Xuan","Ruijie Ren","Kangcheng Liu","Dayan Guan","Abdulmotaleb El Saddik","Shijian Lu","Eric Xing"],"pdf_url":"https://arxiv.org/pdf/2304.00690v1.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2304.00689v1","updated":"2023-04-03T02:38:54Z","published":"2023-04-03T02:38:54Z","title":"Accuracy Improvement of Object Detection in VVC Coded Video Using\n  YOLO-v7 Features","summary":"  With advances in image recognition technology based on deep learning,\nautomatic video analysis by Artificial Intelligence is becoming more\nwidespread. As the amount of video used for image recognition increases,\nefficient compression methods for such video data are necessary. In general,\nwhen the image quality deteriorates due to image encoding, the image\nrecognition accuracy also falls. Therefore, in this paper, we propose a\nneural-network-based approach to improve image recognition accuracy, especially\nthe object detection accuracy by applying post-processing to the encoded video.\nVersatile Video Coding (VVC) will be used for the video compression method,\nsince it is the latest video coding method with the best encoding performance.\nThe neural network is trained using the features of YOLO-v7, the latest object\ndetection model. By using VVC as the video coding method and YOLO-v7 as the\ndetection model, high object detection accuracy is achieved even at low bit\nrates. Experimental results show that the combination of the proposed method\nand VVC achieves better coding performance than regular VVC in object detection\naccuracy.\n","authors":["Takahiro Shindo","Taiju Watanabe","Kein Yamada","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2304.00689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17774v2","updated":"2023-04-03T02:36:17Z","published":"2023-03-31T02:37:36Z","title":"Semi-Weakly Supervised Object Kinematic Motion Prediction","summary":"  Given a 3D object, kinematic motion prediction aims to identify the mobile\nparts as well as the corresponding motion parameters. Due to the large\nvariations in both topological structure and geometric details of 3D objects,\nthis remains a challenging task and the lack of large scale labeled data also\nconstrain the performance of deep learning based approaches. In this paper, we\ntackle the task of object kinematic motion prediction problem in a semi-weakly\nsupervised manner. Our key observations are two-fold. First, although 3D\ndataset with fully annotated motion labels is limited, there are existing\ndatasets and methods for object part semantic segmentation at large scale.\nSecond, semantic part segmentation and mobile part segmentation is not always\nconsistent but it is possible to detect the mobile parts from the underlying 3D\nstructure. Towards this end, we propose a graph neural network to learn the map\nbetween hierarchical part-level segmentation and mobile parts parameters, which\nare further refined based on geometric alignment. This network can be first\ntrained on PartNet-Mobility dataset with fully labeled mobility information and\nthen applied on PartNet dataset with fine-grained and hierarchical part-level\nsegmentation. The network predictions yield a large scale of 3D objects with\npseudo labeled mobility information and can further be used for\nweakly-supervised learning with pre-existing segmentation. Our experiments show\nthere are significant performance boosts with the augmented data for previous\nmethod designed for kinematic motion prediction on 3D partial scans.\n","authors":["Gengxin Liu","Qian Sun","Haibin Huang","Chongyang Ma","Yulan Guo","Li Yi","Hui Huang","Ruizhen Hu"],"pdf_url":"https://arxiv.org/pdf/2303.17774v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2304.00685v1","updated":"2023-04-03T02:17:05Z","published":"2023-04-03T02:17:05Z","title":"Vision-Language Models for Vision Tasks: A Survey","summary":"  Most visual recognition studies rely heavily on crowd-labelled data in deep\nneural networks (DNNs) training, and they usually train a DNN for each single\nvisual recognition task, leading to a laborious and time-consuming visual\nrecognition paradigm. To address the two challenges, Vision-Language Models\n(VLMs) have been intensively investigated recently, which learns rich\nvision-language correlation from web-scale image-text pairs that are almost\ninfinitely available on the Internet and enables zero-shot predictions on\nvarious visual recognition tasks with a single VLM. This paper provides a\nsystematic review of visual language models for various visual recognition\ntasks, including: (1) the background that introduces the development of visual\nrecognition paradigms; (2) the foundations of VLM that summarize the\nwidely-adopted network architectures, pre-training objectives, and downstream\ntasks; (3) the widely-adopted datasets in VLM pre-training and evaluations; (4)\nthe review and categorization of existing VLM pre-training methods, VLM\ntransfer learning methods, and VLM knowledge distillation methods; (5) the\nbenchmarking, analysis and discussion of the reviewed methods; (6) several\nresearch challenges and potential research directions that could be pursued in\nthe future VLM studies for visual recognition. A project associated with this\nsurvey has been created at https://github.com/jingyi0000/VLM_survey.\n","authors":["Jingyi Zhang","Jiaxing Huang","Sheng Jin","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2304.00685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11537v2","updated":"2023-04-03T01:51:55Z","published":"2023-03-21T02:07:36Z","title":"Interactive Geometry Editing of Neural Radiance Fields","summary":"  In this paper, we propose a method that enables interactive geometry editing\nfor neural radiance fields manipulation. We use two proxy cages(inner cage and\nouter cage) to edit a scene. The inner cage defines the operation target, and\nthe outer cage defines the adjustment space. Various operations apply to the\ntwo cages. After cage selection, operations on the inner cage lead to the\ndesired transformation of the inner cage and adjustment of the outer cage.\nUsers can edit the scene with translation, rotation, scaling, or combinations.\nThe operations on the corners and edges of the cage are also supported. Our\nmethod does not need any explicit 3D geometry representations. The interactive\ngeometry editing applies directly to the implicit neural radiance fields.\nExtensive experimental results demonstrate the effectiveness of our approach.\n","authors":["Shaoxu Li","Ye Pan"],"pdf_url":"https://arxiv.org/pdf/2303.11537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12051v3","updated":"2023-04-03T01:14:24Z","published":"2022-11-22T06:54:27Z","title":"Adaptive Dynamic Filtering Network for Image Denoising","summary":"  In image denoising networks, feature scaling is widely used to enlarge the\nreceptive field size and reduce computational costs. This practice, however,\nalso leads to the loss of high-frequency information and fails to consider\nwithin-scale characteristics. Recently, dynamic convolution has exhibited\npowerful capabilities in processing high-frequency information (e.g., edges,\ncorners, textures), but previous works lack sufficient spatial contextual\ninformation in filter generation. To alleviate these issues, we propose to\nemploy dynamic convolution to improve the learning of high-frequency and\nmulti-scale features. Specifically, we design a spatially enhanced kernel\ngeneration (SEKG) module to improve dynamic convolution, enabling the learning\nof spatial context information with a very low computational complexity. Based\non the SEKG module, we propose a dynamic convolution block (DCB) and a\nmulti-scale dynamic convolution block (MDCB). The former enhances the\nhigh-frequency information via dynamic convolution and preserves low-frequency\ninformation via skip connections. The latter utilizes shared adaptive dynamic\nkernels and the idea of dilated convolution to achieve efficient multi-scale\nfeature extraction. The proposed multi-dimension feature integration (MFI)\nmechanism further fuses the multi-scale features, providing precise and\ncontextually enriched feature representations. Finally, we build an efficient\ndenoising network with the proposed DCB and MDCB, named ADFNet. It achieves\nbetter performance with low computational complexity on real-world and\nsynthetic Gaussian noisy datasets. The source code is available at\nhttps://github.com/it-hao/ADFNet.\n","authors":["Hao Shen","Zhong-Qiu Zhao","Wandi Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.12051v3.pdf","comment":"9 pages, Accepted in AAAI Conference on Artificial Intelligence\n  (AAAI) 2023"},{"id":"http://arxiv.org/abs/2304.00673v1","updated":"2023-04-03T00:59:31Z","published":"2023-04-03T00:59:31Z","title":"Partial-View Object View Synthesis via Filtered Inversion","summary":"  We propose Filtering Inversion (FINV), a learning framework and optimization\nprocess that predicts a renderable 3D object representation from one or few\npartial views. FINV addresses the challenge of synthesizing novel views of\nobjects from partial observations, spanning cases where the object is not\nentirely in view, is partially occluded, or is only observed from similar\nviews. To achieve this, FINV learns shape priors by training a 3D generative\nmodel. At inference, given one or more views of a novel real-world object, FINV\nfirst finds a set of latent codes for the object by inverting the generative\nmodel from multiple initial seeds. Maintaining the set of latent codes, FINV\nfilters and resamples them after receiving each new observation, akin to\nparticle filtering. The generator is then finetuned for each latent code on the\navailable views in order to adapt to novel objects. We show that FINV\nsuccessfully synthesizes novel views of real-world objects (e.g., chairs,\ntables, and cars), even if the generative prior is trained only on synthetic\nobjects. The ability to address the sim-to-real problem allows FINV to be used\nfor object categories without real-world datasets. FINV achieves\nstate-of-the-art performance on multiple real-world datasets, recovers object\nshape and texture from partial and sparse views, is robust to occlusion, and is\nable to incrementally improve its representation with more observations.\n","authors":["Fan-Yun Sun","Jonathan Tremblay","Valts Blukis","Kevin Lin","Danfei Xu","Boris Ivanovic","Peter Karkus","Stan Birchfield","Dieter Fox","Ruohan Zhang","Yunzhu Li","Jiajun Wu","Marco Pavone","Nick Haber"],"pdf_url":"https://arxiv.org/pdf/2304.00673v1.pdf","comment":"project website: http://cs.stanford.edu/~sunfanyun/finv"},{"id":"http://arxiv.org/abs/2304.00670v1","updated":"2023-04-03T00:47:37Z","published":"2023-04-03T00:47:37Z","title":"CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception","summary":"  Autonomous driving requires an accurate and fast 3D perception system that\nincludes 3D object detection, tracking, and segmentation. Although recent\nlow-cost camera-based approaches have shown promising results, they are\nsusceptible to poor illumination or bad weather conditions and have a large\nlocalization error. Hence, fusing camera with low-cost radar, which provides\nprecise long-range measurement and operates reliably in all environments, is\npromising but has not yet been thoroughly investigated. In this paper, we\npropose Camera Radar Net (CRN), a novel camera-radar fusion framework that\ngenerates a semantically rich and spatially accurate bird's-eye-view (BEV)\nfeature map for various tasks. To overcome the lack of spatial information in\nan image, we transform perspective view image features to BEV with the help of\nsparse but accurate radar points. We further aggregate image and radar feature\nmaps in BEV using multi-modal deformable attention designed to tackle the\nspatial misalignment between inputs. CRN with real-time setting operates at 20\nFPS while achieving comparable performance to LiDAR detectors on nuScenes, and\neven outperforms at a far distance on 100m setting. Moreover, CRN with offline\nsetting yields 62.4% NDS, 57.5% mAP on nuScenes test set and ranks first among\nall camera and camera-radar 3D object detectors.\n","authors":["Youngseok Kim","Sanmin Kim","Juyeb Shin","Jun Won Choi","Dongsuk Kum"],"pdf_url":"https://arxiv.org/pdf/2304.00670v1.pdf","comment":"International Conference on Learning Representations 2023 Workshop on\n  Scene Representations for Autonomous Driving (ICLR'23 SR4AD)"},{"id":"http://arxiv.org/abs/2304.00668v1","updated":"2023-04-03T00:45:11Z","published":"2023-04-03T00:45:11Z","title":"Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR","summary":"  Synthetic aperture radar automatic target recognition (SAR ATR) is one of the\ncritical technologies for SAR image interpretation, which has an important\napplication prospect in military and civilian fields. Deep learning has been\nwidely used in this area and achieved an excellent recognition rate on the\nbenchmark dataset in recent years. However, the benchmark dataset suffers from\ndata selection bias due to a single data collection condition. This data bias\nenhances deep learning models to overfit non-causal background clutter.\nMoreover, existing methods qualitatively analyze the model causality and do not\ndeeply analyze this data bias. In this paper, we explicitly show that the data\nselection bias leads to the non-causality of the model and spurious correlation\nof clutter. First, we quantify the contribution of the target, clutter, and\nshadow regions during the training process through the Shapley value. The\nclutter contribution has a large proportion during the training process.\nSecond, the causes of the non-causality of deep learning in SAR ATR include\ndata selection bias and model texture bias. Data selection bias results in\nclass-related clutter and false feature representation. Furthermore, the\nspurious correlation of clutter arises from the similar signal-to-clutter\nratios (SCR) between the training and test sets. Finally, we propose a random\nSCR re-weighting method to reduce the overfitting for clutter. However, the\nmodel texture bias increases with model complexity after removing data bias.\nThe experimental results of different models under the standard operating\ncondition of the benchmark MSTAR dataset prove the above conclusions.\n","authors":["Weijie Li","Wei Yang","Li Liu","Wenpeng Zhang","Yongxiang Liu"],"pdf_url":"https://arxiv.org/pdf/2304.00668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06299v2","updated":"2023-04-03T00:29:03Z","published":"2022-12-13T00:45:46Z","title":"Interpretable Diabetic Retinopathy Diagnosis based on Biomarker\n  Activation Map","summary":"  Deep learning classifiers provide the most accurate means of automatically\ndiagnosing diabetic retinopathy (DR) based on optical coherence tomography\n(OCT) and its angiography (OCTA). The power of these models is attributable in\npart to the inclusion of hidden layers that provide the complexity required to\nachieve a desired task. However, hidden layers also render algorithm outputs\ndifficult to interpret. Here we introduce a novel biomarker activation map\n(BAM) framework based on generative adversarial learning that allows clinicians\nto verify and understand classifiers decision-making. A data set including 456\nmacular scans were graded as non-referable or referable DR based on current\nclinical standards. A DR classifier that was used to evaluate our BAM was first\ntrained based on this data set. The BAM generation framework was designed by\ncombing two U-shaped generators to provide meaningful interpretability to this\nclassifier. The main generator was trained to take referable scans as input and\nproduce an output that would be classified by the classifier as non-referable.\nThe BAM is then constructed as the difference image between the output and\ninput of the main generator. To ensure that the BAM only highlights\nclassifier-utilized biomarkers an assistant generator was trained to do the\nopposite, producing scans that would be classified as referable by the\nclassifier from non-referable scans. The generated BAMs highlighted known\npathologic features including nonperfusion area and retinal fluid. A fully\ninterpretable classifier based on these highlights could help clinicians better\nutilize and verify automated DR diagnosis.\n","authors":["Pengxiao Zang","Tristan T. Hormel","Jie Wang","Yukun Guo","Steven T. Bailey","Christina J. Flaxel","David Huang","Thomas S. Hwang","Yali Jia"],"pdf_url":"https://arxiv.org/pdf/2212.06299v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2008.09269v2","updated":"2023-04-03T23:03:54Z","published":"2020-08-21T02:22:06Z","title":"Beyond Fixed Grid: Learning Geometric Image Representation with a\n  Deformable Grid","summary":"  In modern computer vision, images are typically represented as a fixed\nuniform grid with some stride and processed via a deep convolutional neural\nnetwork. We argue that deforming the grid to better align with the\nhigh-frequency image content is a more effective strategy. We introduce\n\\emph{Deformable Grid} DefGrid, a learnable neural network module that predicts\nlocation offsets of vertices of a 2-dimensional triangular grid, such that the\nedges of the deformed grid align with image boundaries. We showcase our DefGrid\nin a variety of use cases, i.e., by inserting it as a module at various levels\nof processing. We utilize DefGrid as an end-to-end \\emph{learnable geometric\ndownsampling} layer that replaces standard pooling methods for reducing feature\nresolution when feeding images into a deep CNN. We show significantly improved\nresults at the same grid resolution compared to using CNNs on uniform grids for\nthe task of semantic segmentation. We also utilize DefGrid at the output layers\nfor the task of object mask annotation, and show that reasoning about object\nboundaries on our predicted polygonal grid leads to more accurate results over\nexisting pixel-wise and curve-based approaches. We finally showcase DefGrid as\na standalone module for unsupervised image partitioning, showing superior\nperformance over existing approaches. Project website:\nhttp://www.cs.toronto.edu/~jungao/def-grid\n","authors":["Jun Gao","Zian Wang","Jinchen Xuan","Sanja Fidler"],"pdf_url":"https://arxiv.org/pdf/2008.09269v2.pdf","comment":"ECCV 2020"},{"id":"http://arxiv.org/abs/2203.17269v2","updated":"2023-04-03T22:49:29Z","published":"2022-03-31T17:59:00Z","title":"A Closer Look at Rehearsal-Free Continual Learning","summary":"  Continual learning is a setting where machine learning models learn novel\nconcepts from continuously shifting training data, while simultaneously\navoiding degradation of knowledge on previously seen classes which may\ndisappear from the training data for extended periods of time (a phenomenon\nknown as the catastrophic forgetting problem). Current approaches for continual\nlearning of a single expanding task (aka class-incremental continual learning)\nrequire extensive rehearsal of previously seen data to avoid this degradation\nof knowledge. Unfortunately, rehearsal comes at a cost to memory, and it may\nalso violate data-privacy. Instead, we explore combining knowledge distillation\nand parameter regularization in new ways to achieve strong continual learning\nperformance without rehearsal. Specifically, we take a deep dive into common\ncontinual learning techniques: prediction distillation, feature distillation,\nL2 parameter regularization, and EWC parameter regularization. We first\ndisprove the common assumption that parameter regularization techniques fail\nfor rehearsal-free continual learning of a single, expanding task. Next, we\nexplore how to leverage knowledge from a pre-trained model in rehearsal-free\ncontinual learning and find that vanilla L2 parameter regularization\noutperforms EWC parameter regularization and feature distillation. Finally, we\nexplore the recently popular ImageNet-R benchmark, and show that L2 parameter\nregularization implemented in self-attention blocks of a ViT transformer\noutperforms recent popular prompting for continual learning methods.\n","authors":["James Seale Smith","Junjiao Tian","Shaunak Halbe","Yen-Chang Hsu","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2203.17269v2.pdf","comment":"Accepted by the 2023 IEEE/CVF Conference on Computer Vision and\n  Pattern (CVPR) Workshop on Continual Learning in Computer Vision (CLVision\n  2023)"},{"id":"http://arxiv.org/abs/2304.01401v1","updated":"2023-04-03T22:34:07Z","published":"2023-04-03T22:34:07Z","title":"U-Netmer: U-Net meets Transformer for medical image segmentation","summary":"  The combination of the U-Net based deep learning models and Transformer is a\nnew trend for medical image segmentation. U-Net can extract the detailed local\nsemantic and texture information and Transformer can learn the long-rang\ndependencies among pixels in the input image. However, directly adapting the\nTransformer for segmentation has ``token-flatten\" problem (flattens the local\npatches into 1D tokens which losses the interaction among pixels within local\npatches) and ``scale-sensitivity\" problem (uses a fixed scale to split the\ninput image into local patches). Compared to directly combining U-Net and\nTransformer, we propose a new global-local fashion combination of U-Net and\nTransformer, named U-Netmer, to solve the two problems. The proposed U-Netmer\nsplits an input image into local patches. The global-context information among\nlocal patches is learnt by the self-attention mechanism in Transformer and\nU-Net segments each local patch instead of flattening into tokens to solve the\n`token-flatten\" problem. The U-Netmer can segment the input image with\ndifferent patch sizes with the identical structure and the same parameter.\nThus, the U-Netmer can be trained with different patch sizes to solve the\n``scale-sensitivity\" problem. We conduct extensive experiments in 7 public\ndatasets on 7 organs (brain, heart, breast, lung, polyp, pancreas and prostate)\nand 4 imaging modalities (MRI, CT, ultrasound, and endoscopy) to show that the\nproposed U-Netmer can be generally applied to improve accuracy of medical image\nsegmentation. These experimental results show that U-Netmer provides\nstate-of-the-art performance compared to baselines and other models. In\naddition, the discrepancy among the outputs of U-Netmer with different scales\nis linearly correlated to the segmentation accuracy which can be considered as\na confidence score to rank test images by difficulty without ground-truth.\n","authors":["Sheng He","Rina Bao","P. Ellen Grant","Yangming Ou"],"pdf_url":"https://arxiv.org/pdf/2304.01401v1.pdf","comment":"10 pages, 5 figures, under review"},{"id":"http://arxiv.org/abs/2304.01399v1","updated":"2023-04-03T22:30:08Z","published":"2023-04-03T22:30:08Z","title":"Fine-tuning of explainable CNNs for skin lesion classification based on\n  dermatologists' feedback towards increasing trust","summary":"  In this paper, we propose a CNN fine-tuning method which enables users to\ngive simultaneous feedback on two outputs: the classification itself and the\nvisual explanation for the classification. We present the effect of this\nfeedback strategy in a skin lesion classification task and measure how CNNs\nreact to the two types of user feedback. To implement this approach, we propose\na novel CNN architecture that integrates the Grad-CAM technique for explaining\nthe model's decision in the training loop. Using simulated user feedback, we\nfound that fine-tuning our model on both classification and explanation\nimproves visual explanation while preserving classification accuracy, thus\npotentially increasing the trust of users in using CNN-based skin lesion\nclassifiers.\n","authors":["Md Abdul Kadir","Fabrizio Nunnari","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2304.01399v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2304.01169v1","updated":"2023-04-03T17:37:14Z","published":"2023-04-03T17:37:14Z","title":"Click-aware Structure Transfer with Sample Weight Assignment for\n  Post-Click Conversion Rate Estimation","summary":"  Post-click Conversion Rate (CVR) prediction task plays an essential role in\nindustrial applications, such as recommendation and advertising. Conventional\nCVR methods typically suffer from the data sparsity problem as they rely only\non samples where the user has clicked. To address this problem, researchers\nhave introduced the method of multi-task learning, which utilizes non-clicked\nsamples and shares feature representations of the Click-Through Rate (CTR) task\nwith the CVR task. However, it should be noted that the CVR and CTR tasks are\nfundamentally different and may even be contradictory. Therefore, introducing a\nlarge amount of CTR information without distinction may drown out valuable\ninformation related to CVR. This phenomenon is called the curse of knowledge\nproblem in this paper. To tackle this issue, we argue that a trade-off should\nbe achieved between the introduction of large amounts of auxiliary information\nand the protection of valuable information related to CVR. Hence, we propose a\nClick-aware Structure Transfer model with sample Weight Assignment, abbreviated\nas CSTWA. It pays more attention to the latent structure information, which can\nfilter the input information that is related to CVR, instead of directly\nsharing feature representations. Meanwhile, to capture the representation\nconflict between CTR and CVR, we calibrate the representation layer and\nreweight the discriminant layer to excavate the click bias information from the\nCTR tower. Moreover, it incorporates a sample weight assignment algorithm\nbiased towards CVR modeling, to make the knowledge from CTR would not mislead\nthe CVR. Extensive experiments on industrial and public datasets have\ndemonstrated that CSTWA significantly outperforms widely used and competitive\nmodels.\n","authors":["Kai Ouyang","Wenhao Zheng","Chen Tang","Xuanji Xiao","Hai-Tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2304.01169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01019v1","updated":"2023-04-03T14:17:00Z","published":"2023-04-03T14:17:00Z","title":"Simple Yet Effective Neural Ranking and Reranking Baselines for\n  Cross-Lingual Information Retrieval","summary":"  The advent of multilingual language models has generated a resurgence of\ninterest in cross-lingual information retrieval (CLIR), which is the task of\nsearching documents in one language with queries from another. However, the\nrapid pace of progress has led to a confusing panoply of methods and\nreproducibility has lagged behind the state of the art. In this context, our\nwork makes two important contributions: First, we provide a conceptual\nframework for organizing different approaches to cross-lingual retrieval using\nmulti-stage architectures for mono-lingual retrieval as a scaffold. Second, we\nimplement simple yet effective reproducible baselines in the Anserini and\nPyserini IR toolkits for test collections from the TREC 2022 NeuCLIR Track, in\nPersian, Russian, and Chinese. Our efforts are built on a collaboration of the\ntwo teams that submitted the most effective runs to the TREC evaluation. These\ncontributions provide a firm foundation for future advances.\n","authors":["Jimmy Lin","David Alfonso-Hermelo","Vitor Jeronymo","Ehsan Kamalloo","Carlos Lassance","Rodrigo Nogueira","Odunayo Ogundepo","Mehdi Rezagholizadeh","Nandan Thakur","Jheng-Hong Yang","Xinyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00969v1","updated":"2023-04-03T13:40:08Z","published":"2023-04-03T13:40:08Z","title":"Is More Always Better? The Effects of Personal Characteristics and Level\n  of Detail on the Perception of Explanations in a Recommender System","summary":"  Despite the acknowledgment that the perception of explanations may vary\nconsiderably between end-users, explainable recommender systems (RS) have\ntraditionally followed a one-size-fits-all model, whereby the same explanation\nlevel of detail is provided to each user, without taking into consideration\nindividual user's context, i.e., goals and personal characteristics. To fill\nthis research gap, we aim in this paper at a shift from a one-size-fits-all to\na personalized approach to explainable recommendation by giving users agency in\ndeciding which explanation they would like to see. We developed a transparent\nRecommendation and Interest Modeling Application (RIMA) that provides on-demand\npersonalized explanations of the recommendations, with three levels of detail\n(basic, intermediate, advanced) to meet the demands of different types of\nend-users. We conducted a within-subject study (N=31) to investigate the\nrelationship between user's personal characteristics and the explanation level\nof detail, and the effects of these two variables on the perception of the\nexplainable RS with regard to different explanation goals. Our results show\nthat the perception of explainable RS with different levels of detail is\naffected to different degrees by the explanation goal and user type.\nConsequently, we suggested some theoretical and design guidelines to support\nthe systematic design of explanatory interfaces in RS tailored to the user's\ncontext.\n","authors":["Mohamed Amine Chatti","Mouadh Guesmi","Laura Vorgerd","Thao Ngo","Shoeb Joarder","Qurat Ul Ain","Arham Muslim"],"pdf_url":"https://arxiv.org/pdf/2304.00969v1.pdf","comment":"Proceedings of the 30th ACM Conference on User Modeling, Adaptation\n  and Personalization (UMAP'22)"},{"id":"http://arxiv.org/abs/2304.00902v1","updated":"2023-04-03T11:46:30Z","published":"2023-04-03T11:46:30Z","title":"FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction","summary":"  Click-through rate (CTR) prediction is one of the fundamental tasks for\nonline advertising and recommendation. While multi-layer perceptron (MLP)\nserves as a core component in many deep CTR prediction models, it has been\nwidely recognized that applying a vanilla MLP network alone is inefficient in\nlearning multiplicative feature interactions. As such, many two-stream\ninteraction models (e.g., DeepFM and DCN) have been proposed by integrating an\nMLP network with another dedicated network for enhanced CTR prediction. As the\nMLP stream learns feature interactions implicitly, existing research focuses\nmainly on enhancing explicit feature interactions in the complementary stream.\nIn contrast, our empirical study shows that a well-tuned two-stream MLP model\nthat simply combines two MLPs can even achieve surprisingly good performance,\nwhich has never been reported before by existing work. Based on this\nobservation, we further propose feature selection and interaction aggregation\nlayers that can be easily plugged to make an enhanced two-stream MLP model,\nFinalMLP. In this way, it not only enables differentiated feature inputs but\nalso effectively fuses stream-level interactions across two streams. Our\nevaluation results on four open benchmark datasets as well as an online A/B\ntest in our industrial system show that FinalMLP achieves better performance\nthan many sophisticated two-stream CTR models. Our source code will be\navailable at MindSpore/models and FuxiCTR/model_zoo.\n","authors":["Kelong Mao","Jieming Zhu","Liangcai Su","Guohao Cai","Yuru Li","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2304.00902v1.pdf","comment":"Accepted by AAAI 2023. Code available at\n  https://github.com/xue-pai/FinalMLP"},{"id":"http://arxiv.org/abs/2207.00494v3","updated":"2023-04-03T11:09:07Z","published":"2022-07-01T15:30:10Z","title":"Learning Job Titles Similarity from Noisy Skill Labels","summary":"  Measuring semantic similarity between job titles is an essential\nfunctionality for automatic job recommendations. This task is usually\napproached using supervised learning techniques, which requires training data\nin the form of equivalent job title pairs. In this paper, we instead propose an\nunsupervised representation learning method for training a job title similarity\nmodel using noisy skill labels. We show that it is highly effective for tasks\nsuch as text ranking and job normalization.\n","authors":["Rabih Zbib","Lucas Alvarez Lacasa","Federico Retyk","Rus Poves","Juan Aizpuru","Hermenegildo Fabregat","Vaidotas Simkus","Emilia García-Casademont"],"pdf_url":"https://arxiv.org/pdf/2207.00494v3.pdf","comment":"Accepted to the International workshop on Fair, Effective And\n  Sustainable Talent management using data science (FEAST) as part of ECML-PKDD\n  2022"},{"id":"http://arxiv.org/abs/2304.00686v1","updated":"2023-04-03T02:22:01Z","published":"2023-04-03T02:22:01Z","title":"DiffuRec: A Diffusion Model for Sequential Recommendation","summary":"  Mainstream solutions to Sequential Recommendation (SR) represent items with\nfixed vectors. These vectors have limited capability in capturing items' latent\naspects and users' diverse preferences. As a new generative paradigm, Diffusion\nmodels have achieved excellent performance in areas like computer vision and\nnatural language processing. To our understanding, its unique merit in\nrepresentation generation well fits the problem setting of sequential\nrecommendation. In this paper, we make the very first attempt to adapt\nDiffusion model to SR and propose DiffuRec, for item representation\nconstruction and uncertainty injection. Rather than modeling item\nrepresentations as fixed vectors, we represent them as distributions in\nDiffuRec, which reflect user's multiple interests and item's various aspects\nadaptively. In diffusion phase, DiffuRec corrupts the target item embedding\ninto a Gaussian distribution via noise adding, which is further applied for\nsequential item distribution representation generation and uncertainty\ninjection. Afterwards, the item representation is fed into an Approximator for\ntarget item representation reconstruction. In reversion phase, based on user's\nhistorical interaction behaviors, we reverse a Gaussian noise into the target\nitem representation, then apply rounding operation for target item prediction.\nExperiments over four datasets show that DiffuRec outperforms strong baselines\nby a large margin.\n","authors":["Zihao Li","Aixin Sun","Chenliang Li"],"pdf_url":"https://arxiv.org/pdf/2304.00686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10658v4","updated":"2023-04-03T00:28:34Z","published":"2022-06-21T18:16:31Z","title":"Questions Are All You Need to Train a Dense Passage Retriever","summary":"  We introduce ART, a new corpus-level autoencoding approach for training dense\nretrieval models that does not require any labeled training data. Dense\nretrieval is a central challenge for open-domain tasks, such as Open QA, where\nstate-of-the-art methods typically require large supervised datasets with\ncustom hard-negative mining and denoising of positive examples. ART, in\ncontrast, only requires access to unpaired inputs and outputs (e.g. questions\nand potential answer documents). It uses a new document-retrieval autoencoding\nscheme, where (1) an input question is used to retrieve a set of evidence\ndocuments, and (2) the documents are then used to compute the probability of\nreconstructing the original question. Training for retrieval based on question\nreconstruction enables effective unsupervised learning of both document and\nquestion encoders, which can be later incorporated into complete Open QA\nsystems without any further finetuning. Extensive experiments demonstrate that\nART obtains state-of-the-art results on multiple QA retrieval benchmarks with\nonly generic initialization from a pre-trained language model, removing the\nneed for labeled data and task-specific losses.\n","authors":["Devendra Singh Sachan","Mike Lewis","Dani Yogatama","Luke Zettlemoyer","Joelle Pineau","Manzil Zaheer"],"pdf_url":"https://arxiv.org/pdf/2206.10658v4.pdf","comment":"Accepted to TACL, pre MIT Press publication version"},{"id":"http://arxiv.org/abs/2204.07496v4","updated":"2023-04-03T00:07:58Z","published":"2022-04-15T14:51:41Z","title":"Improving Passage Retrieval with Zero-Shot Question Generation","summary":"  We propose a simple and effective re-ranking method for improving passage\nretrieval in open question answering. The re-ranker re-scores retrieved\npassages with a zero-shot question generation model, which uses a pre-trained\nlanguage model to compute the probability of the input question conditioned on\na retrieved passage. This approach can be applied on top of any retrieval\nmethod (e.g. neural or keyword-based), does not require any domain- or\ntask-specific training (and therefore is expected to generalize better to data\ndistribution shifts), and provides rich cross-attention between query and\npassage (i.e. it must explain every token in the question). When evaluated on a\nnumber of open-domain retrieval datasets, our re-ranker improves strong\nunsupervised retrieval models by 6%-18% absolute and strong supervised models\nby up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new\nstate-of-the-art results on full open-domain question answering by simply\nadding the new re-ranker to existing models with no further changes.\n","authors":["Devendra Singh Sachan","Mike Lewis","Mandar Joshi","Armen Aghajanyan","Wen-tau Yih","Joelle Pineau","Luke Zettlemoyer"],"pdf_url":"https://arxiv.org/pdf/2204.07496v4.pdf","comment":"EMNLP 2022 camera-ready version. Code is available at:\n  https://github.com/DevSinghSachan/unsupervised-passage-reranking"},{"id":"http://arxiv.org/abs/2304.01352v1","updated":"2023-04-03T20:27:10Z","published":"2023-04-03T20:27:10Z","title":"A Simple and Effective Method of Cross-Lingual Plagiarism Detection","summary":"  We present a simple cross-lingual plagiarism detection method applicable to a\nlarge number of languages. The presented approach leverages open multilingual\nthesauri for candidate retrieval task and pre-trained multilingual BERT-based\nlanguage models for detailed analysis. The method does not rely on machine\ntranslation and word sense disambiguation when in use, and therefore is\nsuitable for a large number of languages, including under-resourced languages.\nThe effectiveness of the proposed approach is demonstrated for several existing\nand new benchmarks, achieving state-of-the-art results for French, Russian, and\nArmenian languages.\n","authors":["Karen Avetisyan","Arthur Malajyan","Tsolak Ghukasyan"],"pdf_url":"https://arxiv.org/pdf/2304.01352v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2304.01203v1","updated":"2023-04-03T17:59:58Z","published":"2023-04-03T17:59:58Z","title":"Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning","summary":"  In goal-reaching reinforcement learning (RL), the optimal value function has\na particular geometry, called quasimetric structure. This paper introduces\nQuasimetric Reinforcement Learning (QRL), a new RL method that utilizes\nquasimetric models to learn optimal value functions. Distinct from prior\napproaches, the QRL objective is specifically designed for quasimetrics, and\nprovides strong theoretical recovery guarantees. Empirically, we conduct\nthorough analyses on a discretized MountainCar environment, identifying\nproperties of QRL and its advantages over alternatives. On offline and online\ngoal-reaching benchmarks, QRL also demonstrates improved sample efficiency and\nperformance, across both state-based and image-based observations.\n","authors":["Tongzhou Wang","Antonio Torralba","Phillip Isola","Amy Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.04522v2","updated":"2023-04-03T17:53:20Z","published":"2021-09-09T19:08:56Z","title":"Asynchronous Iterations in Optimization: New Sequence Results and\n  Sharper Algorithmic Guarantees","summary":"  We introduce novel convergence results for asynchronous iterations that\nappear in the analysis of parallel and distributed optimization algorithms. The\nresults are simple to apply and give explicit estimates for how the degree of\nasynchrony impacts the convergence rates of the iterates. Our results shorten,\nstreamline and strengthen existing convergence proofs for several asynchronous\noptimization methods and allow us to establish convergence guarantees for\npopular algorithms that were thus far lacking a complete theoretical\nunderstanding. Specifically, we use our results to derive better iteration\ncomplexity bounds for proximal incremental aggregated gradient methods, to\nobtain tighter guarantees depending on the average rather than maximum delay\nfor the asynchronous stochastic gradient descent method, to provide less\nconservative analyses of the speedup conditions for asynchronous\nblock-coordinate implementations of Krasnoselskii-Mann iterations, and to\nquantify the convergence rates for totally asynchronous iterations under\nvarious assumptions on communication delays and update rates.\n","authors":["Hamid Reza Feyzmahdavian","Mikael Johansson"],"pdf_url":"https://arxiv.org/pdf/2109.04522v2.pdf","comment":"62 pages, 1 Figure"},{"id":"http://arxiv.org/abs/2303.14186v2","updated":"2023-04-03T17:37:50Z","published":"2023-03-24T17:56:22Z","title":"TRAK: Attributing Model Behavior at Scale","summary":"  The goal of data attribution is to trace model predictions back to training\ndata. Despite a long line of work towards this goal, existing approaches to\ndata attribution tend to force users to choose between computational\ntractability and efficacy. That is, computationally tractable methods can\nstruggle with accurately attributing model predictions in non-convex settings\n(e.g., in the context of deep neural networks), while methods that are\neffective in such regimes require training thousands of models, which makes\nthem impractical for large models or datasets.\n  In this work, we introduce TRAK (Tracing with the Randomly-projected After\nKernel), a data attribution method that is both effective and computationally\ntractable for large-scale, differentiable models. In particular, by leveraging\nonly a handful of trained models, TRAK can match the performance of attribution\nmethods that require training thousands of models. We demonstrate the utility\nof TRAK across various modalities and scales: image classifiers trained on\nImageNet, vision-language models (CLIP), and language models (BERT and mT5). We\nprovide code for using TRAK (and reproducing our work) at\nhttps://github.com/MadryLab/trak .\n","authors":["Sung Min Park","Kristian Georgiev","Andrew Ilyas","Guillaume Leclerc","Aleksander Madry"],"pdf_url":"https://arxiv.org/pdf/2303.14186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12474v3","updated":"2023-04-03T17:37:27Z","published":"2022-12-23T17:02:59Z","title":"Physics-Informed Gaussian Process Regression Generalizes Linear PDE\n  Solvers","summary":"  Linear partial differential equations (PDEs) are an important, widely applied\nclass of mechanistic models, describing physical processes such as heat\ntransfer, electromagnetism, and wave propagation. In practice, specialized\nnumerical methods based on discretization are used to solve PDEs. They\ngenerally use an estimate of the unknown model parameters and, if available,\nphysical measurements for initialization. Such solvers are often embedded into\nlarger scientific models with a downstream application and thus error\nquantification plays a key role. However, by ignoring parameter and measurement\nuncertainty, classical PDE solvers may fail to produce consistent estimates of\ntheir inherent approximation error. In this work, we approach this problem in a\nprincipled fashion by interpreting solving linear PDEs as physics-informed\nGaussian process (GP) regression. Our framework is based on a key\ngeneralization of the Gaussian process inference theorem to observations made\nvia an arbitrary bounded linear operator. Crucially, this probabilistic\nviewpoint allows to (1) quantify the inherent discretization error; (2)\npropagate uncertainty about the model parameters to the solution; and (3)\ncondition on noisy measurements. Demonstrating the strength of this\nformulation, we prove that it strictly generalizes methods of weighted\nresiduals, a central class of PDE solvers including collocation, finite volume,\npseudospectral, and (generalized) Galerkin methods such as finite element and\nspectral methods. This class can thus be directly equipped with a structured\nerror estimate. In summary, our results enable the seamless integration of\nmechanistic models as modular building blocks into probabilistic models by\nblurring the boundaries between numerical analysis and Bayesian inference.\n","authors":["Marvin Pförtner","Ingo Steinwart","Philipp Hennig","Jonathan Wenger"],"pdf_url":"https://arxiv.org/pdf/2212.12474v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01168v1","updated":"2023-04-03T17:37:00Z","published":"2023-04-03T17:37:00Z","title":"DeepAccident: A Motion and Accident Prediction Benchmark for V2X\n  Autonomous Driving","summary":"  Safety is the primary priority of autonomous driving. Nevertheless, no\npublished dataset currently supports the direct and explainable safety\nevaluation for autonomous driving. In this work, we propose DeepAccident, a\nlarge-scale dataset generated via a realistic simulator containing diverse\naccident scenarios that frequently occur in real-world driving. The proposed\nDeepAccident dataset contains 57K annotated frames and 285K annotated samples,\napproximately 7 times more than the large-scale nuScenes dataset with 40k\nannotated samples. In addition, we propose a new task, end-to-end motion and\naccident prediction, based on the proposed dataset, which can be used to\ndirectly evaluate the accident prediction ability for different autonomous\ndriving algorithms. Furthermore, for each scenario, we set four vehicles along\nwith one infrastructure to record data, thus providing diverse viewpoints for\naccident scenarios and enabling V2X (vehicle-to-everything) research on\nperception and prediction tasks. Finally, we present a baseline V2X model named\nV2XFormer that demonstrates superior performance for motion and accident\nprediction and 3D object detection compared to the single-vehicle model.\n","authors":["Tianqi Wang","Sukmin Kim","Wenxuan Ji","Enze Xie","Chongjian Ge","Junsong Chen","Zhenguo Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2304.01168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.01752v3","updated":"2023-04-03T17:34:25Z","published":"2022-02-03T18:18:28Z","title":"Near-Optimal Learning of Extensive-Form Games with Imperfect Information","summary":"  This paper resolves the open question of designing near-optimal algorithms\nfor learning imperfect-information extensive-form games from bandit feedback.\nWe present the first line of algorithms that require only\n$\\widetilde{\\mathcal{O}}((XA+YB)/\\varepsilon^2)$ episodes of play to find an\n$\\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where\n$X,Y$ are the number of information sets and $A,B$ are the number of actions\nfor the two players. This improves upon the best known sample complexity of\n$\\widetilde{\\mathcal{O}}((X^2A+Y^2B)/\\varepsilon^2)$ by a factor of\n$\\widetilde{\\mathcal{O}}(\\max\\{X, Y\\})$, and matches the information-theoretic\nlower bound up to logarithmic factors. We achieve this sample complexity by two\nnew algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual\nRegret Minimization. Both algorithms rely on novel approaches of integrating\n\\emph{balanced exploration policies} into their classical counterparts. We also\nextend our results to learning Coarse Correlated Equilibria in multi-player\ngeneral-sum games.\n","authors":["Yu Bai","Chi Jin","Song Mei","Tiancheng Yu"],"pdf_url":"https://arxiv.org/pdf/2202.01752v3.pdf","comment":"Updated V3 to be consistent with ICML 2022 camera-ready version, with\n  an additional analysis of CFR in full-feedback setting in Appendix F"},{"id":"http://arxiv.org/abs/2303.17649v2","updated":"2023-04-03T17:33:16Z","published":"2023-03-30T18:27:15Z","title":"Aligning a medium-size GPT model in English to a small closed domain in\n  Spanish using reinforcement learning","summary":"  In this paper, we propose a methodology to align a medium-sized GPT model,\noriginally trained in English for an open domain, to a small closed domain in\nSpanish. The application for which the model is finely tuned is the question\nanswering task. To achieve this we also needed to train and implement another\nneural network (which we called the reward model) that could score and\ndetermine whether an answer is appropriate for a given question. This component\nserved to improve the decoding and generation of the answers of the system.\nNumerical metrics such as BLEU and perplexity were used to evaluate the model,\nand human judgment was also used to compare the decoding technique with others.\nFinally, the results favored the proposed method, and it was determined that it\nis feasible to use a reward model to align the generation of responses.\n","authors":["Oscar R. Navarrete-Parra","Victor Uc-Cetina","Jorge Reyes-Magana"],"pdf_url":"https://arxiv.org/pdf/2303.17649v2.pdf","comment":"Under review in the journal Procesamiento del Lenguaje Natural"},{"id":"http://arxiv.org/abs/2304.01161v1","updated":"2023-04-03T17:28:24Z","published":"2023-04-03T17:28:24Z","title":"Is Stochastic Mirror Descent Vulnerable to Adversarial Delay Attacks? A\n  Traffic Assignment Resilience Study","summary":"  \\textit{Intelligent Navigation Systems} (INS) are exposed to an increasing\nnumber of informational attack vectors, which often intercept through the\ncommunication channels between the INS and the transportation network during\nthe data collecting process. To measure the resilience of INS, we use the\nconcept of a Wardrop Non-Equilibrium Solution (WANES), which is characterized\nby the probabilistic outcome of learning within a bounded number of\ninteractions. By using concentration arguments, we have discovered that any\nbounded feedback delaying attack only degrades the systematic performance up to\norder $\\tilde{\\mathcal{O}}(\\sqrt{{d^3}{T^{-1}}})$ along the traffic flow\ntrajectory within the Delayed Mirror Descent (DMD) online-learning framework.\nThis degradation in performance can occur with only mild assumptions imposed.\nOur result implies that learning-based INS infrastructures can achieve Wardrop\nNon-equilibrium even when experiencing a certain period of disruption in the\ninformation structure. These findings provide valuable insights for designing\ndefense mechanisms against possible jamming attacks across different layers of\nthe transportation ecosystem.\n","authors":["Yunian Pan","Tao Li","Quanyan Zhu"],"pdf_url":"https://arxiv.org/pdf/2304.01161v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2304.01159v1","updated":"2023-04-03T17:26:09Z","published":"2023-04-03T17:26:09Z","title":"DribbleBot: Dynamic Legged Manipulation in the Wild","summary":"  DribbleBot (Dexterous Ball Manipulation with a Legged Robot) is a legged\nrobotic system that can dribble a soccer ball under the same real-world\nconditions as humans (i.e., in-the-wild). We adopt the paradigm of training\npolicies in simulation using reinforcement learning and transferring them into\nthe real world. We overcome critical challenges of accounting for variable ball\nmotion dynamics on different terrains and perceiving the ball using\nbody-mounted cameras under the constraints of onboard computing. Our results\nprovide evidence that current quadruped platforms are well-suited for studying\ndynamic whole-body control problems involving simultaneous locomotion and\nmanipulation directly from sensory observations.\n","authors":["Yandong Ji","Gabriel B. Margolis","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2304.01159v1.pdf","comment":"To appear at the IEEE Conference on Robotics and Automation (ICRA),\n  2023. Video is available at https://gmargo11.github.io/dribblebot/"},{"id":"http://arxiv.org/abs/2301.05169v2","updated":"2023-04-03T17:19:51Z","published":"2023-01-12T17:43:38Z","title":"Causal Triplet: An Open Challenge for Intervention-centric Causal\n  Representation Learning","summary":"  Recent years have seen a surge of interest in learning high-level causal\nrepresentations from low-level image pairs under interventions. Yet, existing\nefforts are largely limited to simple synthetic settings that are far away from\nreal-world problems. In this paper, we present Causal Triplet, a causal\nrepresentation learning benchmark featuring not only visually more complex\nscenes, but also two crucial desiderata commonly overlooked in previous works:\n(i) an actionable counterfactual setting, where only certain object-level\nvariables allow for counterfactual observations whereas others do not; (ii) an\ninterventional downstream task with an emphasis on out-of-distribution\nrobustness from the independent causal mechanisms principle. Through extensive\nexperiments, we find that models built with the knowledge of disentangled or\nobject-centric representations significantly outperform their distributed\ncounterparts. However, recent causal representation learning methods still\nstruggle to identify such latent structures, indicating substantial challenges\nand opportunities for future work. Our code and datasets will be available at\nhttps://sites.google.com/view/causaltriplet.\n","authors":["Yuejiang Liu","Alexandre Alahi","Chris Russell","Max Horn","Dominik Zietlow","Bernhard Schölkopf","Francesco Locatello"],"pdf_url":"https://arxiv.org/pdf/2301.05169v2.pdf","comment":"Conference on Causal Learning and Reasoning (CLeaR) 2023"},{"id":"http://arxiv.org/abs/2304.01150v1","updated":"2023-04-03T17:14:19Z","published":"2023-04-03T17:14:19Z","title":"Algebraic and Geometric Models for Space Networking","summary":"  In this paper we introduce some new algebraic and geometric perspectives on\nnetworked space communications. Our main contribution is a novel definition of\na time-varying graph (TVG), defined in terms of a matrix with values in subsets\nof the real line P(R). We leverage semi-ring properties of P(R) to model\nmulti-hop communication in a TVG using matrix multiplication and a truncated\nKleene star. This leads to novel statistics on the communication capacity of\nTVGs called lifetime curves, which we generate for large samples of randomly\nchosen STARLINK satellites, whose connectivity is modeled over day-long\nsimulations. Determining when a large subsample of STARLINK is temporally\nstrongly connected is further analyzed using novel metrics introduced here that\nare inspired by topological data analysis (TDA). To better model networking\nscenarios between the Earth and Mars, we introduce various semi-rings capable\nof modeling propagation delay as well as protocols common to Delay Tolerant\nNetworking (DTN), such as store-and-forward. Finally, we illustrate the\napplicability of zigzag persistence for featurizing different space networks\nand demonstrate the efficacy of K-Nearest Neighbors (KNN) classification for\ndistinguishing Earth-Mars and Earth-Moon satellite systems using time-varying\ntopology alone.\n","authors":["William Bernardoni","Robert Cardona","Jacob Cleveland","Justin Curry","Robert Green","Brian Heller","Alan Hylton","Tung Lam","Robert Kassouf-Short"],"pdf_url":"https://arxiv.org/pdf/2304.01150v1.pdf","comment":"43 pages, 18 figures, comments welcome"},{"id":"http://arxiv.org/abs/2210.13978v2","updated":"2023-04-03T17:09:29Z","published":"2022-10-22T09:40:00Z","title":"Boosting the Cycle Counting Power of Graph Neural Networks with\n  I$^2$-GNNs","summary":"  Message Passing Neural Networks (MPNNs) are a widely used class of Graph\nNeural Networks (GNNs). The limited representational power of MPNNs inspires\nthe study of provably powerful GNN architectures. However, knowing one model is\nmore powerful than another gives little insight about what functions they can\nor cannot express. It is still unclear whether these models are able to\napproximate specific functions such as counting certain graph substructures,\nwhich is essential for applications in biology, chemistry and social network\nanalysis. Motivated by this, we propose to study the counting power of Subgraph\nMPNNs, a recent and popular class of powerful GNN models that extract rooted\nsubgraphs for each node, assign the root node a unique identifier and encode\nthe root node's representation within its rooted subgraph. Specifically, we\nprove that Subgraph MPNNs fail to count more-than-4-cycles at node level,\nimplying that node representations cannot correctly encode the surrounding\nsubstructures like ring systems with more than four atoms. To overcome this\nlimitation, we propose I$^2$-GNNs to extend Subgraph MPNNs by assigning\ndifferent identifiers for the root node and its neighbors in each subgraph.\nI$^2$-GNNs' discriminative power is shown to be strictly stronger than Subgraph\nMPNNs and partially stronger than the 3-WL test. More importantly, I$^2$-GNNs\nare proven capable of counting all 3, 4, 5 and 6-cycles, covering common\nsubstructures like benzene rings in organic chemistry, while still keeping\nlinear complexity. To the best of our knowledge, it is the first linear-time\nGNN model that can count 6-cycles with theoretical guarantees. We validate its\ncounting power in cycle counting tasks and demonstrate its competitive\nperformance in molecular prediction benchmarks.\n","authors":["Yinan Huang","Xingang Peng","Jianzhu Ma","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.13978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07882v2","updated":"2023-04-03T17:06:47Z","published":"2022-11-15T04:15:03Z","title":"Explainable Action Advising for Multi-Agent Reinforcement Learning","summary":"  Action advising is a knowledge transfer technique for reinforcement learning\nbased on the teacher-student paradigm. An expert teacher provides advice to a\nstudent during training in order to improve the student's sample efficiency and\npolicy performance. Such advice is commonly given in the form of state-action\npairs. However, it makes it difficult for the student to reason with and apply\nto novel states. We introduce Explainable Action Advising, in which the teacher\nprovides action advice as well as associated explanations indicating why the\naction was chosen. This allows the student to self-reflect on what it has\nlearned, enabling advice generalization and leading to improved sample\nefficiency and learning performance - even in environments where the teacher is\nsub-optimal. We empirically show that our framework is effective in both\nsingle-agent and multi-agent scenarios, yielding improved policy returns and\nconvergence rates when compared to state-of-the-art methods\n","authors":["Yue Guo","Joseph Campbell","Simon Stepputtis","Ruiyu Li","Dana Hughes","Fei Fang","Katia Sycara"],"pdf_url":"https://arxiv.org/pdf/2211.07882v2.pdf","comment":"This work has been accepted to ICRA 2023"},{"id":"http://arxiv.org/abs/2303.16169v2","updated":"2023-04-03T16:55:54Z","published":"2023-03-28T17:30:35Z","title":"Diffusion Maps for Group-Invariant Manifolds","summary":"  In this article, we consider the manifold learning problem when the data set\nis invariant under the action of a compact Lie group $K$. Our approach consists\nin augmenting the data-induced graph Laplacian by integrating over the\n$K$-orbits of the existing data points, which yields a $K$-invariant graph\nLaplacian $L$. We prove that $L$ can be diagonalized by using the unitary\nirreducible representation matrices of $K$, and we provide an explicit formula\nfor computing its eigenvalues and eigenfunctions. In addition, we show that the\nnormalized Laplacian operator $L_N$ converges to the Laplace-Beltrami operator\nof the data manifold with an improved convergence rate, where the improvement\ngrows with the dimension of the symmetry group $K$. This work extends the\nsteerable graph Laplacian framework of Landa and Shkolnisky from the case of\n$\\operatorname{SO}(2)$ to arbitrary compact Lie groups.\n","authors":["Paulina Hoyos","Joe Kileel"],"pdf_url":"https://arxiv.org/pdf/2303.16169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.00400v2","updated":"2023-04-03T16:35:49Z","published":"2022-07-01T13:17:01Z","title":"WNet: A data-driven dual-domain denoising model for sparse-view computed\n  tomography with a trainable reconstruction layer","summary":"  Deep learning based solutions are being succesfully implemented for a wide\nvariety of applications. Most notably, clinical use-cases have gained an\nincreased interest and have been the main driver behind some of the\ncutting-edge data-driven algorithms proposed in the last years. For\napplications like sparse-view tomographic reconstructions, where the amount of\nmeasurement data is small in order to keep acquisition time short and radiation\ndose low, reduction of the streaking artifacts has prompted the development of\ndata-driven denoising algorithms with the main goal of obtaining diagnostically\nviable images with only a subset of a full-scan data. We propose WNet, a\ndata-driven dual-domain denoising model which contains a trainable\nreconstruction layer for sparse-view artifact denoising. Two encoder-decoder\nnetworks perform denoising in both sinogram- and reconstruction-domain\nsimultaneously, while a third layer implementing the Filtered Backprojection\nalgorithm is sandwiched between the first two and takes care of the\nreconstruction operation. We investigate the performance of the network on\nsparse-view chest CT scans, and we highlight the added benefit of having a\ntrainable reconstruction layer over the more conventional fixed ones. We train\nand test our network on two clinically relevant datasets and we compare the\nobtained results with three different types of sparse-view CT denoising and\nreconstruction algorithms.\n","authors":["Theodor Cheslerean-Boghiu","Felix C. Hofmann","Manuel Schultheiß","Franz Pfeiffer","Daniela Pfeiffer","Tobias Lasser"],"pdf_url":"https://arxiv.org/pdf/2207.00400v2.pdf","comment":"Publisehd at IEEE TCI in January 2023. Supplementary materials are\n  available @IEEE"},{"id":"http://arxiv.org/abs/2304.01120v1","updated":"2023-04-03T16:34:18Z","published":"2023-04-03T16:34:18Z","title":"Synthesis parameter effect detection using quantitative representations\n  and high dimensional distribution distances","summary":"  Detection of effects of the parameters of the synthetic process on the\nmicrostructure of materials is an important, yet elusive goal of materials\nscience. We develop a method for detecting effects based on copula theory, high\ndimensional distribution distances, and permutational statistics to analyze a\ndesigned experiment synthesizing plutonium oxide from Pu(III) Oxalate. We\ndetect effects of strike order and oxalic acid feed on the microstructure of\nthe resulting plutonium oxide, which match the literature well. We also detect\nexcess bivariate effects between the pairs of acid concentration, strike order\nand precipitation temperature.\n","authors":["Alex Hagen","Shane Jackson"],"pdf_url":"https://arxiv.org/pdf/2304.01120v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2304.01117v1","updated":"2023-04-03T16:30:20Z","published":"2023-04-03T16:30:20Z","title":"Interpretable Symbolic Regression for Data Science: Analysis of the 2022\n  Competition","summary":"  Symbolic regression searches for analytic expressions that accurately\ndescribe studied phenomena. The main attraction of this approach is that it\nreturns an interpretable model that can be insightful to users. Historically,\nthe majority of algorithms for symbolic regression have been based on\nevolutionary algorithms. However, there has been a recent surge of new\nproposals that instead utilize approaches such as enumeration algorithms, mixed\nlinear integer programming, neural networks, and Bayesian optimization. In\norder to assess how well these new approaches behave on a set of common\nchallenges often faced in real-world data, we hosted a competition at the 2022\nGenetic and Evolutionary Computation Conference consisting of different\nsynthetic and real-world datasets which were blind to entrants. For the\nreal-world track, we assessed interpretability in a realistic way by using a\ndomain expert to judge the trustworthiness of candidate models.We present an\nin-depth analysis of the results obtained in this competition, discuss current\nchallenges of symbolic regression algorithms and highlight possible\nimprovements for future competitions.\n","authors":["F. O. de Franca","M. Virgolin","M. Kommenda","M. S. Majumder","M. Cranmer","G. Espada","L. Ingelse","A. Fonseca","M. Landajuela","B. Petersen","R. Glatt","N. Mundhenk","C. S. Lee","J. D. Hochhalter","D. L. Randall","P. Kamienny","H. Zhang","G. Dick","A. Simon","B. Burlacu","Jaan Kasak","Meera Machado","Casper Wilstrup","W. G. La Cava"],"pdf_url":"https://arxiv.org/pdf/2304.01117v1.pdf","comment":"13 pages, 13 figures, submitted to IEEE Transactions on Evolutionary\n  Computation"},{"id":"http://arxiv.org/abs/2204.12929v2","updated":"2023-04-03T16:19:11Z","published":"2022-04-21T16:34:53Z","title":"Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump","summary":"  With the proliferation of pump-and-dump schemes (P&Ds) in the cryptocurrency\nmarket, it becomes imperative to detect such fraudulent activities in advance\nto alert potentially susceptible investors. In this paper, we focus on\npredicting the pump probability of all coins listed in the target exchange\nbefore a scheduled pump time, which we refer to as the target coin prediction\ntask. Firstly, we conduct a comprehensive study of the latest 709 P&D events\norganized in Telegram from Jan. 2019 to Jan. 2022. Our empirical analysis\nreveals some interesting patterns of P&Ds, such as that pumped coins exhibit\nintra-channel homogeneity and inter-channel heterogeneity. Here channel refers\na form of group in Telegram that is frequently used to coordinate P&D events.\nThis observation inspires us to develop a novel sequence-based neural network,\ndubbed SNN, which encodes a channel's P&D event history into a sequence\nrepresentation via the positional attention mechanism to enhance the prediction\naccuracy. Positional attention helps to extract useful information and\nalleviates noise, especially when the sequence length is long. Extensive\nexperiments verify the effectiveness and generalizability of proposed methods.\nAdditionally, we release the code and P&D dataset on GitHub:\nhttps://github.com/Bayi-Hu/Pump-and-Dump-Detection-on-Cryptocurrency, and\nregularly update the dataset.\n","authors":["Sihao Hu","Zhen Zhang","Shengliang Lu","Bingsheng He","Zhao Li"],"pdf_url":"https://arxiv.org/pdf/2204.12929v2.pdf","comment":"SIGMOD conference 2023"},{"id":"http://arxiv.org/abs/2209.00905v2","updated":"2023-04-03T16:15:58Z","published":"2022-09-02T09:27:37Z","title":"From latent dynamics to meaningful representations","summary":"  While representation learning has been central to the rise of machine\nlearning and artificial intelligence, a key problem remains in making the\nlearnt representations meaningful. For this the typical approach is to\nregularize the learned representation through prior probability distributions.\nHowever such priors are usually unavailable or ad hoc. To deal with this, we\npropose a dynamics-constrained representation learning framework. Instead of\nusing predefined probabilities, we restrict the latent representation to follow\nspecific dynamics, which is a more natural constraint for representation\nlearning in dynamical systems. Our belief stems from a fundamental observation\nin physics that though different systems can have different marginalized\nprobability distributions, they typically obey the same dynamics, such as\nNewton's and Schrodinger's equations. We validate our framework for different\nsystems including a real-world fluorescent DNA movie dataset. We show that our\nalgorithm can uniquely identify an uncorrelated, isometric and meaningful\nlatent representation.\n","authors":["Dedi Wang","Yihang Wang","Luke Evans","Pratyush Tiwary"],"pdf_url":"https://arxiv.org/pdf/2209.00905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.07316v2","updated":"2023-04-03T16:15:04Z","published":"2022-08-15T16:30:14Z","title":"MENLI: Robust Evaluation Metrics from Natural Language Inference","summary":"  Recently proposed BERT-based evaluation metrics for text generation perform\nwell on standard benchmarks but are vulnerable to adversarial attacks, e.g.,\nrelating to information correctness. We argue that this stems (in part) from\nthe fact that they are models of semantic similarity. In contrast, we develop\nevaluation metrics based on Natural Language Inference (NLI), which we deem a\nmore appropriate modeling. We design a preference-based adversarial attack\nframework and show that our NLI based metrics are much more robust to the\nattacks than the recent BERT-based metrics. On standard benchmarks, our NLI\nbased metrics outperform existing summarization metrics, but perform below SOTA\nMT metrics. However, when combining existing metrics with our NLI metrics, we\nobtain both higher adversarial robustness (15%-30%) and higher quality metrics\nas measured on standard benchmarks (+5% to 30%).\n","authors":["Yanran Chen","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2208.07316v2.pdf","comment":"TACL 2023 camera-ready"},{"id":"http://arxiv.org/abs/2304.01111v1","updated":"2023-04-03T16:14:31Z","published":"2023-04-03T16:14:31Z","title":"Theoretical guarantees for neural control variates in MCMC","summary":"  In this paper, we propose a variance reduction approach for Markov chains\nbased on additive control variates and the minimization of an appropriate\nestimate for the asymptotic variance. We focus on the particular case when\ncontrol variates are represented as deep neural networks. We derive the optimal\nconvergence rate of the asymptotic variance under various ergodicity\nassumptions on the underlying Markov chain. The proposed approach relies upon\nrecent results on the stochastic errors of variance reduction algorithms and\nfunction approximation theory.\n","authors":["Denis Belomestny","Artur Goldman","Alexey Naumov","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2304.01111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01108v1","updated":"2023-04-03T16:08:22Z","published":"2023-04-03T16:08:22Z","title":"Coincidental Generation","summary":"  Generative AI models are emerging as a versatile tool across diverse\nindustries with applications in synthetic data generation computational art\npersonalization of products and services and immersive entertainment Here we\nintroduce a new privacy concern in the adoption and use of generative AI models\nthat of coincidental generation Coincidental generation occurs when a models\noutput inadvertently bears a likeness to a realworld entity Consider for\nexample synthetic portrait generators which are today deployed in commercial\napplications such as virtual modeling agencies and synthetic stock photography\nWe argue that the low intrinsic dimensionality of human face perception implies\nthat every synthetically generated face will coincidentally resemble an actual\nperson all but guaranteeing a privacy violation in the form of a\nmisappropriation of likeness.\n","authors":["Jordan W. Suchow","Necdet Gürkan"],"pdf_url":"https://arxiv.org/pdf/2304.01108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12067v2","updated":"2023-04-03T16:05:50Z","published":"2023-01-28T02:48:14Z","title":"Learning Optimal Features via Partial Invariance","summary":"  Learning models that are robust to distribution shifts is a key concern in\nthe context of their real-life applicability. Invariant Risk Minimization (IRM)\nis a popular framework that aims to learn robust models from multiple\nenvironments. The success of IRM requires an important assumption: the\nunderlying causal mechanisms/features remain invariant across environments.\nWhen not satisfied, we show that IRM can over-constrain the predictor and to\nremedy this, we propose a relaxation via $\\textit{partial invariance}$. In this\nwork, we theoretically highlight the sub-optimality of IRM and then demonstrate\nhow learning from a partition of training domains can help improve invariant\nmodels. Several experiments, conducted both in linear settings as well as with\ndeep neural networks on tasks over both language and image data, allow us to\nverify our conclusions.\n","authors":["Moulik Choraria","Ibtihal Ferwana","Ankur Mani","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2301.12067v2.pdf","comment":"Presented at the 37th AAAI Conference on Artificial Intelligence,\n  2023"},{"id":"http://arxiv.org/abs/2304.01102v1","updated":"2023-04-03T16:02:00Z","published":"2023-04-03T16:02:00Z","title":"RunBugRun -- An Executable Dataset for Automated Program Repair","summary":"  Recently, we can notice a transition to data-driven techniques in Automated\nProgram Repair (APR), in particular towards deep neural networks. This entails\ntraining on hundreds of thousands or even millions of non-executable code\nfragments. We would like to bring more attention to an aspect of code often\nneglected in Neural Program Repair (NPR), namely its execution. Code execution\nhas several significant advantages. It allows for test-based evaluation of\ncandidate fixes and can provide valuable information to aid repair. In this\nwork we present a fully executable dataset of 450,000 small buggy/fixed program\npairs originally submitted to programming competition websites written in eight\ndifferent programming languages. Along with the dataset we provide\ninfrastructure to compile, safely execute and test programs as well as\nfine-grained bug-type labels. To give a point of reference, we provide basic\nevaluation results for two baselines, one based on a generate-and-validate\napproach and one on deep learning. With this dataset we follow several goals:\nwe want to lift Neural Program Repair beyond fully static code representations,\nfoster the use of execution-based features and, by including several different\nlanguages, counterbalance the predominance of Java in the current landscape of\nAPR datasets and benchmarks.\n","authors":["Julian Aron Prenner","Romain Robbes"],"pdf_url":"https://arxiv.org/pdf/2304.01102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.15476v4","updated":"2023-04-03T16:00:22Z","published":"2022-06-30T17:59:22Z","title":"AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly\n  Detection","summary":"  Analyzing the distribution shift of data is a growing research direction in\nnowadays Machine Learning (ML), leading to emerging new benchmarks that focus\non providing a suitable scenario for studying the generalization properties of\nML models. The existing benchmarks are focused on supervised learning, and to\nthe best of our knowledge, there is none for unsupervised learning. Therefore,\nwe introduce an unsupervised anomaly detection benchmark with data that shifts\nover time, built over Kyoto-2006+, a traffic dataset for network intrusion\ndetection. This type of data meets the premise of shifting the input\ndistribution: it covers a large time span ($10$ years), with naturally\noccurring changes over time (eg users modifying their behavior patterns, and\nsoftware updates). We first highlight the non-stationary nature of the data,\nusing a basic per-feature analysis, t-SNE, and an Optimal Transport approach\nfor measuring the overall distribution distances between years. Next, we\npropose AnoShift, a protocol splitting the data in IID, NEAR, and FAR testing\nsplits. We validate the performance degradation over time with diverse models,\nranging from classical approaches to deep learning. Finally, we show that by\nacknowledging the distribution shift problem and properly addressing it, the\nperformance can be improved compared to the classical training which assumes\nindependent and identically distributed data (on average, by up to $3\\%$ for\nour approach). Dataset and code are available at\nhttps://github.com/bit-ml/AnoShift/.\n","authors":["Marius Dragoi","Elena Burceanu","Emanuela Haller","Andrei Manolache","Florin Brad"],"pdf_url":"https://arxiv.org/pdf/2206.15476v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01096v1","updated":"2023-04-03T15:57:49Z","published":"2023-04-03T15:57:49Z","title":"Evolving Artificial Neural Networks To Imitate Human Behaviour In\n  Shinobi III : Return of the Ninja Master","summary":"  Our society is increasingly fond of computational tools. This phenomenon has\ngreatly increased over the past decade following, among other factors, the\nemergence of a new Artificial Intelligence paradigm. Specifically, the coupling\nof two algorithmic techniques, Deep Neural Networks and Stochastic Gradient\nDescent, thrusted by an exponentially increasing computing capacity, has and is\ncontinuing to become a major asset in many modern technologies. However, as\nprogress takes its course, some still wonder whether other methods could\nsimilarly or even more greatly benefit from these various hardware advances. In\norder to further this study, we delve in this thesis into Evolutionary\nAlgorithms and their application to Dynamic Neural Networks, two techniques\nwhich despite enjoying many advantageous properties have yet to find their\nniche in contemporary Artificial Intelligence. We find that by elaborating new\nmethods while exploiting strong computational resources, it becomes possible to\ndevelop strongly performing agents on a variety of benchmarks but also some\nother agents behaving very similarly to human subjects on the video game\nShinobi III : Return of The Ninja Master, typical complex tasks previously out\nof reach for non-gradient-based optimization.\n","authors":["Maximilien Le Clei"],"pdf_url":"https://arxiv.org/pdf/2304.01096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01086v1","updated":"2023-04-03T15:42:28Z","published":"2023-04-03T15:42:28Z","title":"Self-building Neural Networks","summary":"  During the first part of life, the brain develops while it learns through a\nprocess called synaptogenesis. The neurons, growing and interacting with each\nother, create synapses. However, eventually the brain prunes those synapses.\nWhile previous work focused on learning and pruning independently, in this work\nwe propose a biologically plausible model that, thanks to a combination of\nHebbian learning and pruning, aims to simulate the synaptogenesis process. In\nthis way, while learning how to solve the task, the agent translates its\nexperience into a particular network structure. Namely, the network structure\nbuilds itself during the execution of the task. We call this approach\nSelf-building Neural Network (SBNN). We compare our proposed SBNN with\ntraditional neural networks (NNs) over three classical control tasks from\nOpenAI. The results show that our model performs generally better than\ntraditional NNs. Moreover, we observe that the performance decay while\nincreasing the pruning rate is smaller in our model than with NNs. Finally, we\nperform a validation test, testing the models over tasks unseen during the\nlearning phase. In this case, the results show that SBNNs can adapt to new\ntasks better than the traditional NNs, especially when over $80\\%$ of the\nweights are pruned.\n","authors":["Andrea Ferigo","Giovanni Iacca"],"pdf_url":"https://arxiv.org/pdf/2304.01086v1.pdf","comment":"To appear in the Genetic and Evolutionary Computation Conference\n  Companion (GECCO '23 Companion) Proceedings, July 15--19, 2023, Lisbon,\n  Portugal"},{"id":"http://arxiv.org/abs/2304.01083v1","updated":"2023-04-03T15:39:35Z","published":"2023-04-03T15:39:35Z","title":"Can the Inference Logic of Large Language Models be Disentangled into\n  Symbolic Concepts?","summary":"  In this paper, we explain the inference logic of large language models (LLMs)\nas a set of symbolic concepts. Many recent studies have discovered that\ntraditional DNNs usually encode sparse symbolic concepts. However, because an\nLLM has much more parameters than traditional DNNs, whether the LLM also\nencodes sparse symbolic concepts is still an open problem. Therefore, in this\npaper, we propose to disentangle the inference score of LLMs for dialogue tasks\ninto a small number of symbolic concepts. We verify that we can use those\nsparse concepts to well estimate all inference scores of the LLM on all\narbitrarily masking states of the input sentence. We also evaluate the\ntransferability of concepts encoded by an LLM and verify that symbolic concepts\nusually exhibit high transferability across similar input sentences. More\ncrucially, those symbolic concepts can be used to explain the exact reasons\naccountable for the LLM's prediction errors.\n","authors":["Wen Shen","Lei Cheng","Yuxiao Yang","Mingjie Li","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01081v1","updated":"2023-04-03T15:38:53Z","published":"2023-04-03T15:38:53Z","title":"FMGNN: Fused Manifold Graph Neural Network","summary":"  Graph representation learning has been widely studied and demonstrated\neffectiveness in various graph tasks. Most existing works embed graph data in\nthe Euclidean space, while recent works extend the embedding models to\nhyperbolic or spherical spaces to achieve better performance on graphs with\ncomplex structures, such as hierarchical or ring structures. Fusing the\nembedding from different manifolds can further take advantage of the embedding\ncapabilities over different graph structures. However, existing embedding\nfusion methods mostly focus on concatenating or summing up the output\nembeddings, without considering interacting and aligning the embeddings of the\nsame vertices on different manifolds, which can lead to distortion and\nimpression in the final fusion results. Besides, it is also challenging to fuse\nthe embeddings of the same vertices from different coordinate systems. In face\nof these challenges, we propose the Fused Manifold Graph Neural Network\n(FMGNN), a novel GNN architecture that embeds graphs into different Riemannian\nmanifolds with interaction and alignment among these manifolds during training\nand fuses the vertex embeddings through the distances on different manifolds\nbetween vertices and selected landmarks, geometric coresets. Our experiments\ndemonstrate that FMGNN yields superior performance over strong baselines on the\nbenchmarks of node classification and link prediction tasks.\n","authors":["Cheng Deng","Fan Xu","Jiaxing Ding","Luoyi Fu","Weinan Zhang","Xinbing Wang"],"pdf_url":"https://arxiv.org/pdf/2304.01081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01075v1","updated":"2023-04-03T15:32:38Z","published":"2023-04-03T15:32:38Z","title":"Conformal Prediction Regions for Time Series using Linear\n  Complementarity Programming","summary":"  Conformal prediction is a statistical tool for producing prediction regions\nof machine learning models that are valid with high probability. However,\napplying conformal prediction to time series data leads to conservative\nprediction regions. In fact, to obtain prediction regions over $T$ time steps\nwith confidence $1-\\delta$, {previous works require that each individual\nprediction region is valid} with confidence $1-\\delta/T$. We propose an\noptimization-based method for reducing this conservatism to enable long horizon\nplanning and verification when using learning-enabled time series predictors.\nInstead of considering prediction errors individually at each time step, we\nconsider a parameterized prediction error over multiple time steps. By\noptimizing the parameters over an additional dataset, we find prediction\nregions that are not conservative. We show that this problem can be cast as a\nmixed integer linear complementarity program (MILCP), which we then relax into\na linear complementarity program (LCP). Additionally, we prove that the relaxed\nLP has the same optimal cost as the original MILCP. Finally, we demonstrate the\nefficacy of our method on a case study using pedestrian trajectory predictors.\n","authors":["Matthew Cleaveland","Insup Lee","George J. Pappas","Lars Lindemann"],"pdf_url":"https://arxiv.org/pdf/2304.01075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05743v2","updated":"2023-04-03T15:31:56Z","published":"2023-02-11T16:54:20Z","title":"Is Distance Matrix Enough for Geometric Deep Learning?","summary":"  Graph Neural Networks (GNNs) are often used for tasks involving the geometry\nof a given graph, such as molecular dynamics simulation. Although the distance\nmatrix of a geometric graph contains complete geometric information, it has\nbeen demonstrated that Message Passing Neural Networks (MPNNs) are insufficient\nfor learning this geometry. In this work, we expand on the families of\ncounterexamples that MPNNs are unable to distinguish from their distance\nmatrices, by constructing families of novel and symmetric geometric graphs. We\nthen propose $k$-DisGNNs, which can effectively exploit the rich geometry\ncontained in the distance matrix. We demonstrate the high expressive power of\nour models and prove that some existing well-designed geometric models can be\nunified by $k$-DisGNNs as special cases. Most importantly, we establish a\nconnection between geometric deep learning and traditional graph representation\nlearning, showing that those highly expressive GNN models originally designed\nfor graph structure learning can also be applied to geometric deep learning\nproblems with impressive performance, and that existing complex, equivariant\nmodels are not the only solution. Experimental results verify our theory.\n","authors":["Zian Li","Xiyuan Wang","Yinan Huang","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.05743v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.09204v2","updated":"2023-04-03T15:18:21Z","published":"2021-02-18T07:58:47Z","title":"Towards a mathematical theory of trajectory inference","summary":"  We devise a theoretical framework and a numerical method to infer\ntrajectories of a stochastic process from samples of its temporal marginals.\nThis problem arises in the analysis of single cell RNA-sequencing data, which\nprovide high dimensional measurements of cell states but cannot track the\ntrajectories of the cells over time. We prove that for a class of stochastic\nprocesses it is possible to recover the ground truth trajectories from limited\nsamples of the temporal marginals at each time-point, and provide an efficient\nalgorithm to do so in practice. The method we develop, Global Waddington-OT\n(gWOT), boils down to a smooth convex optimization problem posed globally over\nall time-points involving entropy-regularized optimal transport. We demonstrate\nthat this problem can be solved efficiently in practice and yields good\nreconstructions, as we show on several synthetic and real datasets.\n","authors":["Hugo Lavenant","Stephen Zhang","Young-Heon Kim","Geoffrey Schiebinger"],"pdf_url":"https://arxiv.org/pdf/2102.09204v2.pdf","comment":"The first two authors contributed equally to this work; 76 pages"},{"id":"http://arxiv.org/abs/2304.01063v1","updated":"2023-04-03T15:18:16Z","published":"2023-04-03T15:18:16Z","title":"Depth Separation with Multilayer Mean-Field Networks","summary":"  Depth separation -- why a deeper network is more powerful than a shallower\none -- has been a major problem in deep learning theory. Previous results often\nfocus on representation power. For example, arXiv:1904.06984 constructed a\nfunction that is easy to approximate using a 3-layer network but not\napproximable by any 2-layer network. In this paper, we show that this\nseparation is in fact algorithmic: one can learn the function constructed by\narXiv:1904.06984 using an overparameterized network with polynomially many\nneurons efficiently. Our result relies on a new way of extending the mean-field\nlimit to multilayer networks, and a decomposition of loss that factors out the\nerror introduced by the discretization of infinite-width mean-field networks.\n","authors":["Yunwei Ren","Mo Zhou","Rong Ge"],"pdf_url":"https://arxiv.org/pdf/2304.01063v1.pdf","comment":"ICLR 2023"},{"id":"http://arxiv.org/abs/2303.13634v2","updated":"2023-04-03T14:50:13Z","published":"2023-03-22T06:49:34Z","title":"Physics-informed PointNet: On how many irregular geometries can it solve\n  an inverse problem simultaneously? Application to linear elasticity","summary":"  Regular physics-informed neural networks (PINNs) predict the solution of\npartial differential equations using sparse labeled data but only over a single\ndomain. On the other hand, fully supervised learning models are first trained\nusually over a few thousand domains with known solutions (i.e., labeled data)\nand then predict the solution over a few hundred unseen domains.\nPhysics-informed PointNet (PIPN) is primarily designed to fill this gap between\nPINNs (as weakly supervised learning models) and fully supervised learning\nmodels. In this article, we demonstrate that PIPN predicts the solution of\ndesired partial differential equations over a few hundred domains\nsimultaneously, while it only uses sparse labeled data. This framework benefits\nfast geometric designs in the industry when only sparse labeled data are\navailable. Particularly, we show that PIPN predicts the solution of a plane\nstress problem over more than 500 domains with different geometries,\nsimultaneously. Moreover, we pioneer implementing the concept of remarkable\nbatch size (i.e., the number of geometries fed into PIPN at each sub-epoch)\ninto PIPN. Specifically, we try batch sizes of 7, 14, 19, 38, 76, and 133.\nAdditionally, the effect of the PIPN size, symmetric function in the PIPN\narchitecture, and static and dynamic weights for the component of the sparse\nlabeled data in the loss function are investigated.\n","authors":["Ali Kashefi","Leonidas J. Guibas","Tapan Mukerji"],"pdf_url":"https://arxiv.org/pdf/2303.13634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03802v2","updated":"2023-04-03T14:49:49Z","published":"2023-02-07T23:46:34Z","title":"Standing Between Past and Future: Spatio-Temporal Modeling for\n  Multi-Camera 3D Multi-Object Tracking","summary":"  This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT)\nframework. It emphasizes spatio-temporal continuity and integrates both past\nand future reasoning for tracked objects. Thus, we name it \"Past-and-Future\nreasoning for Tracking\" (PF-Track). Specifically, our method adapts the\n\"tracking by attention\" framework and represents tracked instances coherently\nover time with object queries. To explicitly use historical cues, our \"Past\nReasoning\" module learns to refine the tracks and enhance the object features\nby cross-attending to queries from previous frames and other objects. The\n\"Future Reasoning\" module digests historical information and predicts robust\nfuture trajectories. In the case of long-term occlusions, our method maintains\nthe object positions and enables re-association by integrating motion\npredictions. On the nuScenes dataset, our method improves AMOTA by a large\nmargin and remarkably reduces ID-Switches by 90% compared to prior approaches,\nwhich is an order of magnitude less. The code and models are made available at\nhttps://github.com/TRI-ML/PF-Track.\n","authors":["Ziqi Pang","Jie Li","Pavel Tokmakov","Dian Chen","Sergey Zagoruyko","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2302.03802v2.pdf","comment":"CVPR 2023 Camera Ready, 15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.09642v2","updated":"2023-04-03T14:47:11Z","published":"2023-03-16T20:42:24Z","title":"SUD$^2$: Supervision by Denoising Diffusion Models for Image\n  Reconstruction","summary":"  Many imaging inverse problems$\\unicode{x2014}$such as image-dependent\nin-painting and dehazing$\\unicode{x2014}$are challenging because their forward\nmodels are unknown or depend on unknown latent parameters. While one can solve\nsuch problems by training a neural network with vast quantities of paired\ntraining data, such paired training data is often unavailable. In this paper,\nwe propose a generalized framework for training image reconstruction networks\nwhen paired training data is scarce. In particular, we demonstrate the ability\nof image denoising algorithms and, by extension, denoising diffusion models to\nsupervise network training in the absence of paired training data.\n","authors":["Matthew A. Chan","Sean I. Young","Christopher A. Metzler"],"pdf_url":"https://arxiv.org/pdf/2303.09642v2.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2303.06947v2","updated":"2023-04-03T14:46:32Z","published":"2023-03-13T09:31:20Z","title":"A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X\n  Communications in Dynamic Environments","summary":"  Digital Twins (DTs) for physical wireless environments have been recently\nproposed as accurate virtual representations of the propagation environment\nthat can enable multi-layer decisions at the physical communication equipment.\nAt high frequency bands, DTs can help to overcome the challenges emerging in\nthe high mobility conditions featuring vehicular environments. In this paper,\nwe propose a novel data-driven workflow for the creation of the DT of a\nVehicle-to-Everything (V2X) communication scenario and a multi-modal simulation\nframework for the generation of realistic sensor data and accurate\nmmWave/sub-THz wireless channels. The proposed method leverages an automotive\nsimulation and testing framework based on the Unreal Engine game engine and an\naccurate ray-tracing channel simulator. Simulations over an urban scenario show\nthe achievable realistic sensor and channel modelling both at the\ninfrastructure and at an ego-vehicle.\n","authors":["Lorenzo Cazzella","Francesco Linsalata","Maurizio Magarini","Matteo Matteucci","Umberto Spagnolini"],"pdf_url":"https://arxiv.org/pdf/2303.06947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01029v1","updated":"2023-04-03T14:28:29Z","published":"2023-04-03T14:28:29Z","title":"Domain Generalization for Crop Segmentation with Knowledge Distillation","summary":"  In recent years, precision agriculture has gradually oriented farming closer\nto automation processes to support all the activities related to field\nmanagement. Service robotics plays a predominant role in this evolution by\ndeploying autonomous agents that can navigate fields while performing tasks\nwithout human intervention, such as monitoring, spraying, and harvesting. To\nexecute these precise actions, mobile robots need a real-time perception system\nthat understands their surroundings and identifies their targets in the wild.\nGeneralizing to new crops and environmental conditions is critical for\npractical applications, as labeled samples are rarely available. In this paper,\nwe investigate the problem of crop segmentation and propose a novel approach to\nenhance domain generalization using knowledge distillation. In the proposed\nframework, we transfer knowledge from an ensemble of models individually\ntrained on source domains to a student model that can adapt to unseen target\ndomains. To evaluate the proposed method, we present a synthetic multi-domain\ndataset for crop segmentation containing plants of variegate shapes and\ncovering different terrain styles, weather conditions, and light scenarios for\nmore than 50,000 samples. We demonstrate significant improvements in\nperformance over state-of-the-art methods. Our approach provides a promising\nsolution for domain generalization in crop segmentation and has the potential\nto enhance precision agriculture applications.\n","authors":["Simone Angarano","Mauro Martini","Alessandro Navone","Marcello Chiaberge"],"pdf_url":"https://arxiv.org/pdf/2304.01029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01023v1","updated":"2023-04-03T14:21:42Z","published":"2023-04-03T14:21:42Z","title":"Self-Supervised learning for Neural Architecture Search (NAS)","summary":"  The objective of this internship is to propose an innovative method that uses\nunlabelled data, i.e. data that will allow the AI to automatically learn to\npredict the correct outcome. To reach this stage, the steps to be followed can\nbe defined as follows: (1) consult the state of the art and position ourself\nagainst it, (2) come up with ideas for development paths, (3) implement these\nideas, (4) and finally test them to position ourself against the state of the\nart, and then start the sequence again. During my internship, this sequence was\ndone several times and therefore gives the tracks explored during the\ninternship.\n","authors":["Samuel Ducros"],"pdf_url":"https://arxiv.org/pdf/2304.01023v1.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2303.17618v2","updated":"2023-04-03T13:58:43Z","published":"2023-03-30T11:26:40Z","title":"Data-driven abstractions via adaptive refinements and a Kantorovich\n  metric [extended version]","summary":"  We introduce an adaptive refinement procedure for smart, and scalable\nabstraction of dynamical systems. Our technique relies on partitioning the\nstate space depending on the observation of future outputs. However, this\nknowledge is dynamically constructed in an adaptive, asymmetric way. In order\nto learn the optimal structure, we define a Kantorovich-inspired metric between\nMarkov chains, and we use it as a loss function. Our technique is prone to\ndata-driven frameworks, but not restricted to.\n  We also study properties of the above mentioned metric between Markov chains,\nwhich we believe could be of application for wider purpose. We propose an\nalgorithm to approximate it, and we show that our method yields a much better\ncomputational complexity than using classical linear programming techniques.\n","authors":["Adrien Banse","Licio Romao","Alessandro Abate","Raphaël M. Jungers"],"pdf_url":"https://arxiv.org/pdf/2303.17618v2.pdf","comment":"This paper is an extended version of a CDC2023 submission"},{"id":"http://arxiv.org/abs/2304.00991v1","updated":"2023-04-03T13:57:38Z","published":"2023-04-03T13:57:38Z","title":"Federated Kalman Filter for Secure IoT-based Device Monitoring Services","summary":"  Device monitoring services have increased in popularity with the evolution of\nrecent technology and the continuously increased number of Internet of Things\n(IoT) devices. Among the popular services are the ones that use device location\ninformation. However, these services run into privacy issues due to the nature\nof data collection and transmission. In this work, we introduce a platform\nincorporating Federated Kalman Filter (FKF) with a federated learning approach\nand private blockchain technology for privacy preservation. We analyze the\naccuracy of the proposed design against a standard Kalman Filter (KF)\nimplementation of localization based on the Received Signal Strength Indicator\n(RSSI). The experimental results reveal significant potential for improved data\nestimation for RSSI-based localization in device monitoring.\n","authors":["Marc Jayson Baucas","Petros Spachos"],"pdf_url":"https://arxiv.org/pdf/2304.00991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00990v1","updated":"2023-04-03T13:56:01Z","published":"2023-04-03T13:56:01Z","title":"Efficient human-in-loop deep learning model training with iterative\n  refinement and statistical result validation","summary":"  Annotation and labeling of images are some of the biggest challenges in\napplying deep learning to medical data. Current processes are time and\ncost-intensive and, therefore, a limiting factor for the wide adoption of the\ntechnology. Additionally validating that measured performance improvements are\nsignificant is important to select the best model. In this paper, we\ndemonstrate a method for creating segmentations, a necessary part of a data\ncleaning for ultrasound imaging machine learning pipelines. We propose a\nfour-step method to leverage automatically generated training data and fast\nhuman visual checks to improve model accuracy while keeping the time/effort and\ncost low. We also showcase running experiments multiple times to allow the\nusage of statistical analysis. Poor quality automated ground truth data and\nquick visual inspections efficiently train an initial base model, which is\nrefined using a small set of more expensive human-generated ground truth data.\nThe method is demonstrated on a cardiac ultrasound segmentation task, removing\nbackground data, including static PHI. Significance is shown by running the\nexperiments multiple times and using the student's t-test on the performance\ndistributions. The initial segmentation accuracy of a simple thresholding\nalgorithm of 92% was improved to 98%. The performance of models trained on\ncomplicated algorithms can be matched or beaten by pre-training with the poorer\nperforming algorithms and a small quantity of high-quality data. The\nintroduction of statistic significance analysis for deep learning models helps\nto validate the performance improvements measured. The method offers a\ncost-effective and fast approach to achieving high-accuracy models while\nminimizing the cost and effort of acquiring high-quality training data.\n","authors":["Manuel Zahn","Douglas P. Perrin"],"pdf_url":"https://arxiv.org/pdf/2304.00990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18171v2","updated":"2023-04-03T13:53:33Z","published":"2023-03-29T18:52:10Z","title":"How Efficient Are Today's Continual Learning Algorithms?","summary":"  Supervised Continual learning involves updating a deep neural network (DNN)\nfrom an ever-growing stream of labeled data. While most work has focused on\novercoming catastrophic forgetting, one of the major motivations behind\ncontinual learning is being able to efficiently update a network with new\ninformation, rather than retraining from scratch on the training dataset as it\ngrows over time. Despite recent continual learning methods largely solving the\ncatastrophic forgetting problem, there has been little attention paid to the\nefficiency of these algorithms. Here, we study recent methods for incremental\nclass learning and illustrate that many are highly inefficient in terms of\ncompute, memory, and storage. Some methods even require more compute than\ntraining from scratch! We argue that for continual learning to have real-world\napplicability, the research community cannot ignore the resources used by these\nalgorithms. There is more to continual learning than mitigating catastrophic\nforgetting.\n","authors":["Md Yousuf Harun","Jhair Gallardo","Tyler L. Hayes","Christopher Kanan"],"pdf_url":"https://arxiv.org/pdf/2303.18171v2.pdf","comment":"To appear in the IEEE Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision\n  (CLVision) 2023"},{"id":"http://arxiv.org/abs/2304.00971v1","updated":"2023-04-03T13:41:35Z","published":"2023-04-03T13:41:35Z","title":"Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection,\n  Segmentation, and Depth Estimation","summary":"  This report serves as a supplementary document for TaskPrompter, detailing\nits implementation on a new joint 2D-3D multi-task learning benchmark based on\nCityscapes-3D. TaskPrompter presents an innovative multi-task prompting\nframework that unifies the learning of (i) task-generic representations, (ii)\ntask-specific representations, and (iii) cross-task interactions, as opposed to\nprevious approaches that separate these learning objectives into different\nnetwork modules. This unified approach not only reduces the need for meticulous\nempirical structure design but also significantly enhances the multi-task\nnetwork's representation learning capability, as the entire model capacity is\ndevoted to optimizing the three objectives simultaneously. TaskPrompter\nintroduces a new multi-task benchmark based on Cityscapes-3D dataset, which\nrequires the multi-task model to concurrently generate predictions for\nmonocular 3D vehicle detection, semantic segmentation, and monocular depth\nestimation. These tasks are essential for achieving a joint 2D-3D understanding\nof visual scenes, particularly in the development of autonomous driving\nsystems. On this challenging benchmark, our multi-task model demonstrates\nstrong performance compared to single-task state-of-the-art methods and\nestablishes new state-of-the-art results on the challenging 3D detection and\ndepth estimation tasks.\n","authors":["Hanrong Ye"],"pdf_url":"https://arxiv.org/pdf/2304.00971v1.pdf","comment":"A supplementary document for TaskPromtper accepted by ICLR 2023.\n  Project page:\n  https://github.com/prismformore/Multi-Task-Transformer/tree/main/TaskPrompter"},{"id":"http://arxiv.org/abs/2304.00970v1","updated":"2023-04-03T13:41:09Z","published":"2023-04-03T13:41:09Z","title":"Development and Evaluation of Conformal Prediction Methods for QSAR","summary":"  The quantitative structure-activity relationship (QSAR) regression model is a\ncommonly used technique for predicting biological activities of compounds using\ntheir molecular descriptors. Predictions from QSAR models can help, for\nexample, to optimize molecular structure; prioritize compounds for further\nexperimental testing; and estimate their toxicity. In addition to the accurate\nestimation of the activity, it is highly desirable to obtain some estimate of\nthe uncertainty associated with the prediction, e.g., calculate a prediction\ninterval (PI) containing the true molecular activity with a pre-specified\nprobability, say 70%, 90% or 95%. The challenge is that most machine learning\n(ML) algorithms that achieve superior predictive performance require some\nadd-on methods for estimating uncertainty of their prediction. The development\nof these algorithms is an active area of research by statistical and ML\ncommunities but their implementation for QSAR modeling remains limited.\nConformal prediction (CP) is a promising approach. It is agnostic to the\nprediction algorithm and can produce valid prediction intervals under some weak\nassumptions on the data distribution. We proposed computationally efficient CP\nalgorithms tailored to the most advanced ML models, including Deep Neural\nNetworks and Gradient Boosting Machines. The validity and efficiency of\nproposed conformal predictors are demonstrated on a diverse collection of QSAR\ndatasets as well as simulation studies.\n","authors":["Yuting Xu","Andy Liaw","Robert P. Sheridan","Vladimir Svetnik"],"pdf_url":"https://arxiv.org/pdf/2304.00970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.00501v2","updated":"2023-04-03T13:32:08Z","published":"2022-06-01T14:00:37Z","title":"Benign Overfitting in Classification: Provably Counter Label Noise with\n  Larger Models","summary":"  Studies on benign overfitting provide insights for the success of\noverparameterized deep learning models. In this work, we examine whether\noverfitting is truly benign in real-world classification tasks. We start with\nthe observation that a ResNet model overfits benignly on Cifar10 but not\nbenignly on ImageNet. To understand why benign overfitting fails in the\nImageNet experiment, we theoretically analyze benign overfitting under a more\nrestrictive setup where the number of parameters is not significantly larger\nthan the number of data points. Under this mild overparameterization setup, our\nanalysis identifies a phase change: unlike in the previous heavy\noverparameterization settings, benign overfitting can now fail in the presence\nof label noise. Our analysis explains our empirical observations, and is\nvalidated by a set of control experiments with ResNets. Our work highlights the\nimportance of understanding implicit bias in underfitting regimes as a future\ndirection.\n","authors":["Kaiyue Wen","Jiaye Teng","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2206.00501v2.pdf","comment":"Published as a conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2304.00957v1","updated":"2023-04-03T13:25:22Z","published":"2023-04-03T13:25:22Z","title":"Properties and Potential Applications of Random Functional-Linked Types\n  of Neural Networks","summary":"  Random functional-linked types of neural networks (RFLNNs), e.g., the extreme\nlearning machine (ELM) and broad learning system (BLS), which avoid suffering\nfrom a time-consuming training process, offer an alternative way of learning in\ndeep structure. The RFLNNs have achieved excellent performance in various\nclassification and regression tasks, however, the properties and explanations\nof these networks are ignored in previous research. This paper gives some\ninsights into the properties of RFLNNs from the viewpoints of frequency domain,\nand discovers the presence of frequency principle in these networks, that is,\nthey preferentially capture low-frequencies quickly and then fit the high\nfrequency components during the training process. These findings are valuable\nfor understanding the RFLNNs and expanding their applications. Guided by the\nfrequency principle, we propose a method to generate a BLS network with better\nperformance, and design an efficient algorithm for solving Poison's equation in\nview of the different frequency principle presenting in the Jacobi iterative\nmethod and BLS network.\n","authors":["Guang-Yong Chen","Yong-Hang Yu","Min Gan","C. L. Philip Chen","Wenzhong Guo"],"pdf_url":"https://arxiv.org/pdf/2304.00957v1.pdf","comment":"9 pages,14 figures"},{"id":"http://arxiv.org/abs/2303.16458v2","updated":"2023-04-03T13:23:35Z","published":"2023-03-29T05:05:02Z","title":"When to Pre-Train Graph Neural Networks? An Answer from Data Generation\n  Perspective!","summary":"  Recently, graph pre-training has attracted wide research attention, which\naims to learn transferable knowledge from unlabeled graph data so as to improve\ndownstream performance. Despite these recent attempts, the negative transfer is\na major issue when applying graph pre-trained models to downstream tasks.\nExisting works made great efforts on the issue of what to pre-train and how to\npre-train by designing a number of graph pre-training and fine-tuning\nstrategies. However, there are indeed cases where no matter how advanced the\nstrategy is, the \"pre-train and fine-tune\" paradigm still cannot achieve clear\nbenefits. This paper introduces a generic framework W2PGNN to answer the\ncrucial question of when to pre-train (i.e., in what situations could we take\nadvantage of graph pre-training) before performing effortful pre-training or\nfine-tuning. We start from a new perspective to explore the complex generative\nmechanisms from the pre-training data to downstream data. In particular, W2PGNN\nfirst fits the pre-training data into graphon bases, each element of graphon\nbasis (i.e., a graphon) identifies a fundamental transferable pattern shared by\na collection of pre-training graphs. All convex combinations of graphon bases\ngive rise to a generator space, from which graphs generated form the solution\nspace for those downstream data that can benefit from pre-training. In this\nmanner, the feasibility of pre-training can be quantified as the generation\nprobability of the downstream data from any generator in the generator space.\nW2PGNN provides three broad applications, including providing the application\nscope of graph pre-trained models, quantifying the feasibility of performing\npre-training, and helping select pre-training data to enhance downstream\nperformance. We give a theoretically sound solution for the first application\nand extensive empirical justifications for the latter two applications.\n","authors":["Yuxuan Cao","Jiarong Xu","Carl Yang","Jiaan Wang","Yunchao Zhang","Chunping Wang","Lei Chen","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.16458v2.pdf","comment":"This paper was withdrawn because it was submitted without the consent\n  of one of the co-authors. It does not contain any errors that need to be\n  corrected"},{"id":"http://arxiv.org/abs/2304.00952v1","updated":"2023-04-03T13:16:33Z","published":"2023-04-03T13:16:33Z","title":"Optimizing data-flow in Binary Neural Networks","summary":"  Binary Neural Networks (BNNs) can significantly accelerate the inference time\nof a neural network by replacing its expensive floating-point arithmetic with\nbitwise operations. Most existing solutions, however, do not fully optimize\ndata flow through the BNN layers, and intermediate conversions from 1 to 16/32\nbits often further hinder efficiency. We propose a novel training scheme that\ncan increase data flow and parallelism in the BNN pipeline; specifically, we\nintroduce a clipping block that decreases the data-width from 32 bits to 8.\nFurthermore, we reduce the internal accumulator size of a binary layer, usually\nkept using 32-bit to prevent data overflow without losing accuracy.\nAdditionally, we provide an optimization of the Batch Normalization layer that\nboth reduces latency and simplifies deployment. Finally, we present an\noptimized implementation of the Binary Direct Convolution for ARM instruction\nsets. Our experiments show a consistent improvement of the inference speed (up\nto 1.91 and 2.73x compared to two state-of-the-art BNNs frameworks) with no\ndrop in accuracy for at least one full-precision model.\n","authors":["L. Vorabbi","D. Maltoni","S. Santi"],"pdf_url":"https://arxiv.org/pdf/2304.00952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00947v1","updated":"2023-04-03T13:13:12Z","published":"2023-04-03T13:13:12Z","title":"RePAST: Relative Pose Attention Scene Representation Transformer","summary":"  The Scene Representation Transformer (SRT) is a recent method to render novel\nviews at interactive rates. Since SRT uses camera poses with respect to an\narbitrarily chosen reference camera, it is not invariant to the order of the\ninput views. As a result, SRT is not directly applicable to large-scale scenes\nwhere the reference frame would need to be changed regularly. In this work, we\npropose Relative Pose Attention SRT (RePAST): Instead of fixing a reference\nframe at the input, we inject pairwise relative camera pose information\ndirectly into the attention mechanism of the Transformers. This leads to a\nmodel that is by definition invariant to the choice of any global reference\nframe, while still retaining the full capabilities of the original method.\nEmpirical results show that adding this invariance to the model does not lead\nto a loss in quality. We believe that this is a step towards applying fully\nlatent transformer-based rendering methods to large-scale scenes.\n","authors":["Aleksandr Safin","Daniel Durckworth","Mehdi S. M. Sajjadi"],"pdf_url":"https://arxiv.org/pdf/2304.00947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00935v1","updated":"2023-04-03T12:47:18Z","published":"2023-04-03T12:47:18Z","title":"Learning Sparsity of Representations with Discrete Latent Variables","summary":"  Deep latent generative models have attracted increasing attention due to the\ncapacity of combining the strengths of deep learning and probabilistic models\nin an elegant way. The data representations learned with the models are often\ncontinuous and dense. However in many applications, sparse representations are\nexpected, such as learning sparse high dimensional embedding of data in an\nunsupervised setting, and learning multi-labels from thousands of candidate\ntags in a supervised setting. In some scenarios, there could be further\nrestriction on degree of sparsity: the number of non-zero features of a\nrepresentation cannot be larger than a pre-defined threshold $L_0$. In this\npaper we propose a sparse deep latent generative model SDLGM to explicitly\nmodel degree of sparsity and thus enable to learn the sparse structure of the\ndata with the quantified sparsity constraint. The resulting sparsity of a\nrepresentation is not fixed, but fits to the observation itself under the\npre-defined restriction. In particular, we introduce to each observation $i$ an\nauxiliary random variable $L_i$, which models the sparsity of its\nrepresentation. The sparse representations are then generated with a two-step\nsampling process via two Gumbel-Softmax distributions. For inference and\nlearning, we develop an amortized variational method based on MC gradient\nestimator. The resulting sparse representations are differentiable with\nbackpropagation. The experimental evaluation on multiple datasets for\nunsupervised and supervised learning problems shows the benefits of the\nproposed method.\n","authors":["Zhao Xu","Daniel Onoro Rubio","Giuseppe Serra","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2304.00935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00933v1","updated":"2023-04-03T12:45:52Z","published":"2023-04-03T12:45:52Z","title":"Knowledge Accumulation in Continually Learned Representations and the\n  Issue of Feature Forgetting","summary":"  By default, neural networks learn on all training data at once. When such a\nmodel is trained on sequential chunks of new data, it tends to catastrophically\nforget how to handle old data. In this work we investigate how continual\nlearners learn and forget representations. We observe two phenomena: knowledge\naccumulation, i.e. the improvement of a representation over time, and feature\nforgetting, i.e. the loss of task-specific representations. To better\nunderstand both phenomena, we introduce a new analysis technique called task\nexclusion comparison. If a model has seen a task and it has not forgotten all\nthe task-specific features, then its representation for that task should be\nbetter than that of a model that was trained on similar tasks, but not that\nexact one. Our image classification experiments show that most task-specific\nfeatures are quickly forgotten, in contrast to what has been suggested in the\npast. Further, we demonstrate how some continual learning methods, like replay,\nand ideas from representation learning affect a continually learned\nrepresentation. We conclude by observing that representation quality is tightly\ncorrelated with continual learning performance.\n","authors":["Timm Hess","Eli Verwimp","Gido M. van de Ven","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2304.00933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14042v2","updated":"2023-04-03T12:40:57Z","published":"2022-12-20T16:37:20Z","title":"FunkNN: Neural Interpolation for Functional Generation","summary":"  Can we build continuous generative models which generalize across scales, can\nbe evaluated at any coordinate, admit calculation of exact derivatives, and are\nconceptually simple? Existing MLP-based architectures generate worse samples\nthan the grid-based generators with favorable convolutional inductive biases.\nModels that focus on generating images at different scales do better, but\nemploy complex architectures not designed for continuous evaluation of images\nand derivatives. We take a signal-processing perspective and treat continuous\nimage generation as interpolation from samples. Indeed, correctly sampled\ndiscrete images contain all information about the low spatial frequencies. The\nquestion is then how to extrapolate the spectrum in a data-driven way while\nmeeting the above design criteria. Our answer is FunkNN -- a new convolutional\nnetwork which learns how to reconstruct continuous images at arbitrary\ncoordinates and can be applied to any image dataset. Combined with a discrete\ngenerative model it becomes a functional generator which can act as a prior in\ncontinuous ill-posed inverse problems. We show that FunkNN generates\nhigh-quality continuous images and exhibits strong out-of-distribution\nperformance thanks to its patch-based design. We further showcase its\nperformance in several stylized inverse problems with exact spatial\nderivatives.\n","authors":["AmirEhsan Khorashadizadeh","Anadi Chaman","Valentin Debarnot","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2212.14042v2.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2304.00918v1","updated":"2023-04-03T12:18:23Z","published":"2023-04-03T12:18:23Z","title":"Uncertainty Propagation in Node Classification","summary":"  Quantifying predictive uncertainty of neural networks has recently attracted\nincreasing attention. In this work, we focus on measuring uncertainty of graph\nneural networks (GNNs) for the task of node classification. Most existing GNNs\nmodel message passing among nodes. The messages are often deterministic.\nQuestions naturally arise: Does there exist uncertainty in the messages? How\ncould we propagate such uncertainty over a graph together with messages? To\naddress these issues, we propose a Bayesian uncertainty propagation (BUP)\nmethod, which embeds GNNs in a Bayesian modeling framework, and models\npredictive uncertainty of node classification with Bayesian confidence of\npredictive probability and uncertainty of messages. Our method proposes a novel\nuncertainty propagation mechanism inspired by Gaussian models. Moreover, we\npresent an uncertainty oriented loss for node classification that allows the\nGNNs to clearly integrate predictive uncertainty in learning procedure.\nConsequently, the training examples with large predictive uncertainty will be\npenalized. We demonstrate the BUP with respect to prediction reliability and\nout-of-distribution (OOD) predictions. The learned uncertainty is also analyzed\nin depth. The relations between uncertainty and graph topology, as well as\npredictive uncertainty in the OOD cases are investigated with extensive\nexperiments. The empirical results with popular benchmark datasets demonstrate\nthe superior performance of the proposed method.\n","authors":["Zhao Xu","Carolin Lawrence","Ammar Shaker","Raman Siarheyeu"],"pdf_url":"https://arxiv.org/pdf/2304.00918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.07664v3","updated":"2023-04-03T12:13:43Z","published":"2022-04-15T22:26:21Z","title":"Conditional Injective Flows for Bayesian Imaging","summary":"  Most deep learning models for computational imaging regress a single\nreconstructed image. In practice, however, ill-posedness, nonlinearity, model\nmismatch, and noise often conspire to make such point estimates misleading or\ninsufficient. The Bayesian approach models images and (noisy) measurements as\njointly distributed random vectors and aims to approximate the posterior\ndistribution of unknowns. Recent variational inference methods based on\nconditional normalizing flows are a promising alternative to traditional MCMC\nmethods, but they come with drawbacks: excessive memory and compute demands for\nmoderate to high resolution images and underwhelming performance on hard\nnonlinear problems. In this work, we propose C-Trumpets -- conditional\ninjective flows specifically designed for imaging problems, which greatly\ndiminish these challenges. Injectivity reduces memory footprint and training\ntime while low-dimensional latent space together with architectural innovations\nlike fixed-volume-change layers and skip-connection revnet layers, C-Trumpets\noutperform regular conditional flow models on a variety of imaging and image\nrestoration tasks, including limited-view CT and nonlinear inverse scattering,\nwith a lower compute and memory budget. C-Trumpets enable fast approximation of\npoint estimates like MMSE or MAP as well as physically-meaningful uncertainty\nquantification.\n","authors":["AmirEhsan Khorashadizadeh","Konik Kothari","Leonardo Salsi","Ali Aghababaei Harandi","Maarten de Hoop","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2204.07664v3.pdf","comment":"23 pages, 23 figures"},{"id":"http://arxiv.org/abs/2304.00917v1","updated":"2023-04-03T12:13:42Z","published":"2023-04-03T12:13:42Z","title":"Diffusion Bridge Mixture Transports, Schrödinger Bridge Problems and\n  Generative Modeling","summary":"  The dynamic Schr\\\"odinger bridge problem seeks a stochastic process that\ndefines a transport between two target probability measures, while optimally\nsatisfying the criteria of being closest, in terms of Kullback-Leibler\ndivergence, to a reference process.\n  We propose a novel sampling-based iterative algorithm, the iterated diffusion\nbridge mixture transport (IDBM), aimed at solving the dynamic Schr\\\"odinger\nbridge problem. The IDBM procedure exhibits the attractive property of\nrealizing a valid coupling between the target measures at each step. We perform\nan initial theoretical investigation of the IDBM procedure, establishing its\nconvergence properties. The theoretical findings are complemented by numerous\nnumerical experiments illustrating the competitive performance of the IDBM\nprocedure across various applications.\n  Recent advancements in generative modeling employ the time-reversal of a\ndiffusion process to define a generative process that approximately transports\na simple distribution to the data distribution. As an alternative, we propose\nusing the first iteration of the IDBM procedure as an approximation-free method\nfor realizing this transport. This approach offers greater flexibility in\nselecting the generative process dynamics and exhibits faster training and\nsuperior sample quality over longer discretization intervals. In terms of\nimplementation, the necessary modifications are minimally intrusive, being\nlimited to the training loss computation, with no changes necessary for\ngenerative sampling.\n","authors":["Stefano Peluchetti"],"pdf_url":"https://arxiv.org/pdf/2304.00917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.06206v6","updated":"2023-04-03T12:02:02Z","published":"2021-11-11T13:48:20Z","title":"Defining and Quantifying the Emergence of Sparse Concepts in DNNs","summary":"  This paper aims to illustrate the concept-emerging phenomenon in a trained\nDNN. Specifically, we find that the inference score of a DNN can be\ndisentangled into the effects of a few interactive concepts. These concepts can\nbe understood as causal patterns in a sparse, symbolic causal graph, which\nexplains the DNN. The faithfulness of using such a causal graph to explain the\nDNN is theoretically guaranteed, because we prove that the causal graph can\nwell mimic the DNN's outputs on an exponential number of different masked\nsamples. Besides, such a causal graph can be further simplified and re-written\nas an And-Or graph (AOG), without losing much explanation accuracy.\n","authors":["Jie Ren","Mingjie Li","Qirui Chen","Huiqi Deng","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2111.06206v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00909v1","updated":"2023-04-03T11:55:39Z","published":"2023-04-03T11:55:39Z","title":"Laplace-fPINNs: Laplace-based fractional physics-informed neural\n  networks for solving forward and inverse problems of subdiffusion","summary":"  The use of Physics-informed neural networks (PINNs) has shown promise in\nsolving forward and inverse problems of fractional diffusion equations.\nHowever, due to the fact that automatic differentiation is not applicable for\nfractional derivatives, solving fractional diffusion equations using PINNs\nrequires addressing additional challenges. To address this issue, this paper\nproposes an extension to PINNs called Laplace-based fractional physics-informed\nneural networks (Laplace-fPINNs), which can effectively solve the forward and\ninverse problems of fractional diffusion equations. This approach avoids\nintroducing a mass of auxiliary points and simplifies the loss function. We\nvalidate the effectiveness of the Laplace-fPINNs approach using several\nexamples. Our numerical results demonstrate that the Laplace-fPINNs method can\neffectively solve both the forward and inverse problems of high-dimensional\nfractional diffusion equations.\n","authors":["Xiong-Bin Yan","Zhi-Qin John Xu","Zheng Ma"],"pdf_url":"https://arxiv.org/pdf/2304.00909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00906v1","updated":"2023-04-03T11:51:46Z","published":"2023-04-03T11:51:46Z","title":"ScandEval: A Benchmark for Scandinavian Natural Language Processing","summary":"  This paper introduces a Scandinavian benchmarking platform, ScandEval, which\ncan benchmark any pretrained model on four different tasks in the Scandinavian\nlanguages. The datasets used in two of the tasks, linguistic acceptability and\nquestion answering, are new. We develop and release a Python package and\ncommand-line interface, scandeval, which can benchmark any model that has been\nuploaded to the Hugging Face Hub, with reproducible results. Using this\npackage, we benchmark more than 100 Scandinavian or multilingual models and\npresent the results of these in an interactive online leaderboard, as well as\nprovide an analysis of the results. The analysis shows that there is\nsubstantial cross-lingual transfer among the Mainland Scandinavian languages\n(Danish, Swedish and Norwegian), with limited cross-lingual transfer between\nthe group of Mainland Scandinavian languages and the group of Insular\nScandinavian languages (Icelandic and Faroese). The benchmarking results also\nshow that the investment in language technology in Norway, Sweden and Denmark\nhas led to language models that outperform massively multilingual models such\nas XLM-RoBERTa and mDeBERTaV3. We release the source code for both the package\nand leaderboard.\n","authors":["Dan Saattrup Nielsen"],"pdf_url":"https://arxiv.org/pdf/2304.00906v1.pdf","comment":"17 pages, 11 figures, camera-ready NoDaLiDa 2023 submission"},{"id":"http://arxiv.org/abs/2210.00935v4","updated":"2023-04-03T11:40:16Z","published":"2022-10-03T13:47:47Z","title":"Analysis of (sub-)Riemannian PDE-G-CNNs","summary":"  Group equivariant convolutional neural networks (G-CNNs) have been\nsuccessfully applied in geometric deep learning. Typically, G-CNNs have the\nadvantage over CNNs that they do not waste network capacity on training\nsymmetries that should have been hard-coded in the network. The recently\nintroduced framework of PDE-based G-CNNs (PDE-G-CNNs) generalises G-CNNs.\nPDE-G-CNNs have the core advantages that they simultaneously 1) reduce network\ncomplexity, 2) increase classification performance, and 3) provide geometric\ninterpretability. Their implementations primarily consist of linear and\nmorphological convolutions with kernels.\n  In this paper we show that the previously suggested approximative\nmorphological kernels do not always accurately approximate the exact kernels\naccurately. More specifically, depending on the spatial anisotropy of the\nRiemannian metric, we argue that one must resort to sub-Riemannian\napproximations. We solve this problem by providing a new approximative kernel\nthat works regardless of the anisotropy. We provide new theorems with better\nerror estimates of the approximative kernels, and prove that they all carry the\nsame reflectional symmetries as the exact ones.\n  We test the effectiveness of multiple approximative kernels within the\nPDE-G-CNN framework on two datasets, and observe an improvement with the new\napproximative kernels. We report that the PDE-G-CNNs again allow for a\nconsiderable reduction of network complexity while having comparable or better\nperformance than G-CNNs and CNNs on the two datasets. Moreover, PDE-G-CNNs have\nthe advantage of better geometric interpretability over G-CNNs, as the\nmorphological kernels are related to association fields from neurogeometry.\n","authors":["Gijs Bellaard","Daan L. J. Bon","Gautam Pai","Bart M. N. Smets","Remco Duits"],"pdf_url":"https://arxiv.org/pdf/2210.00935v4.pdf","comment":"29 pages, 21 figures"},{"id":"http://arxiv.org/abs/2304.00897v1","updated":"2023-04-03T11:35:10Z","published":"2023-04-03T11:35:10Z","title":"Accuracy is not the only Metric that matters: Estimating the Energy\n  Consumption of Deep Learning Models","summary":"  Modern machine learning models have started to consume incredible amounts of\nenergy, thus incurring large carbon footprints (Strubell et al., 2019). To\naddress this issue, we have created an energy estimation pipeline1, which\nallows practitioners to estimate the energy needs of their models in advance,\nwithout actually running or training them. We accomplished this, by collecting\nhigh-quality energy data and building a first baseline model, capable of\npredicting the energy consumption of DL models by accumulating their estimated\nlayer-wise energies.\n","authors":["Johannes Getzner","Bertrand Charpentier","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2304.00897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.04384v4","updated":"2023-04-03T11:27:19Z","published":"2021-06-08T14:14:24Z","title":"FL-Market: Trading Private Models in Federated Learning","summary":"  The difficulty in acquiring a sufficient amount of training data is a major\nbottleneck for machine learning (ML) based data analytics. Recently,\ncommoditizing ML models has been proposed as an economical and moderate\nsolution to ML-oriented data acquisition. However, existing model marketplaces\nassume that the broker can access data owners' private training data, which may\nnot be realistic in practice. In this paper, to promote trustworthy data\nacquisition for ML tasks, we propose FL-Market, a locally private model\nmarketplace that protects privacy not only against model buyers but also\nagainst the untrusted broker. FL-Market decouples ML from the need to centrally\ngather training data on the broker's side using federated learning, an emerging\nprivacy-preserving ML paradigm in which data owners collaboratively train an ML\nmodel by uploading local gradients (to be aggregated into a global gradient for\nmodel updating). Then, FL-Market enables data owners to locally perturb their\ngradients by local differential privacy and thus further prevents privacy\nrisks. To drive FL-Market, we propose a deep learning-empowered auction\nmechanism for intelligently deciding the local gradients' perturbation levels\nand an optimal aggregation mechanism for aggregating the perturbed gradients.\nOur auction and aggregation mechanisms can jointly maximize the global\ngradient's accuracy, which optimizes model buyers' utility. Our experiments\nverify the effectiveness of the proposed mechanisms.\n","authors":["Shuyuan Zheng","Yang Cao","Masatoshi Yoshikawa","Huizhong Li","Qiang Yan"],"pdf_url":"https://arxiv.org/pdf/2106.04384v4.pdf","comment":"Extended report for our IEEE BigData 2022 paper"},{"id":"http://arxiv.org/abs/2304.00891v1","updated":"2023-04-03T11:26:56Z","published":"2023-04-03T11:26:56Z","title":"Online Algorithms for Hierarchical Inference in Deep Learning\n  applications at the Edge","summary":"  We consider a resource-constrained Edge Device (ED) embedded with a\nsmall-size ML model (S-ML) for a generic classification application, and an\nEdge Server (ES) that hosts a large-size ML model (L-ML). Since the inference\naccuracy of S-ML is lower than that of the L-ML, offloading all the data\nsamples to the ES results in high inference accuracy, but it defeats the\npurpose of embedding S-ML on the ED and deprives the benefits of reduced\nlatency, bandwidth savings, and energy efficiency of doing local inference. To\nget the best out of both worlds, i.e., the benefits of doing inference on the\nED and the benefits of doing inference on ES, we explore the idea of\nHierarchical Inference (HI), wherein S-ML inference is only accepted when it is\ncorrect, otherwise the data sample is offloaded for L-ML inference. However,\nthe ideal implementation of HI is infeasible as the correctness of the S-ML\ninference is not known to the ED. We thus propose an online meta-learning\nframework to predict the correctness of the S-ML inference. The resulting\nonline learning problem turns out to be a Prediction with Expert Advice (PEA)\nproblem with continuous expert space. We consider the full feedback scenario,\nwhere the ED receives feedback on the correctness of the S-ML once it accepts\nthe inference, and the no-local feedback scenario, where the ED does not\nreceive the ground truth for the classification, and propose the HIL-F and\nHIL-N algorithms and prove a regret bound that is sublinear with the number of\ndata samples. We evaluate and benchmark the performance of the proposed\nalgorithms for image classification applications using four datasets, namely,\nImagenette, Imagewoof, MNIST, and CIFAR-10.\n","authors":["Vishnu Narayanan Moothedath","Jaya Prakash Champati","James Gross"],"pdf_url":"https://arxiv.org/pdf/2304.00891v1.pdf","comment":"This work will be appearing in a journal soon and the 'Journal\n  reference' will be updated as and when the information is available. The\n  submission contains 22 pages, 7 figures including subfigures, 2 tables and 2\n  algorithms"},{"id":"http://arxiv.org/abs/2304.00873v1","updated":"2023-04-03T10:55:16Z","published":"2023-04-03T10:55:16Z","title":"Action Pick-up in Dynamic Action Space Reinforcement Learning","summary":"  Most reinforcement learning algorithms are based on a key assumption that\nMarkov decision processes (MDPs) are stationary. However, non-stationary MDPs\nwith dynamic action space are omnipresent in real-world scenarios. Yet problems\nof dynamic action space reinforcement learning have been studied by many\nprevious works, how to choose valuable actions from new and unseen actions to\nimprove learning efficiency remains unaddressed. To tackle this problem, we\npropose an intelligent Action Pick-up (AP) algorithm to autonomously choose\nvaluable actions that are most likely to boost performance from a set of new\nactions. In this paper, we first theoretically analyze and find that a prior\noptimal policy plays an important role in action pick-up by providing useful\nknowledge and experience. Then, we design two different AP methods:\nfrequency-based global method and state clustering-based local method, based on\nthe prior optimal policy. Finally, we evaluate the AP on two simulated but\nchallenging environments where action spaces vary over time. Experimental\nresults demonstrate that our proposed AP has advantages over baselines in\nlearning efficiency.\n","authors":["Jiaqi Ye","Xiaodong Li","Pangjing Wu","Feng Wang"],"pdf_url":"https://arxiv.org/pdf/2304.00873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16589v2","updated":"2023-04-03T10:42:20Z","published":"2023-03-29T10:49:31Z","title":"Poster: Link between Bias, Node Sensitivity and Long-Tail Distribution\n  in trained DNNs","summary":"  Owing to their remarkable learning (and relearning) capabilities, deep neural\nnetworks (DNNs) find use in numerous real-world applications. However, the\nlearning of these data-driven machine learning models is generally as good as\nthe data available to them for training. Hence, training datasets with\nlong-tail distribution pose a challenge for DNNs, since the DNNs trained on\nthem may provide a varying degree of classification performance across\ndifferent output classes. While the overall bias of such networks is already\nhighlighted in existing works, this work identifies the node bias that leads to\na varying sensitivity of the nodes for different output classes. To the best of\nour knowledge, this is the first work highlighting this unique challenge in\nDNNs, discussing its probable causes, and providing open challenges for this\nnew research direction. We support our reasoning using an empirical case study\nof the networks trained on a real-world dataset.\n","authors":["Mahum Naseer","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.16589v2.pdf","comment":"To appear at the 16th IEEE International Conference on Software\n  Testing, Verification and Validation (ICST 2023), Dublin, Ireland"},{"id":"http://arxiv.org/abs/2302.04419v2","updated":"2023-04-03T10:34:44Z","published":"2023-02-09T03:11:21Z","title":"An Investigation into Pre-Training Object-Centric Representations for\n  Reinforcement Learning","summary":"  Unsupervised object-centric representation (OCR) learning has recently drawn\nattention as a new paradigm of visual representation. This is because of its\npotential of being an effective pre-training technique for various downstream\ntasks in terms of sample efficiency, systematic generalization, and reasoning.\nAlthough image-based reinforcement learning (RL) is one of the most important\nand thus frequently mentioned such downstream tasks, the benefit in RL has\nsurprisingly not been investigated systematically thus far. Instead, most of\nthe evaluations have focused on rather indirect metrics such as segmentation\nquality and object property prediction accuracy. In this paper, we investigate\nthe effectiveness of OCR pre-training for image-based reinforcement learning\nvia empirical experiments. For systematic evaluation, we introduce a simple\nobject-centric visual RL benchmark and conduct experiments to answer questions\nsuch as ``Does OCR pre-training improve performance on object-centric tasks?''\nand ``Can OCR pre-training help with out-of-distribution generalization?''. Our\nresults provide empirical evidence for valuable insights into the effectiveness\nof OCR pre-training for RL and the potential limitations of its use in certain\nscenarios. Additionally, this study also examines the critical aspects of\nincorporating OCR pre-training in RL, including performance in a visually\ncomplex environment and the appropriate pooling layer to aggregate the object\nrepresentations.\n","authors":["Jaesik Yoon","Yi-Fu Wu","Heechul Bae","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2302.04419v2.pdf","comment":"We study unsupervised object-centric representations in reinforcement\n  learning through systematic investigation"},{"id":"http://arxiv.org/abs/2304.00860v1","updated":"2023-04-03T10:16:24Z","published":"2023-04-03T10:16:24Z","title":"Designing and Evaluating Speech Emotion Recognition Systems: A reality\n  check case study with IEMOCAP","summary":"  There is an imminent need for guidelines and standard test sets to allow\ndirect and fair comparisons of speech emotion recognition (SER). While\nresources, such as the Interactive Emotional Dyadic Motion Capture (IEMOCAP)\ndatabase, have emerged as widely-adopted reference corpora for researchers to\ndevelop and test models for SER, published work reveals a wide range of\nassumptions and variety in its use that challenge reproducibility and\ngeneralization. Based on a critical review of the latest advances in SER using\nIEMOCAP as the use case, our work aims at two contributions: First, using an\nanalysis of the recent literature, including assumptions made and metrics used\ntherein, we provide a set of SER evaluation guidelines. Second, using recent\npublications with open-sourced implementations, we focus on reproducibility\nassessment in SER.\n","authors":["Nikolaos Antoniou","Athanasios Katsamanis","Theodoros Giannakopoulos","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2304.00860v1.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2302.07517v2","updated":"2023-04-03T09:46:40Z","published":"2023-02-15T08:26:24Z","title":"Extensible Motion-based Identification of XR Users using Non-Specific\n  Motion Data","summary":"  In this paper, we combine the strengths of distance-based and\nclassification-based approaches for the task of identifying extended reality\nusers by their movements. For this we present an embedding-based approach that\nleverages deep metric learning. We train the model on a dataset of users\nplaying the VR game ``Half-Life: Alyx'' and conduct multiple experiments and\nanalyses using a state of the art classification-based model as baseline. The\nresults show that the embedding-based method 1) is able to identify new users\nfrom non-specific movements using only a few minutes of enrollment data, 2) can\nenroll new users within seconds, while retraining the baseline approach takes\nalmost a day, 3) is more reliable than the baseline approach when only little\nenrollment data is available, 4) can be used to identify new users from another\ndataset recorded with different VR devices.\n  Altogether, our solution is a foundation for easily extensible XR user\nidentification systems, applicable to a wide range of user motions. It also\npaves the way for production-ready models that could be used by XR\npractitioners without the requirements of expertise, hardware, or data for\ntraining deep learning models.\n","authors":["Christian Schell","Konstantin Kobs","Tamara Fernando","Andreas Hotho","Marc Erich Latoschik"],"pdf_url":"https://arxiv.org/pdf/2302.07517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07816v7","updated":"2023-04-03T09:45:32Z","published":"2022-11-15T00:40:55Z","title":"Quantifying the Impact of Label Noise on Federated Learning","summary":"  Federated Learning (FL) is a distributed machine learning paradigm where\nclients collaboratively train a model using their local (human-generated)\ndatasets. While existing studies focus on FL algorithm development to tackle\ndata heterogeneity across clients, the important issue of data quality (e.g.,\nlabel noise) in FL is overlooked. This paper aims to fill this gap by providing\na quantitative study on the impact of label noise on FL. We derive an upper\nbound for the generalization error that is linear in the clients' label noise\nlevel. Then we conduct experiments on MNIST and CIFAR-10 datasets using various\nFL algorithms. Our empirical results show that the global model accuracy\nlinearly decreases as the noise level increases, which is consistent with our\ntheoretical analysis. We further find that label noise slows down the\nconvergence of FL training, and the global model tends to overfit when the\nnoise level is high.\n","authors":["Shuqi Ke","Chao Huang","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2211.07816v7.pdf","comment":"Accepted by The AAAI 2023 Workshop on Representation Learning for\n  Responsible Human-Centric AI"},{"id":"http://arxiv.org/abs/2302.07675v2","updated":"2023-04-03T09:38:38Z","published":"2023-02-15T14:09:55Z","title":"Guaranteed Dynamic Scheduling of Ultra-Reliable Low-Latency Traffic via\n  Conformal Prediction","summary":"  The dynamic scheduling of ultra-reliable and low-latency traffic (URLLC) in\nthe uplink can significantly enhance the efficiency of coexisting services,\nsuch as enhanced mobile broadband (eMBB) devices, by only allocating resources\nwhen necessary. The main challenge is posed by the uncertainty in the process\nof URLLC packet generation, which mandates the use of predictors for URLLC\ntraffic in the coming frames. In practice, such prediction may overestimate or\nunderestimate the amount of URLLC data to be generated, yielding either an\nexcessive or an insufficient amount of resources to be pre-emptively allocated\nfor URLLC packets. In this paper, we introduce a novel scheduler for URLLC\npackets that provides formal guarantees on reliability and latency irrespective\nof the quality of the URLLC traffic predictor. The proposed method leverages\nrecent advances in online conformal prediction (CP), and follows the principle\nof dynamically adjusting the amount of allocated resources so as to meet\nreliability and latency requirements set by the designer.\n","authors":["Kfir M. Cohen","Sangwoo Park","Osvaldo Simeone","Petar Popovski","Shlomo Shamai"],"pdf_url":"https://arxiv.org/pdf/2302.07675v2.pdf","comment":"To appear in IEEE Signal Processing Letters"},{"id":"http://arxiv.org/abs/2302.03542v2","updated":"2023-04-03T09:30:12Z","published":"2023-02-07T15:50:49Z","title":"Two Losses Are Better Than One: Faster Optimization Using a Cheaper\n  Proxy","summary":"  We present an algorithm for minimizing an objective with hard-to-compute\ngradients by using a related, easier-to-access function as a proxy. Our\nalgorithm is based on approximate proximal point iterations on the proxy\ncombined with relatively few stochastic gradients from the objective. When the\ndifference between the objective and the proxy is $\\delta$-smooth, our\nalgorithm guarantees convergence at a rate matching stochastic gradient descent\non a $\\delta$-smooth objective, which can lead to substantially better sample\nefficiency. Our algorithm has many potential applications in machine learning,\nand provides a principled means of leveraging synthetic data, physics\nsimulators, mixed public and private data, and more.\n","authors":["Blake Woodworth","Konstantin Mishchenko","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2302.03542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07272v2","updated":"2023-04-03T09:28:55Z","published":"2023-03-10T10:32:18Z","title":"What is the state of the art? Accounting for multiplicity in machine\n  learning benchmark performance","summary":"  Machine learning methods are commonly evaluated and compared by their\nperformance on data sets from public repositories. This allows for multiple\nmethods, oftentimes several thousands, to be evaluated under identical\nconditions and across time. The highest ranked performance on a problem is\nreferred to as state-of-the-art (SOTA) performance, and is used, among other\nthings, as a reference point for publication of new methods. Using the\nhighest-ranked performance as an estimate for SOTA is a biased estimator,\ngiving overly optimistic results. The mechanisms at play are those of\nmultiplicity, a topic that is well-studied in the context of multiple\ncomparisons and multiple testing, but has, as far as the authors are aware of,\nbeen nearly absent from the discussion regarding SOTA estimates. The optimistic\nstate-of-the-art estimate is used as a standard for evaluating new methods, and\nmethods with substantial inferior results are easily overlooked. In this\narticle, we provide a probability distribution for the case of multiple\nclassifiers so that known analyses methods can be engaged and a better SOTA\nestimate can be provided. We demonstrate the impact of multiplicity through a\nsimulated example with independent classifiers. We show how classifier\ndependency impacts the variance, but also that the impact is limited when the\naccuracy is high. Finally, we discuss a real-world example; a Kaggle\ncompetition from 2020.\n","authors":["Kajsa Møllersen","Einar Holsbø"],"pdf_url":"https://arxiv.org/pdf/2303.07272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.04777v6","updated":"2023-04-03T09:23:26Z","published":"2022-02-10T00:13:34Z","title":"Exact Solutions of a Deep Linear Network","summary":"  This work finds the analytical expression of the global minima of a deep\nlinear network with weight decay and stochastic neurons, a fundamental model\nfor understanding the landscape of neural networks. Our result implies that\nzero is a special point in deep neural network architecture. We show that\nweight decay strongly interacts with the model architecture and can create bad\nminima at zero in a network with more than $1$ hidden layer, qualitatively\ndifferent from a network with only $1$ hidden layer. Practically, our result\nimplies that common deep learning initialization methods are insufficient to\nease the optimization of neural networks in general.\n","authors":["Liu Ziyin","Botao Li","Xiangming Meng"],"pdf_url":"https://arxiv.org/pdf/2202.04777v6.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2304.00830v1","updated":"2023-04-03T09:15:51Z","published":"2023-04-03T09:15:51Z","title":"AUDIT: Audio Editing by Following Instructions with Latent Diffusion\n  Models","summary":"  Audio editing is applicable for various purposes, such as adding background\nsound effects, replacing a musical instrument, and repairing damaged audio.\nRecently, some diffusion-based methods achieved zero-shot audio editing by\nusing a diffusion and denoising process conditioned on the text description of\nthe output audio. However, these methods still have some problems: 1) they have\nnot been trained on editing tasks and cannot ensure good editing effects; 2)\nthey can erroneously modify audio segments that do not require editing; 3) they\nneed a complete description of the output audio, which is not always available\nor necessary in practical scenarios. In this work, we propose AUDIT, an\ninstruction-guided audio editing model based on latent diffusion models.\nSpecifically, AUDIT has three main design features: 1) we construct triplet\ntraining data (instruction, input audio, output audio) for different audio\nediting tasks and train a diffusion model using instruction and input (to be\nedited) audio as conditions and generating output (edited) audio; 2) it can\nautomatically learn to only modify segments that need to be edited by comparing\nthe difference between the input and output audio; 3) it only needs edit\ninstructions instead of full target audio descriptions as text input. AUDIT\nachieves state-of-the-art results in both objective and subjective metrics for\nseveral audio editing tasks (e.g., adding, dropping, replacement, inpainting,\nsuper-resolution). Demo samples are available at https://audit-demo.github.io/.\n","authors":["Yuancheng Wang","Zeqian Ju","Xu Tan","Lei He","Zhizheng Wu","Jiang Bian","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.00830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05731v2","updated":"2023-04-03T09:08:55Z","published":"2023-03-10T06:31:30Z","title":"Upper Bound of Real Log Canonical Threshold of Tensor Decomposition and\n  its Application to Bayesian Inference","summary":"  Tensor decomposition is now being used for data analysis, information\ncompression, and knowledge recovery. However, the mathematical property of\ntensor decomposition is not yet fully clarified because it is one of singular\nlearning machines. In this paper, we give the upper bound of its real log\ncanonical threshold (RLCT) of the tensor decomposition by using an algebraic\ngeometrical method and derive its Bayesian generalization error theoretically.\nWe also give considerations about its mathematical property through numerical\nexperiments.\n","authors":["Naoki Yoshida","Sumio Watanabe"],"pdf_url":"https://arxiv.org/pdf/2303.05731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00818v1","updated":"2023-04-03T09:07:17Z","published":"2023-04-03T09:07:17Z","title":"Swarm Reinforcement Learning For Adaptive Mesh Refinement","summary":"  Adaptive Mesh Refinement (AMR) is crucial for mesh-based simulations, as it\nallows for dynamically adjusting the resolution of a mesh to trade off\ncomputational cost with the simulation accuracy. Yet, existing methods for AMR\neither use task-dependent heuristics, expensive error estimators, or do not\nscale well to larger meshes or more complex problems. In this paper, we\nformalize AMR as a Swarm Reinforcement Learning problem, viewing each element\nof a mesh as part of a collaborative system of simple and homogeneous agents.\nWe combine this problem formulation with a novel agent-wise reward function and\nGraph Neural Networks, allowing us to learn reliable and scalable refinement\nstrategies on arbitrary systems of equations. We experimentally demonstrate the\neffectiveness of our approach in improving the accuracy and efficiency of\ncomplex simulations. Our results show that we outperform learned baselines and\nachieve a refinement quality that is on par with a traditional error-based AMR\nrefinement strategy without requiring error indicators during inference.\n","authors":["Niklas Freymuth","Philipp Dahlinger","Tobias Würth","Luise Kärger","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2304.00818v1.pdf","comment":"Accepted as a workshop paper in the ICLR 2023 Workshop on Physics for\n  Machine Learning"},{"id":"http://arxiv.org/abs/2204.09291v2","updated":"2023-04-03T09:03:07Z","published":"2022-04-20T08:15:54Z","title":"Improving generalization of machine learning-identified biomarkers with\n  causal modeling: an investigation into immune receptor diagnostics","summary":"  Machine learning is increasingly used to discover diagnostic and prognostic\nbiomarkers from high-dimensional molecular data. However, a variety of factors\nrelated to experimental design may affect the ability to learn generalizable\nand clinically applicable diagnostics. Here, we argue that a causal perspective\nimproves the identification of these challenges and formalizes their relation\nto the robustness and generalization of machine learning-based diagnostics. To\nmake for a concrete discussion, we focus on a specific, recently established\nhigh-dimensional biomarker - adaptive immune receptor repertoires (AIRRs).\nThrough simulations, we illustrate how major biological and experimental\nfactors of the AIRR domain may influence the learned biomarkers. In conclusion,\nwe argue that causal modeling improves machine learning-based biomarker\nrobustness by identifying stable relations between variables and by guiding the\nadjustment of the relations and variables that vary between populations.\n","authors":["Milena Pavlović","Ghadi S. Al Hajj","Chakravarthi Kanduri","Johan Pensar","Mollie Wood","Ludvig M. Sollid","Victor Greiff","Geir Kjetil Sandve"],"pdf_url":"https://arxiv.org/pdf/2204.09291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01201v4","updated":"2023-04-03T09:02:13Z","published":"2022-11-02T15:23:16Z","title":"Human alignment of neural network representations","summary":"  Today's computer vision models achieve human or near-human level performance\nacross a wide variety of vision tasks. However, their architectures, data, and\nlearning algorithms differ in numerous ways from those that give rise to human\nvision. In this paper, we investigate the factors that affect the alignment\nbetween the representations learned by neural networks and human mental\nrepresentations inferred from behavioral responses. We find that model scale\nand architecture have essentially no effect on the alignment with human\nbehavioral responses, whereas the training dataset and objective function both\nhave a much larger impact. These findings are consistent across three datasets\nof human similarity judgments collected using two different tasks. Linear\ntransformations of neural network representations learned from behavioral\nresponses from one dataset substantially improve alignment with human\nsimilarity judgments on the other two datasets. In addition, we find that some\nhuman concepts such as food and animals are well-represented by neural networks\nwhereas others such as royal or sports-related objects are not. Overall,\nalthough models trained on larger, more diverse datasets achieve better\nalignment with humans than models trained on ImageNet alone, our results\nindicate that scaling alone is unlikely to be sufficient to train neural\nnetworks with conceptual representations that match those used by humans.\n","authors":["Lukas Muttenthaler","Jonas Dippel","Lorenz Linhardt","Robert A. Vandermeulen","Simon Kornblith"],"pdf_url":"https://arxiv.org/pdf/2211.01201v4.pdf","comment":"Accepted for publication at ICLR 2023"},{"id":"http://arxiv.org/abs/2304.00813v1","updated":"2023-04-03T09:01:59Z","published":"2023-04-03T09:01:59Z","title":"Model-Agnostic Reachability Analysis on Deep Neural Networks","summary":"  Verification plays an essential role in the formal analysis of\nsafety-critical systems. Most current verification methods have specific\nrequirements when working on Deep Neural Networks (DNNs). They either target\none particular network category, e.g., Feedforward Neural Networks (FNNs), or\nnetworks with specific activation functions, e.g., RdLU. In this paper, we\ndevelop a model-agnostic verification framework, called DeepAgn, and show that\nit can be applied to FNNs, Recurrent Neural Networks (RNNs), or a mixture of\nboth. Under the assumption of Lipschitz continuity, DeepAgn analyses the\nreachability of DNNs based on a novel optimisation scheme with a global\nconvergence guarantee. It does not require access to the network's internal\nstructures, such as layers and parameters. Through reachability analysis,\nDeepAgn can tackle several well-known robustness problems, including computing\nthe maximum safe radius for a given input, and generating the ground-truth\nadversarial examples. We also empirically demonstrate DeepAgn's superior\ncapability and efficiency in handling a broader class of deep neural networks,\nincluding both FNNs, and RNNs with very deep layers and millions of neurons,\nthan other state-of-the-art verification approaches.\n","authors":["Chi Zhang","Wenjie Ruan","Fu Wang","Peipei Xu","Geyong Min","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2304.00813v1.pdf","comment":"PAKDD 2023"},{"id":"http://arxiv.org/abs/2210.06462v2","updated":"2023-04-03T08:56:41Z","published":"2022-10-12T17:57:58Z","title":"Self-Guided Diffusion Models","summary":"  Diffusion models have demonstrated remarkable progress in image generation\nquality, especially when guidance is used to control the generative process.\nHowever, guidance requires a large amount of image-annotation pairs for\ntraining and is thus dependent on their availability, correctness and\nunbiasedness. In this paper, we eliminate the need for such annotation by\ninstead leveraging the flexibility of self-supervision signals to design a\nframework for self-guided diffusion models. By leveraging a feature extraction\nfunction and a self-annotation function, our method provides guidance signals\nat various image granularities: from the level of holistic images to object\nboxes and even segmentation masks. Our experiments on single-label and\nmulti-label image datasets demonstrate that self-labeled guidance always\noutperforms diffusion models without guidance and may even surpass guidance\nbased on ground-truth labels, especially on unbalanced data. When equipped with\nself-supervised box or mask proposals, our method further generates visually\ndiverse yet semantically consistent images, without the need for any class,\nbox, or segment label annotation. Self-guided diffusion is simple, flexible and\nexpected to profit from deployment at scale.\n","authors":["Vincent Tao Hu","David W Zhang","Yuki M. Asano","Gertjan J. Burghouts","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2210.06462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00803v1","updated":"2023-04-03T08:50:58Z","published":"2023-04-03T08:50:58Z","title":"A Tutorial Introduction to Reinforcement Learning","summary":"  In this paper, we present a brief survey of Reinforcement Learning (RL), with\nparticular emphasis on Stochastic Approximation (SA) as a unifying theme. The\nscope of the paper includes Markov Reward Processes, Markov Decision Processes,\nStochastic Approximation algorithms, and widely used algorithms such as\nTemporal Difference Learning and $Q$-learning.\n","authors":["Mathukumalli Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2304.00803v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2110.11291v5","updated":"2023-04-03T08:50:44Z","published":"2021-10-21T17:18:59Z","title":"Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs\n  Theory","summary":"  Schr\\\"odinger Bridge (SB) is an entropy-regularized optimal transport problem\nthat has received increasing attention in deep generative modeling for its\nmathematical flexibility compared to the Scored-based Generative Model (SGM).\nHowever, it remains unclear whether the optimization principle of SB relates to\nthe modern training of deep generative models, which often rely on constructing\nlog-likelihood objectives.This raises questions on the suitability of SB models\nas a principled alternative for generative applications. In this work, we\npresent a novel computational framework for likelihood training of SB models\ngrounded on Forward-Backward Stochastic Differential Equations Theory - a\nmathematical methodology appeared in stochastic optimal control that transforms\nthe optimality condition of SB into a set of SDEs. Crucially, these SDEs can be\nused to construct the likelihood objectives for SB that, surprisingly,\ngeneralizes the ones for SGM as special cases. This leads to a new optimization\nprinciple that inherits the same SB optimality yet without losing applications\nof modern generative training techniques, and we show that the resulting\ntraining algorithm achieves comparable results on generating realistic images\non MNIST, CelebA, and CIFAR10. Our code is available at\nhttps://github.com/ghliu/SB-FBSDE.\n","authors":["Tianrong Chen","Guan-Horng Liu","Evangelos A. Theodorou"],"pdf_url":"https://arxiv.org/pdf/2110.11291v5.pdf","comment":"fix appendix net arh error"},{"id":"http://arxiv.org/abs/2304.00792v1","updated":"2023-04-03T08:24:40Z","published":"2023-04-03T08:24:40Z","title":"Few-shot Fine-tuning is All You Need for Source-free Domain Adaptation","summary":"  Recently, source-free unsupervised domain adaptation (SFUDA) has emerged as a\nmore practical and feasible approach compared to unsupervised domain adaptation\n(UDA) which assumes that labeled source data are always accessible. However,\nsignificant limitations associated with SFUDA approaches are often overlooked,\nwhich limits their practicality in real-world applications. These limitations\ninclude a lack of principled ways to determine optimal hyperparameters and\nperformance degradation when the unlabeled target data fail to meet certain\nrequirements such as a closed-set and identical label distribution to the\nsource data. All these limitations stem from the fact that SFUDA entirely\nrelies on unlabeled target data. We empirically demonstrate the limitations of\nexisting SFUDA methods in real-world scenarios including out-of-distribution\nand label distribution shifts in target data, and verify that none of these\nmethods can be safely applied to real-world settings. Based on our experimental\nresults, we claim that fine-tuning a source pretrained model with a few labeled\ndata (e.g., 1- or 3-shot) is a practical and reliable solution to circumvent\nthe limitations of SFUDA. Contrary to common belief, we find that carefully\nfine-tuned models do not suffer from overfitting even when trained with only a\nfew labeled data, and also show little change in performance due to sampling\nbias. Our experimental results on various domain adaptation benchmarks\ndemonstrate that the few-shot fine-tuning approach performs comparatively under\nthe standard SFUDA settings, and outperforms comparison methods under realistic\nscenarios. Our code is available at https://github.com/daintlab/fewshot-SFDA .\n","authors":["Suho Lee","Seungwon Seo","Jihyo Kim","Yejin Lee","Sangheum Hwang"],"pdf_url":"https://arxiv.org/pdf/2304.00792v1.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2212.04780v2","updated":"2023-04-03T08:21:02Z","published":"2022-12-09T11:18:40Z","title":"Genie: Show Me the Data for Quantization","summary":"  Zero-shot quantization is a promising approach for developing lightweight\ndeep neural networks when data is inaccessible owing to various reasons,\nincluding cost and issues related to privacy. By exploiting the learned\nparameters ($\\mu$ and $\\sigma$) of batch normalization layers in an\nFP32-pre-trained model, zero-shot quantization schemes focus on generating\nsynthetic data. Subsequently, they distill knowledge from the pre-trained model\n(teacher) to the quantized model (student) such that the quantized model can be\noptimized with the synthetic dataset. However, thus far, zero-shot quantization\nhas primarily been discussed in the context of quantization-aware training\nmethods, which require task-specific losses and long-term optimization as much\nas retraining. We thus introduce a post-training quantization scheme for\nzero-shot quantization that produces high-quality quantized networks within a\nfew hours. Furthermore, we propose a framework called \\genie~that generates\ndata suited for quantization. With the data synthesized by Genie, we can\nproduce robust quantized models without real datasets, which is comparable to\nfew-shot quantization. We also propose a post-training quantization algorithm\nto enhance the performance of quantized models. By combining them, we can\nbridge the gap between zero-shot and few-shot quantization while significantly\nimproving the quantization performance compared to that of existing approaches.\nIn other words, we can obtain a unique state-of-the-art zero-shot quantization\napproach.\n","authors":["Yongkweon Jeon","Chungman Lee","Ho-young Kim"],"pdf_url":"https://arxiv.org/pdf/2212.04780v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2208.09406v3","updated":"2023-04-03T08:16:37Z","published":"2022-08-19T15:48:30Z","title":"Dance Style Transfer with Cross-modal Transformer","summary":"  We present CycleDance, a dance style transfer system to transform an existing\nmotion clip in one dance style to a motion clip in another dance style while\nattempting to preserve motion context of the dance. Our method extends an\nexisting CycleGAN architecture for modeling audio sequences and integrates\nmultimodal transformer encoders to account for music context. We adopt sequence\nlength-based curriculum learning to stabilize training. Our approach captures\nrich and long-term intra-relations between motion frames, which is a common\nchallenge in motion transfer and synthesis work. We further introduce new\nmetrics for gauging transfer strength and content preservation in the context\nof dance movements. We perform an extensive ablation study as well as a human\nstudy including 30 participants with 5 or more years of dance experience. The\nresults demonstrate that CycleDance generates realistic movements with the\ntarget style, significantly outperforming the baseline CycleGAN on naturalness,\ntransfer strength, and content preservation.\n","authors":["Wenjie Yin","Hang Yin","Kim Baraka","Danica Kragic","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2208.09406v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.03445v4","updated":"2023-04-03T08:15:35Z","published":"2021-09-08T06:06:28Z","title":"Convergence of Batch Asynchronous Stochastic Approximation With\n  Applications to Reinforcement Learning","summary":"  The stochastic approximation (SA) algorithm is a widely used probabilistic\nmethod for finding a zero or a fixed point of a vector-valued funtion, when\nonly noisy measurements of the function are available. In the literature to\ndate, one makes a distinction between ``synchronous'' updating, whereby every\ncomponent of the current guess is updated at each time, and ``asynchronous''\nupdating, whereby only one component is updated. In this paper, we study an\nintermediate situation that we call ``batch asynchronous stochastic\napproximation'' (BASA), in which, at each time instant, \\textit{some but not\nall} components of the current estimated solution are updated. BASA allows the\nuser to trade off memory requirements against time complexity. We develop a\ngeneral methodology for proving that such algorithms converge to the fixed\npoint of the map under study. These convergence proofs make use of weaker\nhypotheses than existing results. Specifically, existing convergence proofs\nrequire that the measurement noise is a zero-mean i.i.d\\ sequence or a\nmartingale difference sequence. In the present paper, we permit biased\nmeasurements, that is, measurement noises that have nonzero conditional mean.\nAlso, all convergence results to date assume that the stochastic step sizes\nsatisfy a probabilistic analog of the well-known Robbins-Monro conditions. We\nreplace this assumption by a purely deterministic condition on the\nirreducibility of the underlying Markov processes.\n  As specific applications to Reinforcement Learning, we analyze the temporal\ndifference algorithm $TD(\\lambda)$ for value iteration, and the $Q$-learning\nalgorithm for finding the optimal action-value function. In both cases, we\nestablish the convergence of these algorithms, under milder conditions than in\nthe existing literature.\n","authors":["Rajeeva L. Karandikar","M. Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2109.03445v4.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2304.00776v1","updated":"2023-04-03T07:59:13Z","published":"2023-04-03T07:59:13Z","title":"Chain-of-Thought Predictive Control","summary":"  We study generalizable policy learning from demonstrations for complex\nlow-level control tasks (e.g., contact-rich object manipulations). We propose\nan imitation learning method that incorporates the idea of temporal abstraction\nand the planning capabilities from Hierarchical RL (HRL) in a novel and\neffective manner. As a step towards decision foundation models, our design can\nutilize scalable, albeit highly sub-optimal, demonstrations. Specifically, we\nfind certain short subsequences of the demos, i.e. the chain-of-thought (CoT),\nreflect their hierarchical structures by marking the completion of subgoals in\nthe tasks. Our model learns to dynamically predict the entire CoT as coherent\nand structured long-term action guidance and consistently outperforms typical\ntwo-stage subgoal-conditioned policies. On the other hand, such CoT facilitates\ngeneralizable policy learning as they exemplify the decision patterns shared\namong demos (even those with heavy noises and randomness). Our method,\nChain-of-Thought Predictive Control (CoTPC), significantly outperforms existing\nones on challenging low-level manipulation tasks from scalable yet highly\nsub-optimal demos.\n","authors":["Zhiwei Jia","Fangchen Liu","Vineet Thumuluri","Linghao Chen","Zhiao Huang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2304.00776v1.pdf","comment":"Project page at https://zjia.eng.ucsd.edu/cotpc"},{"id":"http://arxiv.org/abs/2304.00774v1","updated":"2023-04-03T07:55:42Z","published":"2023-04-03T07:55:42Z","title":"MGMT promoter methylation status prediction using MRI scans? An\n  extensive experimental evaluation of deep learning models","summary":"  The number of studies on deep learning for medical diagnosis is expanding,\nand these systems are often claimed to outperform clinicians. However, only a\nfew systems have shown medical efficacy. From this perspective, we examine a\nwide range of deep learning algorithms for the assessment of glioblastoma - a\ncommon brain tumor in older adults that is lethal. Surgery, chemotherapy, and\nradiation are the standard treatments for glioblastoma patients. The\nmethylation status of the MGMT promoter, a specific genetic sequence found in\nthe tumor, affects chemotherapy's effectiveness. MGMT promoter methylation\nimproves chemotherapy response and survival in several cancers. MGMT promoter\nmethylation is determined by a tumor tissue biopsy, which is then genetically\ntested. This lengthy and invasive procedure increases the risk of infection and\nother complications. Thus, researchers have used deep learning models to\nexamine the tumor from brain MRI scans to determine the MGMT promoter's\nmethylation state. We employ deep learning models and one of the largest public\nMRI datasets of 585 participants to predict the methylation status of the MGMT\npromoter in glioblastoma tumors using MRI scans. We test these models using\nGrad-CAM, occlusion sensitivity, feature visualizations, and training loss\nlandscapes. Our results show no correlation between these two, indicating that\nexternal cohort data should be used to verify these models' performance to\nassure the accuracy and reliability of deep learning systems in cancer\ndiagnosis.\n","authors":["Numan Saeed","Muhammad Ridzuan","Hussain Alasmawi","Ikboljon Sobirov","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2304.00774v1.pdf","comment":"12 pages, 10 figures, MedIA"},{"id":"http://arxiv.org/abs/2303.18083v2","updated":"2023-04-03T07:41:26Z","published":"2023-03-31T14:21:53Z","title":"Analysis and Comparison of Two-Level KFAC Methods for Training Deep\n  Neural Networks","summary":"  As a second-order method, the Natural Gradient Descent (NGD) has the ability\nto accelerate training of neural networks. However, due to the prohibitive\ncomputational and memory costs of computing and inverting the Fisher\nInformation Matrix (FIM), efficient approximations are necessary to make NGD\nscalable to Deep Neural Networks (DNNs). Many such approximations have been\nattempted. The most sophisticated of these is KFAC, which approximates the FIM\nas a block-diagonal matrix, where each block corresponds to a layer of the\nneural network. By doing so, KFAC ignores the interactions between different\nlayers. In this work, we investigate the interest of restoring some\nlow-frequency interactions between the layers by means of two-level methods.\nInspired from domain decomposition, several two-level corrections to KFAC using\ndifferent coarse spaces are proposed and assessed. The obtained results show\nthat incorporating the layer interactions in this fashion does not really\nimprove the performance of KFAC. This suggests that it is safe to discard the\noff-diagonal blocks of the FIM, since the block-diagonal approach is\nsufficiently robust, accurate and economical in computation time.\n","authors":["Abdoulaye Koroko","Ani Anciaux-Sedrakian","Ibtihel Ben Gharbia","Valérie Garès","Mounir Haddou","Quang Huy Tran"],"pdf_url":"https://arxiv.org/pdf/2303.18083v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2304.00765v1","updated":"2023-04-03T07:34:43Z","published":"2023-04-03T07:34:43Z","title":"Managing power grids through topology actions: A comparative study\n  between advanced rule-based and reinforcement learning agents","summary":"  The operation of electricity grids has become increasingly complex due to the\ncurrent upheaval and the increase in renewable energy production. As a\nconsequence, active grid management is reaching its limits with conventional\napproaches. In the context of the Learning to Run a Power Network challenge, it\nhas been shown that Reinforcement Learning (RL) is an efficient and reliable\napproach with considerable potential for automatic grid operation. In this\narticle, we analyse the submitted agent from Binbinchen and provide novel\nstrategies to improve the agent, both for the RL and the rule-based approach.\nThe main improvement is a N-1 strategy, where we consider topology actions that\nkeep the grid stable, even if one line is disconnected. More, we also propose a\ntopology reversion to the original grid, which proved to be beneficial. The\nimprovements are tested against reference approaches on the challenge test sets\nand are able to increase the performance of the rule-based agent by 27%. In\ndirect comparison between rule-based and RL agent we find similar performance.\nHowever, the RL agent has a clear computational advantage. We also analyse the\nbehaviour in an exemplary case in more detail to provide additional insights.\nHere, we observe that through the N-1 strategy, the actions of the agents\nbecome more diversified.\n","authors":["Malte Lehna","Jan Viebahn","Christoph Scholz","Antoine Marot","Sven Tomforde"],"pdf_url":"https://arxiv.org/pdf/2304.00765v1.pdf","comment":"12 pages plus references and appendix, 11 figures, submitted to the\n  journal Energy and AI"},{"id":"http://arxiv.org/abs/2303.18200v2","updated":"2023-04-03T07:27:28Z","published":"2023-03-27T15:32:35Z","title":"PADME-SoSci: A Platform for Analytics and Distributed Machine Learning\n  for the Social Sciences","summary":"  Data privacy and ownership are significant in social data science, raising\nlegal and ethical concerns. Sharing and analyzing data is difficult when\ndifferent parties own different parts of it. An approach to this challenge is\nto apply de-identification or anonymization techniques to the data before\ncollecting it for analysis. However, this can reduce data utility and increase\nthe risk of re-identification. To address these limitations, we present PADME,\na distributed analytics tool that federates model implementation and training.\nPADME uses a federated approach where the model is implemented and deployed by\nall parties and visits each data location incrementally for training. This\nenables the analysis of data across locations while still allowing the model to\nbe trained as if all data were in a single location. Training the model on data\nin its original location preserves data ownership. Furthermore, the results are\nnot provided until the analysis is completed on all data locations to ensure\nprivacy and avoid bias in the results.\n","authors":["Zeyd Boukhers","Arnim Bleier","Yeliz Ucer Yediel","Mio Hienstorfer-Heitmann","Mehrshad Jaberansary","Adamantios Koumpis","Oya Beyan"],"pdf_url":"https://arxiv.org/pdf/2303.18200v2.pdf","comment":"accepted to be published @ ACM/IEEE JCDL 2023 - Joint Conference on\n  Digital Libraries"},{"id":"http://arxiv.org/abs/2304.00759v1","updated":"2023-04-03T07:20:43Z","published":"2023-04-03T07:20:43Z","title":"FedIN: Federated Intermediate Layers Learning for Model Heterogeneity","summary":"  Federated learning (FL) facilitates edge devices to cooperatively train a\nglobal shared model while maintaining the training data locally and privately.\nHowever, a common but impractical assumption in FL is that the participating\nedge devices possess the same required resources and share identical global\nmodel architecture. In this study, we propose a novel FL method called\nFederated Intermediate Layers Learning (FedIN), supporting heterogeneous models\nwithout utilizing any public dataset. The training models in FedIN are divided\ninto three parts, including an extractor, the intermediate layers, and a\nclassifier. The model architectures of the extractor and classifier are the\nsame in all devices to maintain the consistency of the intermediate layer\nfeatures, while the architectures of the intermediate layers can vary for\nheterogeneous devices according to their resource capacities. To exploit the\nknowledge from features, we propose IN training, training the intermediate\nlayers in line with the features from other clients. Additionally, we formulate\nand solve a convex optimization problem to mitigate the gradient divergence\nproblem induced by the conflicts between the IN training and the local\ntraining. The experiment results show that FedIN achieves the best performance\nin the heterogeneous model environment compared with the state-of-the-art\nalgorithms. Furthermore, our ablation study demonstrates the effectiveness of\nIN training and the solution to the convex optimization problem.\n","authors":["Chan Yun-Hin","Jiang Zhihan","Deng Jing","Ngai C. -H. Edith"],"pdf_url":"https://arxiv.org/pdf/2304.00759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06945v3","updated":"2023-04-03T07:17:02Z","published":"2023-03-13T09:27:34Z","title":"CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for\n  Protein-Protein Interaction Site Prediction","summary":"  Protein-protein interactions are essential in biochemical processes. Accurate\nprediction of the protein-protein interaction sites (PPIs) deepens our\nunderstanding of biological mechanism and is crucial for new drug design.\nHowever, conventional experimental methods for PPIs prediction are costly and\ntime-consuming so that many computational approaches, especially ML-based\nmethods, have been developed recently. Although these approaches have achieved\ngratifying results, there are still two limitations: (1) Most models have\nexcavated some useful input features, but failed to take coevolutionary\nfeatures into account, which could provide clues for inter-residue\nrelationships; (2) The attention-based models only allocate attention weights\nfor neighboring residues, instead of doing it globally, neglecting that some\nresidues being far away from the target residues might also matter.\n  We propose a coevolution-enhanced global attention neural network, a\nsequence-based deep learning model for PPIs prediction, called CoGANPPIS. It\nutilizes three layers in parallel for feature extraction: (1) Local-level\nrepresentation aggregation layer, which aggregates the neighboring residues'\nfeatures; (2) Global-level representation learning layer, which employs a novel\ncoevolution-enhanced global attention mechanism to allocate attention weights\nto all the residues on the same protein sequences; (3) Coevolutionary\ninformation learning layer, which applies CNN & pooling to coevolutionary\ninformation to obtain the coevolutionary profile representation. Then, the\nthree outputs are concatenated and passed into several fully connected layers\nfor the final prediction. Application on two benchmark datasets demonstrated a\nstate-of-the-art performance of our model. The source code is publicly\navailable at https://github.com/Slam1423/CoGANPPIS_source_code.\n","authors":["Jiaxing Guo","Xuening Zhu","Zixin Hu","Xiaoxi Hu"],"pdf_url":"https://arxiv.org/pdf/2303.06945v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.02998v2","updated":"2023-04-03T06:41:41Z","published":"2021-08-06T08:18:28Z","title":"AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases\n  Treatment: Status Quo","summary":"  The aortic vessel tree is composed of the aorta and its branching arteries,\nand plays a key role in supplying the whole body with blood. Aortic diseases,\nlike aneurysms or dissections, can lead to an aortic rupture, whose treatment\nwith open surgery is highly risky. Therefore, patients commonly undergo drug\ntreatment under constant monitoring, which requires regular inspections of the\nvessels through imaging. The standard imaging modality for diagnosis and\nmonitoring is computed tomography (CT), which can provide a detailed picture of\nthe aorta and its branching vessels if completed with a contrast agent, called\nCT angiography (CTA). Optimally, the whole aortic vessel tree geometry from\nconsecutive CTAs is overlaid and compared. This allows not only detection of\nchanges in the aorta, but also of its branches, caused by the primary pathology\nor newly developed. When performed manually, this reconstruction requires slice\nby slice contouring, which could easily take a whole day for a single aortic\nvessel tree, and is therefore not feasible in clinical practice. Automatic or\nsemi-automatic vessel tree segmentation algorithms, however, can complete this\ntask in a fraction of the manual execution time and run in parallel to the\nclinical routine of the clinicians. In this paper, we systematically review\ncomputing techniques for the automatic and semi-automatic segmentation of the\naortic vessel tree. The review concludes with an in-depth discussion on how\nclose these state-of-the-art approaches are to an application in clinical\npractice and how active this research field is, taking into account the number\nof publications, datasets and challenges.\n","authors":["Yuan Jin","Antonio Pepe","Jianning Li","Christina Gsaxner","Fen-hua Zhao","Kelsey L. Pomykala","Jens Kleesiek","Alejandro F. Frangi","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2108.02998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11152v2","updated":"2023-04-03T06:41:13Z","published":"2022-11-21T02:32:25Z","title":"You Need Multiple Exiting: Dynamic Early Exiting for Accelerating\n  Unified Vision Language Model","summary":"  Large-scale Transformer models bring significant improvements for various\ndownstream vision language tasks with a unified architecture. The performance\nimprovements come with increasing model size, resulting in slow inference speed\nand increased cost for severing. While some certain predictions benefit from\nthe full complexity of the large-scale model, not all of inputs need the same\namount of computation to conduct, potentially leading to computation resource\nwaste. To handle this challenge, early exiting is proposed to adaptively\nallocate computational power in term of input complexity to improve inference\nefficiency. The existing early exiting strategies usually adopt output\nconfidence based on intermediate layers as a proxy of input complexity to incur\nthe decision of skipping following layers. However, such strategies cannot\napply to encoder in the widely-used unified architecture with both encoder and\ndecoder due to difficulty of output confidence estimation in the encoder. It is\nsuboptimal in term of saving computation power to ignore the early exiting in\nencoder component. To handle this challenge, we propose a novel early exiting\nstrategy for unified visual language models, which allows dynamically skip the\nlayers in encoder and decoder simultaneously in term of input layer-wise\nsimilarities with multiple times of early exiting, namely \\textbf{MuE}. By\ndecomposing the image and text modalities in the encoder, MuE is flexible and\ncan skip different layers in term of modalities, advancing the inference\nefficiency while minimizing performance drop. Experiments on the SNLI-VE and MS\nCOCO datasets show that the proposed approach MuE can reduce expected inference\ntime by up to 50\\% and 40\\% while maintaining 99\\% and 96\\% performance\nrespectively.\n","authors":["Shengkun Tang","Yaqing Wang","Zhenglun Kong","Tianchi Zhang","Yao Li","Caiwen Ding","Yanzhi Wang","Yi Liang","Dongkuan Xu"],"pdf_url":"https://arxiv.org/pdf/2211.11152v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.01872v3","updated":"2023-04-03T06:22:10Z","published":"2021-11-02T20:20:28Z","title":"Towards Fairness-Aware Federated Learning","summary":"  Recent advances in Federated Learning (FL) have brought large-scale\ncollaborative machine learning opportunities for massively distributed clients\nwith performance and data privacy guarantees. However, most current works focus\non the interest of the central controller in FL,and overlook the interests of\nthe FL clients. This may result in unfair treatment of clients that discourages\nthem from actively participating in the learning process and damages the\nsustainability of the FL ecosystem. Therefore, the topic of ensuring fairness\nin FL is attracting a great deal of research interest. In recent years, diverse\nFairness-Aware FL (FAFL) approaches have been proposed in an effort to achieve\nfairness in FL from different perspectives. However, there is no comprehensive\nsurvey that helps readers gain insight into this interdisciplinary field. This\npaper aims to provide such a survey. By examining the fundamental and\nsimplifying assumptions, as well as the notions of fairness adopted by existing\nliterature in this field, we propose a taxonomy of FAFL approaches covering\nmajor steps in FL, including client selection, optimization, contribution\nevaluation and incentive distribution. In addition, we discuss the main metrics\nfor experimentally evaluating the performance of FAFL approaches, and suggest\npromising future research directions towards FAFL.\n","authors":["Yuxin Shi","Han Yu","Cyril Leung"],"pdf_url":"https://arxiv.org/pdf/2111.01872v3.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems"},{"id":"http://arxiv.org/abs/2304.00738v1","updated":"2023-04-03T06:19:42Z","published":"2023-04-03T06:19:42Z","title":"Device Image-IV Mapping using Variational Autoencoder for Inverse Design\n  and Forward Prediction","summary":"  This paper demonstrates the learning of the underlying device physics by\nmapping device structure images to their corresponding Current-Voltage (IV)\ncharacteristics using a novel framework based on variational autoencoders\n(VAE). Since VAE is used, domain expertise is not required and the framework\ncan be quickly deployed on any new device and measurement. This is expected to\nbe useful in the compact modeling of novel devices when only device\ncross-sectional images and electrical characteristics are available (e.g. novel\nemerging memory). Technology Computer-Aided Design (TCAD) generated and\nhand-drawn Metal-Oxide-Semiconductor (MOS) device images and noisy\ndrain-current-gate-voltage curves (IDVG) are used for the demonstration. The\nframework is formed by stacking two VAEs (one for image manifold learning and\none for IDVG manifold learning) which communicate with each other through the\nlatent variables. Five independent variables with different strengths are used.\nIt is shown that it can perform inverse design (generate a design structure for\na given IDVG) and forward prediction (predict IDVG for a given structure image,\nwhich can be used for compact modeling if the image is treated as device\nparameters) successfully. Since manifold learning is used, the machine is shown\nto be robust against noise in the inputs (i.e. using hand-drawn images and\nnoisy IDVG curves) and not confused by weak and irrelevant independent\nvariables.\n","authors":["Thomas Lu","Albert Lu","Hiu Yung Wong"],"pdf_url":"https://arxiv.org/pdf/2304.00738v1.pdf","comment":"5 pages 6 figures"},{"id":"http://arxiv.org/abs/2109.07852v3","updated":"2023-04-03T06:17:16Z","published":"2021-09-16T10:31:59Z","title":"OpenFed: A Comprehensive and Versatile Open-Source Federated Learning\n  Framework","summary":"  Recent developments in Artificial Intelligence techniques have enabled their\nsuccessful application across a spectrum of commercial and industrial settings.\nHowever, these techniques require large volumes of data to be aggregated in a\ncentralized manner, forestalling their applicability to scenarios wherein the\ndata is sensitive or the cost of data transmission is prohibitive. Federated\nLearning alleviates these problems by decentralizing model training, thereby\nremoving the need for data transfer and aggregation. To advance the adoption of\nFederated Learning, more research and development needs to be conducted to\naddress some important open questions. In this work, we propose OpenFed, an\nopen-source software framework for end-to-end Federated Learning. OpenFed\nreduces the barrier to entry for both researchers and downstream users of\nFederated Learning by the targeted removal of existing pain points. For\nresearchers, OpenFed provides a framework wherein new methods can be easily\nimplemented and fairly evaluated against an extensive suite of benchmarks. For\ndownstream users, OpenFed allows Federated Learning to be plugged and play\nwithin different subject-matter contexts, removing the need for deep expertise\nin Federated Learning.\n","authors":["Dengsheng Chen","Vince Tan","Zhilin Lu","Jie Hu"],"pdf_url":"https://arxiv.org/pdf/2109.07852v3.pdf","comment":"9 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2304.00737v1","updated":"2023-04-03T06:15:50Z","published":"2023-04-03T06:15:50Z","title":"SparDL: Distributed Deep Learning Training with Efficient Sparse\n  Communication","summary":"  Top-$k$ sparsification has recently been widely used to reduce the\ncommunication volume in distributed deep learning; however, due to Gradient\nAccumulation (GA) dilemma, the performance of top-$k$ sparsification is still\nlimited. Several methods have been proposed to handle the GA dilemma but have\ntwo drawbacks: (1) they are frustrated by the high communication complexity as\nthey introduce a large amount of extra transmission; (2) they are not flexible\nfor non-power-of-two numbers of workers. To solve these two problems, we\npropose a flexible and efficient sparse communication framework, dubbed SparDL.\nSparDL uses the Spar-Reduce-Scatter algorithm to solve the GA dilemma without\nadditional communication operations and is flexible to any number of workers.\nBesides, to further reduce the communication complexity and adjust the\nproportion of latency and bandwidth cost in communication complexity, we\npropose the Spar-All-Gather algorithm as part of SparDL. Extensive experiments\nvalidate the superiority of SparDL.\n","authors":["Minjun Zhao","Yichen Yin","Yuren Mao","Lu Chen","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2304.00737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00732v1","updated":"2023-04-03T05:55:27Z","published":"2023-04-03T05:55:27Z","title":"Leveraging Predictive Models for Adaptive Sampling of Spatiotemporal\n  Fluid Processes","summary":"  Persistent monitoring of a spatiotemporal fluid process requires data\nsampling and predictive modeling of the process being monitored. In this paper\nwe present PASST algorithm: Predictive-model based Adaptive Sampling of a\nSpatio-Temporal process. PASST is an adaptive robotic sampling algorithm that\nleverages predictive models to efficiently and persistently monitor a fluid\nprocess in a given region of interest. Our algorithm makes use of the\npredictions from a learned prediction model to plan a path for an autonomous\nvehicle to adaptively and efficiently survey the region of interest. In turn,\nthe sampled data is used to obtain better predictions by giving an updated\ninitial state to the predictive model. For predictive model, we use\nKnowledged-based Neural Ordinary Differential Equations to train models of\nfluid processes. These models are orders of magnitude smaller in size and run\nmuch faster than fluid data obtained from direct numerical simulations of the\npartial differential equations that describe the fluid processes or other\ncomparable computational fluids models. For path planning, we use reinforcement\nlearning based planning algorithms that use the field predictions as reward\nfunctions. We evaluate our adaptive sampling path planning algorithm on both\nnumerically simulated fluid data and real-world nowcast ocean flow data to show\nthat we can sample the spatiotemporal field in the given region of interest for\nlong time horizons. We also evaluate PASST algorithm's generalization ability\nto sample from fluid processes that are not in the training repertoire of the\nlearned models.\n","authors":["Sandeep Manjanna","Tom Z. Jiahao","M. Ani Hsieh"],"pdf_url":"https://arxiv.org/pdf/2304.00732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00731v1","updated":"2023-04-03T05:55:04Z","published":"2023-04-03T05:55:04Z","title":"An Interpretable Loan Credit Evaluation Method Based on Rule\n  Representation Learner","summary":"  The interpretability of model has become one of the obstacles to its wide\napplication in the high-stake fields. The usual way to obtain interpretability\nis to build a black-box first and then explain it using the post-hoc methods.\nHowever, the explanations provided by the post-hoc method are not always\nreliable. Instead, we design an intrinsically interpretable model based on\nRRL(Rule Representation Learner) for the Lending Club dataset. Specifically,\nfeatures can be divided into three categories according to their\ncharacteristics of themselves and build three sub-networks respectively, each\nof which is similar to a neural network with a single hidden layer but can be\nequivalently converted into a set of rules. During the training, we learned\ntricks from previous research to effectively train binary weights. Finally, our\nmodel is compared with the tree-based model. The results show that our model is\nmuch better than the interpretable decision tree in performance and close to\nother black-box, which is of practical significance to both financial\ninstitutions and borrowers. More importantly, our model is used to test the\ncorrectness of the explanations generated by the post-hoc method, the results\nshow that the post-hoc method is not always reliable.\n","authors":["Zihao Chen","Xiaomeng Wang","Yuanjiang Huang","Tao Jia"],"pdf_url":"https://arxiv.org/pdf/2304.00731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07199v3","updated":"2023-04-03T05:35:31Z","published":"2022-10-13T17:19:22Z","title":"Self-Supervised Geometric Correspondence for Category-Level 6D Object\n  Pose Estimation in the Wild","summary":"  While 6D object pose estimation has wide applications across computer vision\nand robotics, it remains far from being solved due to the lack of annotations.\nThe problem becomes even more challenging when moving to category-level 6D\npose, which requires generalization to unseen instances. Current approaches are\nrestricted by leveraging annotations from simulation or collected from humans.\nIn this paper, we overcome this barrier by introducing a self-supervised\nlearning approach trained directly on large-scale real-world object videos for\ncategory-level 6D pose estimation in the wild. Our framework reconstructs the\ncanonical 3D shape of an object category and learns dense correspondences\nbetween input images and the canonical shape via surface embedding. For\ntraining, we propose novel geometrical cycle-consistency losses which construct\ncycles across 2D-3D spaces, across different instances and different time\nsteps. The learned correspondence can be applied for 6D pose estimation and\nother downstream tasks such as keypoint transfer. Surprisingly, our method,\nwithout any human annotations or simulators, can achieve on-par or even better\nperformance than previous supervised or semi-supervised methods on in-the-wild\nimages. Our project page is: https://kywind.github.io/self-pose .\n","authors":["Kaifeng Zhang","Yang Fu","Shubhankar Borse","Hong Cai","Fatih Porikli","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2210.07199v3.pdf","comment":"Project page: https://kywind.github.io/self-pose"},{"id":"http://arxiv.org/abs/2201.13194v3","updated":"2023-04-03T04:33:04Z","published":"2022-01-31T13:01:37Z","title":"Compactness Score: A Fast Filter Method for Unsupervised Feature\n  Selection","summary":"  Along with the flourish of the information age, massive amounts of data are\ngenerated day by day. Due to the large-scale and high-dimensional\ncharacteristics of these data, it is often difficult to achieve better\ndecision-making in practical applications. Therefore, an efficient big data\nanalytics method is urgently needed. For feature engineering, feature selection\nseems to be an important research content in which is anticipated to select\n\"excellent\" features from candidate ones. Different functions can be realized\nthrough feature selection, such as dimensionality reduction, model effect\nimprovement, and model performance improvement. In many classification tasks,\nresearchers found that data seem to be usually close to each other if they are\nfrom the same class; thus, local compactness is of great importance for the\nevaluation of a feature. In this manuscript, we propose a fast unsupervised\nfeature selection method, named as, Compactness Score (CSUFS), to select\ndesired features. To demonstrate the efficiency and accuracy, several data sets\nare chosen with extensive experiments being performed. Later, the effectiveness\nand superiority of our method are revealed through addressing clustering tasks.\nHere, the performance is indicated by several well-known evaluation metrics,\nwhile the efficiency is reflected by the corresponding running time. As\nrevealed by the simulation results, our proposed algorithm seems to be more\naccurate and efficient compared with existing algorithms.\n","authors":["Peican Zhu","Xin Hou","Keke Tang","Zhen Wang","Feiping Nie"],"pdf_url":"https://arxiv.org/pdf/2201.13194v3.pdf","comment":"Since the experimental section in the current version is not\n  complete, it needs to be withdrawn and resubmitted"},{"id":"http://arxiv.org/abs/2304.00709v1","updated":"2023-04-03T04:01:29Z","published":"2023-04-03T04:01:29Z","title":"Improving Autoencoder-based Outlier Detection with Adjustable\n  Probabilistic Reconstruction Error and Mean-shift Outlier Scoring","summary":"  Autoencoders were widely used in many machine learning tasks thanks to their\nstrong learning ability which has drawn great interest among researchers in the\nfield of outlier detection. However, conventional autoencoder-based methods\nlacked considerations in two aspects. This limited their performance in outlier\ndetection. First, the mean squared error used in conventional autoencoders\nignored the judgment uncertainty of the autoencoder, which limited their\nrepresentation ability. Second, autoencoders suffered from the abnormal\nreconstruction problem: some outliers can be unexpectedly reconstructed well,\nmaking them difficult to identify from the inliers. To mitigate the\naforementioned issues, two novel methods were proposed in this paper. First, a\nnovel loss function named Probabilistic Reconstruction Error (PRE) was\nconstructed to factor in both reconstruction bias and judgment uncertainty. To\nfurther control the trade-off of these two factors, two weights were introduced\nin PRE producing Adjustable Probabilistic Reconstruction Error (APRE), which\nbenefited the outlier detection in different applications. Second, a\nconceptually new outlier scoring method based on mean-shift (MSS) was proposed\nto reduce the false inliers caused by the autoencoder. Experiments on 32\nreal-world outlier detection datasets proved the effectiveness of the proposed\nmethods. The combination of the proposed methods achieved 41% of the relative\nperformance improvement compared to the best baseline. The MSS improved the\nperformance of multiple autoencoder-based outlier detectors by an average of\n20%. The proposed two methods have the potential to advance autoencoder's\ndevelopment in outlier detection. The code is available on www.OutlierNet.com\nfor reproducibility.\n","authors":["Xu Tan","Jiawei Yang","Junqi Chen","Sylwan Rahardja","Susanto Rahardja"],"pdf_url":"https://arxiv.org/pdf/2304.00709v1.pdf","comment":"15 pages, 9 figures. Submitted to IEEE Transactions on Neural\n  Networks and Learning Systems (TNNLS)"},{"id":"http://arxiv.org/abs/2203.05711v2","updated":"2023-04-03T03:52:14Z","published":"2022-03-11T01:45:33Z","title":"Synopses of Movie Narratives: a Video-Language Dataset for Story\n  Understanding","summary":"  Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SYMON), containing\n5,193 video summaries of popular movies and TV series. SYMON captures\nnaturalistic story-telling videos for human audience made by human creators. As\na prototypical and naturalistic story dataset, SYMON features high coverage of\nmultimodal story events, abundant mental-state descriptions, and large semantic\ngaps between the visual and the textual modalities. We establish benchmarks on\nvideo-text retrieval and zero-shot alignment on movie summary videos, which\nshowcase the importance of in-domain data in story understanding. With SYMON,\nwe hope to lay the groundwork for progress in multimodal story understanding.\n","authors":["Yidan Sun","Qin Chao","Yangfeng Ji","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2203.05711v2.pdf","comment":"25 pages, 17 figures"},{"id":"http://arxiv.org/abs/2208.10134v2","updated":"2023-04-03T03:16:43Z","published":"2022-08-22T08:23:53Z","title":"Machine Learning with Confidential Computing: A Systematization of\n  Knowledge","summary":"  Privacy and security challenges in Machine Learning (ML) have become\nincreasingly severe, along with ML's pervasive development and the recent\ndemonstration of large attack surfaces. As a mature system-oriented approach,\nConfidential Computing has been utilized in both academia and industry to\nmitigate privacy and security issues in various ML scenarios. In this paper,\nthe conjunction between ML and Confidential Computing is investigated. We\nsystematize the prior work on Confidential Computing-assisted ML techniques\nthat provide i) confidentiality guarantees and ii) integrity assurances, and\ndiscuss their advanced features and drawbacks. Key challenges are further\nidentified, and we provide dedicated analyses of the limitations in existing\nTrusted Execution Environment (TEE) systems for ML use cases. Finally,\nprospective works are discussed, including grounded privacy definitions for\nclosed-loop protection, partitioned executions of efficient ML, dedicated\nTEE-assisted designs for ML, TEE-aware ML, and ML full pipeline guarantees. By\nproviding these potential solutions in our systematization of knowledge, we aim\nat building the bridge to help achieve a much strong TEE-enabled ML for privacy\nguarantees without introducing computation and system costs.\n","authors":["Fan Mo","Zahra Tarkhani","Hamed Haddadi"],"pdf_url":"https://arxiv.org/pdf/2208.10134v2.pdf","comment":"Survey paper"},{"id":"http://arxiv.org/abs/2209.05732v4","updated":"2023-04-03T03:04:44Z","published":"2022-09-13T04:58:35Z","title":"Rényi Divergence Deep Mutual Learning","summary":"  This paper revisits Deep Mutual Learning (DML), a simple yet effective\ncomputing paradigm. We propose using R\\'{e}nyi divergence instead of the KL\ndivergence, which is more flexible and tunable, to improve vanilla DML. This\nmodification is able to consistently improve performance over vanilla DML with\nlimited additional complexity. The convergence properties of the proposed\nparadigm are analyzed theoretically, and Stochastic Gradient Descent with a\nconstant learning rate is shown to converge with $\\mathcal{O}(1)$-bias in the\nworst case scenario for nonconvex optimization tasks. That is, learning will\nreach nearby local optima but continue searching within a bounded scope, which\nmay help mitigate overfitting. Finally, our extensive empirical results\ndemonstrate the advantage of combining DML and R\\'{e}nyi divergence, which\nfurther improves generalized models.\n","authors":["Weipeng Huang","Junjie Tao","Changbo Deng","Ming Fan","Wenqiang Wan","Qi Xiong","Guangyuan Piao"],"pdf_url":"https://arxiv.org/pdf/2209.05732v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00691v1","updated":"2023-04-03T02:51:47Z","published":"2023-04-03T02:51:47Z","title":"Lithium-ion Battery Online Knee Onset Detection by Matrix Profile","summary":"  Lithium-ion batteries (LiBs) degrade slightly until the knee onset, after\nwhich the deterioration accelerates to end of life (EOL). The knee onset, which\nmarks the initiation of the accelerated degradation rate, is crucial in\nproviding an early warning of the battery's performance changes. However, there\nis only limited literature on online knee onset identification. Furthermore, it\nis good to perform such identification using easily collected measurements. To\nsolve these challenges, an online knee onset identification method is developed\nby exploiting the temporal information within the discharge data. First, the\ntemporal dynamics embedded in the discharge voltage cycles from the slight\ndegradation stage are extracted by the dynamic time warping. Second, the\nanomaly is exposed by Matrix Profile during subsequence similarity search. The\nknee onset is detected when the temporal dynamics of the new cycle exceed the\ncontrol limit and the profile index indicates a change in regime. Finally, the\nidentified knee onset is utilized to categorize the battery into long-range or\nshort-range categories by its strong correlation with the battery's EOL cycles.\nWith the support of the battery categorization and the training data acquired\nunder the same statistic distribution, the proposed SOH estimation model\nachieves enhanced estimation results with a root mean squared error as low as\n0.22%.\n","authors":["Kate Qi Zhou","Yan Qin","Chau Yuen"],"pdf_url":"https://arxiv.org/pdf/2304.00691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00668v1","updated":"2023-04-03T00:45:11Z","published":"2023-04-03T00:45:11Z","title":"Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR","summary":"  Synthetic aperture radar automatic target recognition (SAR ATR) is one of the\ncritical technologies for SAR image interpretation, which has an important\napplication prospect in military and civilian fields. Deep learning has been\nwidely used in this area and achieved an excellent recognition rate on the\nbenchmark dataset in recent years. However, the benchmark dataset suffers from\ndata selection bias due to a single data collection condition. This data bias\nenhances deep learning models to overfit non-causal background clutter.\nMoreover, existing methods qualitatively analyze the model causality and do not\ndeeply analyze this data bias. In this paper, we explicitly show that the data\nselection bias leads to the non-causality of the model and spurious correlation\nof clutter. First, we quantify the contribution of the target, clutter, and\nshadow regions during the training process through the Shapley value. The\nclutter contribution has a large proportion during the training process.\nSecond, the causes of the non-causality of deep learning in SAR ATR include\ndata selection bias and model texture bias. Data selection bias results in\nclass-related clutter and false feature representation. Furthermore, the\nspurious correlation of clutter arises from the similar signal-to-clutter\nratios (SCR) between the training and test sets. Finally, we propose a random\nSCR re-weighting method to reduce the overfitting for clutter. However, the\nmodel texture bias increases with model complexity after removing data bias.\nThe experimental results of different models under the standard operating\ncondition of the benchmark MSTAR dataset prove the above conclusions.\n","authors":["Weijie Li","Wei Yang","Li Liu","Wenpeng Zhang","Yongxiang Liu"],"pdf_url":"https://arxiv.org/pdf/2304.00668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06299v2","updated":"2023-04-03T00:29:03Z","published":"2022-12-13T00:45:46Z","title":"Interpretable Diabetic Retinopathy Diagnosis based on Biomarker\n  Activation Map","summary":"  Deep learning classifiers provide the most accurate means of automatically\ndiagnosing diabetic retinopathy (DR) based on optical coherence tomography\n(OCT) and its angiography (OCTA). The power of these models is attributable in\npart to the inclusion of hidden layers that provide the complexity required to\nachieve a desired task. However, hidden layers also render algorithm outputs\ndifficult to interpret. Here we introduce a novel biomarker activation map\n(BAM) framework based on generative adversarial learning that allows clinicians\nto verify and understand classifiers decision-making. A data set including 456\nmacular scans were graded as non-referable or referable DR based on current\nclinical standards. A DR classifier that was used to evaluate our BAM was first\ntrained based on this data set. The BAM generation framework was designed by\ncombing two U-shaped generators to provide meaningful interpretability to this\nclassifier. The main generator was trained to take referable scans as input and\nproduce an output that would be classified by the classifier as non-referable.\nThe BAM is then constructed as the difference image between the output and\ninput of the main generator. To ensure that the BAM only highlights\nclassifier-utilized biomarkers an assistant generator was trained to do the\nopposite, producing scans that would be classified as referable by the\nclassifier from non-referable scans. The generated BAMs highlighted known\npathologic features including nonperfusion area and retinal fluid. A fully\ninterpretable classifier based on these highlights could help clinicians better\nutilize and verify automated DR diagnosis.\n","authors":["Pengxiao Zang","Tristan T. Hormel","Jie Wang","Yukun Guo","Steven T. Bailey","Christina J. Flaxel","David Huang","Thomas S. Hwang","Yali Jia"],"pdf_url":"https://arxiv.org/pdf/2212.06299v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2304.01409v1","updated":"2023-04-03T22:56:08Z","published":"2023-04-03T22:56:08Z","title":"An Efficient Learning-Based Solver for Two-Stage DC Optimal Power Flow\n  with Feasibility Guarantees","summary":"  In this paper, we consider the scenario-based two-stage stochastic DC optimal\npower flow (OPF) problem for optimal and reliable dispatch when the load is\nfacing uncertainty. Although this problem is a linear program, it remains\ncomputationally challenging to solve due to the large number of scenarios\nneeded to accurately represent the uncertainties. To mitigate the computational\nissues, many techniques have been proposed to approximate the second-stage\ndecisions so they can dealt more efficiently. The challenge of finding good\npolicies to approximate the second-stage decisions is that these solutions need\nto be feasible, which has been difficult to achieve with existing policies.\n  To address these challenges, this paper proposes a learning method to solve\nthe two-stage problem in a more efficient and optimal way. A technique called\nthe gauge map is incorporated into the learning architecture design to\nguarantee the learned solutions' feasibility to the network constraints.\nNamely, we can design policies that are feed forward functions that only output\nfeasible solutions. Simulation results on standard IEEE systems show that,\ncompared to iterative solvers and the widely used affine policy, our proposed\nmethod not only learns solutions of good quality but also accelerates the\ncomputation by orders of magnitude.\n","authors":["Ling Zhang","Daniel Tabas","Baosen Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.17269v2","updated":"2023-04-03T22:49:29Z","published":"2022-03-31T17:59:00Z","title":"A Closer Look at Rehearsal-Free Continual Learning","summary":"  Continual learning is a setting where machine learning models learn novel\nconcepts from continuously shifting training data, while simultaneously\navoiding degradation of knowledge on previously seen classes which may\ndisappear from the training data for extended periods of time (a phenomenon\nknown as the catastrophic forgetting problem). Current approaches for continual\nlearning of a single expanding task (aka class-incremental continual learning)\nrequire extensive rehearsal of previously seen data to avoid this degradation\nof knowledge. Unfortunately, rehearsal comes at a cost to memory, and it may\nalso violate data-privacy. Instead, we explore combining knowledge distillation\nand parameter regularization in new ways to achieve strong continual learning\nperformance without rehearsal. Specifically, we take a deep dive into common\ncontinual learning techniques: prediction distillation, feature distillation,\nL2 parameter regularization, and EWC parameter regularization. We first\ndisprove the common assumption that parameter regularization techniques fail\nfor rehearsal-free continual learning of a single, expanding task. Next, we\nexplore how to leverage knowledge from a pre-trained model in rehearsal-free\ncontinual learning and find that vanilla L2 parameter regularization\noutperforms EWC parameter regularization and feature distillation. Finally, we\nexplore the recently popular ImageNet-R benchmark, and show that L2 parameter\nregularization implemented in self-attention blocks of a ViT transformer\noutperforms recent popular prompting for continual learning methods.\n","authors":["James Seale Smith","Junjiao Tian","Shaunak Halbe","Yen-Chang Hsu","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2203.17269v2.pdf","comment":"Accepted by the 2023 IEEE/CVF Conference on Computer Vision and\n  Pattern (CVPR) Workshop on Continual Learning in Computer Vision (CLVision\n  2023)"},{"id":"http://arxiv.org/abs/2304.01406v1","updated":"2023-04-03T22:44:03Z","published":"2023-04-03T22:44:03Z","title":"Learning with augmented target information: An alternative theory of\n  Feedback Alignment","summary":"  While error backpropagation (BP) has dominated the training of nearly all\nmodern neural networks for a long time, it suffers from several biological\nplausibility issues such as the symmetric weight requirement and synchronous\nupdates. Feedback Alignment (FA) was proposed as an alternative to BP to\naddress those dilemmas and has been demonstrated to be effective on various\ntasks and network architectures. Despite its simplicity and effectiveness, a\nsatisfying explanation of how FA works across different architectures is still\nlacking. Here we propose a novel, architecture-agnostic theory of how FA works\nthrough the lens of information theory: Instead of approximating gradients\ncalculated by BP with the same parameter, FA learns effective representations\nby embedding target information into neural networks to be trained. We show\nthis through the analysis of FA dynamics in idealized settings and then via a\nseries of experiments. Based on the implications of this theory, we designed\nthree variants of FA and show their comparable performance on several tasks.\nThese variants also account for some phenomena and theories in neuroscience\nsuch as predictive coding and representational drift.\n","authors":["Huzi Cheng","Joshua W. Brown"],"pdf_url":"https://arxiv.org/pdf/2304.01406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01404v1","updated":"2023-04-03T22:37:06Z","published":"2023-04-03T22:37:06Z","title":"Adaptive Defective Area Identification in Material Surface Using Active\n  Transfer Learning-based Level Set Estimation","summary":"  In material characterization, identifying defective areas on a material\nsurface is fundamental. The conventional approach involves measuring the\nrelevant physical properties point-by-point at the predetermined mesh grid\npoints on the surface and determining the area at which the property does not\nreach the desired level. To identify defective areas more efficiently, we\npropose adaptive mapping methods in which measurement resources are used\npreferentially to detect the boundaries of defective areas. We interpret this\nproblem as an active-learning (AL) of the level set estimation (LSE) problem.\nThe goal of AL-based LSE is to determine the level set of the physical property\nfunction defined on the surface with as small number of measurements as\npossible. Furthermore, to handle the situations in which materials with similar\nspecifications are repeatedly produced, we introduce a transfer learning\napproach so that the information of previously produced materials can be\neffectively utilized. As a proof-of-concept, we applied the proposed methods to\nthe red-zone estimation problem of silicon wafers and demonstrated that we\ncould identify the defective areas with significantly lower measurement costs\nthan those of conventional methods.\n","authors":["Shota Hozumi","Kentaro Kutsukake","Kota Matsui","Syunya Kusakawa","Toru Ujihara","Ichiro Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2304.01404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01395v1","updated":"2023-04-03T22:06:49Z","published":"2023-04-03T22:06:49Z","title":"Learning Personalized Models with Clustered System Identification","summary":"  We address the problem of learning linear system models from observing\nmultiple trajectories from different system dynamics. This framework\nencompasses a collaborative scenario where several systems seeking to estimate\ntheir dynamics are partitioned into clusters according to their system\nsimilarity. Thus, the systems within the same cluster can benefit from the\nobservations made by the others. Considering this framework, we present an\nalgorithm where each system alternately estimates its cluster identity and\nperforms an estimation of its dynamics. This is then aggregated to update the\nmodel of each cluster. We show that under mild assumptions, our algorithm\ncorrectly estimates the cluster identities and achieves an approximate sample\ncomplexity that scales inversely with the number of systems in the cluster,\nthus facilitating a more efficient and personalized system identification\nprocess.\n","authors":["Leonardo F. Toso","Han Wang","James Anderson"],"pdf_url":"https://arxiv.org/pdf/2304.01395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02614v3","updated":"2023-04-03T21:48:36Z","published":"2022-10-06T00:27:16Z","title":"Federated Learning with Server Learning: Enhancing Performance for\n  Non-IID Data","summary":"  Federated Learning (FL) has emerged as a means of distributed learning using\nlocal data stored at clients with a coordinating server. Recent studies showed\nthat FL can suffer from poor performance and slower convergence when training\ndata at clients are not independent and identically distributed. Here we\nconsider a new complementary approach to mitigating this performance\ndegradation by allowing the server to perform auxiliary learning from a small\ndataset. Our analysis and experiments show that this new approach can achieve\nsignificant improvements in both model accuracy and convergence time even when\nthe server dataset is small and its distribution differs from that of the\naggregated data from all clients.\n","authors":["Van Sy Mai","Richard J. La","Tao Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.02614v3.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2304.01391v1","updated":"2023-04-03T21:42:42Z","published":"2023-04-03T21:42:42Z","title":"Counterfactual Learning on Graphs: A Survey","summary":"  Graph-structured data are pervasive in the real-world such as social\nnetworks, molecular graphs and transaction networks. Graph neural networks\n(GNNs) have achieved great success in representation learning on graphs,\nfacilitating various downstream tasks. However, GNNs have several drawbacks\nsuch as lacking interpretability, can easily inherit the bias of the training\ndata and cannot model the casual relations. Recently, counterfactual learning\non graphs has shown promising results in alleviating these drawbacks. Various\ngraph counterfactual learning approaches have been proposed for counterfactual\nfairness, explainability, link prediction and other applications on graphs. To\nfacilitate the development of this promising direction, in this survey, we\ncategorize and comprehensively review papers on graph counterfactual learning.\nWe divide existing methods into four categories based on research problems\nstudied. For each category, we provide background and motivating examples, a\ngeneral framework summarizing existing works and a detailed review of these\nworks. We point out promising future research directions at the intersection of\ngraph-structured data, counterfactual learning, and real-world applications. To\noffer a comprehensive view of resources for future studies, we compile a\ncollection of open-source implementations, public datasets, and commonly-used\nevaluation metrics. This survey aims to serve as a ``one-stop-shop'' for\nbuilding a unified understanding of graph counterfactual learning categories\nand current resources. We also maintain a repository for papers and resources\nand will keep updating the repository\nhttps://github.com/TimeLovercc/Awesome-Graph-Causal-Learning.\n","authors":["Zhimeng Guo","Teng Xiao","Charu Aggarwal","Hui Liu","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2304.01391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.00396v3","updated":"2023-04-03T21:12:06Z","published":"2022-02-01T13:25:19Z","title":"Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis\n  on the Role of Sentiment in Political Communication","summary":"  Social media has become extremely influential when it comes to policy making\nin modern societies, especially in the western world, where platforms such as\nTwitter allow users to follow politicians, thus making citizens more involved\nin political discussion. In the same vein, politicians use Twitter to express\ntheir opinions, debate among others on current topics and promote their\npolitical agendas aiming to influence voter behaviour. In this paper, we\nattempt to analyse tweets of politicians from three European countries and\nexplore the virality of their tweets. Previous studies have shown that tweets\nconveying negative sentiment are likely to be retweeted more frequently. By\nutilising state-of-the-art pre-trained language models, we performed sentiment\nanalysis on hundreds of thousands of tweets collected from members of\nparliament in Greece, Spain and the United Kingdom, including devolved\nadministrations. We achieved this by systematically exploring and analysing the\ndifferences between influential and less popular tweets. Our analysis indicates\nthat politicians' negatively charged tweets spread more widely, especially in\nmore recent times, and highlights interesting differences between political\nparties as well as between politicians and the general population.\n","authors":["Dimosthenis Antypas","Alun Preece","Jose Camacho-Collados"],"pdf_url":"https://arxiv.org/pdf/2202.00396v3.pdf","comment":"Accepted at \"Online Social Networks and Media, Volume 33\"; for code\n  and data used see https://github.com/cardiffnlp/politics-and-virality-twitter"},{"id":"http://arxiv.org/abs/2304.01376v1","updated":"2023-04-03T20:59:16Z","published":"2023-04-03T20:59:16Z","title":"Faulty Branch Identification in Passive Optical Networks using Machine\n  Learning","summary":"  Passive optical networks (PONs) have become a promising broadband access\nnetwork solution. To ensure a reliable transmission, and to meet service level\nagreements, PON systems have to be monitored constantly in order to quickly\nidentify and localize networks faults. Typically, a service disruption in a PON\nsystem is mainly due to fiber cuts and optical network unit (ONU)\ntransmitter/receiver failures. When the ONUs are located at different distances\nfrom the optical line terminal (OLT), the faulty ONU or branch can be\nidentified by analyzing the recorded optical time domain reflectometry (OTDR)\ntraces. However, faulty branch isolation becomes very challenging when the\nreflections originating from two or more branches with similar length overlap,\nwhich makes it very hard to discriminate the faulty branches given the global\nbackscattered signal. Recently, machine learning (ML) based approaches have\nshown great potential for managing optical faults in PON systems. Such\ntechniques perform well when trained and tested with data derived from the same\nPON system. But their performance may severely degrade, if the PON system\n(adopted for the generation of the training data) has changed, e.g. by adding\nmore branches or varying the length difference between two neighboring\nbranches. etc. A re-training of the ML models has to be conducted for each\nnetwork change, which can be time consuming. In this paper, to overcome the\naforementioned issues, we propose a generic ML approach trained independently\nof the network architecture for identifying the faulty branch in PON systems\ngiven OTDR signals for the cases of branches with close lengths. Such an\napproach can be applied to an arbitrary PON system without requiring to be\nre-trained for each change of the network. The proposed approach is validated\nusing experimental data derived from PON system.\n","authors":["Khouloud Abdelli","Carsten Tropschug","Helmut Griesser","Stephan Pachnicke"],"pdf_url":"https://arxiv.org/pdf/2304.01376v1.pdf","comment":"Journal of Optical Communication and Networking (JOCN) 2023"},{"id":"http://arxiv.org/abs/2105.10759v3","updated":"2023-04-03T20:32:09Z","published":"2021-05-22T16:28:57Z","title":"Universal set of Observables for Forecasting Physical Systems through\n  Causal Embedding","summary":"  We demonstrate when and how an entire left-infinite orbit of an underlying\ndynamical system or observations from such left-infinite orbits can be uniquely\nrepresented by a pair of elements in a different space, a phenomenon which we\ncall \\textit{causal embedding}. The collection of such pairs is derived from a\ndriven dynamical system and is used to learn a function which together with the\ndriven system would: (i). determine a system that is topologically conjugate to\nthe underlying system (ii). enable forecasting the underlying system's dynamics\nsince the conjugacy is computable and universal, i.e., it does not depend on\nthe underlying system (iii). guarantee an attractor containing the image of the\ncausally embedded object even if there is an error made in learning the\nfunction. By accomplishing these we herald a new forecasting scheme that beats\nthe existing reservoir computing schemes that often lead to poor long-term\nconsistency as there is no guarantee of the existence of a learnable function,\nand overcomes the challenges of stability in Takens delay embedding. We\nillustrate accurate modeling of underlying systems where previously known\ntechniques have failed.\n","authors":["G Manjunath","A de Clercq","MJ Steynberg"],"pdf_url":"https://arxiv.org/pdf/2105.10759v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01351v1","updated":"2023-04-03T20:26:59Z","published":"2023-04-03T20:26:59Z","title":"Accelerated parallel MRI using memory efficient and robust monotone\n  operator learning (MOL)","summary":"  Model-based deep learning methods that combine imaging physics with learned\nregularization priors have been emerging as powerful tools for parallel MRI\nacceleration. The main focus of this paper is to determine the utility of the\nmonotone operator learning (MOL) framework in the parallel MRI setting. The MOL\nalgorithm alternates between a gradient descent step using a monotone\nconvolutional neural network (CNN) and a conjugate gradient algorithm to\nencourage data consistency. The benefits of this approach include similar\nguarantees as compressive sensing algorithms including uniqueness, convergence,\nand stability, while being significantly more memory efficient than unrolled\nmethods. We validate the proposed scheme by comparing it with different\nunrolled algorithms in the context of accelerated parallel MRI for static and\ndynamic settings.\n","authors":["Aniket Pramanik","Mathews Jacob"],"pdf_url":"https://arxiv.org/pdf/2304.01351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00436v3","updated":"2023-04-03T20:04:08Z","published":"2023-01-01T16:24:12Z","title":"Hierarchical Explanations for Video Action Recognition","summary":"  To interpret deep neural networks, one main approach is to dissect the visual\ninput and find the prototypical parts responsible for the classification.\nHowever, existing methods often ignore the hierarchical relationship between\nthese prototypes, and thus can not explain semantic concepts at both higher\nlevel (e.g., water sports) and lower level (e.g., swimming). In this paper\ninspired by human cognition system, we leverage hierarchal information to deal\nwith uncertainty: When we observe water and human activity, but no definitive\naction it can be recognized as the water sports parent class. Only after\nobserving a person swimming can we definitively refine it to the swimming\naction. To this end, we propose HIerarchical Prototype Explainer (HIPE) to\nbuild hierarchical relations between prototypes and classes. HIPE enables a\nreasoning process for video action classification by dissecting the input video\nframes on multiple levels of the class hierarchy, our method is also applicable\nto other video tasks. The faithfulness of our method is verified by reducing\naccuracy-explainability trade off on ActivityNet and UCF-101 while providing\nmulti-level explanations.\n","authors":["Sadaf Gulshad","Teng Long","Nanne van Noord"],"pdf_url":"https://arxiv.org/pdf/2301.00436v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03494v8","updated":"2023-04-03T20:02:26Z","published":"2023-02-06T04:21:59Z","title":"A Categorical Archive of ChatGPT Failures","summary":"  Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.\n","authors":["Ali Borji"],"pdf_url":"https://arxiv.org/pdf/2302.03494v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01335v1","updated":"2023-04-03T20:01:52Z","published":"2023-04-03T20:01:52Z","title":"Charting the Topography of the Neural Network Landscape with\n  Thermal-Like Noise","summary":"  The training of neural networks is a complex, high-dimensional, non-convex\nand noisy optimization problem whose theoretical understanding is interesting\nboth from an applicative perspective and for fundamental reasons. A core\nchallenge is to understand the geometry and topography of the landscape that\nguides the optimization. In this work, we employ standard Statistical Mechanics\nmethods, namely, phase-space exploration using Langevin dynamics, to study this\nlandscape for an over-parameterized fully connected network performing a\nclassification task on random data. Analyzing the fluctuation statistics, in\nanalogy to thermal dynamics at a constant temperature, we infer a clear\ngeometric description of the low-loss region. We find that it is a\nlow-dimensional manifold whose dimension can be readily obtained from the\nfluctuations. Furthermore, this dimension is controlled by the number of data\npoints that reside near the classification decision boundary. Importantly, we\nfind that a quadratic approximation of the loss near the minimum is\nfundamentally inadequate due to the exponential nature of the decision boundary\nand the flatness of the low-loss region. This causes the dynamics to sample\nregions with higher curvature at higher temperatures, while producing\nquadratic-like statistics at any given temperature. We explain this behavior by\na simplified loss model which is analytically tractable and reproduces the\nobserved fluctuation statistics.\n","authors":["Theo Jules","Gal Brener","Tal Kachman","Noam Levi","Yohai Bar-Sinai"],"pdf_url":"https://arxiv.org/pdf/2304.01335v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2304.01333v1","updated":"2023-04-03T19:53:31Z","published":"2023-04-03T19:53:31Z","title":"On the Prime Number Divisibility by Deep Learning","summary":"  Certain tasks such as determining whether a given integer can be divided by\n2, 3, or other prime numbers may be trivial for human beings, but can be less\nstraightforward for computers in the absence of pre-specified algorithms. In\nthis paper, we tested multiple deep learning architectures and feature\nengineering approaches, and evaluated the scenario of determining divisibility\nof large finite integers (up to $2^{32}$) by small prime numbers. It turns out\nthat, regardless of the network frameworks or the complexity of the network\nstructures (CNN, RNN, Transformer, etc.), the ability to predict the prime\nnumber divisibility critically depends on the feature space fed into the deep\nlearning models. We also evaluated commercially available Automated Machine\nLearning (AutoML) pipelines from Amazon, Google and Microsoft, and demonstrated\nthat they failed to address this issue unless appropriately engineered features\nwere provided. We further proposed a closed form solution to the problem using\nthe ordinary linear regression on Fourier series basis vectors, and showed its\nsuccess. Finally, we evaluated prompt-based learning using ChatGPT and\ndemonstrated its success on small primes and apparent failures on larger\nprimes. We conclude that feature engineering remains an important task to\nimprove the performance, increase the interpretability, and reduce the\ncomplexity of machine learning/deep learning models, even in the era of AutoML\nand large-language models (LLMs).\n","authors":["Da Wu","Jingye Yang","Mian Umair Ahsan","Kai Wang"],"pdf_url":"https://arxiv.org/pdf/2304.01333v1.pdf","comment":"18 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2304.01329v1","updated":"2023-04-03T19:50:36Z","published":"2023-04-03T19:50:36Z","title":"Learning the Delay Using Neural Delay Differential Equations","summary":"  The intersection of machine learning and dynamical systems has generated\nconsiderable interest recently. Neural Ordinary Differential Equations (NODEs)\nrepresent a rich overlap between these fields. In this paper, we develop a\ncontinuous time neural network approach based on Delay Differential Equations\n(DDEs). Our model uses the adjoint sensitivity method to learn the model\nparameters and delay directly from data. Our approach is inspired by that of\nNODEs and extends earlier neural DDE models, which have assumed that the value\nof the delay is known a priori. We perform a sensitivity analysis on our\nproposed approach and demonstrate its ability to learn DDE parameters from\nbenchmark systems. We conclude our discussion with potential future directions\nand applications.\n","authors":["Maria Oprea","Mark Walth","Robert Stephany","Gabriella Torres Nothaft","Arnaldo Rodriguez-Gonzalez","William Clark"],"pdf_url":"https://arxiv.org/pdf/2304.01329v1.pdf","comment":"Comments welcome!"},{"id":"http://arxiv.org/abs/2304.01315v1","updated":"2023-04-03T19:32:24Z","published":"2023-04-03T19:32:24Z","title":"Empirical Design in Reinforcement Learning","summary":"  Empirical design in reinforcement learning is no small task. Running good\nexperiments requires attention to detail and at times significant computational\nresources. While compute resources available per dollar have continued to grow\nrapidly, so have the scale of typical experiments in reinforcement learning. It\nis now common to benchmark agents with millions of parameters against dozens of\ntasks, each using the equivalent of 30 days of experience. The scale of these\nexperiments often conflict with the need for proper statistical evidence,\nespecially when comparing algorithms. Recent studies have highlighted how\npopular algorithms are sensitive to hyper-parameter settings and implementation\ndetails, and that common empirical practice leads to weak statistical evidence\n(Machado et al., 2018; Henderson et al., 2018). Here we take this one step\nfurther.\n  This manuscript represents both a call to action, and a comprehensive\nresource for how to do good experiments in reinforcement learning. In\nparticular, we cover: the statistical assumptions underlying common performance\nmeasures, how to properly characterize performance variation and stability,\nhypothesis testing, special considerations for comparing multiple agents,\nbaseline and illustrative example construction, and how to deal with\nhyper-parameters and experimenter bias. Throughout we highlight common mistakes\nfound in the literature and the statistical consequences of those in example\nexperiments. The objective of this document is to provide answers on how we can\nuse our unprecedented compute to do good science in reinforcement learning, as\nwell as stay alert to potential pitfalls in our empirical design.\n","authors":["Andrew Patterson","Samuel Neumann","Martha White","Adam White"],"pdf_url":"https://arxiv.org/pdf/2304.01315v1.pdf","comment":"In submission to JMLR"},{"id":"http://arxiv.org/abs/2304.01311v1","updated":"2023-04-03T19:28:43Z","published":"2023-04-03T19:28:43Z","title":"Characterizing the Users, Challenges, and Visualization Needs of\n  Knowledge Graphs in Practice","summary":"  This study presents insights from interviews with nineteen Knowledge Graph\n(KG) practitioners who work in both enterprise and academic settings on a wide\nvariety of use cases. Through this study, we identify critical challenges\nexperienced by KG practitioners when creating, exploring, and analyzing KGs\nthat could be alleviated through visualization design. Our findings reveal\nthree major personas among KG practitioners - KG Builders, Analysts, and\nConsumers - each of whom have their own distinct expertise and needs. We\ndiscover that KG Builders would benefit from schema enforcers, while KG\nAnalysts need customizable query builders that provide interim query results.\nFor KG Consumers, we identify a lack of efficacy for node-link diagrams, and\nthe need for tailored domain-specific visualizations to promote KG adoption and\ncomprehension. Lastly, we find that implementing KGs effectively in practice\nrequires both technical and social solutions that are not addressed with\ncurrent tools, technologies, and collaborative workflows. From the analysis of\nour interviews, we distill several visualization research directions to improve\nKG usability, including knowledge cards that balance digestibility and\ndiscoverability, timeline views to track temporal changes, interfaces that\nsupport organic discovery, and semantic explanations for AI and machine\nlearning predictions.\n","authors":["Harry Li","Gabriel Appleby","Camelia Daniela Brumar","Remco Chang","Ashley Suh"],"pdf_url":"https://arxiv.org/pdf/2304.01311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.12400v2","updated":"2023-04-03T19:17:47Z","published":"2021-09-25T16:43:10Z","title":"Communication-Efficient Federated Linear and Deep Generalized Canonical\n  Correlation Analysis","summary":"  Classic and deep generalized canonical correlation analysis (GCCA) algorithms\nseek low-dimensional common representations of data entities from multiple\n``views'' (e.g., audio and image) using linear transformations and neural\nnetworks, respectively. When the views are acquired and stored at different\ncomputing agents (e.g., organizations and edge devices) and data sharing is\nundesired due to privacy or communication cost considerations, federated\nlearning-based GCCA is well-motivated. In federated learning, the views are\nkept locally at the agents and only derived, limited information exchange with\na central server is allowed. However, applying existing GCCA algorithms onto\nsuch federated learning settings may incur prohibitively high communication\noverhead. This work puts forth a communication-efficient federated learning\nframework for both linear and deep GCCA under the maximum variance (MAX-VAR)\nformulation. The overhead issue is addressed by aggressively compressing (via\nquantization) the exchanging information between the computing agents and a\ncentral controller. Compared to the unquantized version, our empirical study\nshows that the proposed algorithm enjoys a substantial reduction of\ncommunication overheads with virtually no loss in accuracy and convergence\nspeed. Rigorous convergence analyses are also presented, which is a nontrivial\neffort. Generic federated optimization results do not cover the special problem\nstructure of GCCA. Our result shows that the proposed algorithms for both\nlinear and deep GCCA converge to critical points at a sublinear rate, even\nunder heavy quantization and stochastic approximations. In addition, in the\nlinear MAX-VAR case, the quantized algorithm approaches a global optimum in a\ngeometric rate under reasonable conditions. Synthetic and real-data experiments\nare used to showcase the effectiveness of the proposed approach.\n","authors":["Sagar Shrestha","Xiao Fu"],"pdf_url":"https://arxiv.org/pdf/2109.12400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01303v1","updated":"2023-04-03T19:03:22Z","published":"2023-04-03T19:03:22Z","title":"Improved Bound for Mixing Time of Parallel Tempering","summary":"  In the field of sampling algorithms, MCMC (Markov Chain Monte Carlo) methods\nare widely used when direct sampling is not possible. However, multimodality of\ntarget distributions often leads to slow convergence and mixing. One common\nsolution is parallel tempering. Though highly effective in practice,\ntheoretical guarantees on its performance are limited. In this paper, we\npresent a new lower bound for parallel tempering on the spectral gap that has a\npolynomial dependence on all parameters except $\\log L$, where $(L + 1)$ is the\nnumber of levels. This improves the best existing bound which depends\nexponentially on the number of modes. Moreover, we complement our result with a\nhypothetical upper bound on spectral gap that has an exponential dependence on\n$\\log L$, which shows that, in some sense, our bound is tight.\n","authors":["Holden Lee","Zeyu Shen"],"pdf_url":"https://arxiv.org/pdf/2304.01303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01300v1","updated":"2023-04-03T18:52:01Z","published":"2023-04-03T18:52:01Z","title":"Kernel Affine Hull Machines for Differentially Private Learning","summary":"  This paper explores the use of affine hulls of points as a means of\nrepresenting data via learning in Reproducing Kernel Hilbert Spaces (RKHS),\nwith the goal of partitioning the data space into geometric bodies that conceal\nprivacy-sensitive information about individual data points, while preserving\nthe structure of the original learning problem. To this end, we introduce the\nKernel Affine Hull Machine (KAHM), which provides an effective way of computing\na distance measure from the resulting bounded geometric body. KAHM is a\ncritical building block in wide and deep autoencoders, which enable data\nrepresentation learning for classification applications. To ensure\nprivacy-preserving learning, we propose a novel method for generating\nfabricated data, which involves smoothing differentially private data samples\nthrough a transformation process. The resulting fabricated data guarantees not\nonly differential privacy but also ensures that the KAHM modeling error is not\nlarger than that of the original training data samples. We also address the\naccuracy-loss issue that arises with differentially private classifiers by\nusing fabricated data. This approach results in a significant reduction in the\nrisk of membership inference attacks while incurring only a marginal loss of\naccuracy. As an application, a KAHM based differentially private federated\nlearning scheme is introduced featuring that the evaluation of global\nclassifier requires only locally computed distance measures. Overall, our\nfindings demonstrate the potential of KAHM as effective tool for\nprivacy-preserving learning and classification.\n","authors":["Mohit Kumar","Bernhard A. Moser","Lukas Fischer"],"pdf_url":"https://arxiv.org/pdf/2304.01300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01297v1","updated":"2023-04-03T18:47:37Z","published":"2023-04-03T18:47:37Z","title":"Non-Generative Energy Based Models","summary":"  Energy-based models (EBM) have become increasingly popular within computer\nvision. EBMs bring a probabilistic approach to training deep neural networks\n(DNN) and have been shown to enhance performance in areas such as calibration,\nout-of-distribution detection, and adversarial resistance. However, these\nadvantages come at the cost of estimating input data probabilities, usually\nusing a Langevin based method such as Stochastic Gradient Langevin Dynamics\n(SGLD), which bring additional computational costs, require parameterization,\ncaching methods for efficiency, and can run into stability and scaling issues.\nEBMs use dynamical methods to draw samples from the probability density\nfunction (PDF) defined by the current state of the network and compare them to\nthe training data using a maximum log likelihood approach to learn the correct\nPDF.\n  We propose a non-generative training approach, Non-Generative EBM (NG-EBM),\nthat utilizes the {\\it{Approximate Mass}}, identified by Grathwohl et al., as a\nloss term to direct the training. We show that our NG-EBM training strategy\nretains many of the benefits of EBM in calibration, out-of-distribution\ndetection, and adversarial resistance, but without the computational complexity\nand overhead of the traditional approaches. In particular, the NG-EBM approach\nimproves the Expected Calibration Error by a factor of 2.5 for CIFAR10 and 7.5\ntimes for CIFAR100, when compared to traditionally trained models.\n","authors":["Jacob Piland","Christopher Sweet","Priscila Saboia","Charles Vardeman II","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2304.01297v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2304.01296v1","updated":"2023-04-03T18:47:17Z","published":"2023-04-03T18:47:17Z","title":"Dynamic Accommodation Measurement using Purkinje Images and ML\n  Algorithms","summary":"  We developed a prototype device for dynamic gaze and accommodation\nmeasurements based on 4 Purkinje reflections (PR) suitable for use in AR and\nophthalmology applications. PR1&2 and PR3&4 are used for accurate gaze and\naccommodation measurements, respectively. Our eye model was developed in ZEMAX\nand matches the experiments well. Our model predicts the accommodation from 4\ndiopters to 1 diopter with better than 0.25D accuracy. We performed\nrepeatability tests and obtained accurate gaze and accommodation estimations\nfrom subjects. We are generating a large synthetic data set using physically\naccurate models and machine learning.\n","authors":["Faik Ozan Ozhan","Arda Gulersoy","Ugur Aygun","Afsun Sahin","Hakan Urey"],"pdf_url":"https://arxiv.org/pdf/2304.01296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07733v3","updated":"2023-04-03T18:39:30Z","published":"2023-01-18T19:00:50Z","title":"Learning-Rate-Free Learning by D-Adaptation","summary":"  D-Adaptation is an approach to automatically setting the learning rate which\nasymptotically achieves the optimal rate of convergence for minimizing convex\nLipschitz functions, with no back-tracking or line searches, and no additional\nfunction value or gradient evaluations per step. Our approach is the first\nhyper-parameter free method for this class without additional multiplicative\nlog factors in the convergence rate. We present extensive experiments for SGD\nand Adam variants of our method, where the method automatically matches\nhand-tuned learning rates across more than a dozen diverse machine learning\nproblems, including large-scale vision and language problems.\n  An open-source implementation is available at\n\\url{https://github.com/facebookresearch/dadaptation}.\n","authors":["Aaron Defazio","Konstantin Mishchenko"],"pdf_url":"https://arxiv.org/pdf/2301.07733v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01285v1","updated":"2023-04-03T18:20:31Z","published":"2023-04-03T18:20:31Z","title":"X-TIME: An in-memory engine for accelerating machine learning on tabular\n  data with CAMs","summary":"  Structured, or tabular, data is the most common format in data science. While\ndeep learning models have proven formidable in learning from unstructured data\nsuch as images or speech, they are less accurate than simpler approaches when\nlearning from tabular data. In contrast, modern tree-based Machine Learning\n(ML) models shine in extracting relevant information from structured data. An\nessential requirement in data science is to reduce model inference latency in\ncases where, for example, models are used in a closed loop with simulation to\naccelerate scientific discovery. However, the hardware acceleration community\nhas mostly focused on deep neural networks and largely ignored other forms of\nmachine learning. Previous work has described the use of an analog content\naddressable memory (CAM) component for efficiently mapping random forests. In\nthis work, we focus on an overall analog-digital architecture implementing a\nnovel increased precision analog CAM and a programmable network on chip\nallowing the inference of state-of-the-art tree-based ML models, such as\nXGBoost and CatBoost. Results evaluated in a single chip at 16nm technology\nshow 119x lower latency at 9740x higher throughput compared with a\nstate-of-the-art GPU, with a 19W peak power consumption.\n","authors":["Giacomo Pedretti","John Moon","Pedro Bruel","Sergey Serebryakov","Ron M. Roth","Luca Buonanno","Tobias Ziegler","Cong Xu","Martin Foltin","Jim Ignowski","Catherine E. Graves"],"pdf_url":"https://arxiv.org/pdf/2304.01285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10331v2","updated":"2023-04-03T18:09:09Z","published":"2023-02-20T21:54:25Z","title":"Causal Razors","summary":"  When performing causal discovery, assumptions have to be made on how the true\ncausal mechanism corresponds to the underlying joint probability distribution.\nThese assumptions are labeled as causal razors in this work. We review numerous\ncausal razors that appeared in the literature, and offer a comprehensive\nlogical comparison of them. In particular, we scrutinize an unpopular causal\nrazor, namely parameter minimality, in multinomial causal models and its\nlogical relations with other well-studied causal razors. Our logical result\nposes a dilemma in selecting a reasonable scoring criterion for score-based\ncasual search algorithms.\n","authors":["Wai-yin Lam"],"pdf_url":"https://arxiv.org/pdf/2302.10331v2.pdf","comment":"29 pages for the main paper. 14 pages for the supplementary materials"}],"Multimedia":[{"id":"http://arxiv.org/abs/2304.01195v1","updated":"2023-04-03T17:58:54Z","published":"2023-04-03T17:58:54Z","title":"Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior\n  Refinement","summary":"  The popularity of Contrastive Language-Image Pre-training (CLIP) has\npropelled its application to diverse downstream vision tasks. To improve its\ncapacity on downstream tasks, few-shot learning has become a widely-adopted\ntechnique. However, existing methods either exhibit limited performance or\nsuffer from excessive learnable parameters. In this paper, we propose APE, an\nAdaptive Prior rEfinement method for CLIP's pre-trained knowledge, which\nachieves superior accuracy with high computational efficiency. Via a prior\nrefinement module, we analyze the inter-class disparity in the downstream data\nand decouple the domain-specific knowledge from the CLIP-extracted cache model.\nOn top of that, we introduce two model variants, a training-free APE and a\ntraining-required APE-T. We explore the trilateral affinities between the test\nimage, prior cache model, and textual representations, and only enable a\nlightweight category-residual module to be trained. For the average accuracy\nover 11 benchmarks, both APE and APE-T attain state-of-the-art and respectively\noutperform the second-best by +1.59% and +1.99% under 16 shots with x30 less\nlearnable parameters.\n","authors":["Xiangyang Zhu","Renrui Zhang","Bowei He","Aojun Zhou","Dong Wang","Bin Zhao","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2304.01195v1.pdf","comment":"Code is available at https://github.com/yangyangyang127/APE"},{"id":"http://arxiv.org/abs/2303.00448v2","updated":"2023-04-03T11:17:11Z","published":"2023-03-01T12:17:33Z","title":"The style transformer with common knowledge optimization for image-text\n  retrieval","summary":"  Image-text retrieval which associates different modalities has drawn broad\nattention due to its excellent research value and broad real-world application.\nHowever, most of the existing methods haven't taken the high-level semantic\nrelationships (\"style embedding\") and common knowledge from multi-modalities\ninto full consideration. To this end, we introduce a novel style transformer\nnetwork with common knowledge optimization (CKSTN) for image-text retrieval.\nThe main module is the common knowledge adaptor (CKA) with both the style\nembedding extractor (SEE) and the common knowledge optimization (CKO) modules.\nSpecifically, the SEE uses the sequential update strategy to effectively\nconnect the features of different stages in SEE. The CKO module is introduced\nto dynamically capture the latent concepts of common knowledge from different\nmodalities. Besides, to get generalized temporal common knowledge, we propose a\nsequential update strategy to effectively integrate the features of different\nlayers in SEE with previous common feature units. CKSTN demonstrates the\nsuperiorities of the state-of-the-art methods in image-text retrieval on MSCOCO\nand Flickr30K datasets. Moreover, CKSTN is constructed based on the lightweight\ntransformer which is more convenient and practical for the application of real\nscenes, due to the better performance and lower parameters.\n","authors":["Wenrui Li","Zhengyu Ma","Jinqiao Shi","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2303.00448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00827v1","updated":"2023-04-03T09:13:59Z","published":"2023-04-03T09:13:59Z","title":"Multi-modal Fake News Detection on Social Media via Multi-grained\n  Information Fusion","summary":"  The easy sharing of multimedia content on social media has caused a rapid\ndissemination of fake news, which threatens society's stability and security.\nTherefore, fake news detection has garnered extensive research interest in the\nfield of social forensics. Current methods primarily concentrate on the\nintegration of textual and visual features but fail to effectively exploit\nmulti-modal information at both fine-grained and coarse-grained levels.\nFurthermore, they suffer from an ambiguity problem due to a lack of correlation\nbetween modalities or a contradiction between the decisions made by each\nmodality. To overcome these challenges, we present a Multi-grained Multi-modal\nFusion Network (MMFN) for fake news detection. Inspired by the multi-grained\nprocess of human assessment of news authenticity, we respectively employ two\nTransformer-based pre-trained models to encode token-level features from text\nand images. The multi-modal module fuses fine-grained features, taking into\naccount coarse-grained features encoded by the CLIP encoder. To address the\nambiguity problem, we design uni-modal branches with similarity-based weighting\nto adaptively adjust the use of multi-modal features. Experimental results\ndemonstrate that the proposed framework outperforms state-of-the-art methods on\nthree prevalent datasets.\n","authors":["Yangming Zhou","Yuzhou Yang","Qichao Ying","Zhenxing Qian","Xinpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.00827v1.pdf","comment":"Accepted by ICMR 2023"},{"id":"http://arxiv.org/abs/2208.09406v3","updated":"2023-04-03T08:16:37Z","published":"2022-08-19T15:48:30Z","title":"Dance Style Transfer with Cross-modal Transformer","summary":"  We present CycleDance, a dance style transfer system to transform an existing\nmotion clip in one dance style to a motion clip in another dance style while\nattempting to preserve motion context of the dance. Our method extends an\nexisting CycleGAN architecture for modeling audio sequences and integrates\nmultimodal transformer encoders to account for music context. We adopt sequence\nlength-based curriculum learning to stabilize training. Our approach captures\nrich and long-term intra-relations between motion frames, which is a common\nchallenge in motion transfer and synthesis work. We further introduce new\nmetrics for gauging transfer strength and content preservation in the context\nof dance movements. We perform an extensive ablation study as well as a human\nstudy including 30 participants with 5 or more years of dance experience. The\nresults demonstrate that CycleDance generates realistic movements with the\ntarget style, significantly outperforming the baseline CycleGAN on naturalness,\ntransfer strength, and content preservation.\n","authors":["Wenjie Yin","Hang Yin","Kim Baraka","Danica Kragic","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2208.09406v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02339v4","updated":"2023-04-03T06:33:46Z","published":"2022-12-05T15:15:10Z","title":"DeAR: A Deep-learning-based Audio Re-recording Resilient Watermarking","summary":"  Audio watermarking is widely used for leaking source tracing. The robustness\nof the watermark determines the traceability of the algorithm. With the\ndevelopment of digital technology, audio re-recording (AR) has become an\nefficient and covert means to steal secrets. AR process could drastically\ndestroy the watermark signal while preserving the original information. This\nputs forward a new requirement for audio watermarking at this stage, that is,\nto be robust to AR distortions. Unfortunately, none of the existing algorithms\ncan effectively resist AR attacks due to the complexity of the AR process. To\naddress this limitation, this paper proposes DeAR, a deep-learning-based audio\nre-recording resistant watermarking. Inspired by DNN-based image watermarking,\nwe pioneer a deep learning framework for audio carriers, based on which the\nwatermark signal can be effectively embedded and extracted. Meanwhile, in order\nto resist the AR attack, we delicately analyze the distortions that occurred in\nthe AR process and design the corresponding distortion layer to cooperate with\nthe proposed watermarking framework. Extensive experiments show that the\nproposed algorithm can resist not only common electronic channel distortions\nbut also AR distortions. Under the premise of high-quality embedding\n(SNR=25.86dB), in the case of a common re-recording distance (20cm), the\nalgorithm can effectively achieve an average bit recovery accuracy of 98.55%.\n","authors":["Chang Liu","Jie Zhang","Han Fang","Zehua Ma","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2212.02339v4.pdf","comment":"Accepted by AAAI2023"},{"id":"http://arxiv.org/abs/2304.00689v1","updated":"2023-04-03T02:38:54Z","published":"2023-04-03T02:38:54Z","title":"Accuracy Improvement of Object Detection in VVC Coded Video Using\n  YOLO-v7 Features","summary":"  With advances in image recognition technology based on deep learning,\nautomatic video analysis by Artificial Intelligence is becoming more\nwidespread. As the amount of video used for image recognition increases,\nefficient compression methods for such video data are necessary. In general,\nwhen the image quality deteriorates due to image encoding, the image\nrecognition accuracy also falls. Therefore, in this paper, we propose a\nneural-network-based approach to improve image recognition accuracy, especially\nthe object detection accuracy by applying post-processing to the encoded video.\nVersatile Video Coding (VVC) will be used for the video compression method,\nsince it is the latest video coding method with the best encoding performance.\nThe neural network is trained using the features of YOLO-v7, the latest object\ndetection model. By using VVC as the video coding method and YOLO-v7 as the\ndetection model, high object detection accuracy is achieved even at low bit\nrates. Experimental results show that the combination of the proposed method\nand VVC achieves better coding performance than regular VVC in object detection\naccuracy.\n","authors":["Takahiro Shindo","Taiju Watanabe","Kein Yamada","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2304.00689v1.pdf","comment":null}]},"2023-04-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2304.00649v1","updated":"2023-04-02T23:08:11Z","published":"2023-04-02T23:08:11Z","title":"Multilingual Word Error Rate Estimation: e-WER3","summary":"  The success of the multilingual automatic speech recognition systems\nempowered many voice-driven applications. However, measuring the performance of\nsuch systems remains a major challenge, due to its dependency on manually\ntranscribed speech data in both mono- and multilingual scenarios. In this\npaper, we propose a novel multilingual framework -- eWER3 -- jointly trained on\nacoustic and lexical representation to estimate word error rate. We demonstrate\nthe effectiveness of eWER3 to (i) predict WER without using any internal states\nfrom the ASR and (ii) use the multilingual shared latent space to push the\nperformance of the close-related languages. We show our proposed multilingual\nmodel outperforms the previous monolingual word error rate estimation method\n(eWER2) by an absolute 9\\% increase in Pearson correlation coefficient (PCC),\nwith better overall estimation between the predicted and reference WER.\n","authors":["Shammur Absar Chowdhury","Ahmed Ali"],"pdf_url":"https://arxiv.org/pdf/2304.00649v1.pdf","comment":"Accepted in ICASSP, Multilingual WER estimation, End-to-End systems,\n  multilingual model, automatic word error rate estimation"},{"id":"http://arxiv.org/abs/2304.00636v1","updated":"2023-04-02T22:00:27Z","published":"2023-04-02T22:00:27Z","title":"Classifying COVID-19 Related Tweets for Fake News Detection and\n  Sentiment Analysis with BERT-based Models","summary":"  The present paper is about the participation of our team \"techno\" on\nCERIST'22 shared tasks. We used an available dataset \"task1.c\" related to\ncovid-19 pandemic. It comprises 4128 tweets for sentiment analysis task and\n8661 tweets for fake news detection task. We used natural language processing\ntools with the combination of the most renowned pre-trained language models\nBERT (Bidirectional Encoder Representations from Transformers). The results\nshows the efficacy of pre-trained language models as we attained an accuracy of\n0.93 for the sentiment analysis task and 0.90 for the fake news detection task.\n","authors":["Rabia Bounaama","Mohammed El Amine Abderrahim"],"pdf_url":"https://arxiv.org/pdf/2304.00636v1.pdf","comment":"CERIST'22: CERIST NLP Challenge 2022, March 29, 2023, Algeria,\n  Algiers"},{"id":"http://arxiv.org/abs/2301.02748v2","updated":"2023-04-02T21:59:45Z","published":"2023-01-06T23:34:52Z","title":"Conditional Generation of Paired Antibody Chain Sequences through\n  Encoder-Decoder Language Model","summary":"  Protein language models (LMs) have been successful in sequence, structural\nand functional predictions. However, currently, protein LMs are limited to\nencoder- or decoder-only architectures for single sequences while many\nbiological contexts involve protein-protein interactions. Here, we introduce\npAbT5, which models antibody chain pairing as forward- and back-translations\nusing a T5-based architecture. We show that pAbT5 accurately reflects chain\npairing through sequence generation. Our protein LM generates variable-length\nsequences and its next-word prediction probability agrees with\nposition-specific scoring matrix from sequence alignment. Like other works in\nprotein LM, pAbT5 performs state-of-the-art unsupervised prediction on\nexperimental measurements. To the best of our knowledge, pAbT5 is the first\ngenerative encoder-decoder protein LM for protein-protein interactions.\n","authors":["Simon K. S. Chu","Kathy Y. Wei"],"pdf_url":"https://arxiv.org/pdf/2301.02748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00634v1","updated":"2023-04-02T21:39:00Z","published":"2023-04-02T21:39:00Z","title":"MMT: A Multilingual and Multi-Topic Indian Social Media Dataset","summary":"  Social media plays a significant role in cross-cultural communication. A vast\namount of this occurs in code-mixed and multilingual form, posing a significant\nchallenge to Natural Language Processing (NLP) tools for processing such\ninformation, like language identification, topic modeling, and named-entity\nrecognition. To address this, we introduce a large-scale multilingual, and\nmulti-topic dataset (MMT) collected from Twitter (1.7 million Tweets),\nencompassing 13 coarse-grained and 63 fine-grained topics in the Indian\ncontext. We further annotate a subset of 5,346 tweets from the MMT dataset with\nvarious Indian languages and their code-mixed counterparts. Also, we\ndemonstrate that the currently existing tools fail to capture the linguistic\ndiversity in MMT on two downstream tasks, i.e., topic modeling and language\nidentification. To facilitate future research, we will make the anonymized and\nannotated dataset available in the public domain.\n","authors":["Dwip Dalal","Vivek Srivastava","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2304.00634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.02052v3","updated":"2023-04-02T21:13:59Z","published":"2022-08-03T13:18:42Z","title":"Large scale analysis of gender bias and sexism in song lyrics","summary":"  We employ Natural Language Processing techniques to analyse 377808 English\nsong lyrics from the \"Two Million Song Database\" corpus, focusing on the\nexpression of sexism across five decades (1960-2010) and the measurement of\ngender biases. Using a sexism classifier, we identify sexist lyrics at a larger\nscale than previous studies using small samples of manually annotated popular\nsongs. Furthermore, we reveal gender biases by measuring associations in word\nembeddings learned on song lyrics. We find sexist content to increase across\ntime, especially from male artists and for popular songs appearing in Billboard\ncharts. Songs are also shown to contain different language biases depending on\nthe gender of the performer, with male solo artist songs containing more and\nstronger biases. This is the first large scale analysis of this type, giving\ninsights into language usage in such an influential part of popular culture.\n","authors":["Lorenzo Betti","Carlo Abrate","Andreas Kaltenbrunner"],"pdf_url":"https://arxiv.org/pdf/2208.02052v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00612v1","updated":"2023-04-02T20:03:27Z","published":"2023-04-02T20:03:27Z","title":"Eight Things to Know about Large Language Models","summary":"  The widespread public deployment of large language models (LLMs) in recent\nmonths has prompted a wave of new attention and engagement from advocates,\npolicymakers, and scholars from many fields. This attention is a timely\nresponse to the many urgent questions that this technology raises, but it can\nsometimes miss important considerations. This paper surveys the evidence for\neight potentially surprising such points:\n  1. LLMs predictably get more capable with increasing investment, even without\ntargeted innovation.\n  2. Many important LLM behaviors emerge unpredictably as a byproduct of\nincreasing investment.\n  3. LLMs often appear to learn and use representations of the outside world.\n  4. There are no reliable techniques for steering the behavior of LLMs.\n  5. Experts are not yet able to interpret the inner workings of LLMs.\n  6. Human performance on a task isn't an upper bound on LLM performance.\n  7. LLMs need not express the values of their creators nor the values encoded\nin web text.\n  8. Brief interactions with LLMs are often misleading.\n","authors":["Samuel R. Bowman"],"pdf_url":"https://arxiv.org/pdf/2304.00612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00592v1","updated":"2023-04-02T18:23:13Z","published":"2023-04-02T18:23:13Z","title":"PK-Chat: Pointer Network Guided Knowledge Driven Generative Dialogue\n  Model","summary":"  In the research of end-to-end dialogue systems, using real-world knowledge to\ngenerate natural, fluent, and human-like utterances with correct answers is\ncrucial. However, domain-specific conversational dialogue systems may be\nincoherent and introduce erroneous external information to answer questions due\nto the out-of-vocabulary issue or the wrong knowledge from the parameters of\nthe neural network. In this work, we propose PK-Chat, a Pointer network guided\nKnowledge-driven generative dialogue model, incorporating a unified pretrained\nlanguage model and a pointer network over knowledge graphs. The words generated\nby PK-Chat in the dialogue are derived from the prediction of word lists and\nthe direct prediction of the external knowledge graph knowledge. Moreover,\nbased on the PK-Chat, a dialogue system is built for academic scenarios in the\ncase of geosciences. Finally, an academic dialogue benchmark is constructed to\nevaluate the quality of dialogue systems in academic scenarios and the source\ncode is available online.\n","authors":["Cheng Deng","Bo Tong","Luoyi Fu","Jiaxin Ding","Dexing Cao","Xinbing Wang","Chenghu Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.00592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17580v2","updated":"2023-04-02T17:24:47Z","published":"2023-03-30T17:48:28Z","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace","summary":"  Solving complicated AI tasks with different domains and modalities is a key\nstep toward advanced artificial intelligence. While there are abundant AI\nmodels available for different domains and modalities, they cannot handle\ncomplicated AI tasks. Considering large language models (LLMs) have exhibited\nexceptional ability in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks and language could be a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, a\nframework that leverages LLMs (e.g., ChatGPT) to connect various AI models in\nmachine learning communities (e.g., Hugging Face) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHugging Face, execute each subtask with the selected AI model, and summarize\nthe response according to the execution results. By leveraging the strong\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\nHuggingGPT is able to cover numerous sophisticated AI tasks in different\nmodalities and domains and achieve impressive results in language, vision,\nspeech, and other challenging tasks, which paves a new way towards advanced\nartificial intelligence.\n","authors":["Yongliang Shen","Kaitao Song","Xu Tan","Dongsheng Li","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2303.17580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00557v1","updated":"2023-04-02T15:24:08Z","published":"2023-04-02T15:24:08Z","title":"Semi-supervised Neural Machine Translation with Consistency\n  Regularization for Low-Resource Languages","summary":"  The advent of deep learning has led to a significant gain in machine\ntranslation. However, most of the studies required a large parallel dataset\nwhich is scarce and expensive to construct and even unavailable for some\nlanguages. This paper presents a simple yet effective method to tackle this\nproblem for low-resource languages by augmenting high-quality sentence pairs\nand training NMT models in a semi-supervised manner. Specifically, our approach\ncombines the cross-entropy loss for supervised learning with KL Divergence for\nunsupervised fashion given pseudo and augmented target sentences derived from\nthe model. We also introduce a SentenceBERT-based filter to enhance the quality\nof augmenting data by retaining semantically similar sentence pairs.\nExperimental results show that our approach significantly improves NMT\nbaselines, especially on low-resource datasets with 0.46--2.03 BLEU scores. We\nalso demonstrate that using unsupervised training for augmented data is more\nefficient than reusing the ground-truth target sentences for supervised\nlearning.\n","authors":["Viet H. Pham","Thang M. Pham","Giang Nguyen","Long Nguyen","Dien Dinh"],"pdf_url":"https://arxiv.org/pdf/2304.00557v1.pdf","comment":"TMP and GN contributed equally"},{"id":"http://arxiv.org/abs/2302.08956v2","updated":"2023-04-02T14:43:02Z","published":"2023-02-17T15:40:12Z","title":"AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages","summary":"  Africa is home to over 2000 languages from over six language families and has\nthe highest linguistic diversity among all continents. This includes 75\nlanguages with at least one million speakers each. Yet, there is little NLP\nresearch conducted on African languages. Crucial in enabling such research is\nthe availability of high-quality annotated datasets. In this paper, we\nintroduce AfriSenti, which consists of 14 sentiment datasets of 110,000+ tweets\nin 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda,\nMoroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili,\nTigrinya, Twi, Xitsonga, and Yor\\`ub\\'a) from four language families annotated\nby native speakers. The data is used in SemEval 2023 Task 12, the first\nAfro-centric SemEval shared task. We describe the data collection methodology,\nannotation process, and related challenges when curating each of the datasets.\nWe conduct experiments with different sentiment classification baselines and\ndiscuss their usefulness. We hope AfriSenti enables new work on\nunder-represented languages. The dataset is available at\nhttps://github.com/afrisenti-semeval/afrisent-semeval-2023 and can also be\nloaded as a huggingface datasets\n(https://huggingface.co/datasets/shmuhammad/AfriSenti).\n","authors":["Shamsuddeen Hassan Muhammad","Idris Abdulmumin","Abinew Ali Ayele","Nedjma Ousidhoum","David Ifeoluwa Adelani","Seid Muhie Yimam","Ibrahim Sa'id Ahmad","Meriem Beloucif","Saif Mohammad","Sebastian Ruder","Oumaima Hourrane","Pavel Brazdil","Felermino Dário Mário António Ali","Davis Davis","Salomey Osei","Bello Shehu Bello","Falalu Ibrahim","Tajuddeen Gwadabe","Samuel Rutunda","Tadesse Belay","Wendimu Baye Messelle","Hailu Beshada Balcha","Sisay Adugna Chala","Hagos Tesfahun Gebremichael","Bernard Opoku","Steven Arthur"],"pdf_url":"https://arxiv.org/pdf/2302.08956v2.pdf","comment":"15 pages, 6 Figures, 9 Tables"},{"id":"http://arxiv.org/abs/2304.00483v1","updated":"2023-04-02T08:26:38Z","published":"2023-04-02T08:26:38Z","title":"A Data-centric Framework for Improving Domain-specific Machine Reading\n  Comprehension Datasets","summary":"  Low-quality data can cause downstream problems in high-stakes applications.\nData-centric approach emphasizes on improving dataset quality to enhance model\nperformance. High-quality datasets are needed for general-purpose Large\nLanguage Models (LLMs) training, as well as for domain-specific models, which\nare usually small in size as it is costly to engage a large number of domain\nexperts for their creation. Thus, it is vital to ensure high-quality\ndomain-specific training data. In this paper, we propose a framework for\nenhancing the data quality of original datasets. We applied the proposed\nframework to four biomedical datasets and showed relative improvement of up to\n33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when\nusing back translation to enhance the original dataset quality.\n","authors":["Iva Bojic","Josef Halim","Verena Suharman","Sreeja Tar","Qi Chwen Ong","Duy Phung","Mathieu Ravaut","Shafiq Joty","Josip Car"],"pdf_url":"https://arxiv.org/pdf/2304.00483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00468v1","updated":"2023-04-02T06:45:18Z","published":"2023-04-02T06:45:18Z","title":"Words that Wound: The Impact of Biased Language on News Sentiment and\n  Stock Market Index","summary":"  This study investigates the impact of biased language, specifically 'Words\nthat Wound,' on sentiment analysis in a dataset of 45,379 South Korean daily\neconomic news articles. Using Word2Vec, cosine similarity, and an expanded\nlexicon, we analyzed the influence of these words on news titles' sentiment\nscores. Our findings reveal that incorporating biased language significantly\namplifies sentiment scores' intensity, particularly negativity. The research\nexamines the effect of heightened negativity in news titles on the KOSPI200\nindex using linear regression and sentiment analysis. Results indicate that the\naugmented sentiment lexicon (Sent1000), which includes the top 1,000 negative\nwords with high cosine similarity to 'Crisis,' more effectively captures the\nimpact of news sentiment on the stock market index than the original KNU\nsentiment lexicon (Sent0). The ARDL model and Impulse Response Function (IRF)\nanalyses disclose that Sent1000 has a stronger and more persistent impact on\nKOSPI200 compared to Sent0. These findings emphasize the importance of\nunderstanding language's role in shaping market dynamics and investor\nsentiment, particularly the impact of negatively biased language on stock\nmarket indices. The study highlights the need for considering context and\nlinguistic nuances when analyzing news content and its potential effects on\npublic opinion and market dynamics.\n","authors":["Wonseong Kim"],"pdf_url":"https://arxiv.org/pdf/2304.00468v1.pdf","comment":"18 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2304.00457v1","updated":"2023-04-02T05:47:09Z","published":"2023-04-02T05:47:09Z","title":"LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language\n  Models","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\nand demonstrated impressive capabilities in various tasks. Unfortunately, they\nare prone to hallucinations, where the model exposes incorrect or false\ninformation in its responses, which renders diligent evaluation approaches\nmandatory. While LLM performance in specific knowledge fields is often\nevaluated based on question and answer (Q&A) datasets, such evaluations usually\nreport only a single accuracy number for the entire field, a procedure which is\nproblematic with respect to transparency and model improvement. A stratified\nevaluation could instead reveal subfields, where hallucinations are more likely\nto occur and thus help to better assess LLMs' risks and guide their further\ndevelopment. To support such stratified evaluations, we propose LLMMaps as a\nnovel visualization technique that enables users to evaluate LLMs' performance\nwith respect to Q&A datasets. LLMMaps provide detailed insights into LLMs'\nknowledge capabilities in different subfields, by transforming Q&A datasets as\nwell as LLM responses into our internal knowledge structure. An extension for\ncomparative visualization furthermore, allows for the detailed comparison of\nmultiple LLMs. To assess LLMMaps we use them to conduct a comparative analysis\nof several state-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and\nLLaMa-13B, as well as two qualitative user evaluations. All necessary source\ncode and data for generating LLMMaps to be used in scientific publications and\nelsewhere will be available on GitHub.\n","authors":["Patrik Puchert","Poonam Poonam","Christian van Onzenoodt","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2304.00457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00416v1","updated":"2023-04-02T00:39:12Z","published":"2023-04-02T00:39:12Z","title":"Towards Healthy AI: Large Language Models Need Therapists Too","summary":"  Recent advances in large language models (LLMs) have led to the development\nof powerful AI chatbots capable of engaging in natural and human-like\nconversations. However, these chatbots can be potentially harmful, exhibiting\nmanipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to\nbe safe, trustworthy and ethical. To create healthy AI systems, we present the\nSafeguardGPT framework that uses psychotherapy to correct for these harmful\nbehaviors in AI chatbots. The framework involves four types of AI agents: a\nChatbot, a \"User,\" a \"Therapist,\" and a \"Critic.\" We demonstrate the\neffectiveness of SafeguardGPT through a working example of simulating a social\nconversation. Our results show that the framework can improve the quality of\nconversations between AI chatbots and humans. Although there are still several\nchallenges and directions to be addressed in the future, SafeguardGPT provides\na promising approach to improving the alignment between AI chatbots and human\nvalues. By incorporating psychotherapy and reinforcement learning techniques,\nthe framework enables AI chatbots to learn and adapt to human preferences and\nvalues in a safe and ethical way, contributing to the development of a more\nhuman-centric and responsible AI.\n","authors":["Baihan Lin","Djallel Bouneffouf","Guillermo Cecchi","Kush R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2304.00416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01228v1","updated":"2023-04-02T10:59:19Z","published":"2023-04-02T10:59:19Z","title":"Better Language Models of Code through Self-Improvement","summary":"  Pre-trained language models for code (PLMCs) have gained attention in recent\nresearch. These models are pre-trained on large-scale datasets using\nmulti-modal objectives. However, fine-tuning them requires extensive\nsupervision and is limited by the size of the dataset provided. We aim to\nimprove this issue by proposing a simple data augmentation framework. Our\nframework utilizes knowledge gained during the pre-training and fine-tuning\nstage to generate pseudo data, which is then used as training data for the next\nstep. We incorporate this framework into the state-of-the-art language models,\nsuch as CodeT5, CodeBERT, and UnixCoder. The results show that our framework\nsignificantly improves PLMCs' performance in code-related sequence generation\ntasks, such as code summarization and code generation in the CodeXGLUE\nbenchmark.\n","authors":["Hung Quoc To","Nghi D. Q. Bui","Jin Guo","Tien N. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2304.01228v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.12748v3","updated":"2023-04-02T23:01:59Z","published":"2023-03-11T17:14:04Z","title":"Enabling Calibration In The Zero-Shot Inference of Large Vision-Language\n  Models","summary":"  Calibration of deep learning models is crucial to their trustworthiness and\nsafe usage, and as such, has been extensively studied in supervised\nclassification models, with methods crafted to decrease miscalibration.\nHowever, there has yet to be a comprehensive study of the calibration of\nvision-language models that are used for zero-shot inference, like CLIP. We\nmeasure calibration across relevant variables like prompt, dataset, and\narchitecture, and find that zero-shot inference with CLIP is miscalibrated.\nFurthermore, we propose a modified version of temperature scaling that is\naligned with the common use cases of CLIP as a zero-shot inference model, and\nshow that a single learned temperature generalizes for each specific CLIP model\n(defined by a chosen pre-training dataset and architecture) across inference\ndataset and prompt choice.\n","authors":["Will LeVine","Benjamin Pikus","Pranav Raj","Fernando Amat Gil"],"pdf_url":"https://arxiv.org/pdf/2303.12748v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10011v2","updated":"2023-04-02T22:19:08Z","published":"2022-06-20T21:23:15Z","title":"When Does Re-initialization Work?","summary":"  Re-initializing a neural network during training has been observed to improve\ngeneralization in recent works. Yet it is neither widely adopted in deep\nlearning practice nor is it often used in state-of-the-art training protocols.\nThis raises the question of when re-initialization works, and whether it should\nbe used together with regularization techniques such as data augmentation,\nweight decay and learning rate schedules. In this work, we conduct an extensive\nempirical comparison of standard training with a selection of re-initialization\nmethods to answer this question, training over 15,000 models on a variety of\nimage classification benchmarks. We first establish that such methods are\nconsistently beneficial for generalization in the absence of any other\nregularization. However, when deployed alongside other carefully tuned\nregularization techniques, re-initialization methods offer little to no added\nbenefit for generalization, although optimal generalization performance becomes\nless sensitive to the choice of learning rate and weight decay hyperparameters.\nTo investigate the impact of re-initialization methods on noisy data, we also\nconsider learning under label noise. Surprisingly, in this case,\nre-initialization significantly improves upon standard training, even in the\npresence of other carefully tuned regularization techniques.\n","authors":["Sheheryar Zaidi","Tudor Berariu","Hyunjik Kim","Jörg Bornschein","Claudia Clopath","Yee Whye Teh","Razvan Pascanu"],"pdf_url":"https://arxiv.org/pdf/2206.10011v2.pdf","comment":"Published in PMLR Volume 187; spotlight presentation at I Can't\n  Believe It's Not Better Workshop at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2211.05776v3","updated":"2023-04-02T22:01:17Z","published":"2022-11-10T18:58:22Z","title":"High-Quality Entity Segmentation","summary":"  Dense image segmentation tasks e.g., semantic, panoptic) are useful for image\nediting, but existing methods can hardly generalize well in an in-the-wild\nsetting where there are unrestricted image domains, classes, and image\nresolution and quality variations. Motivated by these observations, we\nconstruct a new entity segmentation dataset, with a strong focus on\nhigh-quality dense segmentation in the wild. The dataset contains images\nspanning diverse image domains and entities, along with plentiful\nhigh-resolution images and high-quality mask annotations for training and\ntesting. Given the high-quality and -resolution nature of the dataset, we\npropose CropFormer which is designed to tackle the intractability of\ninstance-level segmentation on high-resolution images. It improves mask\nprediction by fusing high-res image crops that provide more fine-grained image\ndetails and the full image. CropFormer is the first query-based Transformer\narchitecture that can effectively fuse mask predictions from multiple image\nviews, by learning queries that effectively associate the same entities across\nthe full image and its crop. With CropFormer, we achieve a significant AP gain\nof $1.9$ on the challenging entity segmentation task. Furthermore, CropFormer\nconsistently improves the accuracy of traditional segmentation tasks and\ndatasets. The dataset and code will be released at\nhttp://luqi.info/entityv2.github.io/.\n","authors":["Lu Qi","Jason Kuen","Weidong Guo","Tiancheng Shen","Jiuxiang Gu","Jiaya Jia","Zhe Lin","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2211.05776v3.pdf","comment":"The project webiste: http://luqi.info/entityv2.github.io/"},{"id":"http://arxiv.org/abs/2303.12848v3","updated":"2023-04-02T21:27:16Z","published":"2023-03-22T18:14:02Z","title":"Test-time Detection and Repair of Adversarial Samples via Masked\n  Autoencoder","summary":"  Training-time defenses, known as adversarial training, incur high training\ncosts and do not generalize to unseen attacks. Test-time defenses solve these\nissues but most existing test-time defenses require adapting the model weights,\ntherefore they do not work on frozen models and complicate model memory\nmanagement. The only test-time defense that does not adapt model weights aims\nto adapt the input with self-supervision tasks. However, we empirically found\nthese self-supervision tasks are not sensitive enough to detect adversarial\nattacks accurately. In this paper, we propose DRAM, a novel defense method to\ndetect and repair adversarial samples at test time via Masked autoencoder\n(MAE). We demonstrate how to use MAE losses to build a Kolmogorov-Smirnov test\nto detect adversarial samples. Moreover, we use the MAE losses to calculate\ninput reversal vectors that repair adversarial samples resulting from\npreviously unseen attacks. Results on large-scale ImageNet dataset show that,\ncompared to all detection baselines evaluated, DRAM achieves the best detection\nrate (82% on average) on all eight adversarial attacks evaluated. For attack\nrepair, DRAM improves the robust accuracy by 6% ~ 41% for standard ResNet50 and\n3% ~ 8% for robust ResNet50 compared with the baselines that use contrastive\nlearning and rotation prediction.\n","authors":["Yun-Yun Tsai","Ju-Chin Chao","Albert Wen","Zhaoyuan Yang","Chengzhi Mao","Tapan Shah","Junfeng Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12848v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00622v1","updated":"2023-04-02T20:37:22Z","published":"2023-04-02T20:37:22Z","title":"Automatic Detection of Natural Disaster Effect on Paddy Field from\n  Satellite Images using Deep Learning Techniques","summary":"  This paper aims to detect rice field damage from natural disasters in\nBangladesh using high-resolution satellite imagery. The authors developed\nground truth data for rice field damage from the field level. At first, NDVI\ndifferences before and after the disaster are calculated to identify possible\ncrop loss. The areas equal to and above the 0.33 threshold are marked as crop\nloss areas as significant changes are observed. The authors also verified crop\nloss areas by collecting data from local farmers. Later, different bands of\nsatellite data (Red, Green, Blue) and (False Color Infrared) are useful to\ndetect crop loss area. We used the NDVI different images as ground truth to\ntrain the DeepLabV3plus model. With RGB, we got IoU 0.41 and with FCI, we got\nIoU 0.51. As FCI uses NIR, Red, Blue bands and NDVI is normalized difference\nbetween NIR and Red bands, so greater FCI's IoU score than RGB is expected. But\nRGB does not perform very badly here. So, where other bands are not available,\nRGB can use to understand crop loss areas to some extent. The ground truth\ndeveloped in this paper can be used for segmentation models with very high\nresolution RGB only images such as Bing, Google etc.\n","authors":["Tahmid Alavi Ishmam","Amin Ahsan Ali","Md Ahsraful Amin","A K M Mahbubur Rahman"],"pdf_url":"https://arxiv.org/pdf/2304.00622v1.pdf","comment":"6 pages, 13 figures. This paper has been accepted for presentation at\n  the ICCRE2023 conference, held at Nagaoka University of Technology, Japan"},{"id":"http://arxiv.org/abs/2304.00601v1","updated":"2023-04-02T19:09:01Z","published":"2023-04-02T19:09:01Z","title":"Constructive Assimilation: Boosting Contrastive Learning Performance\n  through View Generation Strategies","summary":"  Transformations based on domain expertise (expert transformations), such as\nrandom-resized-crop and color-jitter, have proven critical to the success of\ncontrastive learning techniques such as SimCLR. Recently, several attempts have\nbeen made to replace such domain-specific, human-designed transformations with\ngenerated views that are learned. However for imagery data, so far none of\nthese view-generation methods has been able to outperform expert\ntransformations. In this work, we tackle a different question: instead of\nreplacing expert transformations with generated views, can we constructively\nassimilate generated views with expert transformations? We answer this question\nin the affirmative and propose a view generation method and a simple, effective\nassimilation method that together improve the state-of-the-art by up to ~3.6%\non three different datasets. Importantly, we conduct a detailed empirical study\nthat systematically analyzes a range of view generation and assimilation\nmethods and provides a holistic picture of the efficacy of learned views in\ncontrastive representation learning.\n","authors":["Ligong Han","Seungwook Han","Shivchander Sudalairaj","Charlotte Loh","Rumen Dangovski","Fei Deng","Pulkit Agrawal","Dimitris Metaxas","Leonid Karlinsky","Tsui-Wei Weng","Akash Srivastava"],"pdf_url":"https://arxiv.org/pdf/2304.00601v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2304.00600v1","updated":"2023-04-02T19:08:02Z","published":"2023-04-02T19:08:02Z","title":"Recurrence without Recurrence: Stable Video Landmark Detection with Deep\n  Equilibrium Models","summary":"  Cascaded computation, whereby predictions are recurrently refined over\nseveral stages, has been a persistent theme throughout the development of\nlandmark detection models. In this work, we show that the recently proposed\nDeep Equilibrium Model (DEQ) can be naturally adapted to this form of\ncomputation. Our Landmark DEQ (LDEQ) achieves state-of-the-art performance on\nthe challenging WFLW facial landmark dataset, reaching $3.92$ NME with fewer\nparameters and a training memory cost of $\\mathcal{O}(1)$ in the number of\nrecurrent modules. Furthermore, we show that DEQs are particularly suited for\nlandmark detection in videos. In this setting, it is typical to train on still\nimages due to the lack of labelled videos. This can lead to a ``flickering''\neffect at inference time on video, whereby a model can rapidly oscillate\nbetween different plausible solutions across consecutive frames. By rephrasing\nDEQs as a constrained optimization, we emulate recurrence at inference time,\ndespite not having access to temporal data at training time. This Recurrence\nwithout Recurrence (RwR) paradigm helps in reducing landmark flicker, which we\ndemonstrate by introducing a new metric, normalized mean flicker (NMF), and\ncontributing a new facial landmark video dataset (WFLW-V) targeting landmark\nuncertainty. On the WFLW-V hard subset made up of $500$ videos, our LDEQ with\nRwR improves the NME and NMF by $10$ and $13\\%$ respectively, compared to the\nstrongest previously published model using a hand-tuned conventional filter.\n","authors":["Paul Micaelli","Arash Vahdat","Hongxu Yin","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2304.00600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10703v2","updated":"2023-04-02T18:18:23Z","published":"2023-03-19T16:17:35Z","title":"CCTV-Gun: Benchmarking Handgun Detection in CCTV Images","summary":"  Gun violence is a critical security problem, and it is imperative for the\ncomputer vision community to develop effective gun detection algorithms for\nreal-world scenarios, particularly in Closed Circuit Television (CCTV)\nsurveillance data. Despite significant progress in visual object detection,\ndetecting guns in real-world CCTV images remains a challenging and\nunder-explored task. Firearms, especially handguns, are typically very small in\nsize, non-salient in appearance, and often severely occluded or\nindistinguishable from other small objects. Additionally, the lack of\nprincipled benchmarks and difficulty collecting relevant datasets further\nhinder algorithmic development. In this paper, we present a meticulously\ncrafted and annotated benchmark, called \\textbf{CCTV-Gun}, which addresses the\nchallenges of detecting handguns in real-world CCTV images. Our contribution is\nthree-fold. Firstly, we carefully select and analyze real-world CCTV images\nfrom three datasets, manually annotate handguns and their holders, and assign\neach image with relevant challenge factors such as blur and occlusion.\nSecondly, we propose a new cross-dataset evaluation protocol in addition to the\nstandard intra-dataset protocol, which is vital for gun detection in practical\nsettings. Finally, we comprehensively evaluate both classical and\nstate-of-the-art object detection algorithms, providing an in-depth analysis of\ntheir generalizing abilities. The benchmark will facilitate further research\nand development on this topic and ultimately enhance security. Code,\nannotations, and trained models are available at\nhttps://github.com/srikarym/CCTV-Gun.\n","authors":["Srikar Yellapragada","Zhenghong Li","Kevin Bhadresh Doshi","Purva Makarand Mhasakar","Heng Fan","Jie Wei","Erik Blasch","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2303.10703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00590v1","updated":"2023-04-02T18:13:36Z","published":"2023-04-02T18:13:36Z","title":"Learning Similarity between Scene Graphs and Images with Transformers","summary":"  Scene graph generation is conventionally evaluated by (mean) Recall@K, which\nmeasures the ratio of correctly predicted triplets that appear in the ground\ntruth. However, such triplet-oriented metrics cannot capture the global\nsemantic information of scene graphs, and measure the similarity between images\nand generated scene graphs. The usability of scene graphs is therefore limited\nin downstream tasks. To address this issue, a framework that can measure the\nsimilarity of scene graphs and images is urgently required. Motivated by the\nsuccessful application of Contrastive Language-Image Pre-training (CLIP), we\npropose a novel contrastive learning framework consisting of a graph\nTransformer and an image Transformer to align scene graphs and their\ncorresponding images in the shared latent space. To enable the graph\nTransformer to comprehend the scene graph structure and extract representative\nfeatures, we introduce a graph serialization technique that transforms a scene\ngraph into a sequence with structural encoding. Based on our framework, we\nintroduce R-Precision measuring image retrieval accuracy as a new evaluation\nmetric for scene graph generation and establish new benchmarks for the Visual\nGenome and Open Images datasets. A series of experiments are further conducted\nto demonstrate the effectiveness of the graph Transformer, which shows great\npotential as a scene graph encoder.\n","authors":["Yuren Cong","Wentong Liao","Bodo Rosenhahn","Michael Ying Yang"],"pdf_url":"https://arxiv.org/pdf/2304.00590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04636v2","updated":"2023-04-02T18:13:15Z","published":"2022-12-09T02:25:20Z","title":"Ego-Body Pose Estimation via Ego-Head Pose Estimation","summary":"  Estimating 3D human motion from an egocentric video sequence plays a critical\nrole in human behavior understanding and has various applications in VR/AR.\nHowever, naively learning a mapping between egocentric videos and human motions\nis challenging, because the user's body is often unobserved by the front-facing\ncamera placed on the head of the user. In addition, collecting large-scale,\nhigh-quality datasets with paired egocentric videos and 3D human motions\nrequires accurate motion capture devices, which often limit the variety of\nscenes in the videos to lab-like environments. To eliminate the need for paired\negocentric video and human motions, we propose a new method, Ego-Body Pose\nEstimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the problem\ninto two stages, connected by the head motion as an intermediate\nrepresentation. EgoEgo first integrates SLAM and a learning approach to\nestimate accurate head motion. Subsequently, leveraging the estimated head pose\nas input, EgoEgo utilizes conditional diffusion to generate multiple plausible\nfull-body motions. This disentanglement of head and body pose eliminates the\nneed for training datasets with paired egocentric videos and 3D human motion,\nenabling us to leverage large-scale egocentric video datasets and motion\ncapture datasets separately. Moreover, for systematic benchmarking, we develop\na synthetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentric\nvideos and human motion. On both ARES and real data, our EgoEgo model performs\nsignificantly better than the current state-of-the-art methods.\n","authors":["Jiaman Li","C. Karen Liu","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2212.04636v2.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2210.01115v2","updated":"2023-04-02T18:03:06Z","published":"2022-10-03T17:56:35Z","title":"LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of\n  Vision & Language Models","summary":"  Soft prompt learning has recently emerged as one of the methods of choice for\nadapting V&L models to a downstream task using a few training examples.\nHowever, current methods significantly overfit the training data, suffering\nfrom large accuracy degradation when tested on unseen classes from the same\ndomain. To this end, in this paper, we make the following 4 contributions: (1)\nTo alleviate base class overfitting, we propose a novel Language-Aware Soft\nPrompting (LASP) learning method by means of a text-to-text cross-entropy loss\nthat maximizes the probability of the learned prompts to be correctly\nclassified with respect to pre-defined hand-crafted textual prompts. (2) To\nincrease the representation capacity of the prompts, we propose grouped LASP\nwhere each group of prompts is optimized with respect to a separate subset of\ntextual prompts. (3) We identify a visual-language misalignment introduced by\nprompt learning and LASP, and more importantly, propose a re-calibration\nmechanism to address it. (4) We show that LASP is inherently amenable to\nincluding, during training, virtual classes, i.e. class names for which no\nvisual samples are available, further increasing the robustness of the learned\nprompts. Through evaluations on 11 datasets, we show that our approach (a)\nsignificantly outperforms all prior works on soft prompting, and (b) matches\nand surpasses, for the first time, the accuracy on novel classes obtained by\nhand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be made\navailable at https://www.adrianbulat.com/lasp\n","authors":["Adrian Bulat","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2210.01115v2.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2304.00583v1","updated":"2023-04-02T18:01:51Z","published":"2023-04-02T18:01:51Z","title":"Enhancing Deformable Local Features by Jointly Learning to Detect and\n  Describe Keypoints","summary":"  Local feature extraction is a standard approach in computer vision for\ntackling important tasks such as image matching and retrieval. The core\nassumption of most methods is that images undergo affine transformations,\ndisregarding more complicated effects such as non-rigid deformations.\nFurthermore, incipient works tailored for non-rigid correspondence still rely\non keypoint detectors designed for rigid transformations, hindering performance\ndue to the limitations of the detector. We propose DALF (Deformation-Aware\nLocal Features), a novel deformation-aware network for jointly detecting and\ndescribing keypoints, to handle the challenging problem of matching deformable\nsurfaces. All network components work cooperatively through a feature fusion\napproach that enforces the descriptors' distinctiveness and invariance.\nExperiments using real deforming objects showcase the superiority of our\nmethod, where it delivers 8% improvement in matching scores compared to the\nprevious best results. Our approach also enhances the performance of two\nreal-world applications: deformable object retrieval and non-rigid 3D surface\nregistration. Code for training, inference, and applications are publicly\navailable at https://verlab.dcc.ufmg.br/descriptors/dalf_cvpr23.\n","authors":["Guilherme Potje","Felipe Cadar","Andre Araujo","Renato Martins","Erickson R. Nascimento"],"pdf_url":"https://arxiv.org/pdf/2304.00583v1.pdf","comment":"CVPR 2023; Source code available at\n  https://verlab.dcc.ufmg.br/descriptors/dalf_cvpr23"},{"id":"http://arxiv.org/abs/2303.01991v2","updated":"2023-04-02T17:25:52Z","published":"2023-03-03T15:00:12Z","title":"Unified Perception: Efficient Depth-Aware Video Panoptic Segmentation\n  with Minimal Annotation Costs","summary":"  Depth-aware video panoptic segmentation is a promising approach to camera\nbased scene understanding. However, the current state-of-the-art methods\nrequire costly video annotations and use a complex training pipeline compared\nto their image-based equivalents. In this paper, we present a new approach\ntitled Unified Perception that achieves state-of-the-art performance without\nrequiring video-based training. Our method employs a simple two-stage cascaded\ntracking algorithm that (re)uses object embeddings computed in an image-based\nnetwork. Experimental results on the Cityscapes-DVPS dataset demonstrate that\nour method achieves an overall DVPQ of 57.1, surpassing state-of-the-art\nmethods. Furthermore, we show that our tracking strategies are effective for\nlong-term object association on KITTI-STEP, achieving an STQ of 59.1 which\nexceeded the performance of state-of-the-art methods that employ the same\nbackbone network.\n  Code is available at: https://tue-mps.github.io/unipercept\n","authors":["Kurt Stolle","Gijs Dubbelman"],"pdf_url":"https://arxiv.org/pdf/2303.01991v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2204.10921v2","updated":"2023-04-02T22:01:22Z","published":"2022-04-22T20:22:06Z","title":"Subscriptions and external links help drive resentful users to\n  alternative and extremist YouTube videos","summary":"  Do online platforms facilitate the consumption of potentially harmful\ncontent? Using paired behavioral and survey data provided by participants\nrecruited from a representative sample in 2020 (n=1,181), we show that exposure\nto alternative and extremist channel videos on YouTube is heavily concentrated\namong a small group of people with high prior levels of gender and racial\nresentment. These viewers often subscribe to these channels (prompting\nrecommendations to their videos) and follow external links to them. In\ncontrast, non-subscribers rarely see or follow recommendations to videos from\nthese channels. Our findings suggest YouTube's algorithms were not sending\npeople down \"rabbit holes\" during our observation window in 2020, possibly due\nto changes that the company made to its recommender system in 2019. However,\nthe platform continues to play a key role in facilitating exposure to content\nfrom alternative and extremist channels among dedicated audiences.\n","authors":["Annie Y. Chen","Brendan Nyhan","Jason Reifler","Ronald E. Robertson","Christo Wilson"],"pdf_url":"https://arxiv.org/pdf/2204.10921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00615v1","updated":"2023-04-02T20:13:14Z","published":"2023-04-02T20:13:14Z","title":"An Intrinsic Framework of Information Retrieval Evaluation Measures","summary":"  Information retrieval (IR) evaluation measures are cornerstones for\ndetermining the suitability and task performance efficiency of retrieval\nsystems. Their metric and scale properties enable to compare one system against\nanother to establish differences or similarities. Based on the representational\ntheory of measurement, this paper determines these properties by exploiting the\ninformation contained in a retrieval measure itself. It establishes the\nintrinsic framework of a retrieval measure, which is the common scenario when\nthe domain set is not explicitly specified. A method to determine the metric\nand scale properties of any retrieval measure is provided, requiring knowledge\nof only some of its attained values. The method establishes three main\ncategories of retrieval measures according to their intrinsic properties. Some\ncommon user-oriented and system-oriented evaluation measures are classified\naccording to the presented taxonomy.\n","authors":["Fernando Giner"],"pdf_url":"https://arxiv.org/pdf/2304.00615v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2304.00578v1","updated":"2023-04-02T17:06:07Z","published":"2023-04-02T17:06:07Z","title":"Sequence-aware item recommendations for multiply repeated user-item\n  interactions","summary":"  Recommender systems are one of the most successful applications of machine\nlearning and data science. They are successful in a wide variety of application\ndomains, including e-commerce, media streaming content, email marketing, and\nvirtually every industry where personalisation facilitates better user\nexperience or boosts sales and customer engagement. The main goal of these\nsystems is to analyse past user behaviour to predict which items are of most\ninterest to users. They are typically built with the use of matrix-completion\ntechniques such as collaborative filtering or matrix factorisation. However,\nalthough these approaches have achieved tremendous success in numerous\nreal-world applications, their effectiveness is still limited when users might\ninteract multiple times with the same items, or when user preferences change\nover time.\n  We were inspired by the approach that Natural Language Processing techniques\ntake to compress, process, and analyse sequences of text. We designed a\nrecommender system that induces the temporal dimension in the task of item\nrecommendation and considers sequences of item interactions for each user in\norder to make recommendations. This method is empirically shown to give highly\naccurate predictions of user-items interactions for all users in a retail\nenvironment, without explicit feedback, besides increasing total sales by 5%\nand individual customer expenditure by over 50% in an A/B live test.\n","authors":["Juan Pablo Equihua","Maged Ali","Henrik Nordmark","Berthold Lausen"],"pdf_url":"https://arxiv.org/pdf/2304.00578v1.pdf","comment":"18 pages, 4 figures"},{"id":"http://arxiv.org/abs/2304.00545v1","updated":"2023-04-02T14:41:17Z","published":"2023-04-02T14:41:17Z","title":"FANS: Fast Non-Autoregressive Sequence Generation for Item List\n  Continuation","summary":"  User-curated item lists, such as video-based playlists on Youtube and\nbook-based lists on Goodreads, have become prevalent for content sharing on\nonline platforms. Item list continuation is proposed to model the overall trend\nof a list and predict subsequent items. Recently, Transformer-based models have\nshown promise in comprehending contextual information and capturing item\nrelationships in a list. However, deploying them in real-time industrial\napplications is challenging, mainly because the autoregressive generation\nmechanism used in them is time-consuming. In this paper, we propose a novel\nfast non-autoregressive sequence generation model, namely FANS, to enhance\ninference efficiency and quality for item list continuation. First, we use a\nnon-autoregressive generation mechanism to decode next $K$ items simultaneously\ninstead of one by one in existing models. Then, we design a two-stage\nclassifier to replace the vanilla classifier used in current transformer-based\nmodels to further reduce the decoding time. Moreover, to improve the quality of\nnon-autoregressive generation, we employ a curriculum learning strategy to\noptimize training. Experimental results on four real-world item list\ncontinuation datasets including Zhihu, Spotify, AotM, and Goodreads show that\nour FANS model can significantly improve inference efficiency (up to 8.7x)\nwhile achieving competitive or better generation quality for item list\ncontinuation compared with the state-of-the-art autoregressive models. We also\nvalidate the efficiency of FANS in an industrial setting. Our source code and\ndata will be available at MindSpore/models and Github.\n","authors":["Qijiong Liu","Jieming Zhu","Jiahao Wu","Tiandeng Wu","Zhenhua Dong","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2304.00545v1.pdf","comment":"10 pages, ACM The Web Conference 2023 accepted paper"},{"id":"http://arxiv.org/abs/2304.00413v1","updated":"2023-04-02T00:19:59Z","published":"2023-04-02T00:19:59Z","title":"The Archive Query Log: Mining Millions of Search Result Pages of\n  Hundreds of Search Engines from 25 Years of Web Archives","summary":"  The Archive Query Log (AQL) is a previously unused, comprehensive query log\ncollected at the Internet Archive over the last 25 years. Its first version\nincludes 356 million queries, 166 million search result pages, and 1.7 billion\nsearch results across 550 search providers. Although many query logs have been\nstudied in the literature, the search providers that own them generally do not\npublish their logs to protect user privacy and vital business data. Of the few\nquery logs publicly available, none combines size, scope, and diversity. The\nAQL is the first to do so, enabling research on new retrieval models and\n(diachronic) search engine analyses. Provided in a privacy-preserving manner,\nit promotes open research as well as more transparency and accountability in\nthe search industry.\n","authors":["Jan Heinrich Reimer","Sebastian Schmidt","Maik Fröbe","Lukas Gienapp","Harrisen Scells","Benno Stein","Matthias Hagen","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2304.00413v1.pdf","comment":"12 pages. To be published in the proceedings of SIGIR 2023"},{"id":"http://arxiv.org/abs/2304.01225v1","updated":"2023-04-02T07:25:01Z","published":"2023-04-02T07:25:01Z","title":"A greedy approach for increased vehicle utilization in ridesharing\n  networks","summary":"  In recent years, ridesharing platforms have become a prominent mode of\ntransportation for the residents of urban areas. As a fundamental problem,\nroute recommendation for these platforms is vital for their sustenance. The\nworks done in this direction have recommended routes with higher passenger\ndemand. Despite the existing works, statistics have suggested that these\nservices cause increased greenhouse emissions compared to private vehicles as\nthey roam around in search of riders. This analysis provides finer details\nregarding the functionality of ridesharing systems and it reveals that in the\nface of their boom, they have not utilized the vehicle capacity efficiently. We\npropose to overcome the above limitations and recommend routes that will fetch\nmultiple passengers simultaneously which will result in increased vehicle\nutilization and thereby decrease the effect of these systems on the\nenvironment. As route recommendation is NP-hard, we propose a k-hop-based\nsliding window approximation algorithm that reduces the search space from\nentire road network to a window. We further demonstrate that maximizing\nexpected demand is submodular and greedy algorithms can be used to optimize our\nobjective function within a window. We evaluate our proposed model on\nreal-world datasets and experimental results demonstrate superior performance\nby our proposed model.\n","authors":["Aqsa Ashraf Makhdomi","Iqra Altaf Gillani"],"pdf_url":"https://arxiv.org/pdf/2304.01225v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2304.00653v1","updated":"2023-04-02T23:40:17Z","published":"2023-04-02T23:40:17Z","title":"Enhancing Cluster Quality of Numerical Datasets with Domain Ontology","summary":"  Ontology-based clustering has gained attention in recent years due to the\npotential benefits of ontology. Current ontology-based clustering approaches\nhave mainly been applied to reduce the dimensionality of attributes in text\ndocument clustering. Reduction in dimensionality of attributes using ontology\nhelps to produce high quality clusters for a dataset. However, ontology-based\napproaches in clustering numerical datasets have not been gained enough\nattention. Moreover, some literature mentions that ontology-based clustering\ncan produce either high quality or low-quality clusters from a dataset.\nTherefore, in this paper we present a clustering approach that is based on\ndomain ontology to reduce the dimensionality of attributes in a numerical\ndataset using domain ontology and to produce high quality clusters. For every\ndataset, we produce three datasets using domain ontology. We then cluster these\ndatasets using a genetic algorithm-based clustering technique called\nGenClust++. The clusters of each dataset are evaluated in terms of Sum of\nSquared-Error (SSE). We use six numerical datasets to evaluate the performance\nof our ontology-based approach. The experimental results of our approach\nindicate that cluster quality gradually improves from lower to the higher\nlevels of a domain ontology.\n","authors":["Sudath Rohitha Heiyanthuduwage","Md Anisur Rahman","Md Zahidul Islam"],"pdf_url":"https://arxiv.org/pdf/2304.00653v1.pdf","comment":"6 Pages, IEEE CSDE2022 Conference Paper"},{"id":"http://arxiv.org/abs/2304.00649v1","updated":"2023-04-02T23:08:11Z","published":"2023-04-02T23:08:11Z","title":"Multilingual Word Error Rate Estimation: e-WER3","summary":"  The success of the multilingual automatic speech recognition systems\nempowered many voice-driven applications. However, measuring the performance of\nsuch systems remains a major challenge, due to its dependency on manually\ntranscribed speech data in both mono- and multilingual scenarios. In this\npaper, we propose a novel multilingual framework -- eWER3 -- jointly trained on\nacoustic and lexical representation to estimate word error rate. We demonstrate\nthe effectiveness of eWER3 to (i) predict WER without using any internal states\nfrom the ASR and (ii) use the multilingual shared latent space to push the\nperformance of the close-related languages. We show our proposed multilingual\nmodel outperforms the previous monolingual word error rate estimation method\n(eWER2) by an absolute 9\\% increase in Pearson correlation coefficient (PCC),\nwith better overall estimation between the predicted and reference WER.\n","authors":["Shammur Absar Chowdhury","Ahmed Ali"],"pdf_url":"https://arxiv.org/pdf/2304.00649v1.pdf","comment":"Accepted in ICASSP, Multilingual WER estimation, End-to-End systems,\n  multilingual model, automatic word error rate estimation"},{"id":"http://arxiv.org/abs/2303.12748v3","updated":"2023-04-02T23:01:59Z","published":"2023-03-11T17:14:04Z","title":"Enabling Calibration In The Zero-Shot Inference of Large Vision-Language\n  Models","summary":"  Calibration of deep learning models is crucial to their trustworthiness and\nsafe usage, and as such, has been extensively studied in supervised\nclassification models, with methods crafted to decrease miscalibration.\nHowever, there has yet to be a comprehensive study of the calibration of\nvision-language models that are used for zero-shot inference, like CLIP. We\nmeasure calibration across relevant variables like prompt, dataset, and\narchitecture, and find that zero-shot inference with CLIP is miscalibrated.\nFurthermore, we propose a modified version of temperature scaling that is\naligned with the common use cases of CLIP as a zero-shot inference model, and\nshow that a single learned temperature generalizes for each specific CLIP model\n(defined by a chosen pre-training dataset and architecture) across inference\ndataset and prompt choice.\n","authors":["Will LeVine","Benjamin Pikus","Pranav Raj","Fernando Amat Gil"],"pdf_url":"https://arxiv.org/pdf/2303.12748v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.14048v2","updated":"2023-04-02T22:54:36Z","published":"2022-05-27T15:36:12Z","title":"Average Adjusted Association: Efficient Estimation with High Dimensional\n  Confounders","summary":"  The log odds ratio is a well-established metric for evaluating the\nassociation between binary outcome and exposure variables. Despite its\nwidespread use, there has been limited discussion on how to summarize the log\nodds ratio as a function of confounders through averaging. To address this\nissue, we propose the Average Adjusted Association (AAA), which is a summary\nmeasure of association in a heterogeneous population, adjusted for observed\nconfounders. To facilitate the use of it, we also develop efficient\ndouble/debiased machine learning (DML) estimators of the AAA. Our DML\nestimators use two equivalent forms of the efficient influence function, and\nare applicable in various sampling scenarios, including random sampling,\noutcome-based sampling, and exposure-based sampling. Through real data and\nsimulations, we demonstrate the practicality and effectiveness of our proposed\nestimators in measuring the AAA.\n","authors":["Sung Jae Jun","Sokbae Lee"],"pdf_url":"https://arxiv.org/pdf/2205.14048v2.pdf","comment":"35 pages, 3 tables"},{"id":"http://arxiv.org/abs/2206.10011v2","updated":"2023-04-02T22:19:08Z","published":"2022-06-20T21:23:15Z","title":"When Does Re-initialization Work?","summary":"  Re-initializing a neural network during training has been observed to improve\ngeneralization in recent works. Yet it is neither widely adopted in deep\nlearning practice nor is it often used in state-of-the-art training protocols.\nThis raises the question of when re-initialization works, and whether it should\nbe used together with regularization techniques such as data augmentation,\nweight decay and learning rate schedules. In this work, we conduct an extensive\nempirical comparison of standard training with a selection of re-initialization\nmethods to answer this question, training over 15,000 models on a variety of\nimage classification benchmarks. We first establish that such methods are\nconsistently beneficial for generalization in the absence of any other\nregularization. However, when deployed alongside other carefully tuned\nregularization techniques, re-initialization methods offer little to no added\nbenefit for generalization, although optimal generalization performance becomes\nless sensitive to the choice of learning rate and weight decay hyperparameters.\nTo investigate the impact of re-initialization methods on noisy data, we also\nconsider learning under label noise. Surprisingly, in this case,\nre-initialization significantly improves upon standard training, even in the\npresence of other carefully tuned regularization techniques.\n","authors":["Sheheryar Zaidi","Tudor Berariu","Hyunjik Kim","Jörg Bornschein","Claudia Clopath","Yee Whye Teh","Razvan Pascanu"],"pdf_url":"https://arxiv.org/pdf/2206.10011v2.pdf","comment":"Published in PMLR Volume 187; spotlight presentation at I Can't\n  Believe It's Not Better Workshop at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2107.11732v5","updated":"2023-04-02T22:13:24Z","published":"2021-07-25T05:55:00Z","title":"Federated Causal Inference in Heterogeneous Observational Data","summary":"  We are interested in estimating the effect of a treatment applied to\nindividuals at multiple sites, where data is stored locally for each site. Due\nto privacy constraints, individual-level data cannot be shared across sites;\nthe sites may also have heterogeneous populations and treatment assignment\nmechanisms. Motivated by these considerations, we develop federated methods to\ndraw inference on the average treatment effects of combined data across sites.\nOur methods first compute summary statistics locally using propensity scores\nand then aggregate these statistics across sites to obtain point and variance\nestimators of average treatment effects. We show that these estimators are\nconsistent and asymptotically normal. To achieve these asymptotic properties,\nwe find that the aggregation schemes need to account for the heterogeneity in\ntreatment assignments and in outcomes across sites. We demonstrate the validity\nof our federated methods through a comparative study of two large medical\nclaims databases.\n","authors":["Ruoxuan Xiong","Allison Koenecke","Michael Powell","Zhu Shen","Joshua T. Vogelstein","Susan Athey"],"pdf_url":"https://arxiv.org/pdf/2107.11732v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00634v1","updated":"2023-04-02T21:39:00Z","published":"2023-04-02T21:39:00Z","title":"MMT: A Multilingual and Multi-Topic Indian Social Media Dataset","summary":"  Social media plays a significant role in cross-cultural communication. A vast\namount of this occurs in code-mixed and multilingual form, posing a significant\nchallenge to Natural Language Processing (NLP) tools for processing such\ninformation, like language identification, topic modeling, and named-entity\nrecognition. To address this, we introduce a large-scale multilingual, and\nmulti-topic dataset (MMT) collected from Twitter (1.7 million Tweets),\nencompassing 13 coarse-grained and 63 fine-grained topics in the Indian\ncontext. We further annotate a subset of 5,346 tweets from the MMT dataset with\nvarious Indian languages and their code-mixed counterparts. Also, we\ndemonstrate that the currently existing tools fail to capture the linguistic\ndiversity in MMT on two downstream tasks, i.e., topic modeling and language\nidentification. To facilitate future research, we will make the anonymized and\nannotated dataset available in the public domain.\n","authors":["Dwip Dalal","Vivek Srivastava","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2304.00634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12848v3","updated":"2023-04-02T21:27:16Z","published":"2023-03-22T18:14:02Z","title":"Test-time Detection and Repair of Adversarial Samples via Masked\n  Autoencoder","summary":"  Training-time defenses, known as adversarial training, incur high training\ncosts and do not generalize to unseen attacks. Test-time defenses solve these\nissues but most existing test-time defenses require adapting the model weights,\ntherefore they do not work on frozen models and complicate model memory\nmanagement. The only test-time defense that does not adapt model weights aims\nto adapt the input with self-supervision tasks. However, we empirically found\nthese self-supervision tasks are not sensitive enough to detect adversarial\nattacks accurately. In this paper, we propose DRAM, a novel defense method to\ndetect and repair adversarial samples at test time via Masked autoencoder\n(MAE). We demonstrate how to use MAE losses to build a Kolmogorov-Smirnov test\nto detect adversarial samples. Moreover, we use the MAE losses to calculate\ninput reversal vectors that repair adversarial samples resulting from\npreviously unseen attacks. Results on large-scale ImageNet dataset show that,\ncompared to all detection baselines evaluated, DRAM achieves the best detection\nrate (82% on average) on all eight adversarial attacks evaluated. For attack\nrepair, DRAM improves the robust accuracy by 6% ~ 41% for standard ResNet50 and\n3% ~ 8% for robust ResNet50 compared with the baselines that use contrastive\nlearning and rotation prediction.\n","authors":["Yun-Yun Tsai","Ju-Chin Chao","Albert Wen","Zhaoyuan Yang","Chengzhi Mao","Tapan Shah","Junfeng Yang"],"pdf_url":"https://arxiv.org/pdf/2303.12848v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00629v1","updated":"2023-04-02T21:12:13Z","published":"2023-04-02T21:12:13Z","title":"A principled approach to model validation in domain generalization","summary":"  Domain generalization aims to learn a model with good generalization ability,\nthat is, the learned model should not only perform well on several seen domains\nbut also on unseen domains with different data distributions. State-of-the-art\ndomain generalization methods typically train a representation function\nfollowed by a classifier jointly to minimize both the classification risk and\nthe domain discrepancy. However, when it comes to model selection, most of\nthese methods rely on traditional validation routines that select models solely\nbased on the lowest classification risk on the validation set. In this paper,\nwe theoretically demonstrate a trade-off between minimizing classification risk\nand mitigating domain discrepancy, i.e., it is impossible to achieve the\nminimum of these two objectives simultaneously. Motivated by this theoretical\nresult, we propose a novel model selection method suggesting that the\nvalidation process should account for both the classification risk and the\ndomain discrepancy. We validate the effectiveness of the proposed method by\nnumerical results on several domain generalization datasets.\n","authors":["Boyang Lyu","Thuan Nguyen","Matthias Scheutz","Prakash Ishwar","Shuchin Aeron"],"pdf_url":"https://arxiv.org/pdf/2304.00629v1.pdf","comment":"Accepted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2304.00623v1","updated":"2023-04-02T20:47:08Z","published":"2023-04-02T20:47:08Z","title":"MalIoT: Scalable and Real-time Malware Traffic Detection for IoT\n  Networks","summary":"  The machine learning approach is vital in Internet of Things (IoT) malware\ntraffic detection due to its ability to keep pace with the ever-evolving nature\nof malware. Machine learning algorithms can quickly and accurately analyze the\nvast amount of data produced by IoT devices, allowing for the real-time\nidentification of malicious network traffic. The system can handle the\nexponential growth of IoT devices thanks to the usage of distributed systems\nlike Apache Kafka and Apache Spark, and Intel's oneAPI software stack\naccelerates model inference speed, making it a useful tool for real-time\nmalware traffic detection. These technologies work together to create a system\nthat can give scalable performance and high accuracy, making it a crucial tool\nfor defending against cyber threats in smart communities and medical\ninstitutions.\n","authors":["Ethan Weitkamp","Yusuke Satani","Adam Omundsen","Jingwen Wang","Peilong Li"],"pdf_url":"https://arxiv.org/pdf/2304.00623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00622v1","updated":"2023-04-02T20:37:22Z","published":"2023-04-02T20:37:22Z","title":"Automatic Detection of Natural Disaster Effect on Paddy Field from\n  Satellite Images using Deep Learning Techniques","summary":"  This paper aims to detect rice field damage from natural disasters in\nBangladesh using high-resolution satellite imagery. The authors developed\nground truth data for rice field damage from the field level. At first, NDVI\ndifferences before and after the disaster are calculated to identify possible\ncrop loss. The areas equal to and above the 0.33 threshold are marked as crop\nloss areas as significant changes are observed. The authors also verified crop\nloss areas by collecting data from local farmers. Later, different bands of\nsatellite data (Red, Green, Blue) and (False Color Infrared) are useful to\ndetect crop loss area. We used the NDVI different images as ground truth to\ntrain the DeepLabV3plus model. With RGB, we got IoU 0.41 and with FCI, we got\nIoU 0.51. As FCI uses NIR, Red, Blue bands and NDVI is normalized difference\nbetween NIR and Red bands, so greater FCI's IoU score than RGB is expected. But\nRGB does not perform very badly here. So, where other bands are not available,\nRGB can use to understand crop loss areas to some extent. The ground truth\ndeveloped in this paper can be used for segmentation models with very high\nresolution RGB only images such as Bing, Google etc.\n","authors":["Tahmid Alavi Ishmam","Amin Ahsan Ali","Md Ahsraful Amin","A K M Mahbubur Rahman"],"pdf_url":"https://arxiv.org/pdf/2304.00622v1.pdf","comment":"6 pages, 13 figures. This paper has been accepted for presentation at\n  the ICCRE2023 conference, held at Nagaoka University of Technology, Japan"},{"id":"http://arxiv.org/abs/2304.00613v1","updated":"2023-04-02T20:05:20Z","published":"2023-04-02T20:05:20Z","title":"Improving Few-Shot Inductive Learning on Temporal Knowledge Graphs using\n  Confidence-Augmented Reinforcement Learning","summary":"  Temporal knowledge graph completion (TKGC) aims to predict the missing links\namong the entities in a temporal knwoledge graph (TKG). Most previous TKGC\nmethods only consider predicting the missing links among the entities seen in\nthe training set, while they are unable to achieve great performance in link\nprediction concerning newly-emerged unseen entities. Recently, a new task,\ni.e., TKG few-shot out-of-graph (OOG) link prediction, is proposed, where TKGC\nmodels are required to achieve great link prediction performance concerning\nnewly-emerged entities that only have few-shot observed examples. In this work,\nwe propose a TKGC method FITCARL that combines few-shot learning with\nreinforcement learning to solve this task. In FITCARL, an agent traverses\nthrough the whole TKG to search for the prediction answer. A policy network is\ndesigned to guide the search process based on the traversed path. To better\naddress the data scarcity problem in the few-shot setting, we introduce a\nmodule that computes the confidence of each candidate action and integrate it\ninto the policy for action selection. We also exploit the entity concept\ninformation with a novel concept regularizer to boost model performance.\nExperimental results show that FITCARL achieves stat-of-the-art performance on\nTKG few-shot OOG link prediction.\n","authors":["Zifeng Ding","Jingpei Wu","Zongyue Li","Yunpu Ma","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2304.00613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00601v1","updated":"2023-04-02T19:09:01Z","published":"2023-04-02T19:09:01Z","title":"Constructive Assimilation: Boosting Contrastive Learning Performance\n  through View Generation Strategies","summary":"  Transformations based on domain expertise (expert transformations), such as\nrandom-resized-crop and color-jitter, have proven critical to the success of\ncontrastive learning techniques such as SimCLR. Recently, several attempts have\nbeen made to replace such domain-specific, human-designed transformations with\ngenerated views that are learned. However for imagery data, so far none of\nthese view-generation methods has been able to outperform expert\ntransformations. In this work, we tackle a different question: instead of\nreplacing expert transformations with generated views, can we constructively\nassimilate generated views with expert transformations? We answer this question\nin the affirmative and propose a view generation method and a simple, effective\nassimilation method that together improve the state-of-the-art by up to ~3.6%\non three different datasets. Importantly, we conduct a detailed empirical study\nthat systematically analyzes a range of view generation and assimilation\nmethods and provides a holistic picture of the efficacy of learned views in\ncontrastive representation learning.\n","authors":["Ligong Han","Seungwook Han","Shivchander Sudalairaj","Charlotte Loh","Rumen Dangovski","Fei Deng","Pulkit Agrawal","Dimitris Metaxas","Leonid Karlinsky","Tsui-Wei Weng","Akash Srivastava"],"pdf_url":"https://arxiv.org/pdf/2304.00601v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2302.08647v3","updated":"2023-04-02T19:08:31Z","published":"2023-02-17T01:32:44Z","title":"Multiresolution Graph Transformers and Wavelet Positional Encoding for\n  Learning Hierarchical Structures","summary":"  Contemporary graph learning algorithms are not well-defined for large\nmolecules since they do not consider the hierarchical interactions among the\natoms, which are essential to determine the molecular properties of\nmacromolecules. In this work, we propose Multiresolution Graph Transformers\n(MGT), the first graph transformer architecture that can learn to represent\nlarge molecules at multiple scales. MGT can learn to produce representations\nfor the atoms and group them into meaningful functional groups or repeating\nunits. We also introduce Wavelet Positional Encoding (WavePE), a new positional\nencoding method that can guarantee localization in both spectral and spatial\ndomains. Our proposed model achieves competitive results on two macromolecule\ndatasets consisting of polymers and peptides, and one drug-like molecule\ndataset. Importantly, our model outperforms other state-of-the-art methods and\nachieves chemical accuracy in estimating molecular properties (e.g., GAP, HOMO\nand LUMO) calculated by Density Functional Theory (DFT) in the polymers\ndataset. Furthermore, the visualizations, including clustering results on\nmacromolecules and low-dimensional spaces of their representations, demonstrate\nthe capability of our methodology in learning to represent long-range and\nhierarchical structures. Our PyTorch implementation is publicly available at\nhttps://github.com/HySonLab/Multires-Graph-Transformer\n","authors":["Nhat Khang Ngo","Truong Son Hy","Risi Kondor"],"pdf_url":"https://arxiv.org/pdf/2302.08647v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00600v1","updated":"2023-04-02T19:08:02Z","published":"2023-04-02T19:08:02Z","title":"Recurrence without Recurrence: Stable Video Landmark Detection with Deep\n  Equilibrium Models","summary":"  Cascaded computation, whereby predictions are recurrently refined over\nseveral stages, has been a persistent theme throughout the development of\nlandmark detection models. In this work, we show that the recently proposed\nDeep Equilibrium Model (DEQ) can be naturally adapted to this form of\ncomputation. Our Landmark DEQ (LDEQ) achieves state-of-the-art performance on\nthe challenging WFLW facial landmark dataset, reaching $3.92$ NME with fewer\nparameters and a training memory cost of $\\mathcal{O}(1)$ in the number of\nrecurrent modules. Furthermore, we show that DEQs are particularly suited for\nlandmark detection in videos. In this setting, it is typical to train on still\nimages due to the lack of labelled videos. This can lead to a ``flickering''\neffect at inference time on video, whereby a model can rapidly oscillate\nbetween different plausible solutions across consecutive frames. By rephrasing\nDEQs as a constrained optimization, we emulate recurrence at inference time,\ndespite not having access to temporal data at training time. This Recurrence\nwithout Recurrence (RwR) paradigm helps in reducing landmark flicker, which we\ndemonstrate by introducing a new metric, normalized mean flicker (NMF), and\ncontributing a new facial landmark video dataset (WFLW-V) targeting landmark\nuncertainty. On the WFLW-V hard subset made up of $500$ videos, our LDEQ with\nRwR improves the NME and NMF by $10$ and $13\\%$ respectively, compared to the\nstrongest previously published model using a hand-tuned conventional filter.\n","authors":["Paul Micaelli","Arash Vahdat","Hongxu Yin","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2304.00600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2101.01867v3","updated":"2023-04-02T18:16:37Z","published":"2021-01-06T04:38:57Z","title":"dame-flame: A Python Library Providing Fast Interpretable Matching for\n  Causal Inference","summary":"  dame-flame is a Python package for performing matching for observational\ncausal inference on datasets containing discrete covariates. This package\nimplements the Dynamic Almost Matching Exactly (DAME) and Fast Large-Scale\nAlmost Matching Exactly (FLAME) algorithms, which match treatment and control\nunits on subsets of the covariates. The resulting matched groups are\ninterpretable, because the matches are made on covariates, and high-quality,\nbecause machine learning is used to determine which covariates are important to\nmatch on. DAME solves an optimization problem that matches units on as many\ncovariates as possible, prioritizing matches on important covariates. FLAME\napproximates the solution found by DAME via a much faster backward feature\nselection procedure. The package provides several adjustable parameters to\nadapt the algorithms to specific applications, and can calculate treatment\neffect estimates after matching. Descriptions of these parameters, details on\nestimating treatment effects, and further examples, can be found in the\ndocumentation at\nhttps://almost-matching-exactly.github.io/DAME-FLAME-Python-Package/\n","authors":["Neha R. Gupta","Vittorio Orlandi","Chia-Rui Chang","Tianyu Wang","Marco Morucci","Pritam Dey","Thomas J. Howell","Xian Sun","Angikar Ghosal","Sudeepa Roy","Cynthia Rudin","Alexander Volfovsky"],"pdf_url":"https://arxiv.org/pdf/2101.01867v3.pdf","comment":"26 pages, 2 figures"},{"id":"http://arxiv.org/abs/2210.01115v2","updated":"2023-04-02T18:03:06Z","published":"2022-10-03T17:56:35Z","title":"LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of\n  Vision & Language Models","summary":"  Soft prompt learning has recently emerged as one of the methods of choice for\nadapting V&L models to a downstream task using a few training examples.\nHowever, current methods significantly overfit the training data, suffering\nfrom large accuracy degradation when tested on unseen classes from the same\ndomain. To this end, in this paper, we make the following 4 contributions: (1)\nTo alleviate base class overfitting, we propose a novel Language-Aware Soft\nPrompting (LASP) learning method by means of a text-to-text cross-entropy loss\nthat maximizes the probability of the learned prompts to be correctly\nclassified with respect to pre-defined hand-crafted textual prompts. (2) To\nincrease the representation capacity of the prompts, we propose grouped LASP\nwhere each group of prompts is optimized with respect to a separate subset of\ntextual prompts. (3) We identify a visual-language misalignment introduced by\nprompt learning and LASP, and more importantly, propose a re-calibration\nmechanism to address it. (4) We show that LASP is inherently amenable to\nincluding, during training, virtual classes, i.e. class names for which no\nvisual samples are available, further increasing the robustness of the learned\nprompts. Through evaluations on 11 datasets, we show that our approach (a)\nsignificantly outperforms all prior works on soft prompting, and (b) matches\nand surpasses, for the first time, the accuracy on novel classes obtained by\nhand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be made\navailable at https://www.adrianbulat.com/lasp\n","authors":["Adrian Bulat","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2210.01115v2.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17580v2","updated":"2023-04-02T17:24:47Z","published":"2023-03-30T17:48:28Z","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace","summary":"  Solving complicated AI tasks with different domains and modalities is a key\nstep toward advanced artificial intelligence. While there are abundant AI\nmodels available for different domains and modalities, they cannot handle\ncomplicated AI tasks. Considering large language models (LLMs) have exhibited\nexceptional ability in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks and language could be a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, a\nframework that leverages LLMs (e.g., ChatGPT) to connect various AI models in\nmachine learning communities (e.g., Hugging Face) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHugging Face, execute each subtask with the selected AI model, and summarize\nthe response according to the execution results. By leveraging the strong\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\nHuggingGPT is able to cover numerous sophisticated AI tasks in different\nmodalities and domains and achieve impressive results in language, vision,\nspeech, and other challenging tasks, which paves a new way towards advanced\nartificial intelligence.\n","authors":["Yongliang Shen","Kaitao Song","Xu Tan","Dongsheng Li","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2303.17580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00575v1","updated":"2023-04-02T16:48:43Z","published":"2023-04-02T16:48:43Z","title":"Modelling customer churn for the retail industry in a deep learning\n  based sequential framework","summary":"  As retailers around the world increase efforts in developing targeted\nmarketing campaigns for different audiences, predicting accurately which\ncustomers are most likely to churn ahead of time is crucial for marketing teams\nin order to increase business profits. This work presents a deep survival\nframework to predict which customers are at risk of stopping to purchase with\nretail companies in non-contractual settings. By leveraging the survival model\nparameters to be learnt by recurrent neural networks, we are able to obtain\nindividual level survival models for purchasing behaviour based only on\nindividual customer behaviour and avoid time-consuming feature engineering\nprocesses usually done when training machine learning models.\n","authors":["Juan Pablo Equihua","Henrik Nordmark","Maged Ali","Berthold Lausen"],"pdf_url":"https://arxiv.org/pdf/2304.00575v1.pdf","comment":"21 pages, 12 figures"},{"id":"http://arxiv.org/abs/2304.00573v1","updated":"2023-04-02T16:44:14Z","published":"2023-04-02T16:44:14Z","title":"Risk-Sensitive and Robust Model-Based Reinforcement Learning and\n  Planning","summary":"  Many sequential decision-making problems that are currently automated, such\nas those in manufacturing or recommender systems, operate in an environment\nwhere there is either little uncertainty, or zero risk of catastrophe. As\ncompanies and researchers attempt to deploy autonomous systems in less\nconstrained environments, it is increasingly important that we endow sequential\ndecision-making algorithms with the ability to reason about uncertainty and\nrisk.\n  In this thesis, we will address both planning and reinforcement learning (RL)\napproaches to sequential decision-making. In the planning setting, it is\nassumed that a model of the environment is provided, and a policy is optimised\nwithin that model. Reinforcement learning relies upon extensive random\nexploration, and therefore usually requires a simulator in which to perform\ntraining. In many real-world domains, it is impossible to construct a perfectly\naccurate model or simulator. Therefore, the performance of any policy is\ninevitably uncertain due to the incomplete knowledge about the environment.\nFurthermore, in stochastic domains, the outcome of any given run is also\nuncertain due to the inherent randomness of the environment. These two sources\nof uncertainty are usually classified as epistemic, and aleatoric uncertainty,\nrespectively. The over-arching goal of this thesis is to contribute to\ndeveloping algorithms that mitigate both sources of uncertainty in sequential\ndecision-making problems.\n  We make a number of contributions towards this goal, with a focus on\nmodel-based algorithms...\n","authors":["Marc Rigter"],"pdf_url":"https://arxiv.org/pdf/2304.00573v1.pdf","comment":"DPhil (PhD) thesis, University of Oxford"},{"id":"http://arxiv.org/abs/2304.00570v1","updated":"2023-04-02T16:39:59Z","published":"2023-04-02T16:39:59Z","title":"FedFTN: Personalized Federated Learning with Deep Feature Transformation\n  Network for Multi-institutional Low-count PET Denoising","summary":"  Low-count PET is an efficient way to reduce radiation exposure and\nacquisition time, but the reconstructed images often suffer from low\nsignal-to-noise ratio (SNR), thus affecting diagnosis and other downstream\ntasks. Recent advances in deep learning have shown great potential in improving\nlow-count PET image quality, but acquiring a large, centralized, and diverse\ndataset from multiple institutions for training a robust model is difficult due\nto privacy and security concerns of patient data. Moreover, low-count PET data\nat different institutions may have different data distribution, thus requiring\npersonalized models. While previous federated learning (FL) algorithms enable\nmulti-institution collaborative training without the need of aggregating local\ndata, addressing the large domain shift in the application of\nmulti-institutional low-count PET denoising remains a challenge and is still\nhighly under-explored. In this work, we propose FedFTN, a personalized\nfederated learning strategy that addresses these challenges. FedFTN uses a\nlocal deep feature transformation network (FTN) to modulate the feature outputs\nof a globally shared denoising network, enabling personalized low-count PET\ndenoising for each institution. During the federated learning process, only the\ndenoising network's weights are communicated and aggregated, while the FTN\nremains at the local institutions for feature transformation. We evaluated our\nmethod using a large-scale dataset of multi-institutional low-count PET imaging\ndata from three medical centers located across three continents, and showed\nthat FedFTN provides high-quality low-count PET images, outperforming previous\nbaseline FL reconstruction methods across all low-count levels at all three\ninstitutions.\n","authors":["Bo Zhou","Huidong Xie","Qiong Liu","Xiongchao Chen","Xueqi Guo","Zhicheng Feng","S. Kevin Zhou","Biao Li","Axel Rominger","Kuangyu Shi","James S. Duncan","Chi Liu"],"pdf_url":"https://arxiv.org/pdf/2304.00570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00569v1","updated":"2023-04-02T16:38:13Z","published":"2023-04-02T16:38:13Z","title":"Stability Bounds for Learning-Based Adaptive Control of Discrete-Time\n  Multi-Dimensional Stochastic Linear Systems with Input Constraints","summary":"  We consider the problem of adaptive stabilization for discrete-time,\nmulti-dimensional linear systems with bounded control input constraints and\nunbounded stochastic disturbances, where the parameters of the true system are\nunknown. To address this challenge, we propose a certainty-equivalent control\nscheme which combines online parameter estimation with saturated linear\ncontrol. We establish the existence of a high probability stability bound on\nthe closed-loop system, under additional assumptions on the system and noise\nprocesses. Finally, numerical examples are presented to illustrate our results.\n","authors":["Seth Siriya","Jingge Zhu","Dragan Nešić","Ye Pu"],"pdf_url":"https://arxiv.org/pdf/2304.00569v1.pdf","comment":"21 pages, 1 figure, submitted to 62nd IEEE Conference on Decision and\n  Control"},{"id":"http://arxiv.org/abs/2304.00557v1","updated":"2023-04-02T15:24:08Z","published":"2023-04-02T15:24:08Z","title":"Semi-supervised Neural Machine Translation with Consistency\n  Regularization for Low-Resource Languages","summary":"  The advent of deep learning has led to a significant gain in machine\ntranslation. However, most of the studies required a large parallel dataset\nwhich is scarce and expensive to construct and even unavailable for some\nlanguages. This paper presents a simple yet effective method to tackle this\nproblem for low-resource languages by augmenting high-quality sentence pairs\nand training NMT models in a semi-supervised manner. Specifically, our approach\ncombines the cross-entropy loss for supervised learning with KL Divergence for\nunsupervised fashion given pseudo and augmented target sentences derived from\nthe model. We also introduce a SentenceBERT-based filter to enhance the quality\nof augmenting data by retaining semantically similar sentence pairs.\nExperimental results show that our approach significantly improves NMT\nbaselines, especially on low-resource datasets with 0.46--2.03 BLEU scores. We\nalso demonstrate that using unsupervised training for augmented data is more\nefficient than reusing the ground-truth target sentences for supervised\nlearning.\n","authors":["Viet H. Pham","Thang M. Pham","Giang Nguyen","Long Nguyen","Dien Dinh"],"pdf_url":"https://arxiv.org/pdf/2304.00557v1.pdf","comment":"TMP and GN contributed equally"},{"id":"http://arxiv.org/abs/2210.07420v2","updated":"2023-04-02T15:07:44Z","published":"2022-10-13T23:51:22Z","title":"Learning to Efficiently Plan Robust Frictional Multi-Object Grasps","summary":"  We consider a decluttering problem where multiple rigid convex polygonal\nobjects rest in randomly placed positions and orientations on a planar surface\nand must be efficiently transported to a packing box using both single and\nmulti-object grasps. Prior work considered frictionless multi-object grasping.\nIn this paper, we introduce friction to increase picks per hour. We train a\nneural network using real examples to plan robust multi-object grasps. In\nphysical experiments, we find a 13.7% increase in success rate, a 1.6x increase\nin picks per hour, and a 6.3x decrease in grasp planning time compared to prior\nwork on multi-object grasping. Compared to single object grasping, we find a\n3.1x increase in picks per hour.\n","authors":["Wisdom C. Agboh","Satvik Sharma","Kishore Srinivas","Mallika Parulekar","Gaurav Datta","Tianshuang Qiu","Jeffrey Ichnowski","Eugen Solowjow","Mehmet Dogar","Ken Goldberg"],"pdf_url":"https://arxiv.org/pdf/2210.07420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00553v1","updated":"2023-04-02T15:04:43Z","published":"2023-04-02T15:04:43Z","title":"From Isolated Islands to Pangea: Unifying Semantic Space for Human\n  Action Understanding","summary":"  Action understanding matters and attracts attention. It can be formed as the\nmapping from the action physical space to the semantic space. Typically,\nresearchers built action datasets according to idiosyncratic choices to define\nclasses and push the envelope of benchmarks respectively. Thus, datasets are\nincompatible with each other like \"Isolated Islands\" due to semantic gaps and\nvarious class granularities, e.g., do housework in dataset A and wash plate in\ndataset B. We argue that a more principled semantic space is an urgent need to\nconcentrate the community efforts and enable us to use all datasets together to\npursue generalizable action learning. To this end, we design a Poincare action\nsemantic space given verb taxonomy hierarchy and covering massive actions. By\naligning the classes of previous datasets to our semantic space, we gather\n(image/video/skeleton/MoCap) datasets into a unified database in a unified\nlabel system, i.e., bridging \"isolated islands\" into a \"Pangea\". Accordingly,\nwe propose a bidirectional mapping model between physical and semantic space to\nfully use Pangea. In extensive experiments, our system shows significant\nsuperiority, especially in transfer learning. Code and data will be made\npublicly available.\n","authors":["Yong-Lu Li","Xiaoqian Wu","Xinpeng Liu","Yiming Dou","Yikun Ji","Junyi Zhang","Yixing Li","Jingru Tan","Xudong Lu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2304.00553v1.pdf","comment":"Project Webpage: https://mvig-rhos.com/pangea"},{"id":"http://arxiv.org/abs/2304.00549v1","updated":"2023-04-02T14:56:15Z","published":"2023-04-02T14:56:15Z","title":"Variational Denoising for Variational Quantum Eigensolver","summary":"  The variational quantum eigensolver (VQE) is a hybrid algorithm that has the\npotential to provide a quantum advantage in practical chemistry problems that\nare currently intractable on classical computers. VQE trains parameterized\nquantum circuits using a classical optimizer to approximate the eigenvalues and\neigenstates of a given Hamiltonian. However, VQE faces challenges in\ntask-specific design and machine-specific architecture, particularly when\nrunning on noisy quantum devices. This can have a negative impact on its\ntrainability, accuracy, and efficiency, resulting in noisy quantum data. We\npropose variational denoising, an unsupervised learning method that employs a\nparameterized quantum neural network to improve the solution of VQE by learning\nfrom noisy VQE outputs. Our approach can significantly decrease energy\nestimation errors and increase fidelities with ground states compared to noisy\ninput data for the H2 and LiH molecular Hamiltonians, and surprisingly only\nrequires noisy data for training. Variational denoising can be integrated into\nquantum hardware, increasing its versatility as an end-to-end quantum\nprocessing for quantum data.\n","authors":["Quoc Hoan Tran","Shinji Kikuchi","Hirotaka Oshima"],"pdf_url":"https://arxiv.org/pdf/2304.00549v1.pdf","comment":"main text: 6 pages, 5 figures Supplementary Material: 13 pages, 12\n  figures"},{"id":"http://arxiv.org/abs/2304.00546v1","updated":"2023-04-02T14:46:58Z","published":"2023-04-02T14:46:58Z","title":"Video Pretraining Advances 3D Deep Learning on Chest CT Tasks","summary":"  Pretraining on large natural image classification datasets such as ImageNet\nhas aided model development on data-scarce 2D medical tasks. 3D medical tasks\noften have much less data than 2D medical tasks, prompting practitioners to\nrely on pretrained 2D models to featurize slices. However, these 2D models have\nbeen surpassed by 3D models on 3D computer vision benchmarks since they do not\nnatively leverage cross-sectional or temporal information. In this study, we\nexplore whether natural video pretraining for 3D models can enable higher\nperformance on smaller datasets for 3D medical tasks. We demonstrate video\npretraining improves the average performance of seven 3D models on two chest CT\ndatasets, regardless of finetuning dataset size, and that video pretraining\nallows 3D models to outperform 2D baselines. Lastly, we observe that\npretraining on the large-scale out-of-domain Kinetics dataset improves\nperformance more than pretraining on a typically-sized in-domain CT dataset.\nOur results show consistent benefits of video pretraining across a wide array\nof architectures, tasks, and training dataset sizes, supporting a shift from\nsmall-scale in-domain pretraining to large-scale out-of-domain pretraining for\n3D medical tasks. Our code is available at:\nhttps://github.com/rajpurkarlab/chest-ct-pretraining\n","authors":["Alexander Ke","Shih-Cheng Huang","Chloe P O'Connell","Michal Klimont","Serena Yeung","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2304.00546v1.pdf","comment":"Accepted at MIDL 2023"},{"id":"http://arxiv.org/abs/2211.17260v2","updated":"2023-04-02T14:26:57Z","published":"2022-11-30T18:55:27Z","title":"SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene","summary":"  Generative models have shown great promise in synthesizing photorealistic 3D\nobjects, but they require large amounts of training data. We introduce SinGRAF,\na 3D-aware generative model that is trained with a few input images of a single\nscene. Once trained, SinGRAF generates different realizations of this 3D scene\nthat preserve the appearance of the input while varying scene layout. For this\npurpose, we build on recent progress in 3D GAN architectures and introduce a\nnovel progressive-scale patch discrimination approach during training. With\nseveral experiments, we demonstrate that the results produced by SinGRAF\noutperform the closest related works in both quality and diversity by a large\nmargin.\n","authors":["Minjung Son","Jeong Joon Park","Leonidas Guibas","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2211.17260v2.pdf","comment":"CVPR 2023. Project page:\n  https://www.computationalimaging.org/publications/singraf/"},{"id":"http://arxiv.org/abs/2303.03052v3","updated":"2023-04-02T13:33:20Z","published":"2023-03-06T11:51:28Z","title":"Masked Images Are Counterfactual Samples for Robust Fine-tuning","summary":"  Deep learning models are challenged by the distribution shift between the\ntraining data and test data. Recently, the large models pre-trained on diverse\ndata have demonstrated unprecedented robustness to various distribution shifts.\nHowever, fine-tuning these models can lead to a trade-off between\nin-distribution (ID) performance and out-of-distribution (OOD) robustness.\nExisting methods for tackling this trade-off do not explicitly address the OOD\nrobustness problem. In this paper, based on causal analysis of the\naforementioned problems, we propose a novel fine-tuning method, which uses\nmasked images as counterfactual samples that help improve the robustness of the\nfine-tuning model. Specifically, we mask either the semantics-related or\nsemantics-unrelated patches of the images based on class activation map to\nbreak the spurious correlation, and refill the masked patches with patches from\nother images. The resulting counterfactual samples are used in feature-based\ndistillation with the pre-trained model. Extensive experiments verify that\nregularizing the fine-tuning with the proposed masked images can achieve a\nbetter trade-off between ID and OOD performance, surpassing previous methods on\nthe OOD performance. Our code is available at\nhttps://github.com/Coxy7/robust-finetuning.\n","authors":["Yao Xiao","Ziyi Tang","Pengxu Wei","Cong Liu","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2303.03052v3.pdf","comment":"Accepted by CVPR 2023 (v2: improve the clarity; v3: camera ready\n  version)"},{"id":"http://arxiv.org/abs/2211.16459v2","updated":"2023-04-02T13:09:28Z","published":"2022-11-29T18:40:02Z","title":"A Revenue Function for Comparison-Based Hierarchical Clustering","summary":"  Comparison-based learning addresses the problem of learning when, instead of\nexplicit features or pairwise similarities, one only has access to comparisons\nof the form: \\emph{Object $A$ is more similar to $B$ than to $C$.} Recently, it\nhas been shown that, in Hierarchical Clustering, single and complete linkage\ncan be directly implemented using only such comparisons while several\nalgorithms have been proposed to emulate the behaviour of average linkage.\nHence, finding hierarchies (or dendrograms) using only comparisons is a well\nunderstood problem. However, evaluating their meaningfulness when no\nground-truth nor explicit similarities are available remains an open question.\n  In this paper, we bridge this gap by proposing a new revenue function that\nallows one to measure the goodness of dendrograms using only comparisons. We\nshow that this function is closely related to Dasgupta's cost for hierarchical\nclustering that uses pairwise similarities. On the theoretical side, we use the\nproposed revenue function to resolve the open problem of whether one can\napproximately recover a latent hierarchy using few triplet comparisons. On the\npractical side, we present principled algorithms for comparison-based\nhierarchical clustering based on the maximisation of the revenue and we\nempirically compare them with existing methods.\n","authors":["Aishik Mandal","Michaël Perrot","Debarghya Ghoshdastidar"],"pdf_url":"https://arxiv.org/pdf/2211.16459v2.pdf","comment":"26 pages, 6 figures, 5 tables. Transactions on Machine Learning\n  Research (2023)"},{"id":"http://arxiv.org/abs/2208.13592v3","updated":"2023-04-02T12:35:02Z","published":"2022-08-29T13:39:30Z","title":"Smooth Monotone Stochastic Variational Inequalities and Saddle Point\n  Problems: A Survey","summary":"  This paper is a survey of methods for solving smooth (strongly) monotone\nstochastic variational inequalities. To begin with, we give the deterministic\nfoundation from which the stochastic methods eventually evolved. Then we review\nmethods for the general stochastic formulation, and look at the finite sum\nsetup. The last parts of the paper are devoted to various recent (not\nnecessarily stochastic) advances in algorithms for variational inequalities.\n","authors":["Aleksandr Beznosikov","Boris Polyak","Eduard Gorbunov","Dmitry Kovalev","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2208.13592v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2304.00521v1","updated":"2023-04-02T12:20:49Z","published":"2023-04-02T12:20:49Z","title":"Large Language Models are Few-shot Publication Scoopers","summary":"  Driven by recent advances AI, we passengers are entering a golden age of\nscientific discovery. But golden for whom? Confronting our insecurity that\nothers may beat us to the most acclaimed breakthroughs of the era, we propose a\nnovel solution to the long-standing personal credit assignment problem to\nensure that it is golden for us. At the heart of our approach is a\npip-to-the-post algorithm that assures adulatory Wikipedia pages without\nincurring the substantial capital and career risks of pursuing high impact\nscience with conventional research methodologies. By leveraging the meta trend\nof leveraging large language models for everything, we demonstrate the\nunparalleled potential of our algorithm to scoop groundbreaking findings with\nthe insouciance of a seasoned researcher at a dessert buffet.\n","authors":["Samuel Albanie","Liliane Momeni","João F. Henriques"],"pdf_url":"https://arxiv.org/pdf/2304.00521v1.pdf","comment":"SIGBOVIK 2023"},{"id":"http://arxiv.org/abs/2211.15107v2","updated":"2023-04-02T12:15:52Z","published":"2022-11-28T07:54:06Z","title":"A Light Touch Approach to Teaching Transformers Multi-view Geometry","summary":"  Transformers are powerful visual learners, in large part due to their\nconspicuous lack of manually-specified priors. This flexibility can be\nproblematic in tasks that involve multiple-view geometry, due to the\nnear-infinite possible variations in 3D shapes and viewpoints (requiring\nflexibility), and the precise nature of projective geometry (obeying rigid\nlaws). To resolve this conundrum, we propose a \"light touch\" approach, guiding\nvisual Transformers to learn multiple-view geometry but allowing them to break\nfree when needed. We achieve this by using epipolar lines to guide the\nTransformer's cross-attention maps, penalizing attention values outside the\nepipolar lines and encouraging higher attention along these lines since they\ncontain geometrically plausible matches. Unlike previous methods, our proposal\ndoes not require any camera pose information at test-time. We focus on\npose-invariant object instance retrieval, where standard Transformer networks\nstruggle, due to the large differences in viewpoint between query and retrieved\nimages. Experimentally, our method outperforms state-of-the-art approaches at\nobject retrieval, without needing pose information at test-time.\n","authors":["Yash Bhalgat","Joao F. Henriques","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2211.15107v2.pdf","comment":"Camera-ready version. Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2202.02771v2","updated":"2023-04-02T11:46:07Z","published":"2022-02-06T13:14:02Z","title":"Optimal Algorithms for Decentralized Stochastic Variational Inequalities","summary":"  Variational inequalities are a formalism that includes games, minimization,\nsaddle point, and equilibrium problems as special cases. Methods for\nvariational inequalities are therefore universal approaches for many applied\ntasks, including machine learning problems. This work concentrates on the\ndecentralized setting, which is increasingly important but not well understood.\nIn particular, we consider decentralized stochastic (sum-type) variational\ninequalities over fixed and time-varying networks. We present lower complexity\nbounds for both communication and local iterations and construct optimal\nalgorithms that match these lower bounds. Our algorithms are the best among the\navailable literature not only in the decentralized stochastic case, but also in\nthe decentralized deterministic and non-distributed stochastic cases.\nExperimental results confirm the effectiveness of the presented algorithms.\n","authors":["Dmitry Kovalev","Aleksandr Beznosikov","Abdurakhmon Sadiev","Michael Persiianov","Peter Richtárik","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2202.02771v2.pdf","comment":"Appears in: Advances in Neural Information Processing Systems 35\n  (NeurIPS 2022). Minor modifications with respect to the NeurIPS version. 58\n  pages, 6 algorithms, 9 figures, 4 tables"},{"id":"http://arxiv.org/abs/2110.03313v3","updated":"2023-04-02T11:02:14Z","published":"2021-10-07T10:04:32Z","title":"Distributed Methods with Compressed Communication for Solving\n  Variational Inequalities, with Theoretical Guarantees","summary":"  Variational inequalities in general and saddle point problems in particular\nare increasingly relevant in machine learning applications, including\nadversarial learning, GANs, transport and robust optimization. With increasing\ndata and problem sizes necessary to train high performing models across various\napplications, we need to rely on parallel and distributed computing. However,\nin distributed training, communication among the compute nodes is a key\nbottleneck during training, and this problem is exacerbated for high\ndimensional and over-parameterized models. Due to these considerations, it is\nimportant to equip existing methods with strategies that would allow to reduce\nthe volume of transmitted information during training while obtaining a model\nof comparable quality. In this paper, we present the first theoretically\ngrounded distributed methods for solving variational inequalities and saddle\npoint problems using compressed communication: MASHA1 and MASHA2. Our theory\nand methods allow for the use of both unbiased (such as Rand$k$; MASHA1) and\ncontractive (such as Top$k$; MASHA2) compressors. New algorithms support\nbidirectional compressions, and also can be modified for stochastic setting\nwith batches and for federated learning with partial participation of clients.\nWe empirically validated our conclusions using two experimental setups: a\nstandard bilinear min-max problem, and large-scale distributed adversarial\ntraining of transformers.\n","authors":["Aleksandr Beznosikov","Peter Richtárik","Michael Diskin","Max Ryabinin","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2110.03313v3.pdf","comment":"Appears in: Advances in Neural Information Processing Systems 35\n  (NeurIPS 2022). Minor modifications with respect to the NeurIPS version. 73\n  pages, 9 algorithms, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2106.08315v3","updated":"2023-04-02T10:30:33Z","published":"2021-06-15T17:45:51Z","title":"Decentralized Local Stochastic Extra-Gradient for Variational\n  Inequalities","summary":"  We consider distributed stochastic variational inequalities (VIs) on\nunbounded domains with the problem data that is heterogeneous (non-IID) and\ndistributed across many devices. We make a very general assumption on the\ncomputational network that, in particular, covers the settings of fully\ndecentralized calculations with time-varying networks and centralized\ntopologies commonly used in Federated Learning. Moreover, multiple local\nupdates on the workers can be made for reducing the communication frequency\nbetween the workers. We extend the stochastic extragradient method to this very\ngeneral setting and theoretically analyze its convergence rate in the\nstrongly-monotone, monotone, and non-monotone (when a Minty solution exists)\nsettings. The provided rates explicitly exhibit the dependence on network\ncharacteristics (e.g., mixing time), iteration counter, data heterogeneity,\nvariance, number of devices, and other standard parameters. As a special case,\nour method and analysis apply to distributed stochastic saddle-point problems\n(SPP), e.g., to the training of Deep Generative Adversarial Networks (GANs) for\nwhich decentralized training has been reported to be extremely challenging. In\nexperiments for the decentralized training of GANs we demonstrate the\neffectiveness of our proposed approach.\n","authors":["Aleksandr Beznosikov","Pavel Dvurechensky","Anastasia Koloskova","Valentin Samokhin","Sebastian U Stich","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2106.08315v3.pdf","comment":"Appears in: Advances in Neural Information Processing Systems 35\n  (NeurIPS 2022). Minor modifications with respect to the NeurIPS version. 43\n  pages, 1 algorithm, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2304.00498v1","updated":"2023-04-02T10:18:30Z","published":"2023-04-02T10:18:30Z","title":"Adversary-Aware Partial label learning with Label distillation","summary":"  To ensure that the data collected from human subjects is entrusted with a\nsecret, rival labels are introduced to conceal the information provided by the\nparticipants on purpose. The corresponding learning task can be formulated as a\nnoisy partial-label learning problem. However, conventional partial-label\nlearning (PLL) methods are still vulnerable to the high ratio of noisy partial\nlabels, especially in a large labelling space. To learn a more robust model, we\npresent Adversary-Aware Partial Label Learning and introduce the\n$\\textit{rival}$, a set of noisy labels, to the collection of candidate labels\nfor each instance. By introducing the rival label, the predictive distribution\nof PLL is factorised such that a handy predictive label is achieved with less\nuncertainty coming from the transition matrix, assuming the rival generation\nprocess is known. Nonetheless, the predictive accuracy is still insufficient to\nproduce an sufficiently accurate positive sample set to leverage the clustering\neffect of the contrastive loss function. Moreover, the inclusion of rivals also\nbrings an inconsistency issue for the classifier and risk function due to the\nintractability of the transition matrix. Consequently, an adversarial teacher\nwithin momentum (ATM) disambiguation algorithm is proposed to cope with the\nsituation, allowing us to obtain a provably consistent classifier and risk\nfunction. In addition, our method has shown high resiliency to the choice of\nthe label noise transition matrix. Extensive experiments demonstrate that our\nmethod achieves promising results on the CIFAR10, CIFAR100 and CUB200 datasets.\n","authors":["Cheng Chen","Yueming Lyu","Ivor W. Tsang"],"pdf_url":"https://arxiv.org/pdf/2304.00498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.12321v3","updated":"2023-04-02T09:55:37Z","published":"2021-12-23T02:41:00Z","title":"Physics Constrained Flow Neural Network for Short-Timescale Predictions\n  in Data Communications Networks","summary":"  Machine learning is gaining growing momentum in various recent models for the\ndynamic analysis of information flows in data communications networks. These\npreliminary models often rely on off-the-shelf learning models to predict from\nhistorical statistics while disregarding the physics governing the generating\nbehaviors of these flows. This paper instead introduces Flow Neural Network\n(FlowNN) to improve the feature representation with learned physical bias. This\nis implemented by an induction layer, working upon the embedding layer, to\nimpose the physics connected data correlations, and a self-supervised learning\nstrategy with stop-gradient to make the learned physics universal. For the\nshort-timescale network prediction tasks, FlowNN achieves 17% - 71% of loss\ndecrease than the state-of-the-art baselines on both synthetic and real-world\nnetworking datasets, which shows the strength of this new approach.\n","authors":["Xiangle Cheng","James He","Shihan Xiao","Yingxue Zhang","Zhitang Chen","Pascal Poupart","Fenglin Li"],"pdf_url":"https://arxiv.org/pdf/2112.12321v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12886v3","updated":"2023-04-02T09:31:02Z","published":"2022-11-23T11:44:35Z","title":"OReX: Object Reconstruction from Planar Cross-sections Using Neural\n  Fields","summary":"  Reconstructing 3D shapes from planar cross-sections is a challenge inspired\nby downstream applications like medical imaging and geographic informatics. The\ninput is an in/out indicator function fully defined on a sparse collection of\nplanes in space, and the output is an interpolation of the indicator function\nto the entire volume. Previous works addressing this sparse and ill-posed\nproblem either produce low quality results, or rely on additional priors such\nas target topology, appearance information, or input normal directions. In this\npaper, we present OReX, a method for 3D shape reconstruction from slices alone,\nfeaturing a Neural Field as the interpolation prior. A modest neural network is\ntrained on the input planes to return an inside/outside estimate for a given 3D\ncoordinate, yielding a powerful prior that induces smoothness and\nself-similarities. The main challenge for this approach is high-frequency\ndetails, as the neural prior is overly smoothing. To alleviate this, we offer\nan iterative estimation architecture and a hierarchical input sampling scheme\nthat encourage coarse-to-fine training, allowing the training process to focus\non high frequencies at later stages. In addition, we identify and analyze a\nripple-like effect stemming from the mesh extraction step. We mitigate it by\nregularizing the spatial gradients of the indicator function around input\nin/out boundaries during network training, tackling the problem at the root.\nThrough extensive qualitative and quantitative experimentation, we demonstrate\nour method is robust, accurate, and scales well with the size of the input. We\nreport state-of-the-art results compared to previous approaches and recent\npotential solutions, and demonstrate the benefit of our individual\ncontributions through analysis and ablation studies.\n","authors":["Haim Sawdayee","Amir Vaxman","Amit H. Bermano"],"pdf_url":"https://arxiv.org/pdf/2211.12886v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.13336v2","updated":"2023-04-02T09:27:20Z","published":"2023-03-23T15:17:15Z","title":"A Survey on Audio Diffusion Models: Text To Speech Synthesis and\n  Enhancement in Generative AI","summary":"  Generative AI has demonstrated impressive performance in various fields,\namong which speech synthesis is an interesting direction. With the diffusion\nmodel as the most popular generative model, numerous works have attempted two\nactive tasks: text to speech and speech enhancement. This work conducts a\nsurvey on audio diffusion model, which is complementary to existing surveys\nthat either lack the recent progress of diffusion-based speech synthesis or\nhighlight an overall picture of applying diffusion model in multiple fields.\nSpecifically, this work first briefly introduces the background of audio and\ndiffusion model. As for the text-to-speech task, we divide the methods into\nthree categories based on the stage where diffusion model is adopted: acoustic\nmodel, vocoder and end-to-end framework. Moreover, we categorize various speech\nenhancement tasks by either certain signals are removed or added into the input\nspeech. Comparisons of experimental results and discussions are also covered in\nthis survey.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Sheng Zheng","Mengchun Zhang","Maryam Qamar","Sung-Ho Bae","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.13336v2.pdf","comment":"18 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2304.00500v1","updated":"2023-04-02T10:25:09Z","published":"2023-04-02T10:25:09Z","title":"Parents and Children: Distinguishing Multimodal DeepFakes from Natural\n  Images","summary":"  Recent advancements in diffusion models have enabled the generation of\nrealistic deepfakes by writing textual prompts in natural language. While these\nmodels have numerous benefits across various sectors, they have also raised\nconcerns about the potential misuse of fake images and cast new pressures on\nfake image detection. In this work, we pioneer a systematic study of the\nauthenticity of fake images generated by state-of-the-art diffusion models.\nFirstly, we conduct a comprehensive study on the performance of contrastive and\nclassification-based visual features. Our analysis demonstrates that fake\nimages share common low-level cues, which render them easily recognizable.\nFurther, we devise a multimodal setting wherein fake images are synthesized by\ndifferent textual captions, which are used as seeds for a generator. Under this\nsetting, we quantify the performance of fake detection strategies and introduce\na contrastive-based disentangling strategy which let us analyze the role of the\nsemantics of textual descriptions and low-level perceptual cues. Finally, we\nrelease a new dataset, called COCOFake, containing about 600k images generated\nfrom original COCO images.\n","authors":["Roberto Amoroso","Davide Morelli","Marcella Cornia","Lorenzo Baraldi","Alberto Del Bimbo","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2304.00500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13336v2","updated":"2023-04-02T09:27:20Z","published":"2023-03-23T15:17:15Z","title":"A Survey on Audio Diffusion Models: Text To Speech Synthesis and\n  Enhancement in Generative AI","summary":"  Generative AI has demonstrated impressive performance in various fields,\namong which speech synthesis is an interesting direction. With the diffusion\nmodel as the most popular generative model, numerous works have attempted two\nactive tasks: text to speech and speech enhancement. This work conducts a\nsurvey on audio diffusion model, which is complementary to existing surveys\nthat either lack the recent progress of diffusion-based speech synthesis or\nhighlight an overall picture of applying diffusion model in multiple fields.\nSpecifically, this work first briefly introduces the background of audio and\ndiffusion model. As for the text-to-speech task, we divide the methods into\nthree categories based on the stage where diffusion model is adopted: acoustic\nmodel, vocoder and end-to-end framework. Moreover, we categorize various speech\nenhancement tasks by either certain signals are removed or added into the input\nspeech. Comparisons of experimental results and discussions are also covered in\nthis survey.\n","authors":["Chenshuang Zhang","Chaoning Zhang","Sheng Zheng","Mengchun Zhang","Maryam Qamar","Sung-Ho Bae","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2303.13336v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2303.11599v2","updated":"2023-04-02T05:54:24Z","published":"2023-03-21T05:34:04Z","title":"Low-complexity Deep Video Compression with A Distributed Coding\n  Architecture","summary":"  Prevalent predictive coding-based video compression methods rely on a heavy\nencoder to reduce temporal redundancy, which makes it challenging to deploy\nthem on resource-constrained devices. Since the 1970s, distributed source\ncoding theory has indicated that independent encoding and joint decoding with\nside information (SI) can achieve high-efficient compression of correlated\nsources. This has inspired a distributed coding architecture aiming at reducing\nthe encoding complexity. However, traditional distributed coding methods suffer\nfrom a substantial performance gap to predictive coding ones. Inspired by the\ngreat success of learning-based compression, we propose the first end-to-end\ndistributed deep video compression framework to improve the rate-distortion\nperformance. A key ingredient is an effective SI generation module at the\ndecoder, which helps to effectively exploit inter-frame correlations without\ncomputation-intensive encoder-side motion estimation and compensation.\nExperiments show that our method significantly outperforms conventional\ndistributed video coding and H.264. Meanwhile, it enjoys 6-7x encoding speedup\nagainst DVC [1] with comparable compression performance. Code is released at\nhttps://github.com/Xinjie-Q/Distributed-DVC.\n","authors":["Xinjie Zhang","Jiawei Shao","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.11599v2.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2304.00451v1","updated":"2023-04-02T05:06:51Z","published":"2023-04-02T05:06:51Z","title":"Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild","summary":"  Automatic Perceptual Image Quality Assessment is a challenging problem that\nimpacts billions of internet, and social media users daily. To advance research\nin this field, we propose a Mixture of Experts approach to train two separate\nencoders to learn high-level content and low-level image quality features in an\nunsupervised setting. The unique novelty of our approach is its ability to\ngenerate low-level representations of image quality that are complementary to\nhigh-level features representing image content. We refer to the framework used\nto train the two encoders as Re-IQA. For Image Quality Assessment in the Wild,\nwe deploy the complementary low and high-level image representations obtained\nfrom the Re-IQA framework to train a linear regression model, which is used to\nmap the image representations to the ground truth quality scores, refer Figure\n1. Our method achieves state-of-the-art performance on multiple large-scale\nimage quality assessment databases containing both real and synthetic\ndistortions, demonstrating how deep neural networks can be trained in an\nunsupervised setting to produce perceptually relevant representations. We\nconclude from our experiments that the low and high-level features obtained are\nindeed complementary and positively impact the performance of the linear\nregressor. A public release of all the codes associated with this work will be\nmade available on GitHub.\n","authors":["Avinab Saha","Sandeep Mishra","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2304.00451v1.pdf","comment":"Accepted to IEEE/CVF CVPR 2023. Code will be released post conference\n  in July 2023. Avinab Saha & Sandeep Mishra contributed equally to this work"}]},"2023-04-01T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2303.15100v2","updated":"2023-04-01T19:04:58Z","published":"2023-03-27T11:08:35Z","title":"An Information Extraction Study: Take In Mind the Tokenization!","summary":"  Current research on the advantages and trade-offs of using characters,\ninstead of tokenized text, as input for deep learning models, has evolved\nsubstantially. New token-free models remove the traditional tokenization step;\nhowever, their efficiency remains unclear. Moreover, the effect of tokenization\nis relatively unexplored in sequence tagging tasks. To this end, we investigate\nthe impact of tokenization when extracting information from documents and\npresent a comparative study and analysis of subword-based and character-based\nmodels. Specifically, we study Information Extraction (IE) from biomedical\ntexts. The main outcome is twofold: tokenization patterns can introduce\ninductive bias that results in state-of-the-art performance, and the\ncharacter-based models produce promising results; thus, transitioning to\ntoken-free IE models is feasible.\n","authors":["Christos Theodoropoulos","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2303.15100v2.pdf","comment":"Submitted Manuscript/Preprint (accepted at EUSFLAT 2023, to be\n  published in Lecture Notes in Computer Science (LNCS))"},{"id":"http://arxiv.org/abs/2304.00363v1","updated":"2023-04-01T18:05:14Z","published":"2023-04-01T18:05:14Z","title":"Automatic Authorship Attribution in the Work of Tirso de Molina","summary":"  Automatic Authorship Attribution (AAA) is the result of applying tools and\ntechniques from Digital Humanities to authorship attribution studies. Through a\nquantitative and statistical approach this discipline can draw further\nconclusions about renowned authorship issues which traditional critics have\nbeen dealing with for centuries, opening a new door to style comparison. The\naim of this paper is to prove the potential of these tools and techniques by\ntesting the authorship of five comedies traditionally attributed to Spanish\nplaywright Tirso de Molina (1579-1648): La ninfa del cielo, El burlador de\nSevilla, Tan largo me lo fiais, La mujer por fuerza and El condenado por\ndesconfiado. To accomplish this purpose some experiments concerning clustering\nanalysis by Stylo package from R and four distance measures are carried out on\na corpus built with plays by Tirso, Andres de Claramonte (c. 1560-1626),\nAntonio Mira de Amescua (1577-1644) and Luis Velez de Guevara (1579-1644). The\nresults obtained point to the denial of all the attributions to Tirso except\nfor the case of La mujer por fuerza.\n","authors":["Miguel Cavadas","Pablo Gamallo"],"pdf_url":"https://arxiv.org/pdf/2304.00363v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.14070v3","updated":"2023-04-01T18:00:33Z","published":"2023-03-24T15:29:16Z","title":"ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical\n  Domain Knowledge","summary":"  Recent large language models (LLMs) in the general domain, such as ChatGPT,\nhave shown remarkable success in following instructions and producing\nhuman-like responses. However, such language models have not been tailored to\nthe medical domain, resulting in poor answer accuracy and inability to give\nplausible recommendations for medical diagnosis, medications, etc. To address\nthis issue, we collected more than 700 diseases and their corresponding\nsymptoms, required medical tests, and recommended medications, from which we\ngenerated 5K doctor-patient conversations. In addition, we obtained 200K real\npatient-doctor conversations from online Q\\&A medical consultation sites. By\nfine-tuning LLMs using these 205k doctor-patient conversations, the resulting\nmodels emerge with great potential to understand patients' needs, provide\ninformed advice, and offer valuable assistance in a variety of medical-related\nfields. The integration of these advanced language models into healthcare can\nrevolutionize the way healthcare professionals and patients communicate,\nultimately improving the overall efficiency and quality of patient care and\noutcomes. In addition, we made public all the source codes, datasets, and model\nweights to facilitate the further development of dialogue models in the medical\nfield. The training data, codes, and weights of this project are available at:\nThe training data, codes, and weights of this project are available at:\nhttps://github.com/Kent0n-Li/ChatDoctor.\n","authors":["Yunxiang Li","Zihan Li","Kai Zhang","Ruilong Dan","You Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14070v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13613v2","updated":"2023-04-01T17:13:25Z","published":"2022-11-24T13:59:32Z","title":"Ham2Pose: Animating Sign Language Notation into Pose Sequences","summary":"  Translating spoken languages into Sign languages is necessary for open\ncommunication between the hearing and hearing-impaired communities. To achieve\nthis goal, we propose the first method for animating a text written in\nHamNoSys, a lexical Sign language notation, into signed pose sequences. As\nHamNoSys is universal by design, our proposed method offers a generic solution\ninvariant to the target Sign language. Our method gradually generates pose\npredictions using transformer encoders that create meaningful representations\nof the text and poses while considering their spatial and temporal information.\nWe use weak supervision for the training process and show that our method\nsucceeds in learning from partial and inaccurate data. Additionally, we offer a\nnew distance measurement that considers missing keypoints, to measure the\ndistance between pose sequences using DTW-MJE. We validate its correctness\nusing AUTSL, a large-scale Sign language dataset, show that it measures the\ndistance between pose sequences more accurately than existing measurements, and\nuse it to assess the quality of our generated pose sequences. Code for the data\npre-processing, the model, and the distance measurement is publicly released\nfor future research.\n","authors":["Rotem Shalev-Arkushin","Amit Moryossef","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2211.13613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00350v1","updated":"2023-04-01T16:10:36Z","published":"2023-04-01T16:10:36Z","title":"When Crowd Meets Persona: Creating a Large-Scale Open-Domain Persona\n  Dialogue Corpus","summary":"  Building a natural language dataset requires caution since word semantics is\nvulnerable to subtle text change or the definition of the annotated concept.\nSuch a tendency can be seen in generative tasks like question-answering and\ndialogue generation and also in tasks that create a categorization-based\ncorpus, like topic classification or sentiment analysis. Open-domain\nconversations involve two or more crowdworkers freely conversing about any\ntopic, and collecting such data is particularly difficult for two reasons: 1)\nthe dataset should be ``crafted\" rather than ``obtained\" due to privacy\nconcerns, and 2) paid creation of such dialogues may differ from how\ncrowdworkers behave in real-world settings. In this study, we tackle these\nissues when creating a large-scale open-domain persona dialogue corpus, where\npersona implies that the conversation is performed by several actors with a\nfixed persona and user-side workers from an unspecified crowd.\n","authors":["Won Ik Cho","Yoon Kyung Lee","Seoyeon Bae","Jihwan Kim","Sangah Park","Moosung Kim","Sowon Hahn","Nam Soo Kim"],"pdf_url":"https://arxiv.org/pdf/2304.00350v1.pdf","comment":"Presented at HCOMP 2022 as Works-in-Progress"},{"id":"http://arxiv.org/abs/2303.09184v2","updated":"2023-04-01T12:50:29Z","published":"2023-03-16T09:53:57Z","title":"Block-wise Bit-Compression of Transformer-based Models","summary":"  With the popularity of the recent Transformer-based models represented by\nBERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range\nof natural language processing tasks. However, the massive computations, huge\nmemory footprint, and thus high latency of Transformer-based models is an\ninevitable challenge for the cloud with high real-time requirement. To tackle\nthe issue, we propose BBCT, a method of block-wise bit-compression for\ntransformer without retraining. Our method achieves more fine-grained\ncompression of the whole transformer, including embedding, matrix\nmultiplication, GELU, softmax, layer normalization, and all the intermediate\nresults. As a case, we compress an efficient BERT with the method of BBCT. Our\nbenchmark test results on General Language Understanding Evaluation (GLUE) show\nthat BBCT can achieve less than 1% accuracy drop in most tasks.\n","authors":["Gaochen Dong","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09184v2.pdf","comment":"Need to add figures and adjust languages to improve readability"},{"id":"http://arxiv.org/abs/2205.12191v2","updated":"2023-04-01T07:07:44Z","published":"2022-05-24T16:44:45Z","title":"Reassessing Evaluation Practices in Visual Question Answering: A Case\n  Study on Out-of-Distribution Generalization","summary":"  Vision-and-language (V&L) models pretrained on large-scale multimodal data\nhave demonstrated strong performance on various tasks such as image captioning\nand visual question answering (VQA). The quality of such models is commonly\nassessed by measuring their performance on unseen data that typically comes\nfrom the same distribution as the training data. However, when evaluated under\nout-of-distribution (out-of-dataset) settings for VQA, we observe that these\nmodels exhibit poor generalization. We comprehensively evaluate two pretrained\nV&L models under different settings (i.e. classification and open-ended text\ngeneration) by conducting cross-dataset evaluations. We find that these models\ntend to learn to solve the benchmark, rather than learning the high-level\nskills required by the VQA task. We also find that in most cases generative\nmodels are less susceptible to shifts in data distribution compared to\ndiscriminative ones, and that multimodal pretraining is generally helpful for\nOOD generalization. Finally, we revisit assumptions underlying the use of\nautomatic VQA evaluation metrics, and empirically show that their stringent\nnature repeatedly penalizes models for correct responses.\n","authors":["Aishwarya Agrawal","Ivana Kajić","Emanuele Bugliarello","Elnaz Davoodi","Anita Gergely","Phil Blunsom","Aida Nematzadeh"],"pdf_url":"https://arxiv.org/pdf/2205.12191v2.pdf","comment":"Findings of EACL 2023. Aishwarya, Ivana, Emanuele and Aida had equal\n  first author contributions. Elnaz and Anita had equal contributions. Aida and\n  Aishwarya had equal senior contributions"},{"id":"http://arxiv.org/abs/2304.00235v1","updated":"2023-04-01T05:43:22Z","published":"2023-04-01T05:43:22Z","title":"What Does the Indian Parliament Discuss? An Exploratory Analysis of the\n  Question Hour in the Lok Sabha","summary":"  The TCPD-IPD dataset is a collection of questions and answers discussed in\nthe Lower House of the Parliament of India during the Question Hour between\n1999 and 2019. Although it is difficult to analyze such a huge collection\nmanually, modern text analysis tools can provide a powerful means to navigate\nit. In this paper, we perform an exploratory analysis of the dataset. In\nparticular, we present insightful corpus-level statistics and a detailed\nanalysis of three subsets of the dataset. In the latter analysis, the focus is\non understanding the temporal evolution of topics using a dynamic topic model.\nWe observe that the parliamentary conversation indeed mirrors the political and\nsocio-economic tensions of each period.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2304.00235v1.pdf","comment":"Accepted at the workshop PoliticalNLP co-located with the conference\n  LREC 2022"},{"id":"http://arxiv.org/abs/2304.00228v1","updated":"2023-04-01T05:04:06Z","published":"2023-04-01T05:04:06Z","title":"Large language models can rate news outlet credibility","summary":"  Although large language models (LLMs) have shown exceptional performance in\nvarious natural language processing tasks, they are prone to hallucinations.\nState-of-the-art chatbots, such as the new Bing, attempt to mitigate this issue\nby gathering information directly from the internet to ground their answers. In\nthis setting, the capacity to distinguish trustworthy sources is critical for\nproviding appropriate accuracy contexts to users. Here we assess whether\nChatGPT, a prominent LLM, can evaluate the credibility of news outlets. With\nappropriate instructions, ChatGPT can provide ratings for a diverse set of news\noutlets, including those in non-English languages and satirical sources, along\nwith contextual explanations. Our results show that these ratings correlate\nwith those from human experts (Spearmam's $\\rho=0.54, p<0.001$). These findings\nsuggest that LLMs could be an affordable reference for credibility ratings in\nfact-checking applications. Future LLMs should enhance their alignment with\nhuman expert judgments of source credibility to improve information accuracy.\n","authors":["Kai-Cheng Yang","Filippo Menczer"],"pdf_url":"https://arxiv.org/pdf/2304.00228v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2304.00215v1","updated":"2023-04-01T03:49:47Z","published":"2023-04-01T03:49:47Z","title":"Inductive Relation Prediction from Relational Paths and Context with\n  Hierarchical Transformers","summary":"  Relation prediction on knowledge graphs (KGs) is a key research topic.\nDominant embedding-based methods mainly focus on the transductive setting and\nlack the inductive ability to generalize to new entities for inference.\nExisting methods for inductive reasoning mostly mine the connections between\nentities, i.e., relational paths, without considering the nature of head and\ntail entities contained in the relational context. This paper proposes a novel\nmethod that captures both connections between entities and the intrinsic nature\nof entities, by simultaneously aggregating RElational Paths and cOntext with a\nunified hieRarchical Transformer framework, namely REPORT. REPORT relies solely\non relation semantics and can naturally generalize to the fully-inductive\nsetting, where KGs for training and inference have no common entities. In the\nexperiments, REPORT performs consistently better than all baselines on almost\nall the eight version subsets of two fully-inductive datasets. Moreover. REPORT\nis interpretable by providing each element's contribution to the prediction\nresults.\n","authors":["Jiaang Li","Quan Wang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2304.00215v1.pdf","comment":"Accepted by ICASSP 2023"},{"id":"http://arxiv.org/abs/2212.05935v2","updated":"2023-04-01T10:00:35Z","published":"2022-12-07T10:09:49Z","title":"Hierarchical multimodal transformers for Multi-Page DocVQA","summary":"  Document Visual Question Answering (DocVQA) refers to the task of answering\nquestions from document images. Existing work on DocVQA only considers\nsingle-page documents. However, in real scenarios documents are mostly composed\nof multiple pages that should be processed altogether. In this work we extend\nDocVQA to the multi-page scenario. For that, we first create a new dataset,\nMP-DocVQA, where questions are posed over multi-page documents instead of\nsingle pages. Second, we propose a new hierarchical method, Hi-VT5, based on\nthe T5 architecture, that overcomes the limitations of current methods to\nprocess long multi-page documents. The proposed method is based on a\nhierarchical transformer architecture where the encoder summarizes the most\nrelevant information of every page and then, the decoder takes this summarized\ninformation to generate the final answer. Through extensive experimentation, we\ndemonstrate that our method is able, in a single stage, to answer the questions\nand provide the page that contains the relevant information to find the answer,\nwhich can be used as a kind of explainability measure.\n","authors":["Rubèn Tito","Dimosthenis Karatzas","Ernest Valveny"],"pdf_url":"https://arxiv.org/pdf/2212.05935v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01938v1","updated":"2023-04-01T06:04:58Z","published":"2023-04-01T06:04:58Z","title":"Evaluating Large Language Models on a Highly-specialized Topic,\n  Radiation Oncology Physics","summary":"  We present the first study to investigate Large Language Models (LLMs) in\nanswering radiation oncology physics questions. Because popular exams like AP\nPhysics, LSAT, and GRE have large test-taker populations and ample test\npreparation resources in circulation, they may not allow for accurately\nassessing the true potential of LLMs. This paper proposes evaluating LLMs on a\nhighly-specialized topic, radiation oncology physics, which may be more\npertinent to scientific and medical communities in addition to being a valuable\nbenchmark of LLMs. We developed an exam consisting of 100 radiation oncology\nphysics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT\n(GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against\nmedical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs\nas well as medical physicists, on average. The performance of ChatGPT (GPT-4)\nwas further improved when prompted to explain first, then answer. ChatGPT\n(GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices\nacross a number of trials, whether correct or incorrect, a characteristic that\nwas not observed in the human test groups. In evaluating ChatGPTs (GPT-4)\ndeductive reasoning ability using a novel approach (substituting the correct\nanswer with \"None of the above choices is the correct answer.\"), ChatGPT\n(GPT-4) demonstrated surprising accuracy, suggesting the potential presence of\nan emergent ability. Finally, although ChatGPT (GPT-4) performed well overall,\nits intrinsic properties did not allow for further improvement when scoring\nbased on a majority vote across trials. In contrast, a team of medical\nphysicists were able to greatly outperform ChatGPT (GPT-4) using a majority\nvote. This study suggests a great potential for LLMs to work alongside\nradiation oncology experts as highly knowledgeable assistants.\n","authors":["Jason Holmes","Zhengliang Liu","Lian Zhang","Yuzhen Ding","Terence T. Sio","Lisa A. McGee","Jonathan B. Ashman","Xiang Li","Tianming Liu","Jiajian Shen","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2304.01938v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2304.00353v1","updated":"2023-04-01T16:15:45Z","published":"2023-04-01T16:15:45Z","title":"Reviewer Assignment Problem: A Systematic Review of the Literature","summary":"  Appropriate reviewer assignment significantly impacts the quality of proposal\nevaluation, as accurate and fair reviews are contingent on their assignment to\nrelevant reviewers. The crucial task of assigning reviewers to submitted\nproposals is the starting point of the review process and is also known as the\nreviewer assignment problem (RAP). Due to the obvious restrictions of manual\nassignment, journal editors, conference organizers, and grant managers demand\nautomatic reviewer assignment approaches. Many studies have proposed assignment\nsolutions in response to the demand for automated procedures since 1992. The\nprimary objective of this survey paper is to provide scholars and practitioners\nwith a comprehensive overview of available research on the RAP. To achieve this\ngoal, this article presents an in-depth systematic review of 103 publications\nin the field of reviewer assignment published in the past three decades and\navailable in the Web of Science, Scopus, ScienceDirect, Google Scholar, and\nSemantic Scholar databases. This review paper classified and discussed the RAP\napproaches into two broad categories and numerous subcategories based on their\nunderlying techniques. Furthermore, potential future research directions for\neach category are presented. This survey shows that the research on the RAP is\nbecoming more significant and that more effort is required to develop new\napproaches and a framework.\n","authors":["Meltem Aksoy","Seda Yanik","Mehmet Fatih Amasyali"],"pdf_url":"https://arxiv.org/pdf/2304.00353v1.pdf","comment":"67 pages, 11 figures"},{"id":"http://arxiv.org/abs/2304.00310v1","updated":"2023-04-01T13:18:18Z","published":"2023-04-01T13:18:18Z","title":"On the Feasibility and Robustness of Pointwise Evaluation of Query\n  Performance Prediction","summary":"  Despite the retrieval effectiveness of queries being mutually independent of\none another, the evaluation of query performance prediction (QPP) systems has\nbeen carried out by measuring rank correlation over an entire set of queries.\nSuch a listwise approach has a number of disadvantages, notably that it does\nnot support the common requirement of assessing QPP for individual queries. In\nthis paper, we propose a pointwise QPP framework that allows us to evaluate the\nquality of a QPP system for individual queries by measuring the deviations\nbetween each prediction versus the corresponding true value, and then\naggregating the results over a set of queries. Our experiments demonstrate that\nthis new approach leads to smaller variances in QPP evaluations across a range\nof different target metrics and retrieval models.\n","authors":["Suchana Datta","Debasis Ganguly","Derek Greene","Mandar Mitra"],"pdf_url":"https://arxiv.org/pdf/2304.00310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00241v1","updated":"2023-04-01T06:39:33Z","published":"2023-04-01T06:39:33Z","title":"Bipartite Graph Convolutional Hashing for Effective and Efficient Top-N\n  Search in Hamming Space","summary":"  Searching on bipartite graphs is basal and versatile to many real-world Web\napplications, e.g., online recommendation, database retrieval, and\nquery-document searching. Given a query node, the conventional approaches rely\non the similarity matching with the vectorized node embeddings in the\ncontinuous Euclidean space. To efficiently manage intensive similarity\ncomputation, developing hashing techniques for graph structured data has\nrecently become an emerging research direction. Despite the retrieval\nefficiency in Hamming space, prior work is however confronted with catastrophic\nperformance decay. In this work, we investigate the problem of hashing with\nGraph Convolutional Network on bipartite graphs for effective Top-N search. We\npropose an end-to-end Bipartite Graph Convolutional Hashing approach, namely\nBGCH, which consists of three novel and effective modules: (1) adaptive graph\nconvolutional hashing, (2) latent feature dispersion, and (3) Fourier\nserialized gradient estimation. Specifically, the former two modules achieve\nthe substantial retention of the structural information against the inevitable\ninformation loss in hash encoding; the last module develops Fourier Series\ndecomposition to the hashing function in the frequency domain mainly for more\naccurate gradient estimation. The extensive experiments on six real-world\ndatasets not only show the performance superiority over the competing\nhashing-based counterparts, but also demonstrate the effectiveness of all\nproposed model components contained therein.\n","authors":["Yankai Chen","Yixiang Fang","Yifei Zhang","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2304.00241v1.pdf","comment":"Accepted by WWW 2023"}]},"2023-04-04T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2304.01982v1","updated":"2023-04-04T17:37:06Z","published":"2023-04-04T17:37:06Z","title":"Rethinking the Role of Token Retrieval in Multi-Vector Retrieval","summary":"  Multi-vector retrieval models such as ColBERT [Khattab and Zaharia, 2020]\nallow token-level interactions between queries and documents, and hence achieve\nstate of the art on many information retrieval benchmarks. However, their\nnon-linear scoring function cannot be scaled to millions of documents,\nnecessitating a three-stage process for inference: retrieving initial\ncandidates via token retrieval, accessing all token vectors, and scoring the\ninitial candidate documents. The non-linear scoring function is applied over\nall token vectors of each candidate document, making the inference process\ncomplicated and slow. In this paper, we aim to simplify the multi-vector\nretrieval by rethinking the role of token retrieval. We present XTR,\nConteXtualized Token Retriever, which introduces a simple, yet novel, objective\nfunction that encourages the model to retrieve the most important document\ntokens first. The improvement to token retrieval allows XTR to rank candidates\nonly using the retrieved tokens rather than all tokens in the document, and\nenables a newly designed scoring stage that is two-to-three orders of magnitude\ncheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances the\nstate-of-the-art by 2.8 nDCG@10 without any distillation. Detailed analysis\nconfirms our decision to revisit the token retrieval stage, as XTR demonstrates\nmuch better recall of the token retrieval stage compared to ColBERT.\n","authors":["Jinhyuk Lee","Zhuyun Dai","Sai Meher Karthik Duddu","Tao Lei","Iftekhar Naim","Ming-Wei Chang","Vincent Y. Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.01982v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2304.01974v1","updated":"2023-04-04T17:31:32Z","published":"2023-04-04T17:31:32Z","title":"Dialogue-Contextualized Re-ranking for Medical History-Taking","summary":"  AI-driven medical history-taking is an important component in symptom\nchecking, automated patient intake, triage, and other AI virtual care\napplications. As history-taking is extremely varied, machine learning models\nrequire a significant amount of data to train. To overcome this challenge,\nexisting systems are developed using indirect data or expert knowledge. This\nleads to a training-inference gap as models are trained on different kinds of\ndata than what they observe at inference time. In this work, we present a\ntwo-stage re-ranking approach that helps close the training-inference gap by\nre-ranking the first-stage question candidates using a dialogue-contextualized\nmodel. For this, we propose a new model, global re-ranker, which cross-encodes\nthe dialogue with all questions simultaneously, and compare it with several\nexisting neural baselines. We test both transformer and S4-based language model\nbackbones. We find that relative to the expert system, the best performance is\nachieved by our proposed global re-ranker with a transformer backbone,\nresulting in a 30% higher normalized discount cumulative gain (nDCG) and a 77%\nhigher mean average precision (mAP).\n","authors":["Jian Zhu","Ilya Valmianski","Anitha Kannan"],"pdf_url":"https://arxiv.org/pdf/2304.01974v1.pdf","comment":"Code and pre-trained S4 checkpoints will be available after\n  publication"},{"id":"http://arxiv.org/abs/2304.01969v1","updated":"2023-04-04T17:26:11Z","published":"2023-04-04T17:26:11Z","title":"MEGClass: Text Classification with Extremely Weak Supervision via\n  Mutually-Enhancing Text Granularities","summary":"  Text classification typically requires a substantial amount of\nhuman-annotated data to serve as supervision, which is costly to obtain in\ndynamic emerging domains. Certain methods seek to address this problem by\nsolely relying on the surface text of class names to serve as extremely weak\nsupervision. However, existing methods fail to account for single-class\ndocuments discussing multiple topics. Both topic diversity and vague sentences\nmay introduce noise into the document's underlying representation and\nconsequently the precision of the predicted class. Furthermore, current work\nfocuses on text granularities (documents, sentences, or words) independently,\nwhich limits the degree of coarse- or fine-grained context that we can jointly\nextract from all three to identify significant subtext for classification. In\norder to address this problem, we propose MEGClass, an extremely\nweakly-supervised text classification method to exploit Mutually-Enhancing Text\nGranularities. Specifically, MEGClass constructs class-oriented sentence and\nclass representations based on keywords for performing a sentence-level\nconfidence-weighted label ensemble in order to estimate a document's initial\nclass distribution. This serves as the target distribution for a multi-head\nattention network with a class-weighted contrastive loss. This network learns\ncontextualized sentence representations and weights to form document\nrepresentations that reflect its original document and sentence-level topic\ndiversity. Retaining this heterogeneity allows MEGClass to select the most\nclass-indicative documents to serve as iterative feedback for enhancing the\nclass representations. Finally, these top documents are used to fine-tune a\npre-trained text classifier. As demonstrated through extensive experiments on\nsix benchmark datasets, MEGClass outperforms other weakly and extremely weakly\nsupervised methods.\n","authors":["Priyanka Kargupta","Tanay Komarlu","Susik Yoon","Xuan Wang","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2304.01969v1.pdf","comment":"Code: https://github.com/pkargupta/MEGClass/"},{"id":"http://arxiv.org/abs/2205.12649v2","updated":"2023-04-04T17:16:59Z","published":"2022-05-25T10:44:36Z","title":"Investigating Lexical Replacements for Arabic-English Code-Switched Data\n  Augmentation","summary":"  Data sparsity is a main problem hindering the development of code-switching\n(CS) NLP systems. In this paper, we investigate data augmentation techniques\nfor synthesizing dialectal Arabic-English CS text. We perform lexical\nreplacements using word-aligned parallel corpora where CS points are either\nrandomly chosen or learnt using a sequence-to-sequence model. We compare these\napproaches against dictionary-based replacements. We assess the quality of the\ngenerated sentences through human evaluation and evaluate the effectiveness of\ndata augmentation on machine translation (MT), automatic speech recognition\n(ASR), and speech translation (ST) tasks. Results show that using a predictive\nmodel results in more natural CS sentences compared to the random approach, as\nreported in human judgements. In the downstream tasks, despite the random\napproach generating more data, both approaches perform equally (outperforming\ndictionary-based replacements). Overall, data augmentation achieves 34%\nimprovement in perplexity, 5.2% relative improvement on WER for ASR task,\n+4.0-5.1 BLEU points on MT task, and +2.1-2.2 BLEU points on ST over a baseline\ntrained on available data without augmentation.\n","authors":["Injy Hamed","Nizar Habash","Slim Abdennadher","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2205.12649v2.pdf","comment":"Accepted to LoResMT 2023"},{"id":"http://arxiv.org/abs/2304.01961v1","updated":"2023-04-04T17:11:34Z","published":"2023-04-04T17:11:34Z","title":"AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia\n  Content Creation","summary":"  This paper presents the AToMiC (Authoring Tools for Multimedia Content)\ndataset, designed to advance research in image/text cross-modal retrieval.\nWhile vision-language pretrained transformers have led to significant\nimprovements in retrieval effectiveness, existing research has relied on\nimage-caption datasets that feature only simplistic image-text relationships\nand underspecified user models of retrieval tasks. To address the gap between\nthese oversimplified settings and real-world applications for multimedia\ncontent creation, we introduce a new approach for building retrieval test\ncollections. We leverage hierarchical structures and diverse domains of texts,\nstyles, and types of images, as well as large-scale image-document associations\nembedded in Wikipedia. We formulate two tasks based on a realistic user model\nand validate our dataset through retrieval experiments using baseline models.\nAToMiC offers a testbed for scalable, diverse, and reproducible multimedia\nretrieval research. Finally, the dataset provides the basis for a dedicated\ntrack at the 2023 Text Retrieval Conference (TREC), and is publicly available\nat https://github.com/TREC-AToMiC/AToMiC.\n","authors":["Jheng-Hong Yang","Carlos Lassance","Rafael Sampaio de Rezende","Krishna Srinivasan","Miriam Redi","Stéphane Clinchant","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2304.01961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01933v1","updated":"2023-04-04T16:31:37Z","published":"2023-04-04T16:31:37Z","title":"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of\n  Large Language Models","summary":"  The success of large language models (LLMs), like GPT-3 and ChatGPT, has led\nto the development of numerous cost-effective and accessible alternatives that\nare created by fine-tuning open-access LLMs with task-specific data (e.g.,\nChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning\nmethods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly\none of the most attractive topics, as it only requires fine-tuning a few\nexternal parameters instead of the entire LLMs while achieving comparable or\neven better performance. To enable further research on PEFT methods of LLMs,\nthis paper presents LLM-Adapters, an easy-to-use framework that integrates\nvarious adapters into LLMs and can execute these adapter-based PEFT methods of\nLLMs for different tasks. The framework includes state-of-the-art open-access\nLLMs such as LLaMA, BLOOM, OPT, and GPT-J, as well as widely used adapters such\nas Series adapter, Parallel adapter, and LoRA. The framework is designed to be\nresearch-friendly, efficient, modular, and extendable, allowing the integration\nof new adapters and the evaluation of them with new and larger-scale LLMs.\nFurthermore, to evaluate the effectiveness of adapters in LLMs-Adapters, we\nconduct experiments on six math reasoning datasets. The results demonstrate\nthat using adapter-based PEFT in smaller-scale LLMs (7B) with few extra\ntrainable parameters yields comparable, and in some cases superior, performance\nto that of powerful LLMs (175B) in zero-shot inference on simple math reasoning\ndatasets. Overall, we provide a promising framework for fine-tuning large LLMs\non downstream tasks. We believe the proposed LLMs-Adapters will advance\nadapter-based PEFT research, facilitate the deployment of research pipelines,\nand enable practical applications to real-world systems.\n","authors":["Zhiqiang Hu","Yihuai Lan","Lei Wang","Wanyu Xu","Ee-Peng Lim","Roy Ka-Wei Lee","Lidong Bing","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2304.01933v1.pdf","comment":"Technical Report. The code of our framework can be found at\n  https://github.com/AGI-Edgerunners/LLM-Adapters. We will keep all of the code\n  open-source and continue to update the framework with new adapters, LLMs, and\n  tasks"},{"id":"http://arxiv.org/abs/2209.05135v2","updated":"2023-04-04T16:30:00Z","published":"2022-09-12T10:42:26Z","title":"Signs of Language: Embodied Sign Language Fingerspelling Acquisition\n  from Demonstrations for Human-Robot Interaction","summary":"  Learning fine-grained movements is a challenging topic in robotics,\nparticularly in the context of robotic hands. One specific instance of this\nchallenge is the acquisition of fingerspelling sign language in robots. In this\npaper, we propose an approach for learning dexterous motor imitation from video\nexamples without additional information. To achieve this, we first build a URDF\nmodel of a robotic hand with a single actuator for each joint. We then leverage\npre-trained deep vision models to extract the 3D pose of the hand from RGB\nvideos. Next, using state-of-the-art reinforcement learning algorithms for\nmotion imitation (namely, proximal policy optimization and soft actor-critic),\nwe train a policy to reproduce the movement extracted from the demonstrations.\nWe identify the optimal set of hyperparameters for imitation based on a\nreference motion. Finally, we demonstrate the generalizability of our approach\nby testing it on six different tasks, corresponding to fingerspelled letters.\nOur results show that our approach is able to successfully imitate these\nfine-grained movements without additional information, highlighting its\npotential for real-world applications in robotics.\n","authors":["Federico Tavella","Aphrodite Galata","Angelo Cangelosi"],"pdf_url":"https://arxiv.org/pdf/2209.05135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.05711v3","updated":"2023-04-04T16:27:38Z","published":"2022-03-11T01:45:33Z","title":"Synopses of Movie Narratives: a Video-Language Dataset for Story\n  Understanding","summary":"  Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SYMON), containing\n5,193 video summaries of popular movies and TV series. SYMON captures\nnaturalistic story-telling videos for human audience made by human creators. As\na prototypical and naturalistic story dataset, SYMON features high coverage of\nmultimodal story events, abundant mental-state descriptions, and large semantic\ngaps between the visual and the textual modalities. We establish benchmarks on\nvideo-text retrieval and zero-shot alignment on movie summary videos, which\nshowcase the importance of in-domain data in story understanding. With SYMON,\nwe hope to lay the groundwork for progress in multimodal story understanding.\n","authors":["Yidan Sun","Qin Chao","Yangfeng Ji","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2203.05711v3.pdf","comment":"25 pages, 17 figures"},{"id":"http://arxiv.org/abs/2304.01922v1","updated":"2023-04-04T16:16:25Z","published":"2023-04-04T16:16:25Z","title":"Resources and Few-shot Learners for In-context Learning in Slavic\n  Languages","summary":"  Despite the rapid recent progress in creating accurate and compact in-context\nlearners, most recent work focuses on in-context learning (ICL) for tasks in\nEnglish. However, the ability to interact with users of languages outside\nEnglish presents a great potential for broadening the applicability of language\ntechnologies to non-English speakers.\n  In this work, we collect the infrastructure necessary for training and\nevaluation of ICL in a selection of Slavic languages: Czech, Polish, and\nRussian. We link a diverse set of datasets and cast these into a unified\ninstructional format through a set of transformations and newly-crafted\ntemplates written purely in target languages. Using the newly-curated dataset,\nwe evaluate a set of the most recent in-context learners and compare their\nresults to the supervised baselines. Finally, we train, evaluate and publish a\nset of in-context learning models that we train on the collected resources and\ncompare their performance to previous work.\n  We find that ICL models tuned in English are also able to learn some tasks\nfrom non-English contexts, but multilingual instruction fine-tuning\nconsistently improves the ICL ability. We also find that the massive multitask\ntraining can be outperformed by single-task training in the target language,\nuncovering the potential for specializing in-context learners to the\nlanguage(s) of their application.\n","authors":["Michal Štefánik","Marek Kadlčík","Piotr Gramacki","Petr Sojka"],"pdf_url":"https://arxiv.org/pdf/2304.01922v1.pdf","comment":"EACL 2023 SlavicNLP Long Paper. New instructional templates and\n  models are available on\n  https://github.com/fewshot-goes-multilingual/slavic-incontext-learning"},{"id":"http://arxiv.org/abs/2304.01904v1","updated":"2023-04-04T15:57:28Z","published":"2023-04-04T15:57:28Z","title":"REFINER: Reasoning Feedback on Intermediate Representations","summary":"  Language models (LMs) have recently shown remarkable performance on reasoning\ntasks by explicitly generating intermediate inferences, e.g., chain-of-thought\nprompting. However, these intermediate inference steps may be inappropriate\ndeductions from the initial context and lead to incorrect final predictions.\nHere we introduce REFINER, a framework for finetuning LMs to explicitly\ngenerate intermediate reasoning steps while interacting with a critic model\nthat provides automated feedback on the reasoning. Specifically, the critic\nprovides structured feedback that the reasoning LM uses to iteratively improve\nits intermediate arguments. Empirical evaluations of REFINER on three diverse\nreasoning tasks show significant improvements over baseline LMs of comparable\nscale. Furthermore, when using GPT3.5 as the reasoner, the trained critic\nsignificantly improves reasoning without finetuning the reasoner. Finally, our\ncritic model is trained without expensive human-in-the-loop data but can be\nsubstituted with humans at inference time.\n","authors":["Debjit Paul","Mete Ismayilzada","Maxime Peyrard","Beatriz Borges","Antoine Bosselut","Robert West","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2304.01904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17612v2","updated":"2023-04-04T15:51:59Z","published":"2023-03-30T01:37:19Z","title":"oBERTa: Improving Sparse Transfer Learning via improved initialization,\n  distillation, and pruning regimes","summary":"  In this paper, we introduce the range of oBERTa language models, an\neasy-to-use set of language models which allows Natural Language Processing\n(NLP) practitioners to obtain between 3.8 and 24.3 times faster models without\nexpertise in model compression. Specifically, oBERTa extends existing work on\npruning, knowledge distillation, and quantization and leverages frozen\nembeddings improves distillation and model initialization to deliver higher\naccuracy on a broad range of transfer tasks. In generating oBERTa, we explore\nhow the highly optimized RoBERTa differs from the BERT for pruning during\npre-training and finetuning. We find it less amenable to compression during\nfine-tuning. We explore the use of oBERTa on seven representative NLP tasks and\nfind that the improved compression techniques allow a pruned oBERTa model to\nmatch the performance of BERTbase and exceed the performance of Prune OFA Large\non the SQUAD V1.1 Question Answering dataset, despite being 8x and 2x,\nrespectively faster in inference. We release our code, training regimes, and\nassociated model for broad usage to encourage usage and experimentation\n","authors":["Daniel Campos","Alexandre Marques","Mark Kurtz","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.17612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01894v1","updated":"2023-04-04T15:47:26Z","published":"2023-04-04T15:47:26Z","title":"San-BERT: Extractive Summarization for Sanskrit Documents using BERT and\n  it's variants","summary":"  In this work, we develop language models for the Sanskrit language, namely\nBidirectional Encoder Representations from Transformers (BERT) and its\nvariants: A Lite BERT (ALBERT), and Robustly Optimized BERT (RoBERTa) using\nDevanagari Sanskrit text corpus. Then we extracted the features for the given\ntext from these models. We applied the dimensional reduction and clustering\ntechniques on the features to generate an extractive summary for a given\nSanskrit document. Along with the extractive text summarization techniques, we\nhave also created and released a Sanskrit Devanagari text corpus publicly.\n","authors":["Kartik Bhatnagar","Sampath Lonka","Jammi Kunal","Mahabala Rao M G"],"pdf_url":"https://arxiv.org/pdf/2304.01894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01890v1","updated":"2023-04-04T15:42:08Z","published":"2023-04-04T15:42:08Z","title":"Sociocultural knowledge is needed for selection of shots in hate speech\n  detection tasks","summary":"  We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for\nthe countries of Brazil, Germany, India and Kenya, to aid training and\ninterpretability of models. We demonstrate how our lexicon can be used to\ninterpret model predictions, showing that models developed to classify extreme\nspeech rely heavily on target words when making predictions. Further, we\npropose a method to aid shot selection for training in low-resource settings\nvia HATELEXICON. In few-shot learning, the selection of shots is of paramount\nimportance to model performance. In our work, we simulate a few-shot setting\nfor German and Hindi, using HASOC data for training and the Multilingual\nHateCheck (MHC) as a benchmark. We show that selecting shots based on our\nlexicon leads to models performing better on MHC than models trained on shots\nsampled randomly. Thus, when given only a few training examples, using our\nlexicon to select shots containing more sociocultural information leads to\nbetter few-shot performance.\n","authors":["Antonis Maronikolakis","Abdullatif Köksal","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2304.01890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02427v2","updated":"2023-04-04T15:34:50Z","published":"2023-03-04T14:23:02Z","title":"Self-tuning hyper-parameters for unsupervised cross-lingual tokenization","summary":"  We explore the possibility of meta-learning for the language-independent\nunsupervised tokenization problem for English, Russian, and Chinese. We\nimplement the meta-learning approach for automatic determination of\nhyper-parameters of the unsupervised tokenization model proposed in earlier\nworks, relying on various human-independent fitness functions such as\nnormalised anti-entropy, compression factor and cross-split F1 score, as well\nas additive and multiplicative composite combinations of the three metrics,\ntesting them against the conventional F1 tokenization score. We find a fairly\ngood correlation between the latter and the additive combination of the former\nthree metrics for English and Russian. In case of Chinese, we find a\nsignificant correlation between the F 1 score and the compression factor. Our\nresults suggest the possibility of robust unsupervised tokenization of\nlow-resource and dead languages and allow us to think about human languages in\nterms of the evolution of efficient symbolic communication codes with different\nstructural optimisation schemes that have evolved in different human cultures.\n","authors":["Anton Kolonin"],"pdf_url":"https://arxiv.org/pdf/2303.02427v2.pdf","comment":"5 figures, 2 tables, submitted to KONT-2023 conference"},{"id":"http://arxiv.org/abs/2304.01852v1","updated":"2023-04-04T15:01:06Z","published":"2023-04-04T15:01:06Z","title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of\n  Large Language Models","summary":"  This paper presents a comprehensive survey of ChatGPT and GPT-4,\nstate-of-the-art large language models (LLM) from the GPT series, and their\nprospective applications across diverse domains. Indeed, key innovations such\nas large-scale pre-training that captures knowledge across the entire world\nwide web, instruction fine-tuning and Reinforcement Learning from Human\nFeedback (RLHF) have played significant roles in enhancing LLMs' adaptability\nand performance. We performed an in-depth analysis of 194 relevant papers on\narXiv, encompassing trend analysis, word cloud representation, and distribution\nanalysis across various application domains. The findings reveal a significant\nand increasing interest in ChatGPT/GPT-4 research, predominantly centered on\ndirect natural language processing applications, while also demonstrating\nconsiderable potential in areas ranging from education and history to\nmathematics, medicine, and physics. This study endeavors to furnish insights\ninto ChatGPT's capabilities, potential implications, ethical concerns, and\noffer direction for future advancements in this field.\n","authors":["Yiheng Liu","Tianle Han","Siyuan Ma","Jiayue Zhang","Yuanyuan Yang","Jiaming Tian","Hao He","Antong Li","Mengshen He","Zhengliang Liu","Zihao Wu","Dajiang Zhu","Xiang Li","Ning Qiang","Dingang Shen","Tianming Liu","Bao Ge"],"pdf_url":"https://arxiv.org/pdf/2304.01852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01774v1","updated":"2023-04-04T13:05:10Z","published":"2023-04-04T13:05:10Z","title":"A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System","summary":"  Human-in-the-loop topic modelling incorporates users' knowledge into the\nmodelling process, enabling them to refine the model iteratively. Recent\nresearch has demonstrated the value of user feedback, but there are still\nissues to consider, such as the difficulty in tracking changes, comparing\ndifferent models and the lack of evaluation based on real-world examples of\nuse. We developed a novel, interactive human-in-the-loop topic modeling system\nwith a user-friendly interface that enables users compare and record every step\nthey take, and a novel topic words suggestion feature to help users provide\nfeedback that is faithful to the ground truth. Our system also supports not\nonly what traditional topic models can do, i.e., learning the topics from the\nwhole corpus, but also targeted topic modelling, i.e., learning topics for\nspecific aspects of the corpus. In this article, we provide an overview of the\nsystem and present the results of a series of user studies designed to assess\nthe value of the system in progressively more realistic applications of topic\nmodelling.\n","authors":["Zheng Fang","Lama Alqazlan","Du Liu","Yulan He","Rob Procter"],"pdf_url":"https://arxiv.org/pdf/2304.01774v1.pdf","comment":"The paper is accepted by the 17th Conference of the European Chapter\n  of the Association for Computational Linguistics (EACL)"},{"id":"http://arxiv.org/abs/2304.01752v1","updated":"2023-04-04T12:42:29Z","published":"2023-04-04T12:42:29Z","title":"Black Box Few-Shot Adaptation for Vision-Language models","summary":"  Vision-Language (V-L) models trained with contrastive learning to align the\nvisual and language modalities have been shown to be strong few-shot learners.\nSoft prompt learning is the method of choice for few-shot downstream adaption\naiming to bridge the modality gap caused by the distribution shift induced by\nthe new domain. While parameter-efficient, prompt learning still requires\naccess to the model weights and can be computationally infeasible for large\nmodels with billions of parameters. To address these shortcomings, in this\nwork, we describe a black-box method for V-L few-shot adaptation that (a)\noperates on pre-computed image and text features and hence works without access\nto the model's weights, (b) it is orders of magnitude faster at training time,\n(c) it is amenable to both supervised and unsupervised training, and (d) it can\nbe even used to align image and text features computed from uni-modal models.\nTo achieve this, we propose Linear Feature Alignment (LFA), a simple linear\napproach for V-L re-alignment in the target domain. LFA is initialized from a\nclosed-form solution to a least-squares problem and then it is iteratively\nupdated by minimizing a re-ranking loss. Despite its simplicity, our approach\ncan even surpass soft-prompt learning methods as shown by extensive experiments\non 11 image and 2 video datasets.\n","authors":["Yassine Ouali","Adrian Bulat","Brais Martinez","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2304.01752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01746v1","updated":"2023-04-04T12:33:40Z","published":"2023-04-04T12:33:40Z","title":"Is ChatGPT a Highly Fluent Grammatical Error Correction System? A\n  Comprehensive Evaluation","summary":"  ChatGPT, a large-scale language model based on the advanced GPT-3.5\narchitecture, has shown remarkable potential in various Natural Language\nProcessing (NLP) tasks. However, there is currently a dearth of comprehensive\nstudy exploring its potential in the area of Grammatical Error Correction\n(GEC). To showcase its capabilities in GEC, we design zero-shot\nchain-of-thought (CoT) and few-shot CoT settings using in-context learning for\nChatGPT. Our evaluation involves assessing ChatGPT's performance on five\nofficial test sets in three different languages, along with three\ndocument-level GEC test sets in English. Our experimental results and human\nevaluations demonstrate that ChatGPT has excellent error detection capabilities\nand can freely correct errors to make the corrected sentences very fluent,\npossibly due to its over-correction tendencies and not adhering to the\nprinciple of minimal edits. Additionally, its performance in non-English and\nlow-resource settings highlights its potential in multilingual GEC tasks.\nHowever, further analysis of various types of errors at the document-level has\nshown that ChatGPT cannot effectively correct agreement, coreference, tense\nerrors across sentences, and cross-sentence boundary errors.\n","authors":["Tao Fang","Shu Yang","Kaixin Lan","Derek F. Wong","Jinpeng Hu","Lidia S. Chao","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.15088v3","updated":"2023-04-04T12:26:44Z","published":"2022-10-27T00:22:16Z","title":"Personalized Dialogue Generation with Persona-Adaptive Attention","summary":"  Persona-based dialogue systems aim to generate consistent responses based on\nhistorical context and predefined persona. Unlike conventional dialogue\ngeneration, the persona-based dialogue needs to consider both dialogue context\nand persona, posing a challenge for coherent training. Specifically, this\nrequires a delicate weight balance between context and persona. To achieve\nthat, in this paper, we propose an effective framework with Persona-Adaptive\nAttention (PAA), which adaptively integrates the weights from the persona and\ncontext information via our designed attention. In addition, a dynamic masking\nmechanism is applied to the PAA to not only drop redundant information in\ncontext and persona but also serve as a regularization mechanism to avoid\noverfitting. Experimental results demonstrate the superiority of the proposed\nPAA framework compared to the strong baselines in both automatic and human\nevaluation. Moreover, the proposed PAA approach can perform equivalently well\nin a low-resource regime compared to models trained in a full-data setting,\nwhich achieve a similar result with only 20% to 30% of data compared to the\nlarger models trained in the full-data setting. To fully exploit the\neffectiveness of our design, we designed several variants for handling the\nweighted information in different ways, showing the necessity and sufficiency\nof our weighting and masking designs.\n","authors":["Qiushi Huang","Yu Zhang","Tom Ko","Xubo Liu","Bo Wu","Wenwu Wang","Lilian Tang"],"pdf_url":"https://arxiv.org/pdf/2210.15088v3.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.04054v2","updated":"2023-04-04T11:33:30Z","published":"2022-12-08T03:29:04Z","title":"Learning to Dub Movies via Hierarchical Prosody Models","summary":"  Given a piece of text, a video clip and a reference audio, the movie dubbing\n(also known as visual voice clone V2C) task aims to generate speeches that\nmatch the speaker's emotion presented in the video using the desired speaker\nvoice as reference. V2C is more challenging than conventional text-to-speech\ntasks as it additionally requires the generated speech to exactly match the\nvarying emotions and speaking speed presented in the video. Unlike previous\nworks, we propose a novel movie dubbing architecture to tackle these problems\nvia hierarchical prosody modelling, which bridges the visual information to\ncorresponding speech prosody from three aspects: lip, face, and scene.\nSpecifically, we align lip movement to the speech duration, and convey facial\nexpression to speech energy and pitch via attention mechanism based on valence\nand arousal representations inspired by recent psychology findings. Moreover,\nwe design an emotion booster to capture the atmosphere from global video\nscenes. All these embeddings together are used to generate mel-spectrogram and\nthen convert to speech waves via existing vocoder. Extensive experimental\nresults on the Chem and V2C benchmark datasets demonstrate the favorable\nperformance of the proposed method. The source code and trained models will be\nreleased to the public.\n","authors":["Gaoxiang Cong","Liang Li","Yuankai Qi","Zhengjun Zha","Qi Wu","Wenyu Wang","Bin Jiang","Ming-Hsuan Yang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2212.04054v2.pdf","comment":"accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01712v1","updated":"2023-04-04T11:20:37Z","published":"2023-04-04T11:20:37Z","title":"Rumour Detection and Analysis on Twitter","summary":"  In recent years people have become increasingly reliant on social media to\nread news and get information, and some social media users post unsubstantiated\ninformation to gain attention. Such information is known as rumours. Nowadays,\nrumour detection is receiving a growing amount of attention because of the\npandemic of the New Coronavirus, which has led to a large number of rumours\nbeing spread. In this paper, a Natural Language Processing (NLP) system is\nbuilt to predict rumours. The best model is applied to the COVID-19 tweets to\nconduct exploratory data analysis. The contribution of this study is twofold:\n(1) to compare rumours and facts using state-of-the-art natural language\nprocessing models in two dimensions: language structure and propagation route.\n(2) An analysis of how rumours differ from facts in terms of their lexical use\nand the emotions they imply. This study shows that linguistic structure is a\nbetter feature to distinguish rumours from facts compared to the propagation\npath. In addition, rumour tweets contain more vocabulary related to politics\nand negative emotions.\n","authors":["Yaohou Fan"],"pdf_url":"https://arxiv.org/pdf/2304.01712v1.pdf","comment":"Has been accepted by the 2nd International Conference on Computing\n  Innovation and Applied Physics(CONF-CIAP 2023)"},{"id":"http://arxiv.org/abs/2208.07316v3","updated":"2023-04-04T10:23:20Z","published":"2022-08-15T16:30:14Z","title":"MENLI: Robust Evaluation Metrics from Natural Language Inference","summary":"  Recently proposed BERT-based evaluation metrics for text generation perform\nwell on standard benchmarks but are vulnerable to adversarial attacks, e.g.,\nrelating to information correctness. We argue that this stems (in part) from\nthe fact that they are models of semantic similarity. In contrast, we develop\nevaluation metrics based on Natural Language Inference (NLI), which we deem a\nmore appropriate modeling. We design a preference-based adversarial attack\nframework and show that our NLI based metrics are much more robust to the\nattacks than the recent BERT-based metrics. On standard benchmarks, our NLI\nbased metrics outperform existing summarization metrics, but perform below SOTA\nMT metrics. However, when combining existing metrics with our NLI metrics, we\nobtain both higher adversarial robustness (15%-30%) and higher quality metrics\nas measured on standard benchmarks (+5% to 30%).\n","authors":["Yanran Chen","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2208.07316v3.pdf","comment":"TACL 2023 Camera-ready github link fixed"},{"id":"http://arxiv.org/abs/2304.01680v1","updated":"2023-04-04T10:11:06Z","published":"2023-04-04T10:11:06Z","title":"Can BERT eat RuCoLA? Topological Data Analysis to Explain","summary":"  This paper investigates how Transformer language models (LMs) fine-tuned for\nacceptability classification capture linguistic features. Our approach uses the\nbest practices of topological data analysis (TDA) in NLP: we construct directed\nattention graphs from attention matrices, derive topological features from\nthem, and feed them to linear classifiers. We introduce two novel features,\nchordality, and the matching number, and show that TDA-based classifiers\noutperform fine-tuning baselines. We experiment with two datasets, CoLA and\nRuCoLA in English and Russian, typologically different languages. On top of\nthat, we propose several black-box introspection techniques aimed at detecting\nchanges in the attention mode of the LMs during fine-tuning, defining the LM's\nprediction confidences, and associating individual heads with fine-grained\ngrammar phenomena. Our results contribute to understanding the behavior of\nmonolingual LMs in the acceptability classification task, provide insights into\nthe functional roles of attention heads, and highlight the advantages of\nTDA-based approaches for analyzing LMs. We release the code and the\nexperimental results for further uptake.\n","authors":["Irina Proskurina","Irina Piontkovskaya","Ekaterina Artemova"],"pdf_url":"https://arxiv.org/pdf/2304.01680v1.pdf","comment":"Accepted to the Workshop on Slavic NLP @ EACL 2023"},{"id":"http://arxiv.org/abs/2304.01666v1","updated":"2023-04-04T09:50:19Z","published":"2023-04-04T09:50:19Z","title":"A Survey on Contextualised Semantic Shift Detection","summary":"  Semantic Shift Detection (SSD) is the task of identifying, interpreting, and\nassessing the possible change over time in the meanings of a target word.\nTraditionally, SSD has been addressed by linguists and social scientists\nthrough manual and time-consuming activities. In the recent years,\ncomputational approaches based on Natural Language Processing and word\nembeddings gained increasing attention to automate SSD as much as possible. In\nparticular, over the past three years, significant advancements have been made\nalmost exclusively based on word contextualised embedding models, which can\nhandle the multiple usages/meanings of the words and better capture the related\nsemantic shifts. In this paper, we survey the approaches based on\ncontextualised embeddings for SSD (i.e., CSSDetection) and we propose a\nclassification framework characterised by meaning representation,\ntime-awareness, and learning modality dimensions. The framework is exploited i)\nto review the measures for shift assessment, ii) to compare the approaches on\nperformance, and iii) to discuss the current issues in terms of scalability,\ninterpretability, and robustness. Open challenges and future research\ndirections about CSSDetection are finally outlined.\n","authors":["Stefano Montanelli","Francesco Periti"],"pdf_url":"https://arxiv.org/pdf/2304.01666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01665v1","updated":"2023-04-04T09:50:07Z","published":"2023-04-04T09:50:07Z","title":"Neural Comprehension: Language Models with Compiled Neural Networks","summary":"  Language models have achieved impressive results in natural language\nprocessing tasks, but their ability to perform symbolic operations and\narithmetic operations, remains limited, which attribute to their learn the\nrules implicitly from data. We explore how to incorporate compiled neural\nnetworks (CoNNs) which weight is specially designed, into the architecture of\nlanguage models to enable the language model trained by gradient to obtain\nfully rule comprehension ability. The incorporation of compiled neural networks\noffers a promising direction for improving the performance of language models\non compound tasks, particularly in areas that require a deeper comprehension of\nabstract rules beyond recognizing patterns in training data. Our method, which\ncall \"Neural Comprehension\", helps language models achieve absolute accuracy in\nsymbolic operations, thereby enhancing their ability for rule reasoning,\nsymbolic reasoning, and arithmetic reasoning. Our code is publicly available\nat: \\url{https://github.com/WENGSYX/Neural-Comprehension}.\n","authors":["Yixuan Weng","Minjun Zhu","Fei Xia","Bin Li","Shizhu He","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.01665v1.pdf","comment":"32 pages, 8 tables, 12 figures"},{"id":"http://arxiv.org/abs/2304.01662v1","updated":"2023-04-04T09:33:16Z","published":"2023-04-04T09:33:16Z","title":"Cross-Domain Image Captioning with Discriminative Finetuning","summary":"  Neural captioners are typically trained to mimic human-generated references\nwithout optimizing for any specific communication goal, leading to problems\nsuch as the generation of vague captions. In this paper, we show that\nfine-tuning an out-of-the-box neural captioner with a self-supervised\ndiscriminative communication objective helps to recover a plain, visually\ndescriptive language that is more informative about image contents. Given a\ntarget image, the system must learn to produce a description that enables an\nout-of-the-box text-conditioned image retriever to identify such image among a\nset of candidates. We experiment with the popular ClipCap captioner, also\nreplicating the main results with BLIP. In terms of similarity to ground-truth\nhuman descriptions, the captions emerging from discriminative finetuning lag\nslightly behind those generated by the non-finetuned model, when the latter is\ntrained and tested on the same caption dataset. However, when the model is used\nwithout further tuning to generate captions for out-of-domain datasets, our\ndiscriminatively-finetuned captioner generates descriptions that resemble human\nreferences more than those produced by the same captioner without finetuning.\nWe further show that, on the Conceptual Captions dataset, discriminatively\nfinetuned captions are more helpful than either vanilla ClipCap captions or\nground-truth captions for human annotators tasked with an image discrimination\ntask.\n","authors":["Roberto Dessì","Michele Bevilacqua","Eleonora Gualdoni","Nathanael Carraz Rakotonirina","Francesca Franzon","Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2304.01662v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.17853v2","updated":"2023-04-04T09:30:55Z","published":"2023-03-31T07:29:47Z","title":"Can AI Put Gamma-Ray Astrophysicists Out of a Job?","summary":"  In what will likely be a litany of generative-model-themed arXiv submissions\ncelebrating April the 1st, we evaluate the capacity of state-of-the-art\ntransformer models to create a paper detailing the detection of a Pulsar Wind\nNebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT)\nArray. We do this to evaluate the ability of such models to interpret\nastronomical observations and sources based on language information alone, and\nto assess potential means by which fraudulently generated scientific papers\ncould be identified during peer review (given that reliable generative model\nwatermarking has yet to be deployed for these tools). We conclude that our jobs\nas astronomers are safe for the time being. From this point on, prompts given\nto ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT\nis shown in black, whereas analysis by the (human) authors is in blue.\n","authors":["Samuel T. Spencer","Vikas Joshi","Alison M. W. Mitchell"],"pdf_url":"https://arxiv.org/pdf/2303.17853v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01515v3","updated":"2023-04-04T08:55:04Z","published":"2022-12-03T02:55:14Z","title":"Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for\n  Personality Detection","summary":"  Predicting personality traits based on online posts has emerged as an\nimportant task in many fields such as social network analysis. One of the\nchallenges of this task is assembling information from various posts into an\noverall profile for each user. While many previous solutions simply concatenate\nthe posts into a long document and then encode the document by sequential or\nhierarchical models, they introduce unwarranted orders for the posts, which may\nmislead the models. In this paper, we propose a dynamic deep graph\nconvolutional network (D-DGCN) to overcome the above limitation. Specifically,\nwe design a learn-to-connect approach that adopts a dynamic multi-hop structure\ninstead of a deterministic structure, and combine it with a DGCN module to\nautomatically learn the connections between posts. The modules of post encoder,\nlearn-to-connect, and DGCN are jointly trained in an end-to-end manner.\nExperimental results on the Kaggle and Pandora datasets show the superior\nperformance of D-DGCN to state-of-the-art baselines. Our code is available at\nhttps://github.com/djz233/D-DGCN.\n","authors":["Tao Yang","Jinghao Deng","Xiaojun Quan","Qifan Wang"],"pdf_url":"https://arxiv.org/pdf/2212.01515v3.pdf","comment":"AAAI2023 Camera-ready"},{"id":"http://arxiv.org/abs/2304.01638v1","updated":"2023-04-04T08:49:39Z","published":"2023-04-04T08:49:39Z","title":"Multidimensional Perceptron for Efficient and Explainable Long Text\n  Classification","summary":"  Because of the inevitable cost and complexity of transformer and pre-trained\nmodels, efficiency concerns are raised for long text classification. Meanwhile,\nin the highly sensitive domains, e.g., healthcare and legal long-text mining,\npotential model distrust, yet underrated and underexplored, may hatch vital\napprehension. Existing methods generally segment the long text, encode each\npiece with the pre-trained model, and use attention or RNNs to obtain long text\nrepresentation for classification. In this work, we propose a simple but\neffective model, Segment-aWare multIdimensional PErceptron (SWIPE), to replace\nattention/RNNs in the above framework. Unlike prior efforts, SWIPE can\neffectively learn the label of the entire text with supervised training, while\nperceive the labels of the segments and estimate their contributions to the\nlong-text labeling in an unsupervised manner. As a general classifier, SWIPE\ncan endorse different encoders, and it outperforms SOTA models in terms of\nclassification accuracy and model efficiency. It is noteworthy that SWIPE\nachieves superior interpretability to transparentize long text classification\nresults.\n","authors":["Yexiang Wang","Yating Zhang","Xiaozhong Liu","Changlong Sun"],"pdf_url":"https://arxiv.org/pdf/2304.01638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01196v2","updated":"2023-04-04T08:34:16Z","published":"2023-04-03T17:59:09Z","title":"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on\n  Self-Chat Data","summary":"  Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize. An online\ndemo is also available at\nhttps://huggingface.co/spaces/project-baize/baize-lora-7B.\n","authors":["Canwen Xu","Daya Guo","Nan Duan","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2304.01196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01622v1","updated":"2023-04-04T08:25:12Z","published":"2023-04-04T08:25:12Z","title":"An interpretability framework for Similar case matching","summary":"  Similar Case Matching (SCM) is designed to determine whether two cases are\nsimilar. The task has an essential role in the legal system, helping legal\nprofessionals to find relevant cases quickly and thus deal with them more\nefficiently. Existing research has focused on improving the model's performance\nbut not on its interpretability. Therefore, this paper proposes a pipeline\nframework for interpretable SCM, which consists of four modules: a judicial\nfeature sentence identification module, a case matching module, a feature\nsentence alignment module, and a conflict disambiguation module. Unlike\nexisting SCM methods, our framework will identify feature sentences in a case\nthat contain essential information, perform similar case matching based on the\nextracted feature sentence results, and align the feature sentences in the two\ncases to provide evidence for the similarity of the cases. SCM results may\nconflict with feature sentence alignment results, and our framework further\ndisambiguates against this inconsistency. The experimental results show the\neffectiveness of our framework, and our work provides a new benchmark for\ninterpretable SCM.\n","authors":["Nankai Lin","Haonan Liu","Jiajun Fang","Dong Zhou","Aimin Yang"],"pdf_url":"https://arxiv.org/pdf/2304.01622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01621v1","updated":"2023-04-04T08:24:22Z","published":"2023-04-04T08:24:22Z","title":"SimCSum: Joint Learning of Simplification and Cross-lingual\n  Summarization for Cross-lingual Science Journalism","summary":"  Cross-lingual science journalism generates popular science stories of\nscientific articles different from the source language for a non-expert\naudience. Hence, a cross-lingual popular summary must contain the salient\ncontent of the input document, and the content should be coherent,\ncomprehensible, and in a local language for the targeted audience. We improve\nthese aspects of cross-lingual summary generation by joint training of two\nhigh-level NLP tasks, simplification and cross-lingual summarization. The\nformer task reduces linguistic complexity, and the latter focuses on\ncross-lingual abstractive summarization. We propose a novel multi-task\narchitecture - SimCSum consisting of one shared encoder and two parallel\ndecoders jointly learning simplification and cross-lingual summarization. We\nempirically investigate the performance of SimCSum by comparing it with several\nstrong baselines over several evaluation metrics and by human evaluation.\nOverall, SimCSum demonstrates statistically significant improvements over the\nstate-of-the-art on two non-synthetic cross-lingual scientific datasets.\nFurthermore, we conduct an in-depth investigation into the linguistic\nproperties of generated summaries and an error analysis.\n","authors":["Mehwish Fatima","Tim Kolber","Katja Markert","Michael Strube"],"pdf_url":"https://arxiv.org/pdf/2304.01621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01612v1","updated":"2023-04-04T08:07:07Z","published":"2023-04-04T08:07:07Z","title":"EDeR: A Dataset for Exploring Dependency Relations Between Events","summary":"  Relation extraction is a central task in natural language processing (NLP)\nand information retrieval (IR) research. We argue that an important type of\nrelation not explored in NLP or IR research to date is that of an event being\nan argument - required or optional - of another event. We introduce the\nhuman-annotated Event Dependency Relation dataset (EDeR) which provides this\ndependency relation. The annotation is done on a sample of documents from the\nOntoNotes dataset, which has the added benefit that it integrates with\nexisting, orthogonal, annotations of this dataset. We investigate baseline\napproaches for predicting the event dependency relation, the best of which\nachieves an accuracy of 82.61 for binary argument/non-argument classification.\nWe show that recognizing this relation leads to more accurate event extraction\n(semantic role labelling) and can improve downstream tasks that depend on this,\nsuch as co-reference resolution. Furthermore, we demonstrate that predicting\nthe three-way classification into the required argument, optional argument or\nnon-argument is a more challenging task.\n","authors":["Ruiqi Li","Patrik Haslum","Leyang Cui"],"pdf_url":"https://arxiv.org/pdf/2304.01612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01597v1","updated":"2023-04-04T07:37:06Z","published":"2023-04-04T07:37:06Z","title":"Unsupervised Improvement of Factual Knowledge in Language Models","summary":"  Masked language modeling (MLM) plays a key role in pretraining large language\nmodels. But the MLM objective is often dominated by high-frequency words that\nare sub-optimal for learning factual knowledge. In this work, we propose an\napproach for influencing MLM pretraining in a way that can improve language\nmodel performance on a variety of knowledge-intensive tasks. We force the\nlanguage model to prioritize informative words in a fully unsupervised way.\nExperiments demonstrate that the proposed approach can significantly improve\nthe performance of pretrained language models on tasks such as factual recall,\nquestion answering, sentiment analysis, and natural language inference in a\nclosed-book setting.\n","authors":["Nafis Sadeq","Byungkyu Kang","Prarit Lamba","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2304.01597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.15949v2","updated":"2023-04-04T06:43:19Z","published":"2021-03-29T20:51:33Z","title":"Transformer visualization via dictionary learning: contextualized\n  embedding as a linear superposition of transformer factors","summary":"  Transformer networks have revolutionized NLP representation learning since\nthey were introduced. Though a great effort has been made to explain the\nrepresentation in transformers, it is widely recognized that our understanding\nis not sufficient. One important reason is that there lack enough visualization\ntools for detailed analysis. In this paper, we propose to use dictionary\nlearning to open up these \"black boxes\" as linear superpositions of transformer\nfactors. Through visualization, we demonstrate the hierarchical semantic\nstructures captured by the transformer factors, e.g., word-level polysemy\ndisambiguation, sentence-level pattern formation, and long-range dependency.\nWhile some of these patterns confirm the conventional prior linguistic\nknowledge, the rest are relatively unexpected, which may provide new insights.\nWe hope this visualization tool can bring further knowledge and a better\nunderstanding of how transformer networks work. The code is available at\nhttps://github.com/zeyuyun1/TransformerVis\n","authors":["Zeyu Yun","Yubei Chen","Bruno A Olshausen","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2103.15949v2.pdf","comment":"This paper is published at DeeLIO Workshop@NAACL 2021"},{"id":"http://arxiv.org/abs/2304.01563v1","updated":"2023-04-04T06:39:36Z","published":"2023-04-04T06:39:36Z","title":"Attribute-Consistent Knowledge Graph Representation Learning for\n  Multi-Modal Entity Alignment","summary":"  The multi-modal entity alignment (MMEA) aims to find all equivalent entity\npairs between multi-modal knowledge graphs (MMKGs). Rich attributes and\nneighboring entities are valuable for the alignment task, but existing works\nignore contextual gap problems that the aligned entities have different numbers\nof attributes on specific modality when learning entity representations. In\nthis paper, we propose a novel attribute-consistent knowledge graph\nrepresentation learning framework for MMEA (ACK-MMEA) to compensate the\ncontextual gaps through incorporating consistent alignment knowledge.\nAttribute-consistent KGs (ACKGs) are first constructed via multi-modal\nattribute uniformization with merge and generate operators so that each entity\nhas one and only one uniform feature in each modality. The ACKGs are then fed\ninto a relation-aware graph neural network with random dropouts, to obtain\naggregated relation representations and robust entity representations. In order\nto evaluate the ACK-MMEA facilitated for entity alignment, we specially design\na joint alignment loss for both entity and attribute evaluation. Extensive\nexperiments conducted on two benchmark datasets show that our approach achieves\nexcellent performance compared to its competitors.\n","authors":["Qian Li","Shu Guo","Yangyifei Luo","Cheng Ji","Lihong Wang","Jiawei Sheng","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2304.01563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01515v1","updated":"2023-04-04T03:52:49Z","published":"2023-04-04T03:52:49Z","title":"Text-Conditioned Sampling Framework for Text-to-Image Generation with\n  Masked Generative Models","summary":"  Token-based masked generative models are gaining popularity for their fast\ninference time with parallel decoding. While recent token-based approaches\nachieve competitive performance to diffusion-based models, their generation\nperformance is still suboptimal as they sample multiple tokens simultaneously\nwithout considering the dependence among them. We empirically investigate this\nproblem and propose a learnable sampling model, Text-Conditioned Token\nSelection (TCTS), to select optimal tokens via localized supervision with text\ninformation. TCTS improves not only the image quality but also the semantic\nalignment of the generated images with the given texts. To further improve the\nimage quality, we introduce a cohesive sampling strategy, Frequency Adaptive\nSampling (FAS), to each group of tokens divided according to the self-attention\nmaps. We validate the efficacy of TCTS combined with FAS with various\ngenerative tasks, demonstrating that it significantly outperforms the baselines\nin image-text alignment and image quality. Our text-conditioned sampling\nframework further reduces the original inference time by more than 50% without\nmodifying the original generative model.\n","authors":["Jaewoong Lee","Sangwon Jang","Jaehyeong Jo","Jaehong Yoon","Yunji Kim","Jin-Hwa Kim","Jung-Woo Ha","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2304.01515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14524v2","updated":"2023-04-04T03:51:27Z","published":"2023-03-25T17:37:43Z","title":"Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender\n  System","summary":"  Large language models (LLMs) have demonstrated their significant potential to\nbe applied for addressing various application tasks. However, traditional\nrecommender systems continue to face great challenges such as poor\ninteractivity and explainability, which actually also hinder their broad\ndeployment in real-world systems. To address these limitations, this paper\nproposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender\nSystem) that innovatively augments LLMs for building conversational recommender\nsystems by converting user profiles and historical interactions into prompts.\nChat-Rec is demonstrated to be effective in learning user preferences and\nestablishing connections between users and products through in-context\nlearning, which also makes the recommendation process more interactive and\nexplainable. What's more, within the Chat-Rec framework, user's preferences can\ntransfer to different products for cross-domain recommendations, and\nprompt-based injection of information into LLMs can also handle the cold-start\nscenarios with new items. In our experiments, Chat-Rec effectively improve the\nresults of top-k recommendations and performs better in zero-shot rating\nprediction task. Chat-Rec offers a novel approach to improving recommender\nsystems and presents new practical scenarios for the implementation of AIGC (AI\ngenerated content) in recommender system studies.\n","authors":["Yunfan Gao","Tao Sheng","Youlin Xiang","Yun Xiong","Haofen Wang","Jiawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12582v2","updated":"2023-04-04T03:32:24Z","published":"2023-03-22T14:09:20Z","title":"AfroDigits: A Community-Driven Spoken Digit Dataset for African\n  Languages","summary":"  The advancement of speech technologies has been remarkable, yet its\nintegration with African languages remains limited due to the scarcity of\nAfrican speech corpora. To address this issue, we present AfroDigits, a\nminimalist, community-driven dataset of spoken digits for African languages,\ncurrently covering 38 African languages. As a demonstration of the practical\napplications of AfroDigits, we conduct audio digit classification experiments\non six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo\n(kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R\nmodels. Our experiments reveal a useful insight on the effect of mixing African\nspeech corpora during finetuning. AfroDigits is the first published audio digit\ndataset for African languages and we believe it will, among other things, pave\nthe way for Afro-centric speech applications such as the recognition of\ntelephone numbers, and street numbers. We release the dataset and platform\npublicly at https://huggingface.co/datasets/chrisjay/crowd-speech-africa and\nhttps://huggingface.co/spaces/chrisjay/afro-speech respectively.\n","authors":["Chris Chinenye Emezue","Sanchit Gandhi","Lewis Tunstall","Abubakar Abid","Josh Meyer","Quentin Lhoest","Pete Allen","Patrick Von Platen","Douwe Kiela","Yacine Jernite","Julien Chaumond","Merve Noyan","Omar Sanseviero"],"pdf_url":"https://arxiv.org/pdf/2303.12582v2.pdf","comment":"Accepted to the AfricaNLP Workshop at ICLR 2023"},{"id":"http://arxiv.org/abs/2304.01492v1","updated":"2023-04-04T03:13:03Z","published":"2023-04-04T03:13:03Z","title":"A Unified Contrastive Transfer Framework with Propagation Structure for\n  Boosting Low-Resource Rumor Detection","summary":"  The truth is significantly hampered by massive rumors that spread along with\nbreaking news or popular topics. Since there is sufficient corpus gathered from\nthe same domain for model training, existing rumor detection algorithms show\npromising performance on yesterday's news. However, due to a lack of training\ndata and prior expert knowledge, they are poor at spotting rumors concerning\nunforeseen events, especially those propagated in different languages (i.e.,\nlow-resource regimes). In this paper, we propose a unified contrastive transfer\nframework to detect rumors by adapting the features learned from well-resourced\nrumor data to that of the low-resourced. More specifically, we first represent\nrumor circulated on social media as an undirected topology, and then train a\nMulti-scale Graph Convolutional Network via a unified contrastive paradigm. Our\nmodel explicitly breaks the barriers of the domain and/or language issues, via\nlanguage alignment and a novel domain-adaptive contrastive learning mechanism.\nTo enhance the representation learning from a small set of target events, we\nreveal that rumor-indicative signal is closely correlated with the uniformity\nof the distribution of these events. We design a target-wise contrastive\ntraining mechanism with three data augmentation strategies, capable of unifying\nthe representations by distinguishing target events. Extensive experiments\nconducted on four low-resource datasets collected from real-world microblog\nplatforms demonstrate that our framework achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for detecting rumors\nat early stages.\n","authors":["Hongzhan Lin","Jing Ma","Ruichao Yang","Zhiwei Yang","Mingfei Cheng"],"pdf_url":"https://arxiv.org/pdf/2304.01492v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2204.08143"},{"id":"http://arxiv.org/abs/2304.01487v1","updated":"2023-04-04T03:04:28Z","published":"2023-04-04T03:04:28Z","title":"To ChatGPT, or not to ChatGPT: That is the question!","summary":"  ChatGPT has become a global sensation. As ChatGPT and other Large Language\nModels (LLMs) emerge, concerns of misusing them in various ways increase, such\nas disseminating fake news, plagiarism, manipulating public opinion, cheating,\nand fraud. Hence, distinguishing AI-generated from human-generated becomes\nincreasingly essential. Researchers have proposed various detection\nmethodologies, ranging from basic binary classifiers to more complex\ndeep-learning models. Some detection techniques rely on statistical\ncharacteristics or syntactic patterns, while others incorporate semantic or\ncontextual information to improve accuracy. The primary objective of this study\nis to provide a comprehensive and contemporary assessment of the most recent\ntechniques in ChatGPT detection. Additionally, we evaluated other AI-generated\ntext detection tools that do not specifically claim to detect ChatGPT-generated\ncontent to assess their performance in detecting ChatGPT-generated content. For\nour evaluation, we have curated a benchmark dataset consisting of prompts from\nChatGPT and humans, including diverse questions from medical, open Q&A, and\nfinance domains and user-generated responses from popular social networking\nplatforms. The dataset serves as a reference to assess the performance of\nvarious techniques in detecting ChatGPT-generated content. Our evaluation\nresults demonstrate that none of the existing methods can effectively detect\nChatGPT-generated content.\n","authors":["Alessandro Pegoraro","Kavita Kumari","Hossein Fereidooni","Ahmad-Reza Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2304.01487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01483v1","updated":"2023-04-04T02:55:40Z","published":"2023-04-04T02:55:40Z","title":"Blockwise Compression of Transformer-based Models without Retraining","summary":"  Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have\nrecently attracted increasing interest, research enthusiasm, and business\ndemand. However, their massive computation resources and huge memory footprint\nare inevitable challenges. To tackle this issue, we propose BCT, a framework of\nblockwise compression for transformers without retraining, to lower deployment\nthresholds. BCT achieves more fine-grained compression of the whole\ntransformer, including embedding, matrix multiplication, GELU, Softmax, layer\nnormalization, and all the intermediate results. As a case, we compress an\nefficient model with BCT and evaluate it on several General Language\nUnderstanding Evaluation (GLUE) datasets. The results show that BCT can achieve\na less than 0.90% accuracy drop in most tasks.\n","authors":["Gaochen Dong","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2304.01483v1.pdf","comment":"6 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:2303.09184"},{"id":"http://arxiv.org/abs/2304.01481v1","updated":"2023-04-04T02:54:04Z","published":"2023-04-04T02:54:04Z","title":"The Vector Grounding Problem","summary":"  The remarkable performance of large language models (LLMs) on complex\nlinguistic tasks has sparked a lively debate on the nature of their\ncapabilities. Unlike humans, these models learn language exclusively from\ntextual data, without direct interaction with the real world. Nevertheless,\nthey can generate seemingly meaningful text about a wide range of topics. This\nimpressive accomplishment has rekindled interest in the classical 'Symbol\nGrounding Problem,' which questioned whether the internal representations and\noutputs of classical symbolic AI systems could possess intrinsic meaning.\nUnlike these systems, modern LLMs are artificial neural networks that compute\nover vectors rather than symbols. However, an analogous problem arises for such\nsystems, which we dub the Vector Grounding Problem. This paper has two primary\nobjectives. First, we differentiate various ways in which internal\nrepresentations can be grounded in biological or artificial systems,\nidentifying five distinct notions discussed in the literature: referential,\nsensorimotor, relational, communicative, and epistemic grounding.\nUnfortunately, these notions of grounding are often conflated. We clarify the\ndifferences between them, and argue that referential grounding is the one that\nlies at the heart of the Vector Grounding Problem. Second, drawing on theories\nof representational content in philosophy and cognitive science, we propose\nthat certain LLMs, particularly those fine-tuned with Reinforcement Learning\nfrom Human Feedback (RLHF), possess the necessary features to overcome the\nVector Grounding Problem, as they stand in the requisite causal-historical\nrelations to the world that underpin intrinsic meaning. We also argue that,\nperhaps unexpectedly, multimodality and embodiment are neither necessary nor\nsufficient conditions for referential grounding in artificial systems.\n","authors":["Dimitri Coelho Mollo","Raphaël Millière"],"pdf_url":"https://arxiv.org/pdf/2304.01481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01424v1","updated":"2023-04-04T00:13:55Z","published":"2023-04-04T00:13:55Z","title":"Polarity based Sarcasm Detection using Semigraph","summary":"  Sarcasm is an advanced linguistic expression often found on various online\nplatforms. Sarcasm detection is challenging in natural language processing\ntasks that affect sentiment analysis. This article presents the inventive\nmethod of the semigraph, including semigraph construction and sarcasm detection\nprocesses. A variation of the semigraph is suggested in the pattern-relatedness\nof the text document. The proposed method is to obtain the sarcastic and\nnon-sarcastic polarity scores of a document using a semigraph. The sarcastic\npolarity score represents the possibility that a document will become\nsarcastic. Sarcasm is detected based on the polarity scoring model. The\nperformance of the proposed model enhances the existing prior art approach to\nsarcasm detection. In the Amazon product review, the model achieved the\naccuracy, recall, and f-measure of 0.87, 0.79, and 0.83, respectively.\n","authors":["Swapnil Mane","Vaibhav Khatavkar"],"pdf_url":"https://arxiv.org/pdf/2304.01424v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2304.01423v1","updated":"2023-04-04T00:13:47Z","published":"2023-04-04T00:13:47Z","title":"Thematic context vector association based on event uncertainty for\n  Twitter","summary":"  Keyword extraction is a crucial process in text mining. The extraction of\nkeywords with respective contextual events in Twitter data is a big challenge.\nThe challenging issues are mainly because of the informality in the language\nused. The use of misspelled words, acronyms, and ambiguous terms causes\ninformality. The extraction of keywords with informal language in current\nsystems is pattern based or event based. In this paper, contextual keywords are\nextracted using thematic events with the help of data association. The thematic\ncontext for events is identified using the uncertainty principle in the\nproposed system. The thematic contexts are weighed with the help of vectors\ncalled thematic context vectors which signifies the event as certain or\nuncertain. The system is tested on the Twitter COVID-19 dataset and proves to\nbe effective. The system extracts event-specific thematic context vectors from\nthe test dataset and ranks them. The extracted thematic context vectors are\nused for the clustering of contextual thematic vectors which improves the\nsilhouette coefficient by 0.5% than state of art methods namely TF and TF-IDF.\nThe thematic context vector can be used in other applications like\nCyberbullying, sarcasm detection, figurative language detection, etc.\n","authors":["Vaibhav Khatavkar","Swapnil Mane","Parag Kulkarni"],"pdf_url":"https://arxiv.org/pdf/2304.01423v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2304.02168v1","updated":"2023-04-04T23:51:48Z","published":"2023-04-04T23:51:48Z","title":"I2I: Initializing Adapters with Improvised Knowledge","summary":"  Adapters present a promising solution to the catastrophic forgetting problem\nin continual learning. However, training independent Adapter modules for every\nnew task misses an opportunity for cross-task knowledge transfer. We propose\nImprovise to Initialize (I2I), a continual learning algorithm that initializes\nAdapters for incoming tasks by distilling knowledge from previously-learned\ntasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning\nbenchmark, by conducting experiments on sequences of visual question answering\ntasks. Adapters trained with I2I consistently achieve better task accuracy than\nindependently-trained Adapters, demonstrating that our algorithm facilitates\nknowledge transfer between task Adapters. I2I also results in better cross-task\nknowledge transfer than the state-of-the-art AdapterFusion without incurring\nthe associated parametric cost.\n","authors":["Tejas Srinivasan","Furong Jia","Mohammad Rostami","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2304.02168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02144v1","updated":"2023-04-04T22:05:02Z","published":"2023-04-04T22:05:02Z","title":"A Data Fusion Framework for Multi-Domain Morality Learning","summary":"  Language models can be trained to recognize the moral sentiment of text,\ncreating new opportunities to study the role of morality in human life. As\ninterest in language and morality has grown, several ground truth datasets with\nmoral annotations have been released. However, these datasets vary in the\nmethod of data collection, domain, topics, instructions for annotators, etc.\nSimply aggregating such heterogeneous datasets during training can yield models\nthat fail to generalize well. We describe a data fusion framework for training\non multiple heterogeneous datasets that improve performance and\ngeneralizability. The model uses domain adversarial training to align the\ndatasets in feature space and a weighted loss function to deal with label\nshift. We show that the proposed framework achieves state-of-the-art\nperformance in different datasets compared to prior works in morality\ninference.\n","authors":["Siyi Guo","Negar Mokhberian","Kristina Lerman"],"pdf_url":"https://arxiv.org/pdf/2304.02144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02138v1","updated":"2023-04-04T21:47:41Z","published":"2023-04-04T21:47:41Z","title":"Geotechnical Parrot Tales (GPT): Overcoming GPT hallucinations with\n  prompt engineering for geotechnical applications","summary":"  The widespread adoption of large language models (LLMs), such as OpenAI's\nChatGPT, could revolutionized various industries, including geotechnical\nengineering. However, GPT models can sometimes generate plausible-sounding but\nfalse outputs, leading to hallucinations. In this article, we discuss the\nimportance of prompt engineering in mitigating these risks and harnessing the\nfull potential of GPT for geotechnical applications. We explore the challenges\nand pitfalls associated with LLMs and highlight the role of context in ensuring\naccurate and valuable responses. Furthermore, we examine the development of\ncontext-specific search engines and the potential of LLMs to become a natural\ninterface for complex tasks, such as data analysis and design. We also develop\na unified interface using natural language to handle complex geotechnical\nengineering tasks and data analysis. By integrating GPT into geotechnical\nengineering workflows, professionals can streamline their work and develop\nsustainable and resilient infrastructure systems for the future.\n","authors":["Krishna Kumar"],"pdf_url":"https://arxiv.org/pdf/2304.02138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.11881v2","updated":"2023-04-04T21:10:29Z","published":"2020-12-22T08:46:14Z","title":"Undivided Attention: Are Intermediate Layers Necessary for BERT?","summary":"  In recent times, BERT-based models have been extremely successful in solving\na variety of natural language processing (NLP) tasks such as reading\ncomprehension, natural language inference, sentiment analysis, etc. All\nBERT-based architectures have a self-attention block followed by a block of\nintermediate layers as the basic building component. However, a strong\njustification for the inclusion of these intermediate layers remains missing in\nthe literature. In this work we investigate the importance of intermediate\nlayers on the overall network performance of downstream tasks. We show that\nreducing the number of intermediate layers and modifying the architecture for\nBERT-BASE results in minimal loss in fine-tuning accuracy for downstream tasks\nwhile decreasing the number of parameters and training time of the model.\nAdditionally, we use centered kernel alignment and probing linear classifiers\nto gain insight into our architectural modifications and justify that removal\nof intermediate layers has little impact on the fine-tuned accuracy.\n","authors":["Sharath Nittur Sridhar","Anthony Sarah"],"pdf_url":"https://arxiv.org/pdf/2012.11881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12126v3","updated":"2023-04-04T20:03:36Z","published":"2023-02-23T16:09:42Z","title":"KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate\n  Political Stance Prediction","summary":"  The political stance prediction for news articles has been widely studied to\nmitigate the echo chamber effect -- people fall into their thoughts and\nreinforce their pre-existing beliefs. The previous works for the political\nstance problem focus on (1) identifying political factors that could reflect\nthe political stance of a news article and (2) capturing those factors\neffectively. Despite their empirical successes, they are not sufficiently\njustified in terms of how effective their identified factors are in the\npolitical stance prediction. Motivated by this, in this work, we conduct a user\nstudy to investigate important factors in political stance prediction, and\nobserve that the context and tone of a news article (implicit) and external\nknowledge for real-world entities appearing in the article (explicit) are\nimportant in determining its political stance. Based on this observation, we\npropose a novel knowledge-aware approach to political stance prediction (KHAN),\nemploying (1) hierarchical attention networks (HAN) to learn the relationships\namong words and sentences in three different levels and (2) knowledge encoding\n(KE) to incorporate external knowledge for real-world entities into the process\nof political stance prediction. Also, to take into account the subtle and\nimportant difference between opposite political stances, we build two\nindependent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by\nourselves and learn to fuse the different political knowledge. Through\nextensive evaluations on three real-world datasets, we demonstrate the\nsuperiority of DASH in terms of (1) accuracy, (2) efficiency, and (3)\neffectiveness.\n","authors":["Yunyong Ko","Seongeun Ryu","Soeun Han","Youngseung Jeon","Jaehoon Kim","Sohyun Park","Kyungsik Han","Hanghang Tong","Sang-Wook Kim"],"pdf_url":"https://arxiv.org/pdf/2302.12126v3.pdf","comment":"12 pages, 5 figures, 10 tables, the Web Conference 2023 (WWW)"},{"id":"http://arxiv.org/abs/2304.02080v1","updated":"2023-04-04T19:11:05Z","published":"2023-04-04T19:11:05Z","title":"Scalable and Accurate Self-supervised Multimodal Representation Learning\n  without Aligned Video and Text Data","summary":"  Scaling up weakly-supervised datasets has shown to be highly effective in the\nimage-text domain and has contributed to most of the recent state-of-the-art\ncomputer vision and multimodal neural networks. However, existing large-scale\nvideo-text datasets and mining techniques suffer from several limitations, such\nas the scarcity of aligned data, the lack of diversity in the data, and the\ndifficulty of collecting aligned data. Currently popular video-text data mining\napproach via automatic speech recognition (ASR) used in HowTo100M provides\nlow-quality captions that often do not refer to the video content. Other mining\napproaches do not provide proper language descriptions (video tags) and are\nbiased toward short clips (alt text). In this work, we show how recent advances\nin image captioning allow us to pre-train high-quality video models without any\nparallel video-text data. We pre-train several video captioning models that are\nbased on an OPT language model and a TimeSformer visual backbone. We fine-tune\nthese networks on several video captioning datasets. First, we demonstrate that\nimage captioning pseudolabels work better for pre-training than the existing\nHowTo100M ASR captions. Second, we show that pre-training on both images and\nvideos produces a significantly better network (+4 CIDER on MSR-VTT) than\npre-training on a single modality. Our methods are complementary to the\nexisting pre-training or data mining approaches and can be used in a variety of\nsettings. Given the efficacy of the pseudolabeling method, we are planning to\npublicly release the generated captions.\n","authors":["Vladislav Lialin","Stephen Rawls","David Chan","Shalini Ghosh","Anna Rumshisky","Wael Hamza"],"pdf_url":"https://arxiv.org/pdf/2304.02080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06459v2","updated":"2023-04-04T19:05:47Z","published":"2023-02-13T15:39:08Z","title":"Encoding Sentence Position in Context-Aware Neural Machine Translation\n  with Concatenation","summary":"  Context-aware translation can be achieved by processing a concatenation of\nconsecutive sentences with the standard Transformer architecture. This paper\ninvestigates the intuitive idea of providing the model with explicit\ninformation about the position of the sentences contained in the concatenation\nwindow. We compare various methods to encode sentence positions into token\nrepresentations, including novel methods. Our results show that the Transformer\nbenefits from certain sentence position encoding methods on English to Russian\ntranslation if trained with a context-discounted loss (Lupo et al., 2022).\nHowever, the same benefits are not observed in English to German. Further\nempirical efforts are necessary to define the conditions under which the\nproposed approach is beneficial.\n","authors":["Lorenzo Lupo","Marco Dinarelli","Laurent Besacier"],"pdf_url":"https://arxiv.org/pdf/2302.06459v2.pdf","comment":"Insights2023 camera-ready"},{"id":"http://arxiv.org/abs/2301.02748v3","updated":"2023-04-04T18:48:22Z","published":"2023-01-06T23:34:52Z","title":"Conditional Generation of Paired Antibody Chain Sequences through\n  Encoder-Decoder Language Model","summary":"  Protein language models (LMs) have been successful in sequence, structural\nand functional predictions. However, currently, protein LMs are limited to\nencoder- or decoder-only architectures for single sequences while many\nbiological contexts involve protein-protein interactions. Here, we introduce\npAbT5, which models antibody chain pairing as forward- and back-translations\nusing a T5-based architecture. We show that pAbT5 accurately reflects chain\npairing through sequence generation. Our protein LM generates variable-length\nsequences and its next-word prediction probability agrees with\nposition-specific scoring matrix from sequence alignment. Like other works in\nprotein LM, pAbT5 performs state-of-the-art unsupervised prediction on\nexperimental measurements. To the best of our knowledge, pAbT5 is the first\ngenerative encoder-decoder protein LM for protein-protein interactions.\n","authors":["Simon K. S. Chu","Kathy Y. Wei"],"pdf_url":"https://arxiv.org/pdf/2301.02748v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02034v1","updated":"2023-04-04T18:00:01Z","published":"2023-04-04T18:00:01Z","title":"Effective Theory of Transformers at Initialization","summary":"  We perform an effective-theory analysis of forward-backward signal\npropagation in wide and deep Transformers, i.e., residual neural networks with\nmulti-head self-attention blocks and multilayer perceptron blocks. This\nanalysis suggests particular width scalings of initialization and training\nhyperparameters for these models. We then take up such suggestions, training\nVision and Language Transformers in practical setups.\n","authors":["Emily Dinan","Sho Yaida","Susan Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.02034v1.pdf","comment":"64 pages, 5 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2304.02013v1","updated":"2023-04-04T17:59:22Z","published":"2023-04-04T17:59:22Z","title":"NPC: Neural Point Characters from Video","summary":"  High-fidelity human 3D models can now be learned directly from videos,\ntypically by combining a template-based surface model with neural\nrepresentations. However, obtaining a template surface requires expensive\nmulti-view capture systems, laser scans, or strictly controlled conditions.\nPrevious methods avoid using a template but rely on a costly or ill-posed\nmapping from observation to canonical space. We propose a hybrid point-based\nrepresentation for reconstructing animatable characters that does not require\nan explicit surface model, while being generalizable to novel poses. For a\ngiven video, our method automatically produces an explicit set of 3D points\nrepresenting approximate canonical geometry, and learns an articulated\ndeformation model that produces pose-dependent point transformations. The\npoints serve both as a scaffold for high-frequency neural features and an\nanchor for efficiently mapping between observation and canonical space. We\ndemonstrate on established benchmarks that our representation overcomes\nlimitations of prior work operating in either canonical or in observation\nspace. Moreover, our automatic point extraction approach enables learning\nmodels of human and animal characters alike, matching the performance of the\nmethods using rigged surface templates despite being more general. Project\nwebsite: https://lemonatsu.github.io/npc/\n","authors":["Shih-Yang Su","Timur Bagautdinov","Helge Rhodin"],"pdf_url":"https://arxiv.org/pdf/2304.02013v1.pdf","comment":"Project website: https://lemonatsu.github.io/npc/"},{"id":"http://arxiv.org/abs/2304.02012v1","updated":"2023-04-04T17:59:14Z","published":"2023-04-04T17:59:14Z","title":"EGC: Image Generation and Classification via a Single Energy-Based Model","summary":"  Learning image classification and image generation using the same set of\nnetwork parameters is a challenging problem. Recent advanced approaches perform\nwell in one task often exhibit poor performance in the other. This work\nintroduces an energy-based classifier and generator, namely EGC, which can\nachieve superior performance in both tasks using a single neural network.\nUnlike a conventional classifier that outputs a label given an image (i.e., a\nconditional distribution $p(y|\\mathbf{x})$), the forward pass in EGC is a\nclassifier that outputs a joint distribution $p(\\mathbf{x},y)$, enabling an\nimage generator in its backward pass by marginalizing out the label $y$. This\nis done by estimating the energy and classification probability given a noisy\nimage in the forward pass, while denoising it using the score function\nestimated in the backward pass. EGC achieves competitive generation results\ncompared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN\nChurch, while achieving superior classification accuracy and robustness against\nadversarial attacks on CIFAR-10. This work represents the first successful\nattempt to simultaneously excel in both tasks using a single set of network\nparameters. We believe that EGC bridges the gap between discriminative and\ngenerative learning.\n","authors":["Qiushan Guo","Chuofan Ma","Yi Jiang","Zehuan Yuan","Yizhou Yu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2304.02012v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2304.02010v1","updated":"2023-04-04T17:59:04Z","published":"2023-04-04T17:59:04Z","title":"Multi-Level Contrastive Learning for Dense Prediction Task","summary":"  In this work, we present Multi-Level Contrastive Learning for Dense\nPrediction Task (MCL), an efficient self-supervised method for learning\nregion-level feature representation for dense prediction tasks. Our method is\nmotivated by the three key factors in detection: localization, scale\nconsistency and recognition. To explicitly encode absolute position and scale\ninformation, we propose a novel pretext task that assembles multi-scale images\nin a montage manner to mimic multi-object scenarios. Unlike the existing\nimage-level self-supervised methods, our method constructs a multi-level\ncontrastive loss that considers each sub-region of the montage image as a\nsingleton. Our method enables the neural network to learn regional semantic\nrepresentations for translation and scale consistency while reducing\npre-training epochs to the same as supervised pre-training. Extensive\nexperiments demonstrate that MCL consistently outperforms the recent\nstate-of-the-art methods on various datasets with significant margins. In\nparticular, MCL obtains 42.5 AP$^\\mathrm{bb}$ and 38.3 AP$^\\mathrm{mk}$ on COCO\nwith the 1x schedule fintuning, when using Mask R-CNN with R50-FPN backbone\npre-trained with 100 epochs. In comparison to MoCo, our method surpasses their\nperformance by 4.0 AP$^\\mathrm{bb}$ and 3.1 AP$^\\mathrm{mk}$. Furthermore, we\nexplore the alignment between pretext task and downstream tasks. We extend our\npretext task to supervised pre-training, which achieves a similar performance\nto self-supervised learning. This result demonstrates the importance of the\nalignment between pretext task and downstream tasks, indicating the potential\nfor wider applicability of our method beyond self-supervised settings.\n","authors":["Qiushan Guo","Yizhou Yu","Yi Jiang","Jiannan Wu","Zehuan Yuan","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2304.02010v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2304.02009v1","updated":"2023-04-04T17:59:03Z","published":"2023-04-04T17:59:03Z","title":"OrienterNet: Visual Localization in 2D Public Maps with Neural Matching","summary":"  Humans can orient themselves in their 3D environments using simple 2D maps.\nDifferently, algorithms for visual localization mostly rely on complex 3D point\nclouds that are expensive to build, store, and maintain over time. We bridge\nthis gap by introducing OrienterNet, the first deep neural network that can\nlocalize an image with sub-meter accuracy using the same 2D semantic maps that\nhumans use. OrienterNet estimates the location and orientation of a query image\nby matching a neural Bird's-Eye View with open and globally available maps from\nOpenStreetMap, enabling anyone to localize anywhere such maps are available.\nOrienterNet is supervised only by camera poses but learns to perform semantic\nmatching with a wide range of map elements in an end-to-end manner. To enable\nthis, we introduce a large crowd-sourced dataset of images captured across 12\ncities from the diverse viewpoints of cars, bikes, and pedestrians. OrienterNet\ngeneralizes to new datasets and pushes the state of the art in both robotics\nand AR scenarios. The code and trained model will be released publicly.\n","authors":["Paul-Edouard Sarlin","Daniel DeTone","Tsun-Yi Yang","Armen Avetisyan","Julian Straub","Tomasz Malisiewicz","Samuel Rota Bulo","Richard Newcombe","Peter Kontschieder","Vasileios Balntas"],"pdf_url":"https://arxiv.org/pdf/2304.02009v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2304.02008v1","updated":"2023-04-04T17:58:14Z","published":"2023-04-04T17:58:14Z","title":"GlueStick: Robust Image Matching by Sticking Points and Lines Together","summary":"  Line segments are powerful features complementary to points. They offer\nstructural cues, robust to drastic viewpoint and illumination changes, and can\nbe present even in texture-less areas. However, describing and matching them is\nmore challenging compared to points due to partial occlusions, lack of texture,\nor repetitiveness. This paper introduces a new matching paradigm, where points,\nlines, and their descriptors are unified into a single wireframe structure. We\npropose GlueStick, a deep matching Graph Neural Network (GNN) that takes two\nwireframes from different images and leverages the connectivity information\nbetween nodes to better glue them together. In addition to the increased\nefficiency brought by the joint matching, we also demonstrate a large boost of\nperformance when leveraging the complementary nature of these two features in a\nsingle architecture. We show that our matching strategy outperforms the\nstate-of-the-art approaches independently matching line segments and points for\na wide variety of datasets and tasks. The code is available at\nhttps://github.com/cvg/GlueStick.\n","authors":["Rémi Pautrat","Iago Suárez","Yifan Yu","Marc Pollefeys","Viktor Larsson"],"pdf_url":"https://arxiv.org/pdf/2304.02008v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2304.02001v1","updated":"2023-04-04T17:55:03Z","published":"2023-04-04T17:55:03Z","title":"MonoHuman: Animatable Human Neural Field from Monocular Video","summary":"  Animating virtual avatars with free-view control is crucial for various\napplications like virtual reality and digital entertainment. Previous studies\nhave attempted to utilize the representation power of the neural radiance field\n(NeRF) to reconstruct the human body from monocular videos. Recent works\npropose to graft a deformation network into the NeRF to further model the\ndynamics of the human neural field for animating vivid human motions. However,\nsuch pipelines either rely on pose-dependent representations or fall short of\nmotion coherency due to frame-independent optimization, making it difficult to\ngeneralize to unseen pose sequences realistically. In this paper, we propose a\nnovel framework MonoHuman, which robustly renders view-consistent and\nhigh-fidelity avatars under arbitrary novel poses. Our key insight is to model\nthe deformation field with bi-directional constraints and explicitly leverage\nthe off-the-peg keyframe information to reason the feature correlations for\ncoherent results. Specifically, we first propose a Shared Bidirectional\nDeformation module, which creates a pose-independent generalizable deformation\nfield by disentangling backward and forward deformation correspondences into\nshared skeletal motion weight and separate non-rigid motions. Then, we devise a\nForward Correspondence Search module, which queries the correspondence feature\nof keyframes to guide the rendering network. The rendered results are thus\nmulti-view consistent with high fidelity, even under challenging novel pose\nsettings. Extensive experiments demonstrate the superiority of our proposed\nMonoHuman over state-of-the-art methods.\n","authors":["Zhengming Yu","Wei Cheng","Xian Liu","Wayne Wu","Kwan-Yee Lin"],"pdf_url":"https://arxiv.org/pdf/2304.02001v1.pdf","comment":"15 pages, 14 figures. Accepted to CVPR 2023. Project page:\n  https://yzmblog.github.io/projects/MonoHuman/"},{"id":"http://arxiv.org/abs/2304.01999v1","updated":"2023-04-04T17:54:32Z","published":"2023-04-04T17:54:32Z","title":"Revisiting the Evaluation of Image Synthesis with GANs","summary":"  A good metric, which promises a reliable comparison between solutions, is\nessential to a well-defined task. Unlike most vision tasks that have per-sample\nground-truth, image synthesis targets generating \\emph{unseen} data and hence\nis usually evaluated with a distributional distance between one set of real\nsamples and another set of generated samples. This work provides an empirical\nstudy on the evaluation of synthesis performance by taking the popular\ngenerative adversarial networks (GANs) as a representative of generative\nmodels. In particular, we make in-depth analyses on how to represent a data\npoint in the feature space, how to calculate a fair distance using selected\nsamples, and how many instances to use from each set. Experiments on multiple\ndatasets and settings suggest that (1) a group of models including both\nCNN-based and ViT-based architectures serve as reliable and robust feature\nextractors, (2) Centered Kernel Alignment (CKA) enables better comparison\nacross various extractors and hierarchical layers in one model, and (3) CKA\nshows satisfactory sample efficiency and complements existing metrics\n(\\textit{e.g.}, FID) in characterizing the similarity between two internal data\ncorrelations. These findings help us design a new measurement system, based on\nwhich we re-evaluate the state-of-the-art generative models in a consistent and\nreliable way.\n","authors":["Mengping Yang","Ceyuan Yang","Yichi Zhang","Qingyan Bai","Yujun Shen","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2304.01999v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2304.01994v1","updated":"2023-04-04T17:52:49Z","published":"2023-04-04T17:52:49Z","title":"DWA: Differential Wavelet Amplifier for Image Super-Resolution","summary":"  This work introduces Differential Wavelet Amplifier (DWA), a drop-in module\nfor wavelet-based image Super-Resolution (SR). DWA invigorates an approach\nrecently receiving less attention, namely Discrete Wavelet Transformation\n(DWT). DWT enables an efficient image representation for SR and reduces the\nspatial area of its input by a factor of 4, the overall model size, and\ncomputation cost, framing it as an attractive approach for sustainable ML. Our\nproposed DWA model improves wavelet-based SR models by leveraging the\ndifference between two convolutional filters to refine relevant feature\nextraction in the wavelet domain, emphasizing local contrasts and suppressing\ncommon noise in the input signals. We show its effectiveness by integrating it\ninto existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear\nimprovement in classical SR tasks. Moreover, DWA enables a direct application\nof DWSR and MWCNN to input image space, reducing the DWT representation\nchannel-wise since it omits traditional DWT.\n","authors":["Brian Moser","Stanislav Frolov","Federico Raue","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2304.01994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01992v1","updated":"2023-04-04T17:50:30Z","published":"2023-04-04T17:50:30Z","title":"Cross-modulated Few-shot Image Generation for Colorectal Tissue\n  Classification","summary":"  In this work, we propose a few-shot colorectal tissue image generation method\nfor addressing the scarcity of histopathological training data for rare cancer\ntissues. Our few-shot generation method, named XM-GAN, takes one base and a\npair of reference tissue images as input and generates high-quality yet diverse\nimages. Within our XM-GAN, a novel controllable fusion block densely aggregates\nlocal regions of reference images based on their similarity to those in the\nbase image, resulting in locally consistent features. To the best of our\nknowledge, we are the first to investigate few-shot generation in colorectal\ntissue images. We evaluate our few-shot colorectral tissue image generation by\nperforming extensive qualitative, quantitative and subject specialist\n(pathologist) based evaluations. Specifically, in specialist-based evaluation,\npathologists could differentiate between our XM-GAN generated tissue images and\nreal images only 55% time. Moreover, we utilize these generated images as data\naugmentation to address the few-shot tissue image classification task,\nachieving a gain of 4.4% in terms of mean accuracy over the vanilla few-shot\nclassifier. Code: \\url{https://github.com/VIROBO-15/XM-GAN}\n","authors":["Amandeep Kumar","Ankan kumar Bhunia","Sanath Narayan","Hisham Cholakkal","Rao Muhammad Anwer","Jorma Laaksonen","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2304.01992v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2209.01194v4","updated":"2023-04-04T17:48:17Z","published":"2022-09-02T17:44:50Z","title":"CLONeR: Camera-Lidar Fusion for Occupancy Grid-aided Neural\n  Representations","summary":"  Recent advances in neural radiance fields (NeRFs) achieve state-of-the-art\nnovel view synthesis and facilitate dense estimation of scene properties.\nHowever, NeRFs often fail for large, unbounded scenes that are captured under\nvery sparse views with the scene content concentrated far away from the camera,\nas is typical for field robotics applications. In particular, NeRF-style\nalgorithms perform poorly: (1) when there are insufficient views with little\npose diversity, (2) when scenes contain saturation and shadows, and (3) when\nfinely sampling large unbounded scenes with fine structures becomes\ncomputationally intensive.\n  This paper proposes CLONeR, which significantly improves upon NeRF by\nallowing it to model large outdoor driving scenes that are observed from sparse\ninput sensor views. This is achieved by decoupling occupancy and color learning\nwithin the NeRF framework into separate Multi-Layer Perceptrons (MLPs) trained\nusing LiDAR and camera data, respectively. In addition, this paper proposes a\nnovel method to build differentiable 3D Occupancy Grid Maps (OGM) alongside the\nNeRF model, and leverage this occupancy grid for improved sampling of points\nalong a ray for volumetric rendering in metric space.\n  Through extensive quantitative and qualitative experiments on scenes from the\nKITTI dataset, this paper demonstrates that the proposed method outperforms\nstate-of-the-art NeRF models on both novel view synthesis and dense depth\nprediction tasks when trained on sparse input data.\n","authors":["Alexandra Carlson","Manikandasriram Srinivasan Ramanagopal","Nathan Tseng","Matthew Johnson-Roberson","Ram Vasudevan","Katherine A. Skinner"],"pdf_url":"https://arxiv.org/pdf/2209.01194v4.pdf","comment":"first two authors equally contributed"},{"id":"http://arxiv.org/abs/2304.01988v1","updated":"2023-04-04T17:46:20Z","published":"2023-04-04T17:46:20Z","title":"SM/VIO: Robust Underwater State Estimation Switching Between Model-based\n  and Visual Inertial Odometry","summary":"  This paper addresses the robustness problem of visual-inertial state\nestimation for underwater operations. Underwater robots operating in a\nchallenging environment are required to know their pose at all times. All\nvision-based localization schemes are prone to failure due to poor visibility\nconditions, color loss, and lack of features. The proposed approach utilizes a\nmodel of the robot's kinematics together with proprioceptive sensors to\nmaintain the pose estimate during visual-inertial odometry (VIO) failures.\nFurthermore, the trajectories from successful VIO and the ones from the\nmodel-driven odometry are integrated in a coherent set that maintains a\nconsistent pose at all times. Health-monitoring tracks the VIO process ensuring\ntimely switches between the two estimators. Finally, loop closure is\nimplemented on the overall trajectory. The resulting framework is a robust\nestimator switching between model-based and visual-inertial odometry (SM/VIO).\nExperimental results from numerous deployments of the Aqua2 vehicle demonstrate\nthe robustness of our approach over coral reefs and a shipwreck.\n","authors":["Bharat Joshi","Hunter Damron","Sharmin Rahman","Ioannis Rekleitis"],"pdf_url":"https://arxiv.org/pdf/2304.01988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01986v1","updated":"2023-04-04T17:45:06Z","published":"2023-04-04T17:45:06Z","title":"USTC FLICAR: A Multisensor Fusion Dataset of LiDAR-Inertial-Camera for\n  Heavy-duty Autonomous Aerial Work Robots","summary":"  In this paper, we present the USTC FLICAR Dataset, which is dedicated to the\ndevelopment of simultaneous localization and mapping and precise 3D\nreconstruction of the workspace for heavy-duty autonomous aerial work robots.\nIn recent years, numerous public datasets have played significant roles in the\nadvancement of autonomous cars and unmanned aerial vehicles (UAVs). However,\nthese two platforms differ from aerial work robots: UAVs are limited in their\npayload capacity, while cars are restricted to two-dimensional movements. To\nfill this gap, we create the Giraffe mapping robot based on a bucket truck,\nwhich is equipped with a variety of well-calibrated and synchronized sensors:\nfour 3D LiDARs, two stereo cameras, two monocular cameras, Inertial Measurement\nUnits (IMUs), and a GNSS/INS system. A laser tracker is used to record the\nmillimeter-level ground truth positions. We also make its ground twin, the\nOkapi mapping robot, to gather data for comparison. The proposed dataset\nextends the typical autonomous driving sensing suite to aerial scenes.\nTherefore, the dataset is named FLICAR to denote flying cars. We believe this\ndataset can also represent the flying car scenarios, specifically the takeoff\nand landing of VTOL (Vertical Takeoff and Landing) flying cars. The dataset is\navailable for download at: https://ustc-flicar.github.io.\n","authors":["Ziming Wang","Yujiang Liu","Yifan Duan","Xingchen Li","Xinran Zhang","Jianmin Ji","Erbao Dong","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01986v1.pdf","comment":"16 pages, 19 figures"},{"id":"http://arxiv.org/abs/2304.01973v1","updated":"2023-04-04T17:31:15Z","published":"2023-04-04T17:31:15Z","title":"ERM++: An Improved Baseline for Domain Generalization","summary":"  Multi-source Domain Generalization (DG) measures a classifier's ability to\ngeneralize to new distributions of data it was not trained on, given several\ntraining domains. While several multi-source DG methods have been proposed,\nthey incur additional complexity during training by using domain labels. Recent\nwork has shown that a well-tuned Empirical Risk Minimization (ERM) training\nprocedure, that is simply minimizing the empirical risk on the source domains,\ncan outperform most existing DG methods. We identify several key candidate\ntechniques to further improve ERM performance, such as better utilization of\ntraining data, model parameter selection, and weight-space regularization. We\ncall the resulting method ERM++, and show it significantly improves the\nperformance of DG on five multi-source datasets by over 5% compared to standard\nERM, and beats state-of-the-art despite being less computationally expensive.\nAdditionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset, a\nchallenging DG benchmark. We hope that ERM++ becomes a strong baseline for\nfuture DG research. Code is released at\nhttps://github.com/piotr-teterwak/erm_plusplus.\n","authors":["Piotr Teterwak","Kuniaki Saito","Theodoros Tsiligkaridis","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2304.01973v1.pdf","comment":"An improved baseline for Domain Generalization"},{"id":"http://arxiv.org/abs/2304.01963v1","updated":"2023-04-04T17:13:22Z","published":"2023-04-04T17:13:22Z","title":"Model-corrected learned primal-dual models for fast limited-view\n  photoacoustic tomography","summary":"  Learned iterative reconstructions hold great promise to accelerate\ntomographic imaging with empirical robustness to model perturbations.\nNevertheless, an adoption for photoacoustic tomography is hindered by the need\nto repeatedly evaluate the computational expensive forward model. Computational\nfeasibility can be obtained by the use of fast approximate models, but a need\nto compensate model errors arises. In this work we advance the methodological\nand theoretical basis for model corrections in learned image reconstructions by\nembedding the model correction in a learned primal-dual framework. Here, the\nmodel correction is jointly learned in data space coupled with a learned\nupdating operator in image space within an unrolled end-to-end learned\niterative reconstruction approach. The proposed formulation allows an extension\nto a primal-dual deep equilibrium model providing fixed-point convergence as\nwell as reduced memory requirements for training. We provide theoretical and\nempirical insights into the proposed models with numerical validation in a\nrealistic 2D limited-view setting. The model-corrected learned primal-dual\nmethods show excellent reconstruction quality with fast inference times and\nthus providing a methodological basis for real-time capable and scalable\niterative reconstructions in photoacoustic tomography.\n","authors":["Andreas Hauptmann","Jenni Poimala"],"pdf_url":"https://arxiv.org/pdf/2304.01963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01962v1","updated":"2023-04-04T17:13:06Z","published":"2023-04-04T17:13:06Z","title":"Ethylene Leak Detection Based on Infrared Imaging: A Benchmark","summary":"  Ethylene leakage detection has become one of the most important research\ndirections in the field of target detection due to the fact that ethylene\nleakage in the petrochemical industry is closely related to production safety\nand environmental pollution. Under infrared conditions, there are many factors\nthat affect the texture characteristics of ethylene, such as ethylene\nconcentration, background, and so on. We find that the detection criteria used\nin infrared imaging ethylene leakage detection research cannot fully reflect\nreal-world production conditions, which is not conducive to evaluate the\nperformance of current image-based target detection methods. Therefore, we\ncreate a new infrared image dataset of ethylene leakage with different\nconcentrations and backgrounds, including 54275 images. We use the proposed\ndataset benchmark to evaluate seven advanced image-based target detection\nalgorithms. Experimental results demonstrate the performance and limitations of\nexisting algorithms, and the dataset benchmark has good versatility and\neffectiveness.\n","authors":["Xuanchao Ma","Yuchen Liu"],"pdf_url":"https://arxiv.org/pdf/2304.01962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01961v1","updated":"2023-04-04T17:11:34Z","published":"2023-04-04T17:11:34Z","title":"AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia\n  Content Creation","summary":"  This paper presents the AToMiC (Authoring Tools for Multimedia Content)\ndataset, designed to advance research in image/text cross-modal retrieval.\nWhile vision-language pretrained transformers have led to significant\nimprovements in retrieval effectiveness, existing research has relied on\nimage-caption datasets that feature only simplistic image-text relationships\nand underspecified user models of retrieval tasks. To address the gap between\nthese oversimplified settings and real-world applications for multimedia\ncontent creation, we introduce a new approach for building retrieval test\ncollections. We leverage hierarchical structures and diverse domains of texts,\nstyles, and types of images, as well as large-scale image-document associations\nembedded in Wikipedia. We formulate two tasks based on a realistic user model\nand validate our dataset through retrieval experiments using baseline models.\nAToMiC offers a testbed for scalable, diverse, and reproducible multimedia\nretrieval research. Finally, the dataset provides the basis for a dedicated\ntrack at the 2023 Text Retrieval Conference (TREC), and is publicly available\nat https://github.com/TREC-AToMiC/AToMiC.\n","authors":["Jheng-Hong Yang","Carlos Lassance","Rafael Sampaio de Rezende","Krishna Srinivasan","Miriam Redi","Stéphane Clinchant","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2304.01961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01959v1","updated":"2023-04-04T17:07:06Z","published":"2023-04-04T17:07:06Z","title":"Randomized Adversarial Style Perturbations for Domain Generalization","summary":"  We propose a novel domain generalization technique, referred to as Randomized\nAdversarial Style Perturbation (RASP), which is motivated by the observation\nthat the characteristics of each domain are captured by the feature statistics\ncorresponding to style. The proposed algorithm perturbs the style of a feature\nin an adversarial direction towards a randomly selected class, and makes the\nmodel learn against being misled by the unexpected styles observed in unseen\ntarget domains. While RASP is effective to handle domain shifts, its naive\nintegration into the training procedure might degrade the capability of\nlearning knowledge from source domains because it has no restriction on the\nperturbations of representations. This challenge is alleviated by Normalized\nFeature Mixup (NFM), which facilitates the learning of the original features\nwhile achieving robustness to perturbed representations via their mixup during\ntraining. We evaluate the proposed algorithm via extensive experiments on\nvarious benchmarks and show that our approach improves domain generalization\nperformance, especially in large-scale benchmarks.\n","authors":["Taehoon Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2304.01959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12741v2","updated":"2023-04-04T17:03:59Z","published":"2023-03-07T22:31:28Z","title":"A Method for Animating Children's Drawings of the Human Figure","summary":"  Children's drawings have a wonderful inventiveness, creativity, and variety\nto them. We present a system that automatically animates children's drawings of\nthe human figure, is robust to the variance inherent in these depictions, and\nis simple and straightforward enough for anyone to use. We demonstrate the\nvalue and broad appeal of our approach by building and releasing the Animated\nDrawings Demo, a freely available public website that has been used by millions\nof people around the world. We present a set of experiments exploring the\namount of training data needed for fine-tuning, as well as a perceptual study\ndemonstrating the appeal of a novel twisted perspective retargeting technique.\nFinally, we introduce the Amateur Drawings Dataset, a first-of-its-kind\nannotated dataset, collected via the public demo, containing over 178,000\namateur drawings and corresponding user-accepted character bounding boxes,\nsegmentation masks, and joint location annotations.\n","authors":["Harrison Jesse Smith","Qingyuan Zheng","Yifei Li","Somya Jain","Jessica K. Hodgins"],"pdf_url":"https://arxiv.org/pdf/2303.12741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12149v3","updated":"2023-04-04T16:53:07Z","published":"2023-03-06T16:58:27Z","title":"SPARTAN: Self-supervised Spatiotemporal Transformers Approach to Group\n  Activity Recognition","summary":"  In this paper, we propose a new, simple, and effective Self-supervised\nSpatio-temporal Transformers (SPARTAN) approach to Group Activity Recognition\n(GAR) using unlabeled video data. Given a video, we create local and global\nSpatio-temporal views with varying spatial patch sizes and frame rates. The\nproposed self-supervised objective aims to match the features of these\ncontrasting views representing the same video to be consistent with the\nvariations in spatiotemporal domains. To the best of our knowledge, the\nproposed mechanism is one of the first works to alleviate the weakly supervised\nsetting of GAR using the encoders in video transformers. Furthermore, using the\nadvantage of transformer models, our proposed approach supports long-term\nrelationship modeling along spatio-temporal dimensions. The proposed SPARTAN\napproach performs well on two group activity recognition benchmarks, including\nNBA and Volleyball datasets, by surpassing the state-of-the-art results by a\nsignificant margin in terms of MCA and MPCA metrics.\n","authors":["Naga VS Raviteja Chappa","Pha Nguyen","Alexander H Nelson","Han-Seok Seo","Xin Li","Page Daniel Dobbs","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2303.12149v3.pdf","comment":"Accepted to CVPRW 2023; 11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2209.05135v2","updated":"2023-04-04T16:30:00Z","published":"2022-09-12T10:42:26Z","title":"Signs of Language: Embodied Sign Language Fingerspelling Acquisition\n  from Demonstrations for Human-Robot Interaction","summary":"  Learning fine-grained movements is a challenging topic in robotics,\nparticularly in the context of robotic hands. One specific instance of this\nchallenge is the acquisition of fingerspelling sign language in robots. In this\npaper, we propose an approach for learning dexterous motor imitation from video\nexamples without additional information. To achieve this, we first build a URDF\nmodel of a robotic hand with a single actuator for each joint. We then leverage\npre-trained deep vision models to extract the 3D pose of the hand from RGB\nvideos. Next, using state-of-the-art reinforcement learning algorithms for\nmotion imitation (namely, proximal policy optimization and soft actor-critic),\nwe train a policy to reproduce the movement extracted from the demonstrations.\nWe identify the optimal set of hyperparameters for imitation based on a\nreference motion. Finally, we demonstrate the generalizability of our approach\nby testing it on six different tasks, corresponding to fingerspelled letters.\nOur results show that our approach is able to successfully imitate these\nfine-grained movements without additional information, highlighting its\npotential for real-world applications in robotics.\n","authors":["Federico Tavella","Aphrodite Galata","Angelo Cangelosi"],"pdf_url":"https://arxiv.org/pdf/2209.05135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.05711v3","updated":"2023-04-04T16:27:38Z","published":"2022-03-11T01:45:33Z","title":"Synopses of Movie Narratives: a Video-Language Dataset for Story\n  Understanding","summary":"  Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SYMON), containing\n5,193 video summaries of popular movies and TV series. SYMON captures\nnaturalistic story-telling videos for human audience made by human creators. As\na prototypical and naturalistic story dataset, SYMON features high coverage of\nmultimodal story events, abundant mental-state descriptions, and large semantic\ngaps between the visual and the textual modalities. We establish benchmarks on\nvideo-text retrieval and zero-shot alignment on movie summary videos, which\nshowcase the importance of in-domain data in story understanding. With SYMON,\nwe hope to lay the groundwork for progress in multimodal story understanding.\n","authors":["Yidan Sun","Qin Chao","Yangfeng Ji","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2203.05711v3.pdf","comment":"25 pages, 17 figures"},{"id":"http://arxiv.org/abs/2212.07646v2","updated":"2023-04-04T16:27:18Z","published":"2022-12-15T07:39:50Z","title":"Adaptive Multi-Agent Continuous Learning System","summary":"  We propose an adaptive multi-agent clustering recognition system that can be\nself-supervised driven, based on a temporal sequences continuous learning\nmechanism with adaptability. The system is designed to use some different\nfunctional agents to build up a connection structure to improve adaptability to\ncope with environmental diverse demands, by predicting the input of the agent\nto drive the agent to achieve the act of clustering recognition of sequences\nusing the traditional algorithmic approach. Finally, the feasibility\nexperiments of video behavior clustering demonstrate the feasibility of the\nsystem to cope with dynamic situations. Our work is placed\nhere\\footnote{https://github.com/qian-git/MAMMALS}.\n","authors":["Xingyu Qian","Aximu Yuemaier","Longfei Liang","Wen-Chi Yang","Xiaogang Chen","Shunfen Li","Weibang Dai","Zhitang Song"],"pdf_url":"https://arxiv.org/pdf/2212.07646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16191v3","updated":"2023-04-04T16:19:50Z","published":"2023-03-28T17:54:56Z","title":"Hard Nominal Example-aware Template Mutual Matching for Industrial\n  Anomaly Detection","summary":"  Anomaly detectors are widely used in industrial production to detect and\nlocalize unknown defects in query images. These detectors are trained on\nnominal images and have shown success in distinguishing anomalies from most\nnormal samples. However, hard-nominal examples are scattered and far apart from\nmost normalities, they are often mistaken for anomalies by existing anomaly\ndetectors. To address this problem, we propose a simple yet efficient method:\n\\textbf{H}ard Nominal \\textbf{E}xample-aware \\textbf{T}emplate \\textbf{M}utual\n\\textbf{M}atching (HETMM). Specifically, \\textit{HETMM} aims to construct a\nrobust prototype-based decision boundary, which can precisely distinguish\nbetween hard-nominal examples and anomalies, yielding fewer false-positive and\nmissed-detection rates. Moreover, \\textit{HETMM} mutually explores the\nanomalies in two directions between queries and the template set, and thus it\nis capable to capture the logical anomalies. This is a significant advantage\nover most anomaly detectors that frequently fail to detect logical anomalies.\nAdditionally, to meet the speed-accuracy demands, we further propose\n\\textbf{P}ixel-level \\textbf{T}emplate \\textbf{S}election (PTS) to streamline\nthe original template set. \\textit{PTS} selects cluster centres and\nhard-nominal examples to form a tiny set, maintaining the original decision\nboundaries. Comprehensive experiments on five real-world datasets demonstrate\nthat our methods yield outperformance than existing advances under the\nreal-time inference speed. Furthermore, \\textit{HETMM} can be hot-updated by\ninserting novel samples, which may promptly address some incremental learning\nissues.\n","authors":["Zixuan Chen","Xiaohua Xie","Lingxiao Yang","Jianhuang Lai"],"pdf_url":"https://arxiv.org/pdf/2303.16191v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01917v1","updated":"2023-04-04T16:14:39Z","published":"2023-04-04T16:14:39Z","title":"Strong Baselines for Parameter Efficient Few-Shot Fine-tuning","summary":"  Few-shot classification (FSC) entails learning novel classes given only a few\nexamples per class after a pre-training (or meta-training) phase on a set of\nbase classes. Recent works have shown that simply fine-tuning a pre-trained\nVision Transformer (ViT) on new test classes is a strong approach for FSC.\nFine-tuning ViTs, however, is expensive in time, compute and storage. This has\nmotivated the design of parameter efficient fine-tuning (PEFT) methods which\nfine-tune only a fraction of the Transformer's parameters. While these methods\nhave shown promise, inconsistencies in experimental conditions make it\ndifficult to disentangle their advantage from other experimental factors\nincluding the feature extractor architecture, pre-trained initialization and\nfine-tuning algorithm, amongst others. In our paper, we conduct a large-scale,\nexperimentally consistent, empirical analysis to study PEFTs for few-shot image\nclassification. Through a battery of over 1.8k controlled experiments on\nlarge-scale few-shot benchmarks including Meta-Dataset (MD) and ORBIT, we\nuncover novel insights on PEFTs that cast light on their efficacy in\nfine-tuning ViTs for few-shot classification. Through our controlled empirical\nstudy, we have two main findings: (i) Fine-tuning just the LayerNorm parameters\n(which we call LN-Tune) during few-shot adaptation is an extremely strong\nbaseline across ViTs pre-trained with both self-supervised and supervised\nobjectives, (ii) For self-supervised ViTs, we find that simply learning a set\nof scaling parameters for each attention matrix (which we call AttnScale) along\nwith a domain-residual adapter (DRA) module leads to state-of-the-art\nperformance (while being $\\sim\\!$ 9$\\times$ more parameter-efficient) on MD.\nOur extensive empirical findings set strong baselines and call for rethinking\nthe current design of PEFT methods for FSC.\n","authors":["Samyadeep Basu","Daniela Massiceti","Shell Xu Hu","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2304.01917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11313v2","updated":"2023-04-04T16:12:03Z","published":"2023-03-20T17:52:24Z","title":"CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D\n  Recognition","summary":"  Vision-Language models like CLIP have been widely adopted for various tasks\ndue to their impressive zero-shot capabilities. However, CLIP is not suitable\nfor extracting 3D geometric features as it was trained on only images and text\nby natural language supervision. We work on addressing this limitation and\npropose a new framework termed CG3D (CLIP Goes 3D) where a 3D encoder is\nlearned to exhibit zero-shot capabilities. CG3D is trained using triplets of\npointclouds, corresponding rendered 2D images, and texts using natural language\nsupervision. To align the features in a multimodal embedding space, we utilize\ncontrastive loss on 3D features obtained from the 3D encoder, as well as visual\nand text features extracted from CLIP. We note that the natural images used to\ntrain CLIP and the rendered 2D images in CG3D have a distribution shift.\nAttempting to train the visual and text encoder to account for this shift\nresults in catastrophic forgetting and a notable decrease in performance. To\nsolve this, we employ prompt tuning and introduce trainable parameters in the\ninput space to shift CLIP towards the 3D pre-training dataset utilized in CG3D.\nWe extensively test our pre-trained CG3D framework and demonstrate its\nimpressive capabilities in zero-shot, open scene understanding, and retrieval\ntasks. Further, it also serves as strong starting weights for fine-tuning in\ndownstream 3D recognition tasks.\n","authors":["Deepti Hegde","Jeya Maria Jose Valanarasu","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2303.11313v2.pdf","comment":"Website: https://jeya-maria-jose.github.io/cg3d-web/"},{"id":"http://arxiv.org/abs/2304.01900v1","updated":"2023-04-04T15:49:01Z","published":"2023-04-04T15:49:01Z","title":"PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain\n  Gap Using Pose-Preserved Text-to-Image Diffusion","summary":"  Recently, significant advancements have been made in 3D generative models,\nhowever training these models across diverse domains is challenging and\nrequires an huge amount of training data and knowledge of pose distribution.\nText-guided domain adaptation methods have allowed the generator to be adapted\nto the target domains using text prompts, thereby obviating the need for\nassembling numerous data. Recently, DATID-3D presents impressive quality of\nsamples in text-guided domain, preserving diversity in text by leveraging\ntext-to-image diffusion. However, adapting 3D generators to domains with\nsignificant domain gaps from the source domain still remains challenging due to\nissues in current text-to-image diffusion models as following: 1) shape-pose\ntrade-off in diffusion-based translation, 2) pose bias, and 3) instance bias in\nthe target domain, resulting in inferior 3D shapes, low text-image\ncorrespondence, and low intra-domain diversity in the generated samples. To\naddress these issues, we propose a novel pipeline called PODIA-3D, which uses\npose-preserved text-to-image diffusion-based domain adaptation for 3D\ngenerative models. We construct a pose-preserved text-to-image diffusion model\nthat allows the use of extremely high-level noise for significant domain\nchanges. We also propose specialized-to-general sampling strategies to improve\nthe details of the generated samples. Moreover, to overcome the instance bias,\nwe introduce a text-guided debiasing method that improves intra-domain\ndiversity. Consequently, our method successfully adapts 3D generators across\nsignificant domain gaps. Our qualitative results and user study demonstrates\nthat our approach outperforms existing 3D text-guided domain adaptation methods\nin terms of text-image correspondence, realism, diversity of rendered images,\nand sense of depth of 3D shapes in the generated samples\n","authors":["Gwanghyun Kim","Ji Ha Jang","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2304.01900v1.pdf","comment":"Project page: https://gwang-kim.github.io/podia_3d/"},{"id":"http://arxiv.org/abs/2304.01899v1","updated":"2023-04-04T15:48:09Z","published":"2023-04-04T15:48:09Z","title":"Cross-Class Feature Augmentation for Class Incremental Learning","summary":"  We propose a novel class incremental learning approach by incorporating a\nfeature augmentation technique motivated by adversarial attacks. We employ a\nclassifier learned in the past to complement training examples rather than\nsimply play a role as a teacher for knowledge distillation towards subsequent\nmodels. The proposed approach has a unique perspective to utilize the previous\nknowledge in class incremental learning since it augments features of arbitrary\ntarget classes using examples in other classes via adversarial attacks on a\npreviously learned classifier. By allowing the cross-class feature\naugmentations, each class in the old tasks conveniently populates samples in\nthe feature space, which alleviates the collapse of the decision boundaries\ncaused by sample deficiency for the previous tasks, especially when the number\nof stored exemplars is small. This idea can be easily incorporated into\nexisting class incremental learning algorithms without any architecture\nmodification. Extensive experiments on the standard benchmarks show that our\nmethod consistently outperforms existing class incremental learning methods by\nsignificant margins in various scenarios, especially under an environment with\nan extremely limited memory budget.\n","authors":["Taehoon Kim","Jaeyoo Park","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2304.01899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01893v1","updated":"2023-04-04T15:46:42Z","published":"2023-04-04T15:46:42Z","title":"Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory\n  Diffusion","summary":"  We introduce a method for generating realistic pedestrian trajectories and\nfull-body animations that can be controlled to meet user-defined goals. We draw\non recent advances in guided diffusion modeling to achieve test-time\ncontrollability of trajectories, which is normally only associated with\nrule-based systems. Our guided diffusion model allows users to constrain\ntrajectories through target waypoints, speed, and specified social groups while\naccounting for the surrounding environment context. This trajectory diffusion\nmodel is integrated with a novel physics-based humanoid controller to form a\nclosed-loop, full-body pedestrian animation system capable of placing large\ncrowds in a simulated environment with varying terrains. We further propose\nutilizing the value function learned during RL training of the animation\ncontroller to guide diffusion to produce trajectories better suited for\nparticular scenarios such as collision avoidance and traversing uneven terrain.\nVideo results are available on the project page at\nhttps://nv-tlabs.github.io/trace-pace .\n","authors":["Davis Rempe","Zhengyi Luo","Xue Bin Peng","Ye Yuan","Kris Kitani","Karsten Kreis","Sanja Fidler","Or Litany"],"pdf_url":"https://arxiv.org/pdf/2304.01893v1.pdf","comment":"Conference on Computer Vision and Pattern Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2212.08490v5","updated":"2023-04-04T15:22:07Z","published":"2022-12-16T14:02:12Z","title":"LOANet: A Lightweight Network Using Object Attention for Extracting\n  Buildings and Roads from UAV Aerial Remote Sensing Images","summary":"  Semantic segmentation for extracting buildings and roads, from unmanned\naerial vehicle (UAV) remote sensing images by deep learning becomes a more\nefficient and convenient method than traditional manual segmentation in\nsurveying and mapping field. In order to make the model lightweight and improve\nthe model accuracy, A Lightweight Network Using Object Attention (LOANet) for\nBuildings and Roads from UAV Aerial Remote Sensing Images is proposed. The\nproposed network adopts an encoder-decoder architecture in which a Lightweight\nDensely Connected Network (LDCNet) is developed as the encoder. In the decoder\npart, the dual multi-scale context modules which consist of the Atrous Spatial\nPyramid Pooling module (ASPP) and the Object Attention Module (OAM) are\ndesigned to capture more context information from feature maps of UAV remote\nsensing images. Between ASPP and OAM, a Feature Pyramid Network (FPN) module is\nused to and fuse multi-scale features extracting from ASPP. A private dataset\nof remote sensing images taken by UAV which contains 2431 training sets, 945\nvalidation sets, and 475 test sets is constructed. The proposed model performs\nwell on this dataset, with only 1.4M parameters and 5.48G floating-point\noperations (FLOPs), achieving a mean intersection-over-union ratio (mIoU) of\n71.12%. More extensive experiments on the public LoveDA dataset and CITY-OSM\ndataset to further verify the effectiveness of the proposed model with\nexcellent results on mIoU of 65.27% and 74.39%, respectively.\n","authors":["Xiaoxiang Han","Yiman Liu","Gang Liu","Yuanjie Lin","Qiaohong Liu"],"pdf_url":"https://arxiv.org/pdf/2212.08490v5.pdf","comment":"16 pages, 6 tables, 7 figures"},{"id":"http://arxiv.org/abs/2304.01865v1","updated":"2023-04-04T15:15:25Z","published":"2023-04-04T15:15:25Z","title":"SportsPose -- A Dynamic 3D sports pose dataset","summary":"  Accurate 3D human pose estimation is essential for sports analytics,\ncoaching, and injury prevention. However, existing datasets for monocular pose\nestimation do not adequately capture the challenging and dynamic nature of\nsports movements. In response, we introduce SportsPose, a large-scale 3D human\npose dataset consisting of highly dynamic sports movements. With more than\n176,000 3D poses from 24 different subjects performing 5 different sports\nactivities, SportsPose provides a diverse and comprehensive set of 3D poses\nthat reflect the complex and dynamic nature of sports movements. Contrary to\nother markerless datasets we have quantitatively evaluated the precision of\nSportsPose by comparing our poses with a commercial marker-based system and\nachieve a mean error of 34.5 mm across all evaluation sequences. This is\ncomparable to the error reported on the commonly used 3DPW dataset. We further\nintroduce a new metric, local movement, which describes the movement of the\nwrist and ankle joints in relation to the body. With this, we show that\nSportsPose contains more movement than the Human3.6M and 3DPW datasets in these\nextremum joints, indicating that our movements are more dynamic. The dataset\nwith accompanying code can be downloaded from our website. We hope that\nSportsPose will allow researchers and practitioners to develop and evaluate\nmore effective models for the analysis of sports performance and injury\nprevention. With its realistic and diverse dataset, SportsPose provides a\nvaluable resource for advancing the state-of-the-art in pose estimation in\nsports.\n","authors":["Christian Keilstrup Ingwersen","Christian Mikkelstrup","Janus Nørtoft Jensen","Morten Rieger Hannemose","Anders Bjorholm Dahl"],"pdf_url":"https://arxiv.org/pdf/2304.01865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01864v1","updated":"2023-04-04T15:13:44Z","published":"2023-04-04T15:13:44Z","title":"A Practical Framework for Unsupervised Structure Preservation Medical\n  Image Enhancement","summary":"  Medical images are extremely valuable for supporting medical diagnoses.\nHowever, in practice, low-quality (LQ) medical images, such as images that are\nhazy/blurry, have uneven illumination, or are out of focus, among others, are\noften obtained during data acquisition. This leads to difficulties in the\nscreening and diagnosis of medical diseases. Several generative adversarial\nnetworks (GAN)-based image enhancement methods have been proposed and have\nshown promising results. However, there is a quality-originality trade-off\namong these methods in the sense that they produce visually pleasing results\nbut lose the ability to preserve originality, especially the structural inputs.\nMoreover, to our knowledge, there is no objective metric in evaluating the\nstructure preservation of medical image enhancement methods in unsupervised\nsettings due to the unavailability of paired ground-truth data. In this study,\nwe propose a framework for practical unsupervised medical image enhancement\nthat includes (1) a non-reference objective evaluation of structure\npreservation for medical image enhancement tasks called Laplacian structural\nsimilarity index measure (LaSSIM), which is based on SSIM and the Laplacian\npyramid, and (2) a novel unsupervised GAN-based method called Laplacian medical\nimage enhancement (LaMEGAN) to support the improvement of both originality and\nquality from LQ images. The LaSSIM metric does not require clean reference\nimages and has been shown to be superior to SSIM in capturing image structural\nchanges under image degradations, such as strong blurring on different\ndatasets. The experiments demonstrated that our LaMEGAN achieves a satisfactory\nbalance between quality and originality, with robust structure preservation\nperformance while generating compelling visual results with very high image\nquality scores. The code will be made available at\nhttps://github.com/AillisInc/USPMIE.\n","authors":["Quan Huu Cap","Atsushi Fukuda","Hitoshi Iyatomi"],"pdf_url":"https://arxiv.org/pdf/2304.01864v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2303.10076v2","updated":"2023-04-04T15:05:55Z","published":"2023-03-17T15:57:14Z","title":"A Simple Attempt for 3D Occupancy Estimation in Autonomous Driving","summary":"  The task of estimating 3D occupancy from surrounding view images is an\nexciting development in the field of autonomous driving, following the success\nof Birds Eye View (BEV) perception.This task provides crucial 3D attributes of\nthe driving environment, enhancing the overall understanding and perception of\nthe surrounding space. However, there is still a lack of a baseline to define\nthe task, such as network design, optimization, and evaluation. In this work,\nwe present a simple attempt for 3D occupancy estimation, which is a CNN-based\nframework designed to reveal several key factors for 3D occupancy estimation.\nIn addition, we explore the relationship between 3D occupancy estimation and\nother related tasks, such as monocular depth estimation, stereo matching, and\nBEV perception (3D object detection and map segmentation), which could advance\nthe study on 3D occupancy estimation. For evaluation, we propose a simple\nsampling strategy to define the metric for occupancy evaluation, which is\nflexible for current public datasets. Moreover, we establish a new benchmark in\nterms of the depth estimation metric, where we compare our proposed method with\nmonocular depth estimation methods on the DDAD and Nuscenes datasets.The\nrelevant code will be available in\nhttps://github.com/GANWANSHUI/SimpleOccupancy\n","authors":["Wanshui Gan","Ningkai Mo","Hongbin Xu","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2303.10076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09383v2","updated":"2023-04-04T15:02:24Z","published":"2023-03-16T15:13:09Z","title":"Predicting Human Attention using Computational Attention","summary":"  Most models of visual attention are aimed at predicting either top-down or\nbottom-up control, as studied using different visual search and free-viewing\ntasks. We propose Human Attention Transformer (HAT), a single model predicting\nboth forms of attention control. HAT is the new state-of-the-art (SOTA) in\npredicting the scanpath of fixations made during target-present and\ntarget-absent search, and matches or exceeds SOTA in the prediction of taskless\nfree-viewing fixation scanpaths. HAT achieves this new SOTA by using a novel\ntransformer-based architecture and a simplified foveated retina that\ncollectively create a spatio-temporal awareness akin to the dynamic visual\nworking memory of humans. Unlike previous methods that rely on a coarse grid of\nfixation cells and experience information loss due to fixation discretization,\nHAT features a dense-prediction architecture and outputs a dense heatmap for\neach fixation, thus avoiding discretizing fixations. HAT sets a new standard in\ncomputational attention, which emphasizes both effectiveness and generality.\nHAT's demonstrated scope and applicability will likely inspire the development\nof new attention models that can better predict human behavior in various\nattention-demanding scenarios.\n","authors":["Zhibo Yang","Sounak Mondal","Seoyoung Ahn","Gregory Zelinsky","Minh Hoai","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2303.09383v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.04112v3","updated":"2023-04-04T14:59:59Z","published":"2020-12-07T23:31:59Z","title":"Adaptive Enhancement of Extreme Low-Light Images","summary":"  Existing methods for enhancing dark images captured in a very low-light\nenvironment assume that the intensity level of the optimal output image is\nknown and already included in the training set. However, this assumption often\ndoes not hold, leading to output images that contain visual imperfections such\nas dark regions or low contrast. To facilitate the training and evaluation of\nadaptive models that can overcome this limitation, we have created a dataset of\n1500 raw images taken in both indoor and outdoor low-light conditions. Based on\nour dataset, we introduce a deep learning model capable of enhancing input\nimages with a wide range of intensity levels at runtime, including ones that\nare not seen during training. Our experimental results demonstrate that our\nproposed dataset combined with our model can consistently and effectively\nenhance images across a wide range of diverse and challenging scenarios.\n","authors":["Evgeny Hershkovitch Neiterman","Michael Klyuchka","Gil Ben-Artzi"],"pdf_url":"https://arxiv.org/pdf/2012.04112v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.01962v4","updated":"2023-04-04T14:56:39Z","published":"2022-09-05T13:32:41Z","title":"Adversarial Detection: Attacking Object Detection in Real Time","summary":"  Intelligent robots rely on object detection models to perceive the\nenvironment. Following advances in deep learning security it has been revealed\nthat object detection models are vulnerable to adversarial attacks. However,\nprior research primarily focuses on attacking static images or offline videos.\nTherefore, it is still unclear if such attacks could jeopardize real-world\nrobotic applications in dynamic environments. This paper bridges this gap by\npresenting the first real-time online attack against object detection models.\nWe devise three attacks that fabricate bounding boxes for nonexistent objects\nat desired locations. The attacks achieve a success rate of about 90\\% within\nabout 20 iterations. The demo video is available at\nhttps://youtu.be/zJZ1aNlXsMU.\n","authors":["Han Wu","Syed Yunas","Sareh Rowlands","Wenjie Ruan","Johan Wahlstrom"],"pdf_url":"https://arxiv.org/pdf/2209.01962v4.pdf","comment":"Accepted by IEEE Intelligent Vehicle Symposium, 2023"},{"id":"http://arxiv.org/abs/2103.09151v6","updated":"2023-04-04T14:53:04Z","published":"2021-03-16T15:47:34Z","title":"Adversarial Driving: Attacking End-to-End Autonomous Driving","summary":"  As research in deep neural networks advances, deep convolutional networks\nbecome promising for autonomous driving tasks. In particular, there is an\nemerging trend of employing end-to-end neural network models for autonomous\ndriving. However, previous research has shown that deep neural network\nclassifiers are vulnerable to adversarial attacks. While for regression tasks,\nthe effect of adversarial attacks is not as well understood. In this research,\nwe devise two white-box targeted attacks against end-to-end autonomous driving\nmodels. Our attacks manipulate the behavior of the autonomous driving system by\nperturbing the input image. In an average of 800 attacks with the same attack\nstrength (epsilon=1), the image-specific and image-agnostic attack deviates the\nsteering angle from the original output by 0.478 and 0.111, respectively, which\nis much stronger than random noises that only perturbs the steering angle by\n0.002 (The steering angle ranges from [-1, 1]). Both attacks can be initiated\nin real-time on CPUs without employing GPUs. Demo video:\nhttps://youtu.be/I0i8uN2oOP0.\n","authors":["Han Wu","Syed Yunas","Sareh Rowlands","Wenjie Ruan","Johan Wahlstrom"],"pdf_url":"https://arxiv.org/pdf/2103.09151v6.pdf","comment":"Accepted by IEEE Intelligent Vehicle Symposium, 2023"},{"id":"http://arxiv.org/abs/2304.01842v1","updated":"2023-04-04T14:50:52Z","published":"2023-04-04T14:50:52Z","title":"Evaluating Synthetic Pre-Training for Handwriting Processing Tasks","summary":"  In this work, we explore massive pre-training on synthetic word images for\nenhancing the performance on four benchmark downstream handwriting analysis\ntasks. To this end, we build a large synthetic dataset of word images rendered\nin several handwriting fonts, which offers a complete supervision signal. We\nuse it to train a simple convolutional neural network (ConvNet) with a fully\nsupervised objective. The vector representations of the images obtained from\nthe pre-trained ConvNet can then be considered as encodings of the handwriting\nstyle. We exploit such representations for Writer Retrieval, Writer\nIdentification, Writer Verification, and Writer Classification and demonstrate\nthat our pre-training strategy allows extracting rich representations of the\nwriters' style that enable the aforementioned tasks with competitive results\nwith respect to task-specific State-of-the-Art approaches.\n","authors":["Vittorio Pippi","Silvia Cascianelli","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2304.01842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01838v1","updated":"2023-04-04T14:44:06Z","published":"2023-04-04T14:44:06Z","title":"BugNIST -- A New Large Scale Volumetric 3D Image Dataset for\n  Classification and Detection","summary":"  Progress in 3D volumetric image analysis research is limited by the lack of\ndatasets and most advances in analysis methods for volumetric images are based\non medical data. However, medical data do not necessarily resemble the\ncharacteristics of other volumetric images such as micro-CT. To promote\nresearch in 3D volumetric image analysis beyond medical data, we have created\nthe BugNIST dataset and made it freely available. BugNIST is an extensive\ndataset of micro-CT scans of 12 types of bugs, such as insects and larvae.\nBugNIST contains 9437 volumes where 9087 are of individual bugs and 350 are\nmixtures of bugs and other material. The goal of BugNIST is to benchmark\nclassification and detection methods, and we have designed the detection\nchallenge such that detection models are trained on scans of individual bugs\nand tested on bug mixtures. Models capable of solving this task will be\nindependent of the context, i.e., the surrounding material. This is a great\nadvantage if the context is unknown or changing, as is often the case in\nmicro-CT. Our initial baseline analysis shows that current state-of-the-art\ndeep learning methods classify individual bugs very well, but has great\ndifficulty with the detection challenge. Hereby, BugNIST enables research in\nimage analysis areas that until now have missed relevant data - both\nclassification, detection, and hopefully more.\n","authors":["Anders Bjorholm Dahl","Patrick Møller Jensen","Carsten Gundlach","Rebecca Engberg","Hans Martin Kjer","Vedrana Andersen Dahl"],"pdf_url":"https://arxiv.org/pdf/2304.01838v1.pdf","comment":"11 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2304.01834v1","updated":"2023-04-04T14:39:44Z","published":"2023-04-04T14:39:44Z","title":"Neural Field Convolutions by Repeated Differentiation","summary":"  Neural fields are evolving towards a general-purpose continuous\nrepresentation for visual computing. Yet, despite their numerous appealing\nproperties, they are hardly amenable to signal processing. As a remedy, we\npresent a method to perform general continuous convolutions with general\ncontinuous signals such as neural fields. Observing that piecewise polynomial\nkernels reduce to a sparse set of Dirac deltas after repeated differentiation,\nwe leverage convolution identities and train a repeated integral field to\nefficiently execute large-scale convolutions. We demonstrate our approach on a\nvariety of data modalities and spatially-varying kernels.\n","authors":["Ntumba Elie Nsampi","Adarsh Djeacoumar","Hans-Peter Seidel","Tobias Ritschel","Thomas Leimkühler"],"pdf_url":"https://arxiv.org/pdf/2304.01834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01830v1","updated":"2023-04-04T14:34:44Z","published":"2023-04-04T14:34:44Z","title":"Learning to Name Classes for Vision and Language Models","summary":"  Large scale vision and language models can achieve impressive zero-shot\nrecognition performance by mapping class specific text queries to image\ncontent. Two distinct challenges that remain however, are high sensitivity to\nthe choice of handcrafted class names that define queries, and the difficulty\nof adaptation to new, smaller datasets. Towards addressing these problems, we\npropose to leverage available data to learn, for each class, an optimal word\nembedding as a function of the visual content. By learning new word embeddings\non an otherwise frozen model, we are able to retain zero-shot capabilities for\nnew classes, easily adapt models to new datasets, and adjust potentially\nerroneous, non-descriptive or ambiguous class names. We show that our solution\ncan easily be integrated in image classification and object detection\npipelines, yields significant performance gains in multiple scenarios and\nprovides insights into model biases and labelling errors.\n","authors":["Sarah Parisot","Yongxin Yang","Steven McDonagh"],"pdf_url":"https://arxiv.org/pdf/2304.01830v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2304.00996v2","updated":"2023-04-04T14:30:48Z","published":"2023-03-31T16:30:31Z","title":"Deep Learning-based Diffusion Tensor Cardiac Magnetic Resonance\n  Reconstruction: A Comparison Study","summary":"  In vivo cardiac diffusion tensor imaging (cDTI) is a promising Magnetic\nResonance Imaging (MRI) technique for evaluating the micro-structure of\nmyocardial tissue in the living heart, providing insights into cardiac function\nand enabling the development of innovative therapeutic strategies. However, the\nintegration of cDTI into routine clinical practice is challenging due to the\ntechnical obstacles involved in the acquisition, such as low signal-to-noise\nratio and long scanning times. In this paper, we investigate and implement\nthree different types of deep learning-based MRI reconstruction models for cDTI\nreconstruction. We evaluate the performance of these models based on\nreconstruction quality assessment and diffusion tensor parameter assessment.\nOur results indicate that the models we discussed in this study can be applied\nfor clinical use at an acceleration factor (AF) of $\\times 2$ and $\\times 4$,\nwith the D5C5 model showing superior fidelity for reconstruction and the SwinMR\nmodel providing higher perceptual scores. There is no statistical difference\nwith the reference for all diffusion tensor parameters at AF $\\times 2$ or most\nDT parameters at AF $\\times 4$, and the quality of most diffusion tensor\nparameter maps are visually acceptable. SwinMR is recommended as the optimal\napproach for reconstruction at AF $\\times 2$ and AF $\\times 4$. However, we\nbelieved the models discussed in this studies are not prepared for clinical use\nat a higher AF. At AF $\\times 8$, the performance of all models discussed\nremains limited, with only half of the diffusion tensor parameters being\nrecovered to a level with no statistical difference from the reference. Some\ndiffusion tensor parameter maps even provide wrong and misleading information.\n","authors":["Jiahao Huang","Pedro F. Ferreira","Lichao Wang","Yinzhe Wu","Angelica I. Aviles-Rivero","Carola-Bibiane Schonlieb","Andrew D. Scott","Zohya Khalique","Maria Dwornik","Ramyah Rajakulasingam","Ranil De Silva","Dudley J. Pennell","Sonia Nielles-Vallespin","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2304.00996v2.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2201.01014v5","updated":"2023-04-04T14:25:10Z","published":"2022-01-04T07:20:46Z","title":"Local Motion and Contrast Priors Driven Deep Network for Infrared Small\n  Target Super-Resolution","summary":"  Infrared small target super-resolution (SR) aims to recover reliable and\ndetailed high-resolution image with high-contrast targets from its\nlow-resolution counterparts. Since the infrared small target lacks color and\nfine structure information, it is significant to exploit the supplementary\ninformation among sequence images to enhance the target. In this paper, we\npropose the first infrared small target SR method named local motion and\ncontrast prior driven deep network (MoCoPnet) to integrate the domain knowledge\nof infrared small target into deep network, which can mitigate the intrinsic\nfeature scarcity of infrared small targets. Specifically, motivated by the\nlocal motion prior in the spatio-temporal dimension, we propose a local\nspatio-temporal attention module to perform implicit frame alignment and\nincorporate the local spatio-temporal information to enhance the local features\n(especially for small targets). Motivated by the local contrast prior in the\nspatial dimension, we propose a central difference residual group to\nincorporate the central difference convolution into the feature extraction\nbackbone, which can achieve center-oriented gradient-aware feature extraction\nto further improve the target contrast. Extensive experiments have demonstrated\nthat our method can recover accurate spatial dependency and improve the target\ncontrast. Comparative results show that MoCoPnet can outperform the\nstate-of-the-art video SR and single image SR methods in terms of both SR\nperformance and target enhancement. Based on the SR results, we further\ninvestigate the influence of SR on infrared small target detection and the\nexperimental results demonstrate that MoCoPnet promotes the detection\nperformance. The code is available at https://github.com/XinyiYing/MoCoPnet.\n","authors":["Xinyi Ying","Yingqian Wang","Longguang Wang","Weidong Sheng","Li Liu","Zaiping Lin","Shilin Zhou"],"pdf_url":"https://arxiv.org/pdf/2201.01014v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12798v2","updated":"2023-04-04T14:17:15Z","published":"2023-02-24T18:19:49Z","title":"3D Generative Model Latent Disentanglement via Local Eigenprojection","summary":"  Designing realistic digital humans is extremely complex. Most data-driven\ngenerative models used to simplify the creation of their underlying geometric\nshape do not offer control over the generation of local shape attributes. In\nthis paper, we overcome this limitation by introducing a novel loss function\ngrounded in spectral geometry and applicable to different neural-network-based\ngenerative models of 3D head and body meshes. Encouraging the latent variables\nof mesh variational autoencoders (VAEs) or generative adversarial networks\n(GANs) to follow the local eigenprojections of identity attributes, we improve\nlatent disentanglement and properly decouple the attribute creation.\nExperimental results show that our local eigenprojection disentangled (LED)\nmodels not only offer improved disentanglement with respect to the\nstate-of-the-art, but also maintain good generation capabilities with training\ntimes comparable to the vanilla implementations of the models.\n","authors":["Simone Foti","Bongjin Koo","Danail Stoyanov","Matthew J. Clarkson"],"pdf_url":"https://arxiv.org/pdf/2302.12798v2.pdf","comment":"Computer Graphics Forum 2023"},{"id":"http://arxiv.org/abs/2304.01816v1","updated":"2023-04-04T14:14:16Z","published":"2023-04-04T14:14:16Z","title":"Toward Verifiable and Reproducible Human Evaluation for Text-to-Image\n  Generation","summary":"  Human evaluation is critical for validating the performance of text-to-image\ngenerative models, as this highly cognitive process requires deep comprehension\nof text and images. However, our survey of 37 recent papers reveals that many\nworks rely solely on automatic measures (e.g., FID) or perform poorly described\nhuman evaluations that are not reliable or repeatable. This paper proposes a\nstandardized and well-defined human evaluation protocol to facilitate\nverifiable and reproducible human evaluation in future works. In our pilot data\ncollection, we experimentally show that the current automatic measures are\nincompatible with human perception in evaluating the performance of the\ntext-to-image generation results. Furthermore, we provide insights for\ndesigning human evaluation experiments reliably and conclusively. Finally, we\nmake several resources publicly available to the community to facilitate easy\nand fast implementations.\n","authors":["Mayu Otani","Riku Togashi","Yu Sawai","Ryosuke Ishigami","Yuta Nakashima","Esa Rahtu","Janne Heikkilä","Shin'ichi Satoh"],"pdf_url":"https://arxiv.org/pdf/2304.01816v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01814v1","updated":"2023-04-04T14:13:13Z","published":"2023-04-04T14:13:13Z","title":"CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for\n  Low-Dose CT Denoising and Generalization","summary":"  Low-dose computed tomography (CT) images suffer from noise and artifacts due\nto photon starvation and electronic noise. Recently, some works have attempted\nto use diffusion models to address the over-smoothness and training instability\nencountered by previous deep-learning-based denoising models. However,\ndiffusion models suffer from long inference times due to the large number of\nsampling steps involved. Very recently, cold diffusion model generalizes\nclassical diffusion models and has greater flexibility. Inspired by the cold\ndiffusion, this paper presents a novel COntextual eRror-modulated gEneralized\nDiffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First,\nCoreDiff utilizes LDCT images to displace the random Gaussian noise and employs\na novel mean-preserving degradation operator to mimic the physical process of\nCT degradation, significantly reducing sampling steps thanks to the informative\nLDCT images as the starting point of the sampling process. Second, to alleviate\nthe error accumulation problem caused by the imperfect restoration operator in\nthe sampling process, we propose a novel ContextuaL Error-modulAted Restoration\nNetwork (CLEAR-Net), which can leverage contextual information to constrain the\nsampling process from structural distortion and modulate time step embedding\nfeatures for better alignment with the input at the next time step. Third, to\nrapidly generalize to a new, unseen dose level with as few resources as\npossible, we devise a one-shot learning framework to make CoreDiff generalize\nfaster and better using only a single LDCT image (un)paired with NDCT.\nExtensive experimental results on two datasets demonstrate that our CoreDiff\noutperforms competing methods in denoising and generalization performance, with\na clinically acceptable inference time.\n","authors":["Qi Gao","Zilong Li","Junping Zhang","Yi Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2304.01814v1.pdf","comment":"11 pages, 12 figures"},{"id":"http://arxiv.org/abs/2304.01811v1","updated":"2023-04-04T14:08:42Z","published":"2023-04-04T14:08:42Z","title":"HarsanyiNet: Computing Accurate Shapley Values in a Single Forward\n  Propagation","summary":"  The Shapley value is widely regarded as a trustworthy attribution metric.\nHowever, when people use Shapley values to explain the attribution of input\nvariables of a deep neural network (DNN), it usually requires a very high\ncomputational cost to approximate relatively accurate Shapley values in\nreal-world applications. Therefore, we propose a novel network architecture,\nthe HarsanyiNet, which makes inferences on the input sample and simultaneously\ncomputes the exact Shapley values of the input variables in a single forward\npropagation. The HarsanyiNet is designed on the theoretical foundation that the\nShapley value can be reformulated as the redistribution of Harsanyi\ninteractions encoded by the network.\n","authors":["Lu Chen","Siyu Lou","Keyan Zhang","Jin Huang","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.06313v2","updated":"2023-04-04T14:03:37Z","published":"2022-08-12T14:53:47Z","title":"Voxels Intersecting along Orthogonal Levels Attention U-Net for\n  Intracerebral Haemorrhage Segmentation in Head CT","summary":"  We propose a novel and flexible attention based U-Net architecture referred\nto as \"Voxels-Intersecting Along Orthogonal Levels Attention U-Net\"\n(viola-Unet), for intracranial hemorrhage (ICH) segmentation task in the\nINSTANCE 2022 Data Challenge on non-contrast computed tomography (CT). The\nperformance of ICH segmentation was improved by efficiently incorporating fused\nspatially orthogonal and cross-channel features via our proposed Viola\nattention plugged into the U-Net decoding branches. The viola-Unet outperformed\nthe strong baseline nnU-Net models during both 5-fold cross validation and\nonline validation. Our solution was the winner of the challenge validation\nphase in terms of all four performance metrics (i.e., DSC, HD, NSD, and RVD).\nThe code base, pretrained weights, and docker image of the viola-Unet AI tool\nare publicly available at \\url{https://github.com/samleoqh/Viola-Unet}.\n","authors":["Qinghui Liu","Bradley J MacIntosh","Till Schellhorn","Karoline Skogen","KyrreEeg Emblem","Atle Bjørnerud"],"pdf_url":"https://arxiv.org/pdf/2208.06313v2.pdf","comment":"Accepted by ISBI2023, 5 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2304.01805v1","updated":"2023-04-04T14:02:42Z","published":"2023-04-04T14:02:42Z","title":"Exploration of Lightweight Single Image Denoising with Transformers and\n  Truly Fair Training","summary":"  As multimedia content often contains noise from intrinsic defects of digital\ndevices, image denoising is an important step for high-level vision recognition\ntasks. Although several studies have developed the denoising field employing\nadvanced Transformers, these networks are too momory-intensive for real-world\napplications. Additionally, there is a lack of research on lightweight denosing\n(LWDN) with Transformers. To handle this, this work provides seven comparative\nbaseline Transformers for LWDN, serving as a foundation for future research. We\nalso demonstrate the parts of randomly cropped patches significantly affect the\ndenoising performances during training. While previous studies have overlooked\nthis aspect, we aim to train our baseline Transformers in a truly fair manner.\nFurthermore, we conduct empirical analyses of various components to determine\nthe key considerations for constructing LWDN Transformers. Codes are available\nat https://github.com/rami0205/LWDN.\n","authors":["Haram Choi","Cheolwoong Na","Jinseop Kim","Jihoon Yang"],"pdf_url":"https://arxiv.org/pdf/2304.01805v1.pdf","comment":"Technical report. Will be further revised. Codes are available at\n  https://github.com/rami0205/LWDN"},{"id":"http://arxiv.org/abs/2304.01804v1","updated":"2023-04-04T14:00:59Z","published":"2023-04-04T14:00:59Z","title":"Bridging the Gap between Model Explanations in Partially Annotated\n  Multi-label Classification","summary":"  Due to the expensive costs of collecting labels in multi-label classification\ndatasets, partially annotated multi-label classification has become an emerging\nfield in computer vision. One baseline approach to this task is to assume\nunobserved labels as negative labels, but this assumption induces label noise\nas a form of false negative. To understand the negative impact caused by false\nnegative labels, we study how these labels affect the model's explanation. We\nobserve that the explanation of two models, trained with full and partial\nlabels each, highlights similar regions but with different scaling, where the\nlatter tends to have lower attribution scores. Based on these findings, we\npropose to boost the attribution scores of the model trained with partial\nlabels to make its explanation resemble that of the model trained with full\nlabels. Even with the conceptually simple approach, the multi-label\nclassification performance improves by a large margin in three different\ndatasets on a single positive label setting and one on a large-scale partial\nlabel setting. Code is available at\nhttps://github.com/youngwk/BridgeGapExplanationPAMC.\n","authors":["Youngwook Kim","Jae Myung Kim","Jieun Jeong","Cordelia Schmid","Zeynep Akata","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2304.01804v1.pdf","comment":"CVPR2023 Camera-ready"},{"id":"http://arxiv.org/abs/2110.00273v3","updated":"2023-04-04T13:57:13Z","published":"2021-10-01T09:00:34Z","title":"From SLAM to Situational Awareness: Challenges and Survey","summary":"  The capability of a mobile robot to efficiently and safely perform complex\nmissions is limited by its knowledge of the environment, namely the situation.\nAdvanced reasoning, decision-making, and execution skills enable an intelligent\nagent to act autonomously in unknown environments. Situational Awareness (SA)\nis a fundamental capability of humans that has been deeply studied in various\nfields, such as psychology, military, aerospace, and education. Nevertheless,\nit has yet to be considered in robotics, which has focused on single\ncompartmentalized concepts such as sensing, spatial perception, sensor fusion,\nstate estimation, and Simultaneous Localization and Mapping (SLAM). Hence, the\npresent research aims to connect the broad multidisciplinary existing knowledge\nto pave the way for a complete SA system for mobile robotics that we deem\nparamount for autonomy. To this aim, we define the principal components to\nstructure a robotic SA and their area of competence. Accordingly, this paper\ninvestigates each aspect of SA, surveying the state-of-the-art robotics\nalgorithms that cover them, and discusses their current limitations.\nRemarkably, essential aspects of SA are still immature since the current\nalgorithmic development restricts their performance to only specific\nenvironments. Nevertheless, Artificial Intelligence (AI), particularly Deep\nLearning (DL), has brought new methods to bridge the gap that maintains these\nfields apart from the deployment to real-world scenarios. Furthermore, an\nopportunity has been discovered to interconnect the vastly fragmented space of\nrobotic comprehension algorithms through the mechanism of Situational Graph\n(S-Graph), a generalization of the well-known scene graph. Therefore, we\nfinally shape our vision for the future of robotic Situational Awareness by\ndiscussing interesting recent research directions.\n","authors":["Hriday Bavle","Jose Luis Sanchez-Lopez","Claudio Cimarelli","Ali Tourani Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2110.00273v3.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2304.01796v1","updated":"2023-04-04T13:54:59Z","published":"2023-04-04T13:54:59Z","title":"Influence of Myocardial Infraction on QRS Properties: A Simulation Study","summary":"  The interplay between structural and electrical changes in the heart after\nmyocardial infarction (MI) plays a key role in the initiation and maintenance\nof arrhythmia. The anatomical and electrophysiological properties of scar,\nborder zone, and normal myocardium modify the electrocardiographic morphology,\nwhich is routinely analysed in clinical settings. However, the influence of\nvarious MI properties on the QRS is not intuitively predictable.In this work,\nwe have systematically investigated the effects of 17 post-MI scenarios,\nvarying the location, size, transmural extent, and conductive level of scarring\nand border zone area, on the forward-calculated QRS. Additionally, we have\ncompared the contributions of different QRS score criteria for quantifying\npost-MI pathophysiology.The propagation of electrical activity in the\nventricles is simulated via a Eikonal model on a unified coordinate system.The\nanalysis has been performed on 49 subjects, and the results imply that the QRS\nis capable of identifying MI, suggesting the feasibility of inversely\nreconstructing infarct regions from QRS.There exist sensitivity variations of\ndifferent QRS criteria for identifying 17 MI scenarios, which is informative\nfor solving the inverse problem.\n","authors":["Lei Li","Julia Camps"," Zhinuo"," Wang","Abhirup Banerjee","Blanca Rodriguez","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2304.01796v1.pdf","comment":"10 pages, accpeted by FIMH 2022"},{"id":"http://arxiv.org/abs/2303.14829v2","updated":"2023-04-04T13:51:34Z","published":"2023-03-26T21:42:40Z","title":"SEM-POS: Grammatically and Semantically Correct Video Captioning","summary":"  Generating grammatically and semantically correct captions in video\ncaptioning is a challenging task. The captions generated from the existing\nmethods are either word-by-word that do not align with grammatical structure or\nmiss key information from the input videos. To address these issues, we\nintroduce a novel global-local fusion network, with a Global-Local Fusion Block\n(GLFB) that encodes and fuses features from different parts of speech (POS)\ncomponents with visual-spatial features. We use novel combinations of different\nPOS components - 'determinant + subject', 'auxiliary verb', 'verb', and\n'determinant + object' for supervision of the POS blocks - Det + Subject, Aux\nVerb, Verb, and Det + Object respectively. The novel global-local fusion\nnetwork together with POS blocks helps align the visual features with language\ndescription to generate grammatically and semantically correct captions.\nExtensive qualitative and quantitative experiments on benchmark MSVD and MSRVTT\ndatasets demonstrate that the proposed approach generates more grammatically\nand semantically correct captions compared to the existing methods, achieving\nthe new state-of-the-art. Ablations on the POS blocks and the GLFB demonstrate\nthe impact of the contributions on the proposed method.\n","authors":["Asmar Nadeem","Adrian Hilton","Robert Dawes","Graham Thomas","Armin Mustafa"],"pdf_url":"https://arxiv.org/pdf/2303.14829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03373v2","updated":"2023-04-04T13:48:30Z","published":"2023-03-06T18:56:26Z","title":"Detecting Human-Object Contact in Images","summary":"  Humans constantly contact objects to move and perform tasks. Thus, detecting\nhuman-object contact is important for building human-centered artificial\nintelligence. However, there exists no robust method to detect contact between\nthe body and the scene from an image, and there exists no dataset to learn such\na detector. We fill this gap with HOT (\"Human-Object conTact\"), a new dataset\nof human-object contacts for images. To build HOT, we use two data sources: (1)\nWe use the PROX dataset of 3D human meshes moving in 3D scenes, and\nautomatically annotate 2D image areas for contact via 3D mesh proximity and\nprojection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask\ntrained annotators to draw polygons for the 2D image areas where contact takes\nplace. We also annotate the involved body part of the human body. We use our\nHOT dataset to train a new contact detector, which takes a single color image\nas input, and outputs 2D contact heatmaps as well as the body-part labels that\nare in contact. This is a new and challenging task that extends current\nfoot-ground or hand-object contact detectors to the full generality of the\nwhole body. The detector uses a part-attention branch to guide contact\nestimation through the context of the surrounding body parts and scene. We\nevaluate our detector extensively, and quantitative results show that our model\noutperforms baselines, and that all components contribute to better\nperformance. Results on images from an online repository show reasonable\ndetections and generalizability.\n","authors":["Yixin Chen","Sai Kumar Dwivedi","Michael J. Black","Dimitrios Tzionas"],"pdf_url":"https://arxiv.org/pdf/2303.03373v2.pdf","comment":"Accepted at CVPR 2023"},{"id":"http://arxiv.org/abs/2212.05867v3","updated":"2023-04-04T13:37:28Z","published":"2022-12-12T13:10:19Z","title":"ALSO: Automotive Lidar Self-supervision by Occupancy estimation","summary":"  We propose a new self-supervised method for pre-training the backbone of deep\nperception models operating on point clouds. The core idea is to train the\nmodel on a pretext task which is the reconstruction of the surface on which the\n3D points are sampled, and to use the underlying latent vectors as input to the\nperception head. The intuition is that if the network is able to reconstruct\nthe scene surface, given only sparse input points, then it probably also\ncaptures some fragments of semantic information, that can be used to boost an\nactual perception task. This principle has a very simple formulation, which\nmakes it both easy to implement and widely applicable to a large range of 3D\nsensors and deep networks performing semantic segmentation or object detection.\nIn fact, it supports a single-stream pipeline, as opposed to most contrastive\nlearning approaches, allowing training on limited resources. We conducted\nextensive experiments on various autonomous driving datasets, involving very\ndifferent kinds of lidars, for both semantic segmentation and object detection.\nThe results show the effectiveness of our method to learn useful\nrepresentations without any annotation, compared to existing approaches. Code\nis available at https://github.com/valeoai/ALSO\n","authors":["Alexandre Boulch","Corentin Sautier","Björn Michele","Gilles Puy","Renaud Marlet"],"pdf_url":"https://arxiv.org/pdf/2212.05867v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2212.08123v2","updated":"2023-04-04T13:26:22Z","published":"2022-12-15T20:23:09Z","title":"Bayesian posterior approximation with stochastic ensembles","summary":"  We introduce ensembles of stochastic neural networks to approximate the\nBayesian posterior, combining stochastic methods such as dropout with deep\nensembles. The stochastic ensembles are formulated as families of distributions\nand trained to approximate the Bayesian posterior with variational inference.\nWe implement stochastic ensembles based on Monte Carlo dropout, DropConnect and\na novel non-parametric version of dropout and evaluate them on a toy problem\nand CIFAR image classification. For both tasks, we test the quality of the\nposteriors directly against Hamiltonian Monte Carlo simulations. Our results\nshow that stochastic ensembles provide more accurate posterior estimates than\nother popular baselines for Bayesian inference.\n","authors":["Oleksandr Balabanov","Bernhard Mehlig","Hampus Linander"],"pdf_url":"https://arxiv.org/pdf/2212.08123v2.pdf","comment":"19 pages, CVPR 2023"},{"id":"http://arxiv.org/abs/2210.01438v5","updated":"2023-04-04T13:09:22Z","published":"2022-10-04T07:50:28Z","title":"Complementary consistency semi-supervised learning for 3D left atrial\n  image segmentation","summary":"  A network based on complementary consistency training, called CC-Net, has\nbeen proposed for semi-supervised left atrium image segmentation. CC-Net\nefficiently utilizes unlabeled data from the perspective of complementary\ninformation to address the problem of limited ability of existing\nsemi-supervised segmentation algorithms to extract information from unlabeled\ndata. The complementary symmetric structure of CC-Net includes a main model and\ntwo auxiliary models. The complementary model inter-perturbations between the\nmain and auxiliary models force consistency to form complementary consistency.\nThe complementary information obtained by the two auxiliary models helps the\nmain model to effectively focus on ambiguous areas, while enforcing consistency\nbetween the models is advantageous in obtaining decision boundaries with low\nuncertainty. CC-Net has been validated on two public datasets. In the case of\nspecific proportions of labeled data, compared with current advanced\nalgorithms, CC-Net has the best semi-supervised segmentation performance. Our\ncode is publicly available at https://github.com/Cuthbert-Huang/CC-Net.\n","authors":["Hejun Huang","Zuguo Chen","Chaoyang Chen","Ming Lu","Ying Zou"],"pdf_url":"https://arxiv.org/pdf/2210.01438v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01752v1","updated":"2023-04-04T12:42:29Z","published":"2023-04-04T12:42:29Z","title":"Black Box Few-Shot Adaptation for Vision-Language models","summary":"  Vision-Language (V-L) models trained with contrastive learning to align the\nvisual and language modalities have been shown to be strong few-shot learners.\nSoft prompt learning is the method of choice for few-shot downstream adaption\naiming to bridge the modality gap caused by the distribution shift induced by\nthe new domain. While parameter-efficient, prompt learning still requires\naccess to the model weights and can be computationally infeasible for large\nmodels with billions of parameters. To address these shortcomings, in this\nwork, we describe a black-box method for V-L few-shot adaptation that (a)\noperates on pre-computed image and text features and hence works without access\nto the model's weights, (b) it is orders of magnitude faster at training time,\n(c) it is amenable to both supervised and unsupervised training, and (d) it can\nbe even used to align image and text features computed from uni-modal models.\nTo achieve this, we propose Linear Feature Alignment (LFA), a simple linear\napproach for V-L re-alignment in the target domain. LFA is initialized from a\nclosed-form solution to a least-squares problem and then it is iteratively\nupdated by minimizing a re-ranking loss. Despite its simplicity, our approach\ncan even surpass soft-prompt learning methods as shown by extensive experiments\non 11 image and 2 video datasets.\n","authors":["Yassine Ouali","Adrian Bulat","Brais Martinez","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2304.01752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.00728v3","updated":"2023-04-04T12:41:44Z","published":"2022-07-02T03:47:13Z","title":"Multi-scale Attentive Image De-raining Networks via Neural Architecture\n  Search","summary":"  Multi-scale architectures and attention modules have shown effectiveness in\nmany deep learning-based image de-raining methods. However, manually designing\nand integrating these two components into a neural network requires a bulk of\nlabor and extensive expertise. In this article, a high-performance multi-scale\nattentive neural architecture search (MANAS) framework is technically developed\nfor image deraining. The proposed method formulates a new multi-scale attention\nsearch space with multiple flexible modules that are favorite to the image\nde-raining task. Under the search space, multi-scale attentive cells are built,\nwhich are further used to construct a powerful image de-raining network. The\ninternal multiscale attentive architecture of the de-raining network is\nsearched automatically through a gradient-based search algorithm, which avoids\nthe daunting procedure of the manual design to some extent. Moreover, in order\nto obtain a robust image de-raining model, a practical and effective\nmulti-to-one training strategy is also presented to allow the de-raining\nnetwork to get sufficient background information from multiple rainy images\nwith the same background scene, and meanwhile, multiple loss functions\nincluding external loss, internal loss, architecture regularization loss, and\nmodel complexity loss are jointly optimized to achieve robust de-raining\nperformance and controllable model complexity. Extensive experimental results\non both synthetic and realistic rainy images, as well as the down-stream vision\napplications (i.e., objection detection and segmentation) consistently\ndemonstrate the superiority of our proposed method. The code is publicly\navailable at https://github.com/lcai-gz/MANAS.\n","authors":["Lei Cai","Yuli Fu","Wanliang Huo","Youjun Xiang","Tao Zhu","Ying Zhang","Huanqiang Zeng","Delu Zeng"],"pdf_url":"https://arxiv.org/pdf/2207.00728v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.07217v3","updated":"2023-04-04T12:36:53Z","published":"2021-06-14T08:04:18Z","title":"Influential Rank: A New Perspective of Post-training for Robust Model\n  against Noisy Labels","summary":"  Deep neural network can easily overfit to even noisy labels due to its high\ncapacity, which degrades the generalization performance of a model. To overcome\nthis issue, we propose a new approach for learning from noisy labels (LNL) via\npost-training, which can significantly improve the generalization performance\nof any pre-trained model on noisy label data. To this end, we rather exploit\nthe overfitting property of a trained model to identify mislabeled samples.\nSpecifically, our post-training approach gradually removes samples with high\ninfluence on the decision boundary and refines the decision boundary to improve\ngeneralization performance. Our post-training approach creates great synergies\nwhen combined with the existing LNL methods. Experimental results on various\nreal-world and synthetic benchmark datasets demonstrate the validity of our\napproach in diverse realistic scenarios.\n","authors":["Seulki Park","Hwanjun Song","Daeho Um","Dae Ung Jo","Sangdoo Yun","Jin Young Choi"],"pdf_url":"https://arxiv.org/pdf/2106.07217v3.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2304.01747v1","updated":"2023-04-04T12:35:33Z","published":"2023-04-04T12:35:33Z","title":"Learning Invariant Representation via Contrastive Feature Alignment for\n  Clutter Robust SAR Target Recognition","summary":"  The deep neural networks (DNNs) have freed the synthetic aperture radar\nautomatic target recognition (SAR ATR) from expertise-based feature designing\nand demonstrated superiority over conventional solutions. There has been shown\nthe unique deficiency of ground vehicle benchmarks in shapes of strong\nbackground correlation results in DNNs overfitting the clutter and being\nnon-robust to unfamiliar surroundings. However, the gap between fixed\nbackground model training and varying background application remains\nunderexplored. Inspired by contrastive learning, this letter proposes a\nsolution called Contrastive Feature Alignment (CFA) aiming to learn invariant\nrepresentation for robust recognition. The proposed method contributes a mixed\nclutter variants generation strategy and a new inference branch equipped with\nchannel-weighted mean square error (CWMSE) loss for invariant representation\nlearning. In specific, the generation strategy is delicately designed to better\nattract clutter-sensitive deviation in feature space. The CWMSE loss is further\ndevised to better contrast this deviation and align the deep features activated\nby the original images and corresponding clutter variants. The proposed CFA\ncombines both classification and CWMSE losses to train the model jointly, which\nallows for the progressive learning of invariant target representation.\nExtensive evaluations on the MSTAR dataset and six DNN models prove the\neffectiveness of our proposal. The results demonstrated that the CFA-trained\nmodels are capable of recognizing targets among unfamiliar surroundings that\nare not included in the dataset, and are robust to varying signal-to-clutter\nratios.\n","authors":["Bowen Peng","Jianyue Xie","Bo Peng","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2304.01747v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.03667v2","updated":"2023-04-04T12:15:24Z","published":"2023-03-07T06:05:30Z","title":"Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks","summary":"  To design fast neural networks, many works have been focusing on reducing the\nnumber of floating-point operations (FLOPs). We observe that such reduction in\nFLOPs, however, does not necessarily lead to a similar level of reduction in\nlatency. This mainly stems from inefficiently low floating-point operations per\nsecond (FLOPS). To achieve faster networks, we revisit popular operators and\ndemonstrate that such low FLOPS is mainly due to frequent memory access of the\noperators, especially the depthwise convolution. We hence propose a novel\npartial convolution (PConv) that extracts spatial features more efficiently, by\ncutting down redundant computation and memory access simultaneously. Building\nupon our PConv, we further propose FasterNet, a new family of neural networks,\nwhich attains substantially higher running speed than others on a wide range of\ndevices, without compromising on accuracy for various vision tasks. For\nexample, on ImageNet-1k, our tiny FasterNet-T0 is $2.8\\times$, $3.3\\times$, and\n$2.4\\times$ faster than MobileViT-XXS on GPU, CPU, and ARM processors,\nrespectively, while being $2.9\\%$ more accurate. Our large FasterNet-L achieves\nimpressive $83.5\\%$ top-1 accuracy, on par with the emerging Swin-B, while\nhaving $36\\%$ higher inference throughput on GPU, as well as saving $37\\%$\ncompute time on CPU. Code is available at\n\\url{https://github.com/JierunChen/FasterNet}.\n","authors":["Jierun Chen","Shiu-hong Kao","Hao He","Weipeng Zhuo","Song Wen","Chul-Ho Lee","S. -H. Gary Chan"],"pdf_url":"https://arxiv.org/pdf/2303.03667v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.15263v2","updated":"2023-04-04T11:32:43Z","published":"2023-03-27T14:52:08Z","title":"Joint Person Identity, Gender and Age Estimation from Hand Images using\n  Deep Multi-Task Representation Learning","summary":"  In this paper, we propose a multi-task representation learning framework to\njointly estimate the identity, gender and age of individuals from their hand\nimages for the purpose of criminal investigations since the hand images are\noften the only available information in cases of serious crime such as sexual\nabuse. We investigate different up-to-date deep learning architectures and\ncompare their performance for joint estimation of identity, gender and age from\nhand images of perpetrators of serious crime. To overcome the data imbalance\nand simplify the age prediction, we create age groups for the age estimation.\nWe make extensive evaluations and comparisons of both convolution-based and\ntransformer-based deep learning architectures on a publicly available 11k hands\ndataset. Our experimental analysis shows that it is possible to efficiently\nestimate not only identity but also other attributes such as gender and age of\nsuspects jointly from hand images for criminal investigations, which is crucial\nin assisting international police forces in the court to identify and convict\nabusers.\n","authors":["Nathanael L. Baisa"],"pdf_url":"https://arxiv.org/pdf/2303.15263v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.04821"},{"id":"http://arxiv.org/abs/2209.04821v2","updated":"2023-04-04T11:26:56Z","published":"2022-09-11T09:43:42Z","title":"Local-Aware Global Attention Network for Person Re-Identification","summary":"  Learning representative, robust and discriminative information from images is\nessential for effective person re-identification (Re-Id). In this paper, we\npropose a compound approach for end-to-end discriminative deep feature learning\nfor person Re-Id based on both body and hand images. We carefully design the\nLocal-Aware Global Attention Network (LAGA-Net), a multi-branch deep network\narchitecture consisting of one branch for spatial attention, one branch for\nchannel attention, one branch for global feature representations and another\nbranch for local feature representations. The attention branches focus on the\nrelevant features of the image while suppressing the irrelevant backgrounds. In\norder to overcome the weakness of the attention mechanisms, equivariant to\npixel shuffling, we integrate relative positional encodings into the spatial\nattention module to capture the spatial positions of pixels. The global branch\nintends to preserve the global context or structural information. For the the\nlocal branch, which intends to capture the fine-grained information, we perform\nuniform partitioning to generate stripes on the conv-layer horizontally. We\nretrieve the parts by conducting a soft partition without explicitly\npartitioning the images or requiring external cues such as pose estimation. A\nset of ablation study shows that each component contributes to the increased\nperformance of the LAGA-Net. Extensive evaluations on four popular body-based\nperson Re-Id benchmarks and two publicly available hand datasets demonstrate\nthat our proposed method consistently outperforms existing state-of-the-art\nmethods.\n","authors":["Nathanael L. Baisa"],"pdf_url":"https://arxiv.org/pdf/2209.04821v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2108.02234"},{"id":"http://arxiv.org/abs/2304.01716v1","updated":"2023-04-04T11:25:44Z","published":"2023-04-04T11:25:44Z","title":"Decoupling Dynamic Monocular Videos for Dynamic View Synthesis","summary":"  The challenge of dynamic view synthesis from dynamic monocular videos, i.e.,\nsynthesizing novel views for free viewpoints given a monocular video of a\ndynamic scene captured by a moving camera, mainly lies in accurately modeling\nthe dynamic objects of a scene using limited 2D frames, each with a varying\ntimestamp and viewpoint. Existing methods usually require pre-processed 2D\noptical flow and depth maps by additional methods to supervise the network,\nmaking them suffer from the inaccuracy of the pre-processed supervision and the\nambiguity when lifting the 2D information to 3D. In this paper, we tackle this\nchallenge in an unsupervised fashion. Specifically, we decouple the motion of\nthe dynamic objects into object motion and camera motion, respectively\nregularized by proposed unsupervised surface consistency and patch-based\nmulti-view constraints. The former enforces the 3D geometric surfaces of moving\nobjects to be consistent over time, while the latter regularizes their\nappearances to be consistent across different viewpoints. Such a fine-grained\nmotion formulation can alleviate the learning difficulty for the network, thus\nenabling it to produce not only novel views with higher quality but also more\naccurate scene flows and depth than existing methods requiring extra\nsupervision. We will make the code publicly available.\n","authors":["Meng You","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2304.01716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01715v1","updated":"2023-04-04T11:25:23Z","published":"2023-04-04T11:25:23Z","title":"Towards Open-Vocabulary Video Instance Segmentation","summary":"  Video Instance Segmentation(VIS) aims at segmenting and categorizing objects\nin videos from a closed set of training categories, lacking the generalization\nability to handle novel categories in real-world videos. To address this\nlimitation, we make the following three contributions. First, we introduce the\nnovel task of Open-Vocabulary Video Instance Segmentation, which aims to\nsimultaneously segment, track, and classify objects in videos from open-set\ncategories, including novel categories unseen during training. Second, to\nbenchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video Instance\nSegmentation dataset(LV-VIS), that contains well-annotated objects from 1,212\ndiverse categories, significantly surpassing the category size of existing\ndatasets by more than one order of magnitude. Third, we propose an efficient\nMemory-Induced Vision-Language Transformer, MindVLT, to first achieve\nOpen-Vocabulary VIS in an end-to-end manner with near real-time inference\nspeed. Extensive experiments on LV-VIS and four existing VIS datasets\ndemonstrate the strong zero-shot generalization ability of MindVLT on novel\ncategories. We will release the dataset and code to facilitate future\nendeavors.\n","authors":["Haochen Wang","Shuai Wang","Cilin Yan","Xiaolong Jiang","XU Tang","Yao Hu","Weidi Xie","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2304.01715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01705v1","updated":"2023-04-04T11:01:46Z","published":"2023-04-04T11:01:46Z","title":"Cross-modal tumor segmentation using generative blending augmentation\n  and self training","summary":"  Deep learning for medical imaging is limited by data scarcity and domain\nshift, which lead to biased training sets that do not accurately represent\ndeployment conditions. A related practical problem is cross-modal segmentation\nwhere the objective is to segment unlabelled domains using previously labelled\nimages from other modalites, which is the context of the MICCAI CrossMoDA 2022\nchallenge on vestibular schwannoma (VS) segmentation. In this context, we\npropose a VS segmentation method that leverages conventional image-to-image\ntranslation and segmentation using iterative self training combined to a\ndedicated data augmentation technique called Generative Blending Augmentation\n(GBA). GBA is based on a one-shot 2D SinGAN generative model that allows to\nrealistically diversify target tumor appearances in a downstream segmentation\nmodel, improving its generalization power at test time. Our solution ranked\nfirst on the VS segmentation task during the validation and test phase of the\nCrossModa 2022 challenge.\n","authors":["Guillaume Sallé","Pierre-Henri Conze","Julien Bert","Nicolas Boussion","Dimitris Visvikis","Vincent Jaouen"],"pdf_url":"https://arxiv.org/pdf/2304.01705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05323v2","updated":"2023-04-04T10:59:43Z","published":"2023-03-09T15:13:51Z","title":"Controllable Video Generation by Learning the Underlying Dynamical\n  System with Neural ODE","summary":"  Videos depict the change of complex dynamical systems over time in the form\nof discrete image sequences. Generating controllable videos by learning the\ndynamical system is an important yet underexplored topic in the computer vision\ncommunity. This paper presents a novel framework, TiV-ODE, to generate highly\ncontrollable videos from a static image and a text caption. Specifically, our\nframework leverages the ability of Neural Ordinary Differential\nEquations~(Neural ODEs) to represent complex dynamical systems as a set of\nnonlinear ordinary differential equations. The resulting framework is capable\nof generating videos with both desired dynamics and content. Experiments\ndemonstrate the ability of the proposed method in generating highly\ncontrollable and visually consistent videos, and its capability of modeling\ndynamical systems. Overall, this work is a significant step towards developing\nadvanced controllable video generation models that can handle complex and\ndynamic scenes.\n","authors":["Yucheng Xu","Li Nanbo","Arushi Goel","Zijian Guo","Zonghai Yao","Hamidreza Kasaei","Mohammadreze Kasaei","Zhibin Li"],"pdf_url":"https://arxiv.org/pdf/2303.05323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.11927v2","updated":"2023-04-04T10:59:17Z","published":"2021-11-23T15:09:03Z","title":"Hierarchical Graph Networks for 3D Human Pose Estimation","summary":"  Recent 2D-to-3D human pose estimation works tend to utilize the graph\nstructure formed by the topology of the human skeleton. However, we argue that\nthis skeletal topology is too sparse to reflect the body structure and suffer\nfrom serious 2D-to-3D ambiguity problem. To overcome these weaknesses, we\npropose a novel graph convolution network architecture, Hierarchical Graph\nNetworks (HGN). It is based on denser graph topology generated by our\nmulti-scale graph structure building strategy, thus providing more delicate\ngeometric information. The proposed architecture contains three sparse-to-fine\nrepresentation subnetworks organized in parallel, in which multi-scale\ngraph-structured features are processed and exchange information through a\nnovel feature fusion strategy, leading to rich hierarchical representations. We\nalso introduce a 3D coarse mesh constraint to further boost detail-related\nfeature learning. Extensive experiments demonstrate that our HGN achieves the\nstate-of-the art performance with reduced network parameters. Code is released\nat\nhttps://github.com/qingshi9974/BMVC2021-Hierarchical-Graph-Networks-for-3D-Human-Pose-Estimation.\n","authors":["Han Li","Bowen Shi","Wenrui Dai","Yabo Chen","Botao Wang","Yu Sun","Min Guo","Chenlin Li","Junni Zou","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2111.11927v2.pdf","comment":"accepted by BMVC 2021"},{"id":"http://arxiv.org/abs/2303.17595v2","updated":"2023-04-04T10:32:52Z","published":"2023-03-30T17:59:02Z","title":"Neglected Free Lunch; Learning Image Classifiers Using Annotation\n  Byproducts","summary":"  Supervised learning of image classifiers distills human knowledge into a\nparametric model through pairs of images and corresponding labels (X,Y). We\nargue that this simple and widely used representation of human knowledge\nneglects rich auxiliary information from the annotation procedure, such as the\ntime-series of mouse traces and clicks left after image selection. Our insight\nis that such annotation byproducts Z provide approximate human attention that\nweakly guides the model to focus on the foreground cues, reducing spurious\ncorrelations and discouraging shortcut learning. To verify this, we create\nImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with\nsample-wise annotation byproducts, collected by replicating the respective\noriginal annotation tasks. We refer to the new paradigm of training models with\nannotation byproducts as learning using annotation byproducts (LUAB). We show\nthat a simple multitask loss for regressing Z together with Y already improves\nthe generalisability and robustness of the learned models. Compared to the\noriginal supervised learning, LUAB does not require extra annotation costs.\nImageNet-AB and COCO-AB are at https://github.com/naver-ai/NeglectedFreeLunch.\n","authors":["Dongyoon Han","Junsuk Choe","Seonghyeok Chun","John Joon Young Chung","Minsuk Chang","Sangdoo Yun","Jean Y. Song","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2303.17595v2.pdf","comment":"Code at https://github.com/naver-ai/NeglectedFreeLunch"},{"id":"http://arxiv.org/abs/2304.01686v1","updated":"2023-04-04T10:29:42Z","published":"2023-04-04T10:29:42Z","title":"HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised\n  Ordering","summary":"  We consider the challenging task of training models for image-to-video\ndeblurring, which aims to recover a sequence of sharp images corresponding to a\ngiven blurry image input. A critical issue disturbing the training of an\nimage-to-video model is the ambiguity of the frame ordering since both the\nforward and backward sequences are plausible solutions. This paper proposes an\neffective self-supervised ordering scheme that allows training high-quality\nimage-to-video deblurring models. Unlike previous methods that rely on\norder-invariant losses, we assign an explicit order for each video sequence,\nthus avoiding the order-ambiguity issue. Specifically, we map each video\nsequence to a vector in a latent high-dimensional space so that there exists a\nhyperplane such that for every video sequence, the vectors extracted from it\nand its reversed sequence are on different sides of the hyperplane. The side of\nthe vectors will be used to define the order of the corresponding sequence.\nLast but not least, we propose a real-image dataset for the image-to-video\ndeblurring problem that covers a variety of popular domains, including face,\nhand, and street. Extensive experimental results confirm the effectiveness of\nour method. Code and data are available at\nhttps://github.com/VinAIResearch/HyperCUT.git\n","authors":["Bang-Dang Pham","Phong Tran","Anh Tran","Cuong Pham","Rang Nguyen","Minh Hoai"],"pdf_url":"https://arxiv.org/pdf/2304.01686v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2303.14017v2","updated":"2023-04-04T10:24:13Z","published":"2023-03-24T14:18:40Z","title":"CF-Font: Content Fusion for Few-shot Font Generation","summary":"  Content and style disentanglement is an effective way to achieve few-shot\nfont generation. It allows to transfer the style of the font image in a source\ndomain to the style defined with a few reference images in a target domain.\nHowever, the content feature extracted using a representative font might not be\noptimal. In light of this, we propose a content fusion module (CFM) to project\nthe content feature into a linear space defined by the content features of\nbasis fonts, which can take the variation of content features caused by\ndifferent fonts into consideration. Our method also allows to optimize the\nstyle representation vector of reference images through a lightweight iterative\nstyle-vector refinement (ISR) strategy. Moreover, we treat the 1D projection of\na character image as a probability distribution and leverage the distance\nbetween two distributions as the reconstruction loss (namely projected\ncharacter loss, PCL). Compared to L2 or L1 reconstruction loss, the\ndistribution distance pays more attention to the global shape of characters. We\nhave evaluated our method on a dataset of 300 fonts with 6.5k characters each.\nExperimental results verify that our method outperforms existing\nstate-of-the-art few-shot font generation methods by a large margin. The source\ncode can be found at https://github.com/wangchi95/CF-Font.\n","authors":["Chi Wang","Min Zhou","Tiezheng Ge","Yuning Jiang","Hujun Bao","Weiwei Xu"],"pdf_url":"https://arxiv.org/pdf/2303.14017v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01682v1","updated":"2023-04-04T10:13:13Z","published":"2023-04-04T10:13:13Z","title":"High-resolution tomographic reconstruction of optical absorbance through\n  scattering media using neural fields","summary":"  Light scattering imposes a major obstacle for imaging objects seated deeply\nin turbid media, such as biological tissues and foggy air. Diffuse optical\ntomography (DOT) tackles scattering by volumetrically recovering the optical\nabsorbance and has shown significance in medical imaging, remote sensing and\nautonomous driving. A conventional DOT reconstruction paradigm necessitates\ndiscretizing the object volume into voxels at a pre-determined resolution for\nmodelling diffuse light propagation and the resulting spatial resolution of the\nreconstruction is generally limited. We propose NeuDOT, a novel DOT scheme\nbased on neural fields (NF) to continuously encode the optical absorbance\nwithin the volume and subsequently bridge the gap between model accuracy and\nhigh resolution. Comprehensive experiments demonstrate that NeuDOT achieves\nsubmillimetre lateral resolution and resolves complex 3D objects at 14\nmm-depth, outperforming the state-of-the-art methods. NeuDOT is a non-invasive,\nhigh-resolution and computationally efficient tomographic method, and unlocks\nfurther applications of NF involving light scattering.\n","authors":["Wuwei Ren","Siyuan Shen","Linlin Li","Shengyu Gao","Yuehan Wang","Liangtao Gu","Shiying Li","Xingjun Zhu","Jiahua Jiang","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2304.01682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01672v1","updated":"2023-04-04T09:58:58Z","published":"2023-04-04T09:58:58Z","title":"Motion-R3: Fast and Accurate Motion Annotation via Representation-based\n  Representativeness Ranking","summary":"  In this paper, we follow a data-centric philosophy and propose a novel motion\nannotation method based on the inherent representativeness of motion data in a\ngiven dataset. Specifically, we propose a Representation-based\nRepresentativeness Ranking R3 method that ranks all motion data in a given\ndataset according to their representativeness in a learned motion\nrepresentation space. We further propose a novel dual-level motion constrastive\nlearning method to learn the motion representation space in a more informative\nway. Thanks to its high efficiency, our method is particularly responsive to\nfrequent requirements change and enables agile development of motion annotation\nmodels. Experimental results on the HDM05 dataset against state-of-the-art\nmethods demonstrate the superiority of our method.\n","authors":["Jubo Yu","Tianxiang Ren","Shihui Guo","Fengyi Fang","Kai Wang","Zijiao Zeng","Yazhan Zhang","Andreas Aristidou","Yipeng Qin"],"pdf_url":"https://arxiv.org/pdf/2304.01672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01669v1","updated":"2023-04-04T09:58:07Z","published":"2023-04-04T09:58:07Z","title":"Re-thinking Model Inversion Attacks Against Deep Neural Networks","summary":"  Model inversion (MI) attacks aim to infer and reconstruct private training\ndata by abusing access to a model. MI attacks have raised concerns about the\nleaking of sensitive information (e.g. private face images used in training a\nface recognition system). Recently, several algorithms for MI have been\nproposed to improve the attack performance. In this work, we revisit MI, study\ntwo fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms,\nand propose solutions to these issues which lead to a significant boost in\nattack performance for all SOTA MI. In particular, our contributions are\ntwo-fold: 1) We analyze the optimization objective of SOTA MI algorithms, argue\nthat the objective is sub-optimal for achieving MI, and propose an improved\noptimization objective that boosts attack performance significantly. 2) We\nanalyze \"MI overfitting\", show that it would prevent reconstructed images from\nlearning semantics of training data, and propose a novel \"model augmentation\"\nidea to overcome this issue. Our proposed solutions are simple and improve all\nSOTA MI attack accuracy significantly. E.g., in the standard CelebA benchmark,\nour solutions improve accuracy by 11.8% and achieve for the first time over 90%\nattack accuracy. Our findings demonstrate that there is a clear risk of leaking\nsensitive information from deep learning models. We urge serious consideration\nto be given to the privacy implications. Our code, demo, and models are\navailable at\nhttps://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/\n","authors":["Ngoc-Bao Nguyen","Keshigeyan Chandrasegaran","Milad Abdollahzadeh","Ngai-Man Cheung"],"pdf_url":"https://arxiv.org/pdf/2304.01669v1.pdf","comment":"Accepted to CVPR 2023. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2304.01663v1","updated":"2023-04-04T09:34:14Z","published":"2023-04-04T09:34:14Z","title":"On the Stability-Plasticity Dilemma of Class-Incremental Learning","summary":"  A primary goal of class-incremental learning is to strike a balance between\nstability and plasticity, where models should be both stable enough to retain\nknowledge learned from previously seen classes, and plastic enough to learn\nconcepts from new classes. While previous works demonstrate strong performance\non class-incremental benchmarks, it is not clear whether their success comes\nfrom the models being stable, plastic, or a mixture of both. This paper aims to\nshed light on how effectively recent class-incremental learning algorithms\naddress the stability-plasticity trade-off. We establish analytical tools that\nmeasure the stability and plasticity of feature representations, and employ\nsuch tools to investigate models trained with various algorithms on large-scale\nclass-incremental benchmarks. Surprisingly, we find that the majority of\nclass-incremental learning algorithms heavily favor stability over plasticity,\nto the extent that the feature extractor of a model trained on the initial set\nof classes is no less effective than that of the final incremental model. Our\nobservations not only inspire two simple algorithms that highlight the\nimportance of feature representation analysis, but also suggest that\nclass-incremental learning approaches, in general, should strive for better\nfeature representation learning.\n","authors":["Dongwan Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2304.01663v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2211.12501v3","updated":"2023-04-04T09:34:04Z","published":"2022-11-22T18:59:52Z","title":"AeDet: Azimuth-invariant Multi-view 3D Object Detection","summary":"  Recent LSS-based multi-view 3D object detection has made tremendous progress,\nby processing the features in Brid-Eye-View (BEV) via the convolutional\ndetector. However, the typical convolution ignores the radial symmetry of the\nBEV features and increases the difficulty of the detector optimization. To\npreserve the inherent property of the BEV features and ease the optimization,\nwe propose an azimuth-equivariant convolution (AeConv) and an\nazimuth-equivariant anchor. The sampling grid of AeConv is always in the radial\ndirection, thus it can learn azimuth-invariant BEV features. The proposed\nanchor enables the detection head to learn predicting azimuth-irrelevant\ntargets. In addition, we introduce a camera-decoupled virtual depth to unify\nthe depth prediction for the images with different camera intrinsic parameters.\nThe resultant detector is dubbed Azimuth-equivariant Detector (AeDet).\nExtensive experiments are conducted on nuScenes, and AeDet achieves a 62.0%\nNDS, surpassing the recent multi-view 3D object detectors such as PETRv2 and\nBEVDepth by a large margin. Project page: https://fcjian.github.io/aedet.\n","authors":["Chengjian Feng","Zequn Jie","Yujie Zhong","Xiangxiang Chu","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2211.12501v3.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2304.01662v1","updated":"2023-04-04T09:33:16Z","published":"2023-04-04T09:33:16Z","title":"Cross-Domain Image Captioning with Discriminative Finetuning","summary":"  Neural captioners are typically trained to mimic human-generated references\nwithout optimizing for any specific communication goal, leading to problems\nsuch as the generation of vague captions. In this paper, we show that\nfine-tuning an out-of-the-box neural captioner with a self-supervised\ndiscriminative communication objective helps to recover a plain, visually\ndescriptive language that is more informative about image contents. Given a\ntarget image, the system must learn to produce a description that enables an\nout-of-the-box text-conditioned image retriever to identify such image among a\nset of candidates. We experiment with the popular ClipCap captioner, also\nreplicating the main results with BLIP. In terms of similarity to ground-truth\nhuman descriptions, the captions emerging from discriminative finetuning lag\nslightly behind those generated by the non-finetuned model, when the latter is\ntrained and tested on the same caption dataset. However, when the model is used\nwithout further tuning to generate captions for out-of-domain datasets, our\ndiscriminatively-finetuned captioner generates descriptions that resemble human\nreferences more than those produced by the same captioner without finetuning.\nWe further show that, on the Conceptual Captions dataset, discriminatively\nfinetuned captions are more helpful than either vanilla ClipCap captions or\nground-truth captions for human annotators tasked with an image discrimination\ntask.\n","authors":["Roberto Dessì","Michele Bevilacqua","Eleonora Gualdoni","Nathanael Carraz Rakotonirina","Francesca Franzon","Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2304.01662v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2303.05275v2","updated":"2023-04-04T09:30:03Z","published":"2023-03-09T14:14:29Z","title":"Detecting Images Generated by Diffusers","summary":"  This paper explores the task of detecting images generated by text-to-image\ndiffusion models. To evaluate this, we consider images generated from captions\nin the MSCOCO and Wikimedia datasets using two state-of-the-art models: Stable\nDiffusion and GLIDE. Our experiments show that it is possible to detect the\ngenerated images using simple Multi-Layer Perceptrons (MLPs), starting from\nfeatures extracted by CLIP, or traditional Convolutional Neural Networks\n(CNNs). We also observe that models trained on images generated by Stable\nDiffusion can detect images generated by GLIDE relatively well, however, the\nreverse is not true. Lastly, we find that incorporating the associated textual\ninformation with the images rarely leads to significant improvement in\ndetection results but that the type of subject depicted in the image can have a\nsignificant impact on performance. This work provides insights into the\nfeasibility of detecting generated images, and has implications for security\nand privacy concerns in real-world applications. The code to reproduce our\nresults is available at:\nhttps://github.com/davide-coccomini/Detecting-Images-Generated-by-Diffusers\n","authors":["Davide Alessandro Coccomini","Andrea Esuli","Fabrizio Falchi","Claudio Gennaro","Giuseppe Amato"],"pdf_url":"https://arxiv.org/pdf/2303.05275v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01658v1","updated":"2023-04-04T09:28:36Z","published":"2023-04-04T09:28:36Z","title":"Fully Convolutional Networks for Dense Water Flow Intensity Prediction\n  in Swedish Catchment Areas","summary":"  Intensifying climate change will lead to more extreme weather events,\nincluding heavy rainfall and drought. Accurate stream flow prediction models\nwhich are adaptable and robust to new circumstances in a changing climate will\nbe an important source of information for decisions on climate adaptation\nefforts, especially regarding mitigation of the risks of and damages associated\nwith flooding. In this work we propose a machine learning-based approach for\npredicting water flow intensities in inland watercourses based on the physical\ncharacteristics of the catchment areas, obtained from geospatial data\n(including elevation and soil maps, as well as satellite imagery), in addition\nto temporal information about past rainfall quantities and temperature\nvariations. We target the one-day-ahead regime, where a fully convolutional\nneural network model receives spatio-temporal inputs and predicts the water\nflow intensity in every coordinate of the spatial input for the subsequent day.\nTo the best of our knowledge, we are the first to tackle the task of dense\nwater flow intensity prediction; earlier works have considered predicting flow\nintensities at a sparse set of locations at a time. An extensive set of model\nevaluations and ablations are performed, which empirically justify our various\ndesign choices. Code and preprocessed data have been made publicly available at\nhttps://github.com/aleksispi/fcn-water-flow.\n","authors":["Aleksis Pirinen","Olof Mogren","Mårten Västerdal"],"pdf_url":"https://arxiv.org/pdf/2304.01658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08609v5","updated":"2023-04-04T09:19:37Z","published":"2022-11-16T01:43:39Z","title":"Two-Stage Context-Aware model for Predicting Future Motion of Dynamic\n  Agents","summary":"  Predicting the future motion of dynamic agents is of paramount importance to\nensuring safety and assessing risks in motion planning for autonomous robots.\nIn this study, we propose a two-stage motion prediction method, called R-Pred,\ndesigned to effectively utilize both scene and interaction context using a\ncascade of the initial trajectory proposal and trajectory refinement networks.\nThe initial trajectory proposal network produces M trajectory proposals\ncorresponding to the M modes of the future trajectory distribution. The\ntrajectory refinement network enhances each of the M proposals using 1)\ntube-query scene attention (TQSA) and 2) proposal-level interaction attention\n(PIA) mechanisms. TQSA uses tube-queries to aggregate local scene context\nfeatures pooled from proximity around trajectory proposals of interest. PIA\nfurther enhances the trajectory proposals by modeling inter-agent interactions\nusing a group of trajectory proposals selected by their distances from\nneighboring agents. Our experiments conducted on Argoverse and nuScenes\ndatasets demonstrate that the proposed refinement network provides significant\nperformance improvements compared to the single-stage baseline and that R-Pred\nachieves state-of-the-art performance in some categories of the benchmarks.\n","authors":["Sehwan Choi","Jungho Kim","Junyong Yun","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2211.08609v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12656v2","updated":"2023-04-04T09:06:52Z","published":"2023-02-24T14:24:58Z","title":"COVERED, CollabOratiVE Robot Environment Dataset for 3D Semantic\n  segmentation","summary":"  Safe human-robot collaboration (HRC) has recently gained a lot of interest\nwith the emerging Industry 5.0 paradigm. Conventional robots are being replaced\nwith more intelligent and flexible collaborative robots (cobots). Safe and\nefficient collaboration between cobots and humans largely relies on the cobot's\ncomprehensive semantic understanding of the dynamic surrounding of industrial\nenvironments. Despite the importance of semantic understanding for such\napplications, 3D semantic segmentation of collaborative robot workspaces lacks\nsufficient research and dedicated datasets. The performance limitation caused\nby insufficient datasets is called 'data hunger' problem. To overcome this\ncurrent limitation, this work develops a new dataset specifically designed for\nthis use case, named \"COVERED\", which includes point-wise annotated point\nclouds of a robotic cell. Lastly, we also provide a benchmark of current\nstate-of-the-art (SOTA) algorithm performance on the dataset and demonstrate a\nreal-time semantic segmentation of a collaborative robot workspace using a\nmulti-LiDAR system. The promising results from using the trained Deep Networks\non a real-time dynamically changing situation shows that we are on the right\ntrack. Our perception pipeline achieves 20Hz throughput with a prediction point\naccuracy of $>$96\\% and $>$92\\% mean intersection over union (mIOU) while\nmaintaining an 8Hz throughput.\n","authors":["Charith Munasinghe","Fatemeh Mohammadi Amin","Davide Scaramuzza","Hans Wernher van de Venn"],"pdf_url":"https://arxiv.org/pdf/2302.12656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01647v1","updated":"2023-04-04T09:05:11Z","published":"2023-04-04T09:05:11Z","title":"SC-ML: Self-supervised Counterfactual Metric Learning for Debiased\n  Visual Question Answering","summary":"  Visual question answering (VQA) is a critical multimodal task in which an\nagent must answer questions according to the visual cue. Unfortunately,\nlanguage bias is a common problem in VQA, which refers to the model generating\nanswers only by associating with the questions while ignoring the visual\ncontent, resulting in biased results. We tackle the language bias problem by\nproposing a self-supervised counterfactual metric learning (SC-ML) method to\nfocus the image features better. SC-ML can adaptively select the\nquestion-relevant visual features to answer the question, reducing the negative\ninfluence of question-irrelevant visual features on inferring answers. In\naddition, question-irrelevant visual features can be seamlessly incorporated\ninto counterfactual training schemes to further boost robustness. Extensive\nexperiments have proved the effectiveness of our method with improved results\non the VQA-CP dataset. Our code will be made publicly available.\n","authors":["Xinyao Shu","Shiyang Yan","Xu Yang","Ziheng Wu","Zhongfeng Chen","Zhenyu Lu"],"pdf_url":"https://arxiv.org/pdf/2304.01647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00553v2","updated":"2023-04-04T09:04:27Z","published":"2023-04-02T15:04:43Z","title":"From Isolated Islands to Pangea: Unifying Semantic Space for Human\n  Action Understanding","summary":"  Action understanding matters and attracts attention. It can be formed as the\nmapping from the action physical space to the semantic space. Typically,\nresearchers built action datasets according to idiosyncratic choices to define\nclasses and push the envelope of benchmarks respectively. Thus, datasets are\nincompatible with each other like \"Isolated Islands\" due to semantic gaps and\nvarious class granularities, e.g., do housework in dataset A and wash plate in\ndataset B. We argue that a more principled semantic space is an urgent need to\nconcentrate the community efforts and enable us to use all datasets together to\npursue generalizable action learning. To this end, we design a Poincare action\nsemantic space given verb taxonomy hierarchy and covering massive actions. By\naligning the classes of previous datasets to our semantic space, we gather\n(image/video/skeleton/MoCap) datasets into a unified database in a unified\nlabel system, i.e., bridging \"isolated islands\" into a \"Pangea\". Accordingly,\nwe propose a bidirectional mapping model between physical and semantic space to\nfully use Pangea. In extensive experiments, our system shows significant\nsuperiority, especially in transfer learning. Code and data will be made\npublicly available.\n","authors":["Yong-Lu Li","Xiaoqian Wu","Xinpeng Liu","Yiming Dou","Yikun Ji","Junyi Zhang","Yixing Li","Jingru Tan","Xudong Lu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2304.00553v2.pdf","comment":"Project Webpage: https://mvig-rhos.com/pangea"},{"id":"http://arxiv.org/abs/2304.01636v1","updated":"2023-04-04T08:46:47Z","published":"2023-04-04T08:46:47Z","title":"Label-guided Attention Distillation for Lane Segmentation","summary":"  Contemporary segmentation methods are usually based on deep fully\nconvolutional networks (FCNs). However, the layer-by-layer convolutions with a\ngrowing receptive field is not good at capturing long-range contexts such as\nlane markers in the scene. In this paper, we address this issue by designing a\ndistillation method that exploits label structure when training segmentation\nnetwork. The intuition is that the ground-truth lane annotations themselves\nexhibit internal structure. We broadcast the structure hints throughout a\nteacher network, i.e., we train a teacher network that consumes a lane label\nmap as input and attempts to replicate it as output. Then, the attention maps\nof the teacher network are adopted as supervisors of the student segmentation\nnetwork. The teacher network, with label structure information embedded, knows\ndistinctly where the convolution layers should pay visual attention into. The\nproposed method is named as Label-guided Attention Distillation (LGAD). It\nturns out that the student network learns significantly better with LGAD than\nwhen learning alone. As the teacher network is deprecated after training, our\nmethod do not increase the inference time. Note that LGAD can be easily\nincorporated in any lane segmentation network.\n","authors":["Zhikang Liu","Lanyun Zhu"],"pdf_url":"https://arxiv.org/pdf/2304.01636v1.pdf","comment":"Accepted to Neurocomputing 2021"},{"id":"http://arxiv.org/abs/2304.01627v1","updated":"2023-04-04T08:30:50Z","published":"2023-04-04T08:30:50Z","title":"Self-Supervised Image Denoising for Real-World Images with Context-aware\n  Transformer","summary":"  In recent years, the development of deep learning has been pushing image\ndenoising to a new level. Among them, self-supervised denoising is increasingly\npopular because it does not require any prior knowledge. Most of the existing\nself-supervised methods are based on convolutional neural networks (CNN), which\nare restricted by the locality of the receptive field and would cause color\nshifts or textures loss. In this paper, we propose a novel Denoise Transformer\nfor real-world image denoising, which is mainly constructed with Context-aware\nDenoise Transformer (CADT) units and Secondary Noise Extractor (SNE) block.\nCADT is designed as a dual-branch structure, where the global branch uses a\nwindow-based Transformer encoder to extract the global information, while the\nlocal branch focuses on the extraction of local features with small receptive\nfield. By incorporating CADT as basic components, we build a hierarchical\nnetwork to directly learn the noise distribution information through residual\nlearning and obtain the first stage denoised output. Then, we design SNE in low\ncomputation for secondary global noise extraction. Finally the blind spots are\ncollected from the Denoise Transformer output and reconstructed, forming the\nfinal denoised image. Extensive experiments on the real-world SIDD benchmark\nachieve 50.62/0.990 for PSNR/SSIM, which is competitive with the current\nstate-of-the-art method and only 0.17/0.001 lower. Visual comparisons on public\nsRGB, Raw-RGB and greyscale datasets prove that our proposed Denoise\nTransformer has a competitive performance, especially on blurred textures and\nlow-light images, without using additional knowledge, e.g., noise level or\nnoise type, regarding the underlying unknown noise.\n","authors":["Dan Zhang","Fangfang Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.01627v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2304.01110v2","updated":"2023-04-04T08:22:43Z","published":"2023-04-03T16:13:41Z","title":"AutoLabel: CLIP-based framework for Open-set Video Domain Adaptation","summary":"  Open-set Unsupervised Video Domain Adaptation (OUVDA) deals with the task of\nadapting an action recognition model from a labelled source domain to an\nunlabelled target domain that contains \"target-private\" categories, which are\npresent in the target but absent in the source. In this work we deviate from\nthe prior work of training a specialized open-set classifier or weighted\nadversarial learning by proposing to use pre-trained Language and Vision Models\n(CLIP). The CLIP is well suited for OUVDA due to its rich representation and\nthe zero-shot recognition capabilities. However, rejecting target-private\ninstances with the CLIP's zero-shot protocol requires oracle knowledge about\nthe target-private label names. To circumvent the impossibility of the\nknowledge of label names, we propose AutoLabel that automatically discovers and\ngenerates object-centric compositional candidate target-private class names.\nDespite its simplicity, we show that CLIP when equipped with AutoLabel can\nsatisfactorily reject the target-private instances, thereby facilitating better\nalignment between the shared classes of the two domains. The code is available.\n","authors":["Giacomo Zara","Subhankar Roy","Paolo Rota","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2304.01110v2.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01620v1","updated":"2023-04-04T08:21:46Z","published":"2023-04-04T08:21:46Z","title":"Image Blind Denoising Using Dual Convolutional Neural Network with Skip\n  Connection","summary":"  In recent years, deep convolutional neural networks have shown fascinating\nperformance in the field of image denoising. However, deeper network\narchitectures are often accompanied with large numbers of model parameters,\nleading to high training cost and long inference time, which limits their\napplication in practical denoising tasks. In this paper, we propose a novel\ndual convolutional blind denoising network with skip connection (DCBDNet),\nwhich is able to achieve a desirable balance between the denoising effect and\nnetwork complexity. The proposed DCBDNet consists of a noise estimation network\nand a dual convolutional neural network (CNN). The noise estimation network is\nused to estimate the noise level map, which improves the flexibility of the\nproposed model. The dual CNN contains two branches: a u-shaped sub-network is\ndesigned for the upper branch, and the lower branch is composed of the dilated\nconvolution layers. Skip connections between layers are utilized in both the\nupper and lower branches. The proposed DCBDNet was evaluated on several\nsynthetic and real-world image denoising benchmark datasets. Experimental\nresults have demonstrated that the proposed DCBDNet can effectively remove\ngaussian noise in a wide range of levels, spatially variant noise and real\nnoise. With a simple model structure, our proposed DCBDNet still can obtain\ncompetitive denoising performance compared to the state-of-the-art image\ndenoising models containing complex architectures. Namely, a favorable\ntrade-off between denoising performance and model complexity is achieved. Codes\nare available at https://github.com/WenCongWu/DCBDNet.\n","authors":["Wencong Wu","Shicheng Liao","Guannan Lv","Peng Liang","Yungang Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09101v4","updated":"2023-04-04T08:19:34Z","published":"2023-03-16T06:14:18Z","title":"Contrastive Semi-supervised Learning for Underwater Image Restoration\n  via Reliable Bank","summary":"  Despite the remarkable achievement of recent underwater image restoration\ntechniques, the lack of labeled data has become a major hurdle for further\nprogress. In this work, we propose a mean-teacher based Semi-supervised\nUnderwater Image Restoration (Semi-UIR) framework to incorporate the unlabeled\ndata into network training. However, the naive mean-teacher method suffers from\ntwo main problems: (1) The consistency loss used in training might become\nineffective when the teacher's prediction is wrong. (2) Using L1 distance may\ncause the network to overfit wrong labels, resulting in confirmation bias. To\naddress the above problems, we first introduce a reliable bank to store the\n\"best-ever\" outputs as pseudo ground truth. To assess the quality of outputs,\nwe conduct an empirical analysis based on the monotonicity property to select\nthe most trustworthy NR-IQA method. Besides, in view of the confirmation bias\nproblem, we incorporate contrastive regularization to prevent the overfitting\non wrong labels. Experimental results on both full-reference and non-reference\nunderwater benchmarks demonstrate that our algorithm has obvious improvement\nover SOTA methods quantitatively and qualitatively. Code has been released at\nhttps://github.com/Huang-ShiRui/Semi-UIR.\n","authors":["Shirui Huang","Keyan Wang","Huan Liu","Jun Chen","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2303.09101v4.pdf","comment":"CVPR2023"},{"id":"http://arxiv.org/abs/2304.01611v1","updated":"2023-04-04T08:06:40Z","published":"2023-04-04T08:06:40Z","title":"Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder","summary":"  Medical Visual Question Answering (VQA) systems play a supporting role to\nunderstand clinic-relevant information carried by medical images. The questions\nto a medical image include two categories: close-end (such as Yes/No question)\nand open-end. To obtain answers, the majority of the existing medical VQA\nmethods relies on classification approaches, while a few works attempt to use\ngeneration approaches or a mixture of the two. The classification approaches\nare relatively simple but perform poorly on long open-end questions. To bridge\nthis gap, in this paper, we propose a new Transformer based framework for\nmedical VQA (named as Q2ATransformer), which integrates the advantages of both\nthe classification and the generation approaches and provides a unified\ntreatment for the close-end and open-end questions. Specifically, we introduce\nan additional Transformer decoder with a set of learnable candidate answer\nembeddings to query the existence of each answer class to a given\nimage-question pair. Through the Transformer attention, the candidate answer\nembeddings interact with the fused features of the image-question pair to make\nthe decision. In this way, despite being a classification-based approach, our\nmethod provides a mechanism to interact with the answer information for\nprediction like the generation-based approaches. On the other hand, by\nclassification, we mitigate the task difficulty by reducing the search space of\nanswers. Our method achieves new state-of-the-art performance on two medical\nVQA benchmarks. Especially, for the open-end questions, we achieve 79.19% on\nVQA-RAD and 54.85% on PathVQA, with 16.09% and 41.45% absolute improvements,\nrespectively.\n","authors":["Yunyi Liu","Zhanyu Wang","Dong Xu","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.01611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11906v2","updated":"2023-04-04T08:04:19Z","published":"2023-03-21T14:52:52Z","title":"Solving Oscillation Problem in Post-Training Quantization Through a\n  Theoretical Perspective","summary":"  Post-training quantization (PTQ) is widely regarded as one of the most\nefficient compression methods practically, benefitting from its data privacy\nand low computation costs. We argue that an overlooked problem of oscillation\nis in the PTQ methods. In this paper, we take the initiative to explore and\npresent a theoretical proof to explain why such a problem is essential in PTQ.\nAnd then, we try to solve this problem by introducing a principled and\ngeneralized framework theoretically. In particular, we first formulate the\noscillation in PTQ and prove the problem is caused by the difference in module\ncapacity. To this end, we define the module capacity (ModCap) under\ndata-dependent and data-free scenarios, where the differentials between\nadjacent modules are used to measure the degree of oscillation. The problem is\nthen solved by selecting top-k differentials, in which the corresponding\nmodules are jointly optimized and quantized. Extensive experiments demonstrate\nthat our method successfully reduces the performance drop and is generalized to\ndifferent neural networks and PTQ methods. For example, with 2/4 bit ResNet-50\nquantization, our method surpasses the previous state-of-the-art method by\n1.9%. It becomes more significant on small model quantization, e.g. surpasses\nBRECQ method by 6.61% on MobileNetV2*0.5.\n","authors":["Yuexiao Ma","Huixia Li","Xiawu Zheng","Xuefeng Xiao","Rui Wang","Shilei Wen","Xin Pan","Fei Chao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2303.11906v2.pdf","comment":"Accepted by CVPR 2023"},{"id":"http://arxiv.org/abs/2212.09713v2","updated":"2023-04-04T07:52:40Z","published":"2022-12-19T18:42:19Z","title":"A Probabilistic Framework for Lifelong Test-Time Adaptation","summary":"  Test-time adaptation (TTA) is the problem of updating a pre-trained source\nmodel at inference time given test input(s) from a different target domain.\nMost existing TTA approaches assume the setting in which the target domain is\nstationary, i.e., all the test inputs come from a single target domain.\nHowever, in many practical settings, the test input distribution might exhibit\na lifelong/continual shift over time. Moreover, existing TTA approaches also\nlack the ability to provide reliable uncertainty estimates, which is crucial\nwhen distribution shifts occur between the source and target domain. To address\nthese issues, we present PETAL (Probabilistic lifElong Test-time Adaptation\nwith seLf-training prior), which solves lifelong TTA using a probabilistic\napproach, and naturally results in (1) a student-teacher framework, where the\nteacher model is an exponential moving average of the student model, and (2)\nregularizing the model updates at inference time using the source model as a\nregularizer. To prevent model drift in the lifelong/continual TTA setting, we\nalso propose a data-driven parameter restoration technique which contributes to\nreducing the error accumulation and maintaining the knowledge of recent domains\nby restoring only the irrelevant parameters. In terms of predictive error rate\nas well as uncertainty based metrics such as Brier score and negative\nlog-likelihood, our method achieves better results than the current\nstate-of-the-art for online lifelong test-time adaptation across various\nbenchmarks, such as CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC\ndatasets. The source code for our approach is accessible at\nhttps://github.com/dhanajitb/petal.\n","authors":["Dhanajit Brahma","Piyush Rai"],"pdf_url":"https://arxiv.org/pdf/2212.09713v2.pdf","comment":"Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01603v1","updated":"2023-04-04T07:46:40Z","published":"2023-04-04T07:46:40Z","title":"Locate Then Generate: Bridging Vision and Language with Bounding Box for\n  Scene-Text VQA","summary":"  In this paper, we propose a novel multi-modal framework for Scene Text Visual\nQuestion Answering (STVQA), which requires models to read scene text in images\nfor question answering. Apart from text or visual objects, which could exist\nindependently, scene text naturally links text and visual modalities together\nby conveying linguistic semantics while being a visual object in an image\nsimultaneously. Different to conventional STVQA models which take the\nlinguistic semantics and visual semantics in scene text as two separate\nfeatures, in this paper, we propose a paradigm of \"Locate Then Generate\" (LTG),\nwhich explicitly unifies this two semantics with the spatial bounding box as a\nbridge connecting them. Specifically, at first, LTG locates the region in an\nimage that may contain the answer words with an answer location module (ALM)\nconsisting of a region proposal network and a language refinement network, both\nof which can transform to each other with one-to-one mapping via the scene text\nbounding box. Next, given the answer words selected by ALM, LTG generates a\nreadable answer sequence with an answer generation module (AGM) based on a\npre-trained language model. As a benefit of the explicit alignment of the\nvisual and linguistic semantics, even without any scene text based pre-training\ntasks, LTG can boost the absolute accuracy by +6.06% and +6.92% on the TextVQA\ndataset and the ST-VQA dataset respectively, compared with a non-pre-training\nbaseline. We further demonstrate that LTG effectively unifies visual and text\nmodalities through the spatial bounding box connection, which is\nunderappreciated in previous methods.\n","authors":["Yongxin Zhu","Zhen Liu","Yukang Liang","Xin Li","Hao Liu","Changcun Bao","Linli Xu"],"pdf_url":"https://arxiv.org/pdf/2304.01603v1.pdf","comment":"accepted in AAAI 2023"},{"id":"http://arxiv.org/abs/2304.01601v1","updated":"2023-04-04T07:43:56Z","published":"2023-04-04T07:43:56Z","title":"Primitive Simultaneous Optimization of Similarity Metrics for Image\n  Registration","summary":"  Even though simultaneous optimization of similarity metrics represents a\nstandard procedure in the field of semantic segmentation, surprisingly, this\ndoes not hold true for image registration. To close this unexpected gap in the\nliterature, we investigate in a complex multi-modal 3D setting whether\nsimultaneous optimization of registration metrics, here implemented by means of\nprimitive summation, can benefit image registration. We evaluate two\nchallenging datasets containing collections of pre- to post-operative and pre-\nto intra-operative Magnetic Resonance Imaging (MRI) of glioma. Employing the\nproposed optimization we demonstrate improved registration accuracy in terms of\nTarget Registration Error (TRE) on expert neuroradiologists' landmark\nannotations.\n","authors":["Diana Waldmannstetter","Florian Kofler","Benedikt Wiestler","Julian Schwarting","Ivan Ezhov","Marie Metz","Daniel Rueckert","Jan S. Kirschke","Marie Piraud","Bjoern H. Menze"],"pdf_url":"https://arxiv.org/pdf/2304.01601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01598v1","updated":"2023-04-04T07:38:14Z","published":"2023-04-04T07:38:14Z","title":"MM-BSN: Self-Supervised Image Denoising for Real-World with Multi-Mask\n  based on Blind-Spot Network","summary":"  Recent advances in deep learning have been pushing image denoising techniques\nto a new level. In self-supervised image denoising, blind-spot network (BSN) is\none of the most common methods. However, most of the existing BSN algorithms\nuse a dot-based central mask, which is recognized as inefficient for images\nwith large-scale spatially correlated noise. In this paper, we give the\ndefinition of large-noise and propose a multi-mask strategy using multiple\nconvolutional kernels masked in different shapes to further break the noise\nspatial correlation. Furthermore, we propose a novel self-supervised image\ndenoising method that combines the multi-mask strategy with BSN (MM-BSN). We\nshow that different masks can cause significant performance differences, and\nthe proposed MM-BSN can efficiently fuse the features extracted by multi-masked\nlayers, while recovering the texture structures destroyed by multi-masking and\ninformation transmission. Our MM-BSN can be used to address the problem of\nlarge-noise denoising, which cannot be efficiently handled by other BSN\nmethods. Extensive experiments on public real-world datasets demonstrate that\nthe proposed MM-BSN achieves state-of-the-art performance among self-supervised\nand even unpaired image denoising methods for sRGB images denoising, without\nany labelling effort or prior knowledge. Code can be found in\nhttps://github.com/dannie125/MM-BSN.\n","authors":["Dan Zhang","Fangfang Zhou","Yuwen Jiang","Zhengming Fu"],"pdf_url":"https://arxiv.org/pdf/2304.01598v1.pdf","comment":"denoising, self-supervised, sRGB, BSN"},{"id":"http://arxiv.org/abs/2203.12459v3","updated":"2023-04-04T07:24:58Z","published":"2022-03-23T14:54:29Z","title":"Importance Sampling CAMs for Weakly-Supervised Segmentation","summary":"  Classification networks can be used to localize and segment objects in images\nby means of class activation maps (CAMs). However, without pixel-level\nannotations, classification networks are known to (1) mainly focus on\ndiscriminative regions, and (2) to produce diffuse CAMs without well-defined\nprediction contours. In this work, we approach both problems with two\ncontributions for improving CAM learning. First, we incorporate importance\nsampling based on the class-wise probability mass function induced by the CAMs\nto produce stochastic image-level class predictions. This results in CAMs which\nactivate over a larger extent of objects. Second, we formulate a feature\nsimilarity loss term which aims to match the prediction contours with edges in\nthe image. As a third contribution, we conduct experiments on the PASCAL VOC\n2012 benchmark dataset to demonstrate that these modifications significantly\nincrease the performance in terms of contour accuracy, while being comparable\nto current state-of-the-art methods in terms of region similarity.\n","authors":["Arvi Jonnarth","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2203.12459v3.pdf","comment":"Updated to the version published at ICASSP2022"},{"id":"http://arxiv.org/abs/2304.01585v1","updated":"2023-04-04T07:24:51Z","published":"2023-04-04T07:24:51Z","title":"Multi-Channel Time-Series Person and Soft-Biometric Identification","summary":"  Multi-channel time-series datasets are popular in the context of human\nactivity recognition (HAR). On-body device (OBD) recordings of human movements\nare often preferred for HAR applications not only for their reliability but as\nan approach for identity protection, e.g., in industrial settings.\nContradictory, the gait activity is a biometric, as the cyclic movement is\ndistinctive and collectable. In addition, the gait cycle has proven to contain\nsoft-biometric information of human groups, such as age and height. Though\ngeneral human movements have not been considered a biometric, they might\ncontain identity information. This work investigates person and soft-biometrics\nidentification from OBD recordings of humans performing different activities\nusing deep architectures. Furthermore, we propose the use of attribute\nrepresentation for soft-biometric identification. We evaluate the method on\nfour datasets of multi-channel time-series HAR, measuring the performance of a\nperson and soft-biometrics identification and its relation concerning performed\nactivities. We find that person identification is not limited to gait activity.\nThe impact of activities on the identification performance was found to be\ntraining and dataset specific. Soft-biometric based attribute representation\nshows promising results and emphasis the necessity of larger datasets.\n","authors":["Nilah Ravi Nair","Fernando Moya Rueda","Christopher Reining","Gernot A. Fink"],"pdf_url":"https://arxiv.org/pdf/2304.01585v1.pdf","comment":"Accepted at the ICPR 2022 workshop: 12th International Workshop on\n  Human Behavior Understanding"},{"id":"http://arxiv.org/abs/2304.01583v1","updated":"2023-04-04T07:20:06Z","published":"2023-04-04T07:20:06Z","title":"HALO: Hazard-Aware Landing Optimization for Autonomous Systems","summary":"  With autonomous aerial vehicles enacting safety-critical missions, such as\nthe Mars Science Laboratory Curiosity rover's landing on Mars, the tasks of\nautomatically identifying and reasoning about potentially hazardous landing\nsites is paramount. This paper presents a coupled perception-planning solution\nwhich addresses the hazard detection, optimal landing trajectory generation,\nand contingency planning challenges encountered when landing in uncertain\nenvironments. Specifically, we develop and combine two novel algorithms,\nHazard-Aware Landing Site Selection (HALSS) and Adaptive Deferred-Decision\nTrajectory Optimization (Adaptive-DDTO), to address the perception and planning\nchallenges, respectively. The HALSS framework processes point cloud information\nto identify feasible safe landing zones, while Adaptive-DDTO is a multi-target\ncontingency planner that adaptively replans as new perception information is\nreceived. We demonstrate the efficacy of our approach using a simulated Martian\nenvironment and show that our coupled perception-planning method achieves\ngreater landing success whilst being more fuel efficient compared to a\nnonadaptive DDTO approach.\n","authors":["Christopher R. Hayner","Samuel C. Buckner","Daniel Broyles","Evelyn Madewell","Karen Leung","Behcet Acikmese"],"pdf_url":"https://arxiv.org/pdf/2304.01583v1.pdf","comment":"The first two authors have contributed equally to this work. This\n  work is to be published in the proceedings of the 2023 IEEE International\n  Conference on Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2204.06862v2","updated":"2023-04-04T07:19:17Z","published":"2022-04-14T10:27:19Z","title":"An Identity-Preserved Framework for Human Motion Transfer","summary":"  Human motion transfer (HMT) aims to generate a video clip for the target\nsubject by imitating the source subject's motion. Although previous methods\nhave achieved remarkable results in synthesizing good-quality videos, those\nmethods omit the effects of individualized motion information from the source\nand target motions, \\textit{e.g.}, fine and high-frequency motion details, on\nthe realism of the motion in the generated video. To address this problem, we\npropose an identity-preserved HMT network (\\textit{IDPres}), which follows the\npipeline of the skeleton-based method. \\textit{IDpres} takes the individualized\nmotion and skeleton information to enhance motion representations and improve\nthe reality of motions in the generated videos. With individualized motion, our\nmethod focuses on fine-grained disentanglement and synthesis of motion. In\norder to improve the representation capability in latent space and facilitate\nthe training of \\textit{IDPres}, we design a training scheme, which allows\n\\textit{IDPres} to disentangle different representations simultaneously and\ncontrol them to synthesize ideal motions accurately. Furthermore, to our best\nknowledge, there are no available metrics for evaluating the proportion of\nidentity information (both individualized motion and skeleton information) in\nthe generated video. Therefore, we propose a novel quantitative metric called\nIdentity Score (\\textit{IDScore}) based on gait recognition. We also collected\na dataset with 101 subjects' solo-dance videos from the public domain, named\n$Dancer101$, to evaluate the method. The comprehensive experiments show the\nproposed method outperforms state-of-the-art methods in terms of reconstruction\naccuracy and realistic motion.\n","authors":["Jingzhe Ma","Xiaoqing Zhang","Shiqi Yu"],"pdf_url":"https://arxiv.org/pdf/2204.06862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01580v1","updated":"2023-04-04T07:17:31Z","published":"2023-04-04T07:17:31Z","title":"Untargeted Near-collision Attacks in Biometric Recognition","summary":"  A biometric recognition system can operate in two distinct modes,\nidentification or verification. In the first mode, the system recognizes an\nindividual by searching the enrolled templates of all the users for a match. In\nthe second mode, the system validates a claimed identity by comparing the fresh\ntemplate with the enrolled template for this identity. Both the experimentally\ndetermined false match rate and false non-match rate through recognition\nthreshold adjustment define the recognition accuracy, and hence the security of\nthe system. The biometric transformation schemes usually produce binary\ntemplates that are better handled by cryptographic schemes. One of the\nrequirements for these transformation schemes is their irreversibility. In this\nwork, we rely on probabilistic modelling to quantify the security strength of\nbinary templates. We investigate the influence of template size, database size\nand threshold on the probability of having a near-collision, and we highlight\ntwo attacks on biometric systems. We discuss the choice of parameters through\nthe generic presented attacks.\n","authors":["Axel Durbet","Paul-Marie Grollemund","Kevin Thiry-Atighehchi"],"pdf_url":"https://arxiv.org/pdf/2304.01580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00254v2","updated":"2023-04-04T07:08:41Z","published":"2023-04-01T08:06:43Z","title":"DOAD: Decoupled One Stage Action Detection Network","summary":"  Localizing people and recognizing their actions from videos is a challenging\ntask towards high-level video understanding. Existing methods are mostly\ntwo-stage based, with one stage for person bounding box generation and the\nother stage for action recognition. However, such two-stage methods are\ngenerally with low efficiency. We observe that directly unifying detection and\naction recognition normally suffers from (i) inferior learning due to different\ndesired properties of context representation for detection and action\nrecognition; (ii) optimization difficulty with insufficient training data. In\nthis work, we present a decoupled one-stage network dubbed DOAD, to mitigate\nabove issues and improve the efficiency for spatio-temporal action detection.\nTo achieve it, we decouple detection and action recognition into two branches.\nSpecifically, one branch focuses on detection representation for actor\ndetection, and the other one for action recognition. For the action branch, we\ndesign a transformer-based module (TransPC) to model pairwise relationships\nbetween people and context. Different from commonly used vector-based dot\nproduct in self-attention, it is built upon a novel matrix-based key and value\nfor Hadamard attention to model person-context information. It not only\nexploits relationships between person pairs but also takes into account context\nand relative position information. The results on AVA and UCF101-24 datasets\nshow that our method is competitive with two-stage state-of-the-art methods\nwith significant efficiency improvement.\n","authors":["Shuning Chang","Pichao Wang","Fan Wang","Jiashi Feng","Mike Zheng Show"],"pdf_url":"https://arxiv.org/pdf/2304.00254v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2303.15553v2","updated":"2023-04-04T07:06:55Z","published":"2023-03-27T19:12:02Z","title":"MoViT: Memorizing Vision Transformers for Medical Image Analysis","summary":"  The synergy of long-range dependencies from transformers and local\nrepresentations of image content from convolutional neural networks (CNNs) has\nled to advanced architectures and increased performance for various medical\nimage analysis tasks due to their complementary benefits. However, compared\nwith CNNs, transformers require considerably more training data, due to a\nlarger number of parameters and an absence of inductive bias. The need for\nincreasingly large datasets continues to be problematic, particularly in the\ncontext of medical imaging, where both annotation efforts and data protection\nresult in limited data availability. In this work, inspired by the human\ndecision-making process of correlating new ``evidence'' with previously\nmemorized ``experience'', we propose a Memorizing Vision Transformer (MoViT) to\nalleviate the need for large-scale datasets to successfully train and deploy\ntransformer-based architectures. MoViT leverages an external memory structure\nto cache history attention snapshots during the training stage. To prevent\noverfitting, we incorporate an innovative memory update scheme, attention\ntemporal moving average, to update the stored external memories with the\nhistorical moving average. For inference speedup, we design a prototypical\nattention learning method to distill the external memory into smaller\nrepresentative subsets. We evaluate our method on a public histology image\ndataset and an in-house MRI dataset, demonstrating that MoViT applied to varied\nmedical image analysis tasks, can outperform vanilla transformer models across\nvaried data regimes, especially in cases where only a small amount of annotated\ndata is available. More importantly, MoViT can reach a competitive performance\nof ViT with only 3.0% of the training data.\n","authors":["Yiqing Shen","Pengfei Guo","Jingpu Wu","Qianqi Huang","Jinyuan Zhou","Shanshan Jiang","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2303.15553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18044v2","updated":"2023-04-04T07:05:46Z","published":"2023-03-31T13:28:06Z","title":"Long-Short Temporal Co-Teaching for Weakly Supervised Video Anomaly\n  Detection","summary":"  Weakly supervised video anomaly detection (WS-VAD) is a challenging problem\nthat aims to learn VAD models only with video-level annotations. In this work,\nwe propose a Long-Short Temporal Co-teaching (LSTC) method to address the\nWS-VAD problem. It constructs two tubelet-based spatio-temporal transformer\nnetworks to learn from short- and long-term video clips respectively. Each\nnetwork is trained with respect to a multiple instance learning (MIL)-based\nranking loss, together with a cross-entropy loss when clip-level pseudo labels\nare available. A co-teaching strategy is adopted to train the two networks.\nThat is, clip-level pseudo labels generated from each network are used to\nsupervise the other one at the next training round, and the two networks are\nlearned alternatively and iteratively. Our proposed method is able to better\ndeal with the anomalies with varying durations as well as subtle anomalies.\nExtensive experiments on three public datasets demonstrate that our method\noutperforms state-of-the-art WS-VAD methods.\n","authors":["Shengyang Sun","Xiaojin Gong"],"pdf_url":"https://arxiv.org/pdf/2303.18044v2.pdf","comment":"Accepted by ICME 2023"},{"id":"http://arxiv.org/abs/2304.01576v1","updated":"2023-04-04T07:05:15Z","published":"2023-04-04T07:05:15Z","title":"MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network\n  with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan","summary":"  Accurate lung nodule segmentation is crucial for early-stage lung cancer\ndiagnosis, as it can substantially enhance patient survival rates. Computed\ntomography (CT) images are widely employed for early diagnosis in lung nodule\nanalysis. However, the heterogeneity of lung nodules, size diversity, and the\ncomplexity of the surrounding environment pose challenges for developing robust\nnodule segmentation methods. In this study, we propose an efficient end-to-end\nframework, the multi-encoder-based self-adaptive hard attention network\n(MESAHA-Net), for precise lung nodule segmentation in CT scans. MESAHA-Net\ncomprises three encoding paths, an attention block, and a decoder block,\nfacilitating the integration of three types of inputs: CT slice patches,\nforward and backward maximum intensity projection (MIP) images, and region of\ninterest (ROI) masks encompassing the nodule. By employing a novel adaptive\nhard attention mechanism, MESAHA-Net iteratively performs slice-by-slice 2D\nsegmentation of lung nodules, focusing on the nodule region in each slice to\ngenerate 3D volumetric segmentation of lung nodules. The proposed framework has\nbeen comprehensively evaluated on the LIDC-IDRI dataset, the largest publicly\navailable dataset for lung nodule segmentation. The results demonstrate that\nour approach is highly robust for various lung nodule types, outperforming\nprevious state-of-the-art techniques in terms of segmentation accuracy and\ncomputational complexity, rendering it suitable for real-time clinical\nimplementation.\n","authors":["Muhammad Usman","Azka Rehman","Abdullah Shahid","Siddique Latif","Shi Sub Byon","Sung Hyun Kim","Tariq Mahmood Khan","Yeong Gil Shin"],"pdf_url":"https://arxiv.org/pdf/2304.01576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01567v1","updated":"2023-04-04T06:44:13Z","published":"2023-04-04T06:44:13Z","title":"A real-time algorithm for human action recognition in RGB and thermal\n  video","summary":"  Monitoring the movement and actions of humans in video in real-time is an\nimportant task. We present a deep learning based algorithm for human action\nrecognition for both RGB and thermal cameras. It is able to detect and track\nhumans and recognize four basic actions (standing, walking, running, lying) in\nreal-time on a notebook with a NVIDIA GPU. For this, it combines state of the\nart components for object detection (Scaled YoloV4), optical flow (RAFT) and\npose estimation (EvoSkeleton). Qualitative experiments on a set of tunnel\nvideos show that the proposed algorithm works robustly for both RGB and thermal\nvideo.\n","authors":["Hannes Fassold","Karlheinz Gutjahr","Anna Weber","Roland Perko"],"pdf_url":"https://arxiv.org/pdf/2304.01567v1.pdf","comment":"Accepted for SPIE Real-Time Image Processing and Deep Learning\n  Conference 2023"},{"id":"http://arxiv.org/abs/2111.08954v3","updated":"2023-04-04T06:43:35Z","published":"2021-11-17T07:53:45Z","title":"Tracklet-Switch Adversarial Attack against Pedestrian Multi-Object\n  Tracking Trackers","summary":"  Multi-Object Tracking (MOT) has achieved aggressive progress and derived many\nexcellent deep learning trackers. Meanwhile, most deep learning models are\nknown to be vulnerable to adversarial examples that are crafted with small\nperturbations but could mislead the model prediction. In this work, we observe\nthat the robustness on the MOT trackers is rarely studied, and it is\nchallenging to attack the MOT system since its mature association algorithms\nare designed to be robust against errors during the tracking. To this end, we\nanalyze the vulnerability of popular MOT trackers and propose a novel\nadversarial attack method called Tracklet-Switch (TraSw) against the complete\ntracking pipeline of MOT. The proposed TraSw can fool the advanced deep\npedestrian trackers (i.e., FairMOT and ByteTrack), causing them fail to track\nthe targets in the subsequent frames by perturbing very few frames. Experiments\non the MOT-Challenge datasets (i.e., 2DMOT15, MOT17, and MOT20) show that TraSw\ncan achieve an extraordinarily high success attack rate of over 95% by\nattacking only four frames on average. To our knowledge, this is the first work\non the adversarial attack against the pedestrian MOT trackers. Code is\navailable at https://github.com/JHL-HUST/TraSw .\n","authors":["Delv Lin","Qi Chen","Chengyu Zhou","Kun He"],"pdf_url":"https://arxiv.org/pdf/2111.08954v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01565v1","updated":"2023-04-04T06:41:15Z","published":"2023-04-04T06:41:15Z","title":"A Survey on Graph Diffusion Models: Generative AI in Science for\n  Molecule, Protein and Material","summary":"  Diffusion models have become a new SOTA generative modeling method in various\nfields, for which there are multiple survey works that provide an overall\nsurvey. With the number of articles on diffusion models increasing\nexponentially in the past few years, there is an increasing need for surveys of\ndiffusion models on specific fields. In this work, we are committed to\nconducting a survey on the graph diffusion models. Even though our focus is to\ncover the progress of diffusion models in graphs, we first briefly summarize\nhow other generative modeling methods are used for graphs. After that, we\nintroduce the mechanism of diffusion models in various forms, which facilitates\nthe discussion on the graph diffusion models. The applications of graph\ndiffusion models mainly fall into the category of AI-generated content (AIGC)\nin science, for which we mainly focus on how graph diffusion models are\nutilized for generating molecules and proteins but also cover other cases,\nincluding materials design. Moreover, we discuss the issue of evaluating\ndiffusion models in the graph domain and the existing challenges.\n","authors":["Mengchun Zhang","Maryam Qamar","Taegoo Kang","Yuna Jung","Chenshuang Zhang","Sung-Ho Bae","Chaoning Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07700v2","updated":"2023-04-04T06:31:14Z","published":"2023-03-14T08:28:36Z","title":"PATS: Patch Area Transportation with Subdivision for Local Feature\n  Matching","summary":"  Local feature matching aims at establishing sparse correspondences between a\npair of images. Recently, detector-free methods present generally better\nperformance but are not satisfactory in image pairs with large scale\ndifferences. In this paper, we propose Patch Area Transportation with\nSubdivision (PATS) to tackle this issue. Instead of building an expensive image\npyramid, we start by splitting the original image pair into equal-sized patches\nand gradually resizing and subdividing them into smaller patches with the same\nscale. However, estimating scale differences between these patches is\nnon-trivial since the scale differences are determined by both relative camera\nposes and scene structures, and thus spatially varying over image pairs.\nMoreover, it is hard to obtain the ground truth for real scenes. To this end,\nwe propose patch area transportation, which enables learning scale differences\nin a self-supervised manner. In contrast to bipartite graph matching, which\nonly handles one-to-one matching, our patch area transportation can deal with\nmany-to-many relationships. PATS improves both matching accuracy and coverage,\nand shows superior performance in downstream tasks, such as relative pose\nestimation, visual localization, and optical flow estimation. The source code\nis available at \\url{https://zju3dv.github.io/pats/}.\n","authors":["Junjie Ni","Yijin Li","Zhaoyang Huang","Hongsheng Li","Hujun Bao","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.07700v2.pdf","comment":"Accepted to CVPR 2023. Project page: https://zju3dv.github.io/pats"},{"id":"http://arxiv.org/abs/2304.01555v1","updated":"2023-04-04T06:15:53Z","published":"2023-04-04T06:15:53Z","title":"Real-time Driver Monitoring Systems on Edge AI Device","summary":"  As road accident cases are increasing due to the inattention of the driver,\nautomated driver monitoring systems (DMS) have gained an increase in\nacceptance. In this report, we present a real-time DMS system that runs on a\nhardware-accelerator-based edge device. The system consists of an InfraRed\ncamera to record the driver footage and an edge device to process the data. To\nsuccessfully port the deep learning models to run on the edge device taking\nfull advantage of the hardware accelerators, model surgery was performed. The\nfinal DMS system achieves 63 frames per second (FPS) on the TI-TDA4VM edge\ndevice.\n","authors":["Jyothi Hariharan","Rahul Rama Varior","Sunil Karunakaran"],"pdf_url":"https://arxiv.org/pdf/2304.01555v1.pdf","comment":"Driver Monitoring System tech report - Ignitarium Technology\n  Solutions Private Limited"},{"id":"http://arxiv.org/abs/2304.01554v1","updated":"2023-04-04T06:13:33Z","published":"2023-04-04T06:13:33Z","title":"\\emph{MEnsA}: Mix-up Ensemble Average for Unsupervised Multi Target\n  Domain Adaptation on 3D Point Clouds","summary":"  Unsupervised domain adaptation (UDA) addresses the problem of distribution\nshift between the unlabeled target domain and labelled source domain. While the\nsingle target domain adaptation (STDA) is well studied in both 2D and 3D vision\nliterature, multi-target domain adaptation (MTDA) is barely explored for 3D\ndata despite its wide real-world applications such as autonomous driving\nsystems for various geographical and climatic conditions. We establish an MTDA\nbaseline for 3D point cloud data by proposing to mix the feature\nrepresentations from all domains together to achieve better domain adaptation\nperformance by an ensemble average, which we call \\emph{{\\bf M}ixup {\\bf\nEns}emble {\\bf A}verage} or {\\bf \\emph{MEnsA}}. With the mixed representation,\nwe use a domain classifier to improve at distinguishing the feature\nrepresentations of source domain from those of target domains in a shared\nlatent space. In extensive empirical validations on the challenging PointDA-10\ndataset, we showcase a clear benefit of our simple method over previous\nunsupervised STDA and MTDA methods by large margins (up to $17.10\\%$ and\n$4.76\\%$ on averaged over all domain shifts). We make the code publicly\navailable\n\\href{https://github.com/sinAshish/MEnsA_mtda}{here}\\footnote{\\url{https://github.com/sinAshish/MEnsA_mtda}}.\n","authors":["Ashish Sinha","Jonghyun Choi"],"pdf_url":"https://arxiv.org/pdf/2304.01554v1.pdf","comment":"Accepted as poster at L3D-IVU (arxival track) and CL VIsion\n  (non-arxvial track) CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01552v1","updated":"2023-04-04T06:06:59Z","published":"2023-04-04T06:06:59Z","title":"Meta-Learning with a Geometry-Adaptive Preconditioner","summary":"  Model-agnostic meta-learning (MAML) is one of the most successful\nmeta-learning algorithms. It has a bi-level optimization structure where the\nouter-loop process learns a shared initialization and the inner-loop process\noptimizes task-specific weights. Although MAML relies on the standard gradient\ndescent in the inner-loop, recent studies have shown that controlling the\ninner-loop's gradient descent with a meta-learned preconditioner can be\nbeneficial. Existing preconditioners, however, cannot simultaneously adapt in a\ntask-specific and path-dependent way. Additionally, they do not satisfy the\nRiemannian metric condition, which can enable the steepest descent learning\nwith preconditioned gradient. In this study, we propose Geometry-Adaptive\nPreconditioned gradient descent (GAP) that can overcome the limitations in\nMAML; GAP can efficiently meta-learn a preconditioner that is dependent on\ntask-specific parameters, and its preconditioner can be shown to be a\nRiemannian metric. Thanks to the two properties, the geometry-adaptive\npreconditioner is effective for improving the inner-loop optimization.\nExperiment results show that GAP outperforms the state-of-the-art MAML family\nand preconditioned gradient descent-MAML (PGD-MAML) family in a variety of\nfew-shot learning tasks. Code is available at:\nhttps://github.com/Suhyun777/CVPR23-GAP.\n","authors":["Suhyun Kang","Duhun Hwang","Moonjung Eo","Taesup Kim","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2304.01552v1.pdf","comment":"Accepted at CVPR 2023. Code is available at:\n  https://github.com/Suhyun777/CVPR23-GAP"},{"id":"http://arxiv.org/abs/2304.00801v2","updated":"2023-04-04T05:59:43Z","published":"2023-04-03T08:46:56Z","title":"Noisy Image Segmentation With Soft-Dice","summary":"  This paper presents a study on the soft-Dice loss, one of the most popular\nloss functions in medical image segmentation, for situations where noise is\npresent in target labels. In particular, the set of optimal solutions are\ncharacterized and sharp bounds on the volume bias of these solutions are\nprovided. It is further shown that a sequence of soft segmentations converging\nto optimal soft-Dice also converges to optimal Dice when converted to hard\nsegmentations using thresholding. This is an important result because soft-Dice\nis often used as a proxy for maximizing the Dice metric. Finally, experiments\nconfirming the theoretical results are provided.\n","authors":["Marcus Nordström","Henrik Hult","Atsuto Maki","Fredrik Löfman"],"pdf_url":"https://arxiv.org/pdf/2304.00801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.01146v3","updated":"2023-04-04T05:55:13Z","published":"2023-01-03T15:11:41Z","title":"Rethinking Mobile Block for Efficient Attention-based Models","summary":"  This paper focuses on developing modern, efficient, lightweight models for\ndense predictions while trading off parameters, FLOPs, and performance.\nInverted Residual Block (IRB) serves as the infrastructure for lightweight\nCNNs, but no counterpart has been recognized by attention-based studies. This\nwork rethinks lightweight infrastructure from efficient IRB and effective\ncomponents of Transformer from a unified perspective, extending CNN-based IRB\nto attention-based models and abstracting a one-residual Meta Mobile Block\n(MMB) for lightweight model design. Following simple but effective design\ncriterion, we deduce a modern Inverted Residual Mobile Block (iRMB) and build a\nResNet-like Efficient MOdel (EMO) with only iRMB for down-stream tasks.\nExtensive experiments on ImageNet-1K, COCO2017, and ADE20K benchmarks\ndemonstrate the superiority of our EMO over state-of-the-art methods, e.g.,\nEMO-1M/2M/5M achieve 71.5, 75.1, and 78.4 Top-1 that surpass equal-order\nCNN-/Attention-based models, while trading-off the parameter, efficiency, and\naccuracy well: running 2.8-4.0 faster than EdgeNeXt on iPhone14. Code is\navailable.\n","authors":["Jiangning Zhang","Xiangtai Li","Jian Li","Liang Liu","Zhucun Xue","Boshen Zhang","Zhengkai Jiang","Tianxin Huang","Yabiao Wang","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2301.01146v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00001v2","updated":"2023-04-04T05:52:42Z","published":"2023-01-11T15:24:09Z","title":"Determination of cutting positions of honeycomb blocks using computer\n  vision","summary":"  The article discusses a method for automating the process of cutting a\nhoneycomb block, and specifically obtaining points and cutting angles for the\nrequired faces. The following requirements are taken into account in the\ncalculations: the allowable location of the cut plane is 0.4 of the length of\nthe cell face, the cut plane must be perpendicular to the cell wall. The\nalgorithm itself consists of two main stages: determining the honeycomb\nstructure and searching for cut points. In the absence of significant defects\nin honeycomb blocks (deformation of the cell profile and a dent on the edges of\nthe cells), the structure determination algorithm works without significant\ninaccuracies. The results of the cut point search algorithm can be considered\nsatisfactory.\n","authors":["Alexander Razumovsky","Yakov Pikalov","Mikhail Saramud"],"pdf_url":"https://arxiv.org/pdf/2304.00001v2.pdf","comment":"5 pages, in Russian language, 5 figures, for ASEDU-III"},{"id":"http://arxiv.org/abs/2206.05897v3","updated":"2023-04-04T05:44:39Z","published":"2022-06-13T04:03:49Z","title":"$\\texttt{GradICON}$: Approximate Diffeomorphisms via Gradient Inverse\n  Consistency","summary":"  We present an approach to learning regular spatial transformations between\nimage pairs in the context of medical image registration. Contrary to\noptimization-based registration techniques and many modern learning-based\nmethods, we do not directly penalize transformation irregularities but instead\npromote transformation regularity via an inverse consistency penalty. We use a\nneural network to predict a map between a source and a target image as well as\nthe map when swapping the source and target images. Different from existing\napproaches, we compose these two resulting maps and regularize deviations of\nthe $\\bf{Jacobian}$ of this composition from the identity matrix. This\nregularizer -- $\\texttt{GradICON}$ -- results in much better convergence when\ntraining registration models compared to promoting inverse consistency of the\ncomposition of maps directly while retaining the desirable implicit\nregularization effects of the latter. We achieve state-of-the-art registration\nperformance on a variety of real-world medical image datasets using a single\nset of hyperparameters and a single non-dataset-specific training protocol.\n","authors":["Lin Tian","Hastings Greer","François-Xavier Vialard","Roland Kwitt","Raúl San José Estépar","Richard Jarrett Rushmore","Nikolaos Makris","Sylvain Bouix","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2206.05897v3.pdf","comment":"29 pages, 16 figures, CVPR 2023"},{"id":"http://arxiv.org/abs/2203.05565v2","updated":"2023-04-04T05:26:19Z","published":"2022-03-10T00:32:45Z","title":"LiftReg: Limited Angle 2D/3D Deformable Registration","summary":"  We propose LiftReg, a 2D/3D deformable registration approach. LiftReg is a\ndeep registration framework which is trained using sets of digitally\nreconstructed radiographs (DRR) and computed tomography (CT) image pairs. By\nusing simulated training data, LiftReg can use a high-quality CT-CT image\nsimilarity measure, which helps the network to learn a high-quality deformation\nspace. To further improve registration quality and to address the inherent\ndepth ambiguities of very limited angle acquisitions, we propose to use\nfeatures extracted from the backprojected 2D images and a statistical\ndeformation model. We test our approach on the DirLab lung registration dataset\nand show that it outperforms an existing learning-based pairwise registration\napproach.\n","authors":["Lin Tian","Yueh Z. Lee","Raúl San José Estépar","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2203.05565v2.pdf","comment":"MICCAI 2022"},{"id":"http://arxiv.org/abs/2304.01537v1","updated":"2023-04-04T05:21:23Z","published":"2023-04-04T05:21:23Z","title":"PartMix: Regularization Strategy to Learn Part Discovery for\n  Visible-Infrared Person Re-identification","summary":"  Modern data augmentation using a mixture-based technique can regularize the\nmodels from overfitting to the training data in various computer vision\napplications, but a proper data augmentation technique tailored for the\npart-based Visible-Infrared person Re-IDentification (VI-ReID) models remains\nunexplored. In this paper, we present a novel data augmentation technique,\ndubbed PartMix, that synthesizes the augmented samples by mixing the part\ndescriptors across the modalities to improve the performance of part-based\nVI-ReID models. Especially, we synthesize the positive and negative samples\nwithin the same and across different identities and regularize the backbone\nmodel through contrastive learning. In addition, we also present an\nentropy-based mining strategy to weaken the adverse impact of unreliable\npositive and negative samples. When incorporated into existing part-based\nVI-ReID model, PartMix consistently boosts the performance. We conduct\nexperiments to demonstrate the effectiveness of our PartMix over the existing\nVI-ReID methods and provide ablation studies.\n","authors":["Minsu Kim","Seungryong Kim","JungIn Park","Seongheon Park","Kwanghoon Sohn"],"pdf_url":"https://arxiv.org/pdf/2304.01537v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01534v1","updated":"2023-04-04T05:14:25Z","published":"2023-04-04T05:14:25Z","title":"FedBEVT: Federated Learning Bird's Eye View Perception Transformer in\n  Road Traffic Systems","summary":"  Bird's eye view (BEV) perception is becoming increasingly important in the\nfield of autonomous driving. It uses multi-view camera data to learn a\ntransformer model that directly projects the perception of the road environment\nonto the BEV perspective. However, training a transformer model often requires\na large amount of data, and as camera data for road traffic is often private,\nit is typically not shared. Federated learning offers a solution that enables\nclients to collaborate and train models without exchanging data. In this paper,\nwe propose FedBEVT, a federated transformer learning approach for BEV\nperception. We address two common data heterogeneity issues in FedBEVT: (i)\ndiverse sensor poses and (ii) varying sensor numbers in perception systems. We\npresent federated learning with camera-attentive personalization~(FedCaP) and\nadaptive multi-camera masking~(AMCM) to enhance the performance in real-world\nscenarios. To evaluate our method in real-world settings, we create a dataset\nconsisting of four typical federated use cases. Our findings suggest that\nFedBEVT outperforms the baseline approaches in all four use cases,\ndemonstrating the potential of our approach for improving BEV perception in\nautonomous driving. We will make all codes and data publicly available.\n","authors":["Rui Song","Runsheng Xu","Andreas Festag","Jiaqi Ma","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2304.01534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01529v1","updated":"2023-04-04T04:47:44Z","published":"2023-04-04T04:47:44Z","title":"IterativePFN: True Iterative Point Cloud Filtering","summary":"  The quality of point clouds is often limited by noise introduced during their\ncapture process. Consequently, a fundamental 3D vision task is the removal of\nnoise, known as point cloud filtering or denoising. State-of-the-art learning\nbased methods focus on training neural networks to infer filtered displacements\nand directly shift noisy points onto the underlying clean surfaces. In high\nnoise conditions, they iterate the filtering process. However, this iterative\nfiltering is only done at test time and is less effective at ensuring points\nconverge quickly onto the clean surfaces. We propose IterativePFN (iterative\npoint cloud filtering network), which consists of multiple IterationModules\nthat model the true iterative filtering process internally, within a single\nnetwork. We train our IterativePFN network using a novel loss function that\nutilizes an adaptive ground truth target at each iteration to capture the\nrelationship between intermediate filtering results during training. This\nensures that the filtered results converge faster to the clean surfaces. Our\nmethod is able to obtain better performance compared to state-of-the-art\nmethods. The source code can be found at:\nhttps://github.com/ddsediri/IterativePFN.\n","authors":["Dasith de Silva Edirimuni","Xuequan Lu","Zhiwen Shao","Gang Li","Antonio Robles-Kelly","Ying He"],"pdf_url":"https://arxiv.org/pdf/2304.01529v1.pdf","comment":"This paper has been accepted to the IEEE/CVF CVPR Conference, 2023"},{"id":"http://arxiv.org/abs/2304.01524v1","updated":"2023-04-04T04:30:25Z","published":"2023-04-04T04:30:25Z","title":"FisHook -- An Optimized Approach to Marine Specie Classification using\n  MobileNetV2","summary":"  Marine ecosystems are vital for the planet's health, but human activities\nsuch as climate change, pollution, and overfishing pose a constant threat to\nmarine species. Accurate classification and monitoring of these species can aid\nin understanding their distribution, population dynamics, and the impact of\nhuman activities on them. However, classifying marine species can be\nchallenging due to their vast diversity and the complex underwater environment.\nWith advancements in computer performance and GPU-based computing,\ndeep-learning algorithms can now efficiently classify marine species, making it\neasier to monitor and manage marine ecosystems. In this paper, we propose an\noptimization to the MobileNetV2 model to achieve a 99.83% average validation\naccuracy by highlighting specific guidelines for creating a dataset and\naugmenting marine species images. This transfer learning algorithm can be\ndeployed successfully on a mobile application for on-site classification at\nfisheries.\n","authors":["Kohav Dey","Krishna Bajaj","K S Ramalakshmi","Samuel Thomas","Sriram Radhakrishna"],"pdf_url":"https://arxiv.org/pdf/2304.01524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01519v1","updated":"2023-04-04T04:05:56Z","published":"2023-04-04T04:05:56Z","title":"LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation","summary":"  Bird's-Eye View (BEV) features are popular intermediate scene representations\nshared by the 3D backbone and the detector head in LiDAR-based object\ndetectors. However, little research has been done to investigate how to\nincorporate additional supervision on the BEV features to improve proposal\ngeneration in the detector head, while still balancing the number of powerful\n3D layers and efficient 2D network operations. This paper proposes a novel\nscene representation that encodes both the semantics and geometry of the 3D\nenvironment in 2D, which serves as a dense supervision signal for better BEV\nfeature learning. The key idea is to use auxiliary networks to predict a\ncombination of explicit and implicit semantic probabilities by exploiting their\ncomplementary properties. Extensive experiments show that our simple yet\neffective design can be easily integrated into most state-of-the-art 3D object\ndetectors and consistently improves upon baseline models.\n","authors":["Haitao Yang","Zaiwei Zhang","Xiangru Huang","Min Bai","Chen Song","Bo Sun","Li Erran Li","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2304.01519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01515v1","updated":"2023-04-04T03:52:49Z","published":"2023-04-04T03:52:49Z","title":"Text-Conditioned Sampling Framework for Text-to-Image Generation with\n  Masked Generative Models","summary":"  Token-based masked generative models are gaining popularity for their fast\ninference time with parallel decoding. While recent token-based approaches\nachieve competitive performance to diffusion-based models, their generation\nperformance is still suboptimal as they sample multiple tokens simultaneously\nwithout considering the dependence among them. We empirically investigate this\nproblem and propose a learnable sampling model, Text-Conditioned Token\nSelection (TCTS), to select optimal tokens via localized supervision with text\ninformation. TCTS improves not only the image quality but also the semantic\nalignment of the generated images with the given texts. To further improve the\nimage quality, we introduce a cohesive sampling strategy, Frequency Adaptive\nSampling (FAS), to each group of tokens divided according to the self-attention\nmaps. We validate the efficacy of TCTS combined with FAS with various\ngenerative tasks, demonstrating that it significantly outperforms the baselines\nin image-text alignment and image quality. Our text-conditioned sampling\nframework further reduces the original inference time by more than 50% without\nmodifying the original generative model.\n","authors":["Jaewoong Lee","Sangwon Jang","Jaehyeong Jo","Jaehong Yoon","Yunji Kim","Jin-Hwa Kim","Jung-Woo Ha","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2304.01515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01514v1","updated":"2023-04-04T03:48:56Z","published":"2023-04-04T03:48:56Z","title":"Robust Outlier Rejection for 3D Registration with Variational Bayes","summary":"  Learning-based outlier (mismatched correspondence) rejection for robust 3D\nregistration generally formulates the outlier removal as an inlier/outlier\nclassification problem. The core for this to be successful is to learn the\ndiscriminative inlier/outlier feature representations. In this paper, we\ndevelop a novel variational non-local network-based outlier rejection framework\nfor robust alignment. By reformulating the non-local feature learning with\nvariational Bayesian inference, the Bayesian-driven long-range dependencies can\nbe modeled to aggregate discriminative geometric context information for\ninlier/outlier distinction. Specifically, to achieve such Bayesian-driven\ncontextual dependencies, each query/key/value component in our non-local\nnetwork predicts a prior feature distribution and a posterior one. Embedded\nwith the inlier/outlier label, the posterior feature distribution is\nlabel-dependent and discriminative. Thus, pushing the prior to be close to the\ndiscriminative posterior in the training step enables the features sampled from\nthis prior at test time to model high-quality long-range dependencies. Notably,\nto achieve effective posterior feature guidance, a specific probabilistic\ngraphical model is designed over our non-local model, which lets us derive a\nvariational low bound as our optimization objective for model training.\nFinally, we propose a voting-based inlier searching strategy to cluster the\nhigh-quality hypothetical inliers for transformation estimation. Extensive\nexperiments on 3DMatch, 3DLoMatch, and KITTI datasets verify the effectiveness\nof our method.\n","authors":["Haobo Jiang","Zheng Dang","Zhen Wei","Jin Xie","Jian Yang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2304.01514v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2304.01508v1","updated":"2023-04-04T03:36:14Z","published":"2023-04-04T03:36:14Z","title":"EPVT: Environment-aware Prompt Vision Transformer for Domain\n  Generalization in Skin Lesion Recognition","summary":"  Skin lesion recognition using deep learning has made remarkable progress, and\nthere is an increasing need for deploying these systems in real-world\nscenarios. However, recent research has revealed that deep neural networks for\nskin lesion recognition may overly depend on disease-irrelevant image artifacts\n(i.e. dark corners, dense hairs), leading to poor generalization in unseen\nenvironments. To address this issue, we propose a novel domain generalization\nmethod called EPVT, which involves embedding prompts into the vision\ntransformer to collaboratively learn knowledge from diverse domains.\nConcretely, EPVT leverages a set of domain prompts, each of which plays as a\ndomain expert, to capture domain-specific knowledge; and a shared prompt for\ngeneral knowledge over the entire dataset. To facilitate knowledge sharing and\nthe interaction of different prompts, we introduce a domain prompt generator\nthat enables low-rank multiplicative updates between domain prompts and the\nshared prompt. A domain mixup strategy is additionally devised to reduce the\nco-occurring artifacts in each domain, which allows for more flexible decision\nmargins and mitigates the issue of incorrectly assigned domain labels.\nExperiments on four out-of-distribution datasets and six different biased ISIC\ndatasets demonstrate the superior generalization ability of EPVT in skin lesion\nrecognition across various environments. Our code and dataset will be released\nat https://github.com/SiyuanYan1/EPVT.\n","authors":["Siyuan Yan","Chi Liu","Zhen Yu","Lie Ju","Dwarikanath Mahapatrainst","Victoria Mar","Monika Janda","Peter Soyer","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2304.01508v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2211.12497v3","updated":"2023-04-04T03:29:39Z","published":"2022-11-22T18:59:31Z","title":"MagicPony: Learning Articulated 3D Animals in the Wild","summary":"  We consider the problem of predicting the 3D shape, articulation, viewpoint,\ntexture, and lighting of an articulated animal like a horse given a single test\nimage as input. We present a new method, dubbed MagicPony, that learns this\npredictor purely from in-the-wild single-view images of the object category,\nwith minimal assumptions about the topology of deformation. At its core is an\nimplicit-explicit representation of articulated shape and appearance, combining\nthe strengths of neural fields and meshes. In order to help the model\nunderstand an object's shape and pose, we distil the knowledge captured by an\noff-the-shelf self-supervised vision transformer and fuse it into the 3D model.\nTo overcome local optima in viewpoint estimation, we further introduce a new\nviewpoint sampling scheme that comes at no additional training cost. MagicPony\noutperforms prior work on this challenging task and demonstrates excellent\ngeneralisation in reconstructing art, despite the fact that it is only trained\non real images.\n","authors":["Shangzhe Wu","Ruining Li","Tomas Jakab","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2211.12497v3.pdf","comment":"CVPR 2023. Project Page: https://3dmagicpony.github.io/"},{"id":"http://arxiv.org/abs/2303.11126v2","updated":"2023-04-04T03:28:27Z","published":"2023-03-20T14:04:40Z","title":"Robustifying Token Attention for Vision Transformers","summary":"  Despite the success of vision transformers (ViTs), they still suffer from\nsignificant drops in accuracy in the presence of common corruptions, such as\nnoise or blur. Interestingly, we observe that the attention mechanism of ViTs\ntends to rely on few important tokens, a phenomenon we call token overfocusing.\nMore critically, these tokens are not robust to corruptions, often leading to\nhighly diverging attention patterns. In this paper, we intend to alleviate this\noverfocusing issue and make attention more stable through two general\ntechniques: First, our Token-aware Average Pooling (TAP) module encourages the\nlocal neighborhood of each token to take part in the attention mechanism.\nSpecifically, TAP learns average pooling schemes for each token such that the\ninformation of potentially important tokens in the neighborhood can adaptively\nbe taken into account. Second, we force the output tokens to aggregate\ninformation from a diverse set of input tokens rather than focusing on just a\nfew by using our Attention Diversification Loss (ADL). We achieve this by\npenalizing high cosine similarity between the attention vectors of different\ntokens. In experiments, we apply our methods to a wide range of transformer\narchitectures and improve robustness significantly. For example, we improve\ncorruption robustness on ImageNet-C by 2.4% while simultaneously improving\naccuracy by 0.4% based on state-of-the-art robust architecture FAN. Also, when\nfinetuning on semantic segmentation tasks, we improve robustness on\nCityScapes-C by 2.4% and ACDC by 3.1%.\n","authors":["Yong Guo","David Stutz","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2303.11126v2.pdf","comment":"7 figures and 5 tables in the main paper"},{"id":"http://arxiv.org/abs/2304.01498v1","updated":"2023-04-04T03:18:27Z","published":"2023-04-04T03:18:27Z","title":"DCANet: Dual Convolutional Neural Network with Attention for Image Blind\n  Denoising","summary":"  Noise removal of images is an essential preprocessing procedure for many\ncomputer vision tasks. Currently, many denoising models based on deep neural\nnetworks can perform well in removing the noise with known distributions (i.e.\nthe additive Gaussian white noise). However eliminating real noise is still a\nvery challenging task, since real-world noise often does not simply follow one\nsingle type of distribution, and the noise may spatially vary. In this paper,\nwe present a new dual convolutional neural network (CNN) with attention for\nimage blind denoising, named as the DCANet. To the best of our knowledge, the\nproposed DCANet is the first work that integrates both the dual CNN and\nattention mechanism for image denoising. The DCANet is composed of a noise\nestimation network, a spatial and channel attention module (SCAM), and a CNN\nwith a dual structure. The noise estimation network is utilized to estimate the\nspatial distribution and the noise level in an image. The noisy image and its\nestimated noise are combined as the input of the SCAM, and a dual CNN contains\ntwo different branches is designed to learn the complementary features to\nobtain the denoised image. The experimental results have verified that the\nproposed DCANet can suppress both synthetic and real noise effectively. The\ncode of DCANet is available at https://github.com/WenCongWu/DCANet.\n","authors":["Wencong Wu","Guannan Lv","Yingying Duan","Peng Liang","Yungang Zhang","Yuelong Xia"],"pdf_url":"https://arxiv.org/pdf/2304.01498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01489v1","updated":"2023-04-04T03:08:02Z","published":"2023-04-04T03:08:02Z","title":"Improved Visual Fine-tuning with Natural Language Supervision","summary":"  Fine-tuning a pre-trained model can leverage the semantic information from\nlarge-scale pre-training data and mitigate the over-fitting problem on\ndownstream tasks with limited training examples. While the problem of\ncatastrophic forgetting in backbone has been extensively studied, the potential\nbias existing in a pre-trained model due to the corresponding pre-training task\nand data, attracts less attention. In this work, we investigate this problem by\ndemonstrating that the obtained classifier after fine-tuning will be close to\nthat induced by the pre-trained model. To reduce the bias in the classifier\neffectively, we introduce a reference distribution obtained from a fixed text\nclassifier, which can help regularize the learned vision classifier. The\nproposed method, Text Supervised fine-tuning (TeS), is evaluated with diverse\npre-trained vision models including ResNet and ViT, and text encoders including\nBERT and CLIP, on 11 downstream tasks. The consistent improvement with a clear\nmargin over distinct scenarios confirms the effectiveness of our proposal.\n","authors":["Junyang Wang","Yuanhong Xu","Juhua Hu","Ming Yan","Jitao Sang","Qi Qian"],"pdf_url":"https://arxiv.org/pdf/2304.01489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00464v2","updated":"2023-04-04T03:05:50Z","published":"2023-04-02T06:32:19Z","title":"UniDexGrasp++: Improving Dexterous Grasping Policy Learning via\n  Geometry-aware Curriculum and Iterative Generalist-Specialist Learning","summary":"  We propose a novel, object-agnostic method for learning a universal policy\nfor dexterous object grasping from realistic point cloud observations and\nproprioceptive information under a table-top setting, namely UniDexGrasp++. To\naddress the challenge of learning the vision-based policy across thousands of\nobject instances, we propose Geometry-aware Curriculum Learning (GeoCurriculum)\nand Geometry-aware iterative Generalist-Specialist Learning (GiGSL) which\nleverage the geometry feature of the task and significantly improve the\ngeneralizability. With our proposed techniques, our final policy shows\nuniversal dexterous grasping on thousands of object instances with 85.4% and\n78.2% success rate on the train set and test set which outperforms the\nstate-of-the-art baseline UniDexGrasp by 11.7% and 11.3%, respectively.\n","authors":["Weikang Wan","Haoran Geng","Yun Liu","Zikang Shan","Yaodong Yang","Li Yi","He Wang"],"pdf_url":"https://arxiv.org/pdf/2304.00464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01488v1","updated":"2023-04-04T03:04:44Z","published":"2023-04-04T03:04:44Z","title":"End-to-End Latency Optimization of Multi-view 3D Reconstruction for\n  Disaster Response","summary":"  In order to plan rapid response during disasters, first responder agencies\noften adopt `bring your own device' (BYOD) model with inexpensive mobile edge\ndevices (e.g., drones, robots, tablets) for complex video analytics\napplications, e.g., 3D reconstruction of a disaster scene. Unlike simpler video\napplications, widely used Multi-view Stereo (MVS) based 3D reconstruction\napplications (e.g., openMVG/openMVS) are exceedingly time consuming, especially\nwhen run on such computationally constrained mobile edge devices. Additionally,\nreducing the reconstruction latency of such inherently sequential algorithms is\nchallenging as unintelligent, application-agnostic strategies can drastically\ndegrade the reconstruction (i.e., application outcome) quality making them\nuseless. In this paper, we aim to design a latency optimized MVS algorithm\npipeline, with the objective to best balance the end-to-end latency and\nreconstruction quality by running the pipeline on a collaborative mobile edge\nenvironment. The overall optimization approach is two-pronged where: (a)\napplication optimizations introduce data-level parallelism by splitting the\npipeline into high frequency and low frequency reconstruction components and\n(b) system optimizations incorporate task-level parallelism to the pipelines by\nrunning them opportunistically on available resources with online quality\ncontrol in order to balance both latency and quality. Our evaluation on a\nhardware testbed using publicly available datasets shows upto ~54% reduction in\nlatency with negligible loss (~4-7%) in reconstruction quality.\n","authors":["Xiaojie Zhang","Mingjun Li","Andrew Hilton","Amitangshu Pal","Soumyabrata Dey","Saptarshi Debroy"],"pdf_url":"https://arxiv.org/pdf/2304.01488v1.pdf","comment":"2022 10th IEEE International Conference on Mobile Cloud Computing,\n  Services, and Engineering (MobileCloud)"},{"id":"http://arxiv.org/abs/2304.01484v1","updated":"2023-04-04T02:55:57Z","published":"2023-04-04T02:55:57Z","title":"Mapping Degeneration Meets Label Evolution: Learning Infrared Small\n  Target Detection with Single Point Supervision","summary":"  Training a convolutional neural network (CNN) to detect infrared small\ntargets in a fully supervised manner has gained remarkable research interests\nin recent years, but is highly labor expensive since a large number of\nper-pixel annotations are required. To handle this problem, in this paper, we\nmake the first attempt to achieve infrared small target detection with\npoint-level supervision. Interestingly, during the training phase supervised by\npoint labels, we discover that CNNs first learn to segment a cluster of pixels\nnear the targets, and then gradually converge to predict groundtruth point\nlabels. Motivated by this \"mapping degeneration\" phenomenon, we propose a label\nevolution framework named label evolution with single point supervision (LESPS)\nto progressively expand the point label by leveraging the intermediate\npredictions of CNNs. In this way, the network predictions can finally\napproximate the updated pseudo labels, and a pixel-level target mask can be\nobtained to train CNNs in an end-to-end manner. We conduct extensive\nexperiments with insightful visualizations to validate the effectiveness of our\nmethod. Experimental results show that CNNs equipped with LESPS can well\nrecover the target masks from corresponding point labels, {and can achieve over\n70% and 95% of their fully supervised performance in terms of pixel-level\nintersection over union (IoU) and object-level probability of detection (Pd),\nrespectively. Code is available at https://github.com/XinyiYing/LESPS.\n","authors":["Xinyi Ying","Li Liu","Yingqian Wang","Ruojing Li","Nuo Chen","Zaiping Lin","Weidong Sheng","Shilin Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.01484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01482v1","updated":"2023-04-04T02:54:49Z","published":"2023-04-04T02:54:49Z","title":"Defending Against Patch-based Backdoor Attacks on Self-Supervised\n  Learning","summary":"  Recently, self-supervised learning (SSL) was shown to be vulnerable to\npatch-based data poisoning backdoor attacks. It was shown that an adversary can\npoison a small part of the unlabeled data so that when a victim trains an SSL\nmodel on it, the final model will have a backdoor that the adversary can\nexploit. This work aims to defend self-supervised learning against such\nattacks. We use a three-step defense pipeline, where we first train a model on\nthe poisoned data. In the second step, our proposed defense algorithm\n(PatchSearch) uses the trained model to search the training data for poisoned\nsamples and removes them from the training set. In the third step, a final\nmodel is trained on the cleaned-up training set. Our results show that\nPatchSearch is an effective defense. As an example, it improves a model's\naccuracy on images containing the trigger from 38.2% to 63.7% which is very\nclose to the clean model's accuracy, 64.6%. Moreover, we show that PatchSearch\noutperforms baselines and state-of-the-art defense approaches including those\nusing additional clean, trusted data. Our code is available at\nhttps://github.com/UCDvision/PatchSearch\n","authors":["Ajinkya Tejankar","Maziar Sanjabi","Qifan Wang","Sinong Wang","Hamed Firooz","Hamed Pirsiavash","Liang Tan"],"pdf_url":"https://arxiv.org/pdf/2304.01482v1.pdf","comment":"Accepted to CVPR 2023"},{"id":"http://arxiv.org/abs/2304.01480v1","updated":"2023-04-04T02:50:29Z","published":"2023-04-04T02:50:29Z","title":"FineRecon: Depth-aware Feed-forward Network for Detailed 3D\n  Reconstruction","summary":"  Recent works on 3D reconstruction from posed images have demonstrated that\ndirect inference of scene-level 3D geometry without iterative optimization is\nfeasible using a deep neural network, showing remarkable promise and high\nefficiency. However, the reconstructed geometries, typically represented as a\n3D truncated signed distance function (TSDF), are often coarse without fine\ngeometric details. To address this problem, we propose three effective\nsolutions for improving the fidelity of inference-based 3D reconstructions. We\nfirst present a resolution-agnostic TSDF supervision strategy to provide the\nnetwork with a more accurate learning signal during training, avoiding the\npitfalls of TSDF interpolation seen in previous work. We then introduce a depth\nguidance strategy using multi-view depth estimates to enhance the scene\nrepresentation and recover more accurate surfaces. Finally, we develop a novel\narchitecture for the final layers of the network, conditioning the output TSDF\nprediction on high-resolution image features in addition to coarse voxel\nfeatures, enabling sharper reconstruction of fine details. Our method produces\nsmooth and highly accurate reconstructions, showing significant improvements\nacross multiple depth and 3D reconstruction metrics.\n","authors":["Noah Stier","Anurag Ranjan","Alex Colburn","Yajie Yan","Liang Yang","Fangchang Ma","Baptiste Angles"],"pdf_url":"https://arxiv.org/pdf/2304.01480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04701v2","updated":"2023-04-04T02:42:35Z","published":"2022-12-09T07:26:49Z","title":"4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions","summary":"  In this paper, we present a novel and effective framework, named 4K-NeRF, to\npursue high fidelity view synthesis on the challenging scenarios of ultra high\nresolutions, building on the methodology of neural radiance fields (NeRF). The\nrendering procedure of NeRF-based methods typically relies on a pixel-wise\nmanner in which rays (or pixels) are treated independently on both training and\ninference phases, limiting its representational ability on describing subtle\ndetails, especially when lifting to a extremely high resolution. We address the\nissue by exploring ray correlation to enhance high-frequency details recovery.\nParticularly, we use the 3D-aware encoder to model geometric information\neffectively in a lower resolution space and recover fine details through the\n3D-aware decoder, conditioned on ray features and depths estimated by the\nencoder. Joint training with patch-based sampling further facilitates our\nmethod incorporating the supervision from perception oriented regularization\nbeyond pixel-wise loss. Benefiting from the use of geometry-aware local\ncontext, our method can significantly boost rendering quality on high-frequency\ndetails compared with modern NeRF methods, and achieve the state-of-the-art\nvisual quality on 4K ultra-high-resolution scenarios. Code Available at\n\\url{https://github.com/frozoul/4K-NeRF}\n","authors":["Zhongshu Wang","Lingzhi Li","Zhen Shen","Li Shen","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2212.04701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01472v1","updated":"2023-04-04T02:28:25Z","published":"2023-04-04T02:28:25Z","title":"Unsupervised Brain Tumor Segmentation with Image-based Prompts","summary":"  Automated brain tumor segmentation based on deep learning (DL) has achieved\npromising performance. However, it generally relies on annotated images for\nmodel training, which is not always feasible in clinical settings. Therefore,\nthe development of unsupervised DL-based brain tumor segmentation approaches\nwithout expert annotations is desired. Motivated by the success of prompt\nlearning (PL) in natural language processing, we propose an approach to\nunsupervised brain tumor segmentation by designing image-based prompts that\nallow indication of brain tumors, and this approach is dubbed as PL-based Brain\nTumor Segmentation (PL-BTS). Specifically, instead of directly training a model\nfor brain tumor segmentation with a large amount of annotated data, we seek to\ntrain a model that can answer the question: is a voxel in the input image\nassociated with tumor-like hyper-/hypo-intensity? Such a model can be trained\nby artificially generating tumor-like hyper-/hypo-intensity on images without\ntumors with hand-crafted designs. Since the hand-crafted designs may be too\nsimplistic to represent all kinds of real tumors, the trained model may overfit\nthe simplistic hand-crafted task rather than actually answer the question of\nabnormality. To address this problem, we propose the use of a validation task,\nwhere we generate a different hand-crafted task to monitor overfitting. In\naddition, we propose PL-BTS+ that further improves PL-BTS by exploiting\nunannotated images with brain tumors. Compared with competing unsupervised\nmethods, the proposed method has achieved marked improvements on both public\nand in-house datasets, and we have also demonstrated its possible extension to\nother brain lesion segmentation tasks.\n","authors":["Xinru Zhang","Ni Ou","Chenghao Liu","Zhizheng Zhuo","Yaou Liu","Chuyang Ye"],"pdf_url":"https://arxiv.org/pdf/2304.01472v1.pdf","comment":"Currently under review (from November 14th, 2022 until now)"},{"id":"http://arxiv.org/abs/2304.01464v1","updated":"2023-04-04T02:09:32Z","published":"2023-04-04T02:09:32Z","title":"Hierarchical Supervision and Shuffle Data Augmentation for 3D\n  Semi-Supervised Object Detection","summary":"  State-of-the-art 3D object detectors are usually trained on large-scale\ndatasets with high-quality 3D annotations. However, such 3D annotations are\noften expensive and time-consuming, which may not be practical for real\napplications. A natural remedy is to adopt semi-supervised learning (SSL) by\nleveraging a limited amount of labeled samples and abundant unlabeled samples.\nCurrent pseudolabeling-based SSL object detection methods mainly adopt a\nteacher-student framework, with a single fixed threshold strategy to generate\nsupervision signals, which inevitably brings confused supervision when guiding\nthe student network training. Besides, the data augmentation of the point cloud\nin the typical teacher-student framework is too weak, and only contains basic\ndown sampling and flip-and-shift (i.e., rotate and scaling), which hinders the\neffective learning of feature information. Hence, we address these issues by\nintroducing a novel approach of Hierarchical Supervision and Shuffle Data\nAugmentation (HSSDA), which is a simple yet effective teacher-student\nframework. The teacher network generates more reasonable supervision for the\nstudent network by designing a dynamic dual-threshold strategy. Besides, the\nshuffle data augmentation strategy is designed to strengthen the feature\nrepresentation ability of the student network. Extensive experiments show that\nHSSDA consistently outperforms the recent state-of-the-art methods on different\ndatasets. The code will be released at https://github.com/azhuantou/HSSDA.\n","authors":["Chuandong Liu","Chenqiang Gao","Fangcen Liu","Pengcheng Li","Deyu Meng","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2304.01464v1.pdf","comment":"Accepted by CVPR2023"},{"id":"http://arxiv.org/abs/2304.01457v1","updated":"2023-04-04T01:56:16Z","published":"2023-04-04T01:56:16Z","title":"Exploring Vision-Language Models for Imbalanced Learning","summary":"  Vision-Language models (VLMs) that use contrastive language-image\npre-training have shown promising zero-shot classification performance.\nHowever, their performance on imbalanced dataset is relatively poor, where the\ndistribution of classes in the training dataset is skewed, leading to poor\nperformance in predicting minority classes. For instance, CLIP achieved only 5%\naccuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder\nto VLMs to avoid OOM (out of memory) problem caused by large number of classes\nand capture nuanced features for tail classes. Then, we explore improvements of\nVLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms\nsuch as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments\ndemonstrate that the performance of VLMs can be further boosted when used with\ndecoder and imbalanced methods. Specifically, our improved VLMs significantly\noutperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,\nand 6.17%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. We\nfurther analyze the influence of pre-training data size, backbones, and\ntraining cost. Our study highlights the significance of imbalanced learning\nalgorithms in face of VLMs pre-trained by huge data. We release our code at\nhttps://github.com/Imbalance-VLM/Imbalance-VLM.\n","authors":["Yidong Wang","Zhuohao Yu","Jindong Wang","Qiang Heng","Hao Chen","Wei Ye","Rui Xie","Xing Xie","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01457v1.pdf","comment":"Technical report; 14 pages; code:\n  https://github.com/Imbalance-VLM/Imbalance-VLM"},{"id":"http://arxiv.org/abs/2304.01452v1","updated":"2023-04-04T01:51:53Z","published":"2023-04-04T01:51:53Z","title":"Attention Map Guided Transformer Pruning for Edge Device","summary":"  Due to its significant capability of modeling long-range dependencies, vision\ntransformer (ViT) has achieved promising success in both holistic and occluded\nperson re-identification (Re-ID) tasks. However, the inherent problems of\ntransformers such as the huge computational cost and memory footprint are still\ntwo unsolved issues that will block the deployment of ViT based person Re-ID\nmodels on resource-limited edge devices. Our goal is to reduce both the\ninference complexity and model size without sacrificing the comparable accuracy\non person Re-ID, especially for tasks with occlusion. To this end, we propose a\nnovel attention map guided (AMG) transformer pruning method, which removes both\nredundant tokens and heads with the guidance of the attention map in a\nhardware-friendly way. We first calculate the entropy in the key dimension and\nsum it up for the whole map, and the corresponding head parameters of maps with\nhigh entropy will be removed for model size reduction. Then we combine the\nsimilarity and first-order gradients of key tokens along the query dimension\nfor token importance estimation and remove redundant key and value tokens to\nfurther reduce the inference complexity. Comprehensive experiments on Occluded\nDukeMTMC and Market-1501 demonstrate the effectiveness of our proposals. For\nexample, our proposed pruning strategy on ViT-Base enjoys\n\\textup{\\textbf{29.4\\%}} \\textup{\\textbf{FLOPs}} savings with\n\\textup{\\textbf{0.2\\%}} drop on Rank-1 and \\textup{\\textbf{0.4\\%}} improvement\non mAP, respectively.\n","authors":["Junzhu Mao","Yazhou Yao","Zeren Sun","Xingguo Huang","Fumin Shen","Heng-Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2304.01452v1.pdf","comment":"accepted by IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2206.09474v2","updated":"2023-04-04T01:46:59Z","published":"2022-06-19T19:43:11Z","title":"3D Object Detection for Autonomous Driving: A Comprehensive Survey","summary":"  Autonomous driving, in recent years, has been receiving increasing attention\nfor its potential to relieve drivers' burdens and improve the safety of\ndriving. In modern autonomous driving pipelines, the perception system is an\nindispensable component, aiming to accurately estimate the status of\nsurrounding environments and provide reliable observations for prediction and\nplanning. 3D object detection, which intelligently predicts the locations,\nsizes, and categories of the critical 3D objects near an autonomous vehicle, is\nan important part of a perception system. This paper reviews the advances in 3D\nobject detection for autonomous driving. First, we introduce the background of\n3D object detection and discuss the challenges in this task. Second, we conduct\na comprehensive survey of the progress in 3D object detection from the aspects\nof models and sensory inputs, including LiDAR-based, camera-based, and\nmulti-modal detection approaches. We also provide an in-depth analysis of the\npotentials and challenges in each category of methods. Additionally, we\nsystematically investigate the applications of 3D object detection in driving\nsystems. Finally, we conduct a performance analysis of the 3D object detection\napproaches, and we further summarize the research trends over the years and\nprospect the future directions of this area.\n","authors":["Jiageng Mao","Shaoshuai Shi","Xiaogang Wang","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2206.09474v2.pdf","comment":"Accepted to International Journal of Computer Vision (IJCV). Project\n  page is at\n  https://github.com/PointsCoder/Awesome-3D-Object-Detection-for-Autonomous-Driving"},{"id":"http://arxiv.org/abs/2304.01443v1","updated":"2023-04-04T01:34:23Z","published":"2023-04-04T01:34:23Z","title":"Virtual Avatar Stream: a cost-down approach to the Metaverse experience","summary":"  The Metaverse through VR headsets is a rapidly growing concept, but the high\ncost of entry currently limits access for many users. This project aims to\nprovide an accessible entry point to the immersive Metaverse experience by\nleveraging web technologies. The platform developed allows users to engage with\nrendered avatars using only a web browser, microphone, and webcam. By employing\nthe WebGL and MediaPipe face tracking AI model from Google, the application\ngenerates real-time 3D face meshes for users. It uses a client-to-client\nstreaming cluster to establish a connection, and clients negotiate SRTP\nprotocol through WebRTC for direct data streaming. Additionally, the project\naddresses backend challenges through an architecture that is serverless,\ndistributive, auto-scaling, highly resilient, and secure. The platform offers a\nscalable, hardware-free solution for users to experience a near-immersive\nMetaverse, with the potential for future integration with game server clusters.\nThis project provides an important step toward a more inclusive Metaverse\naccessible to a wider audience.\n","authors":["Joseph Chang"],"pdf_url":"https://arxiv.org/pdf/2304.01443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01441v1","updated":"2023-04-04T01:29:51Z","published":"2023-04-04T01:29:51Z","title":"NetFlick: Adversarial Flickering Attacks on Deep Learning Based Video\n  Compression","summary":"  Video compression plays a significant role in IoT devices for the efficient\ntransport of visual data while satisfying all underlying bandwidth constraints.\nDeep learning-based video compression methods are rapidly replacing traditional\nalgorithms and providing state-of-the-art results on edge devices. However,\nrecently developed adversarial attacks demonstrate that digitally crafted\nperturbations can break the Rate-Distortion relationship of video compression.\nIn this work, we present a real-world LED attack to target video compression\nframeworks. Our physically realizable attack, dubbed NetFlick, can degrade the\nspatio-temporal correlation between successive frames by injecting flickering\ntemporal perturbations. In addition, we propose universal perturbations that\ncan downgrade performance of incoming video without prior knowledge of the\ncontents. Experimental results demonstrate that NetFlick can successfully\ndeteriorate the performance of video compression frameworks in both digital-\nand physical-settings and can be further extended to attack downstream video\nclassification networks.\n","authors":["Jung-Woo Chang","Nojan Sheybani","Shehzeen Samarah Hussain","Mojan Javaheripi","Seira Hidano","Farinaz Koushanfar"],"pdf_url":"https://arxiv.org/pdf/2304.01441v1.pdf","comment":"8 pages; Accepted to ICLR 2023 ML4IoT workshop"},{"id":"http://arxiv.org/abs/2304.00782v2","updated":"2023-04-04T01:13:03Z","published":"2023-04-03T08:12:18Z","title":"NeMF: Inverse Volume Rendering with Neural Microflake Field","summary":"  Recovering the physical attributes of an object's appearance from its images\ncaptured under an unknown illumination is challenging yet essential for\nphoto-realistic rendering. Recent approaches adopt the emerging implicit scene\nrepresentations and have shown impressive results.However, they unanimously\nadopt a surface-based representation,and hence can not well handle scenes with\nvery complex geometry, translucent object and etc. In this paper, we propose to\nconduct inverse volume rendering, in contrast to surface-based, by representing\na scene using microflake volume, which assumes the space is filled with\ninfinite small flakes and light reflects or scatters at each spatial location\naccording to microflake distributions. We further adopt the coordinate networks\nto implicitly encode the microflake volume, and develop a differentiable\nmicroflake volume renderer to train the network in an end-to-end way in\nprinciple.Our NeMF enables effective recovery of appearance attributes for\nhighly complex geometry and scattering object, enables high-quality relighting,\nmaterial editing, and especially simulates volume rendering effects, such as\nscattering, which is infeasible for surface-based approaches.\n","authors":["Youjia Zhang","Teng Xu","Junqing Yu","Yuteng Ye","Junle Wang","Yanqing Jing","Jingyi Yu","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2304.00782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18125v2","updated":"2023-04-04T01:12:31Z","published":"2023-03-31T15:09:18Z","title":"Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter\n  Correction","summary":"  This paper addresses the problem of rolling shutter correction in complex\nnonlinear and dynamic scenes with extreme occlusion. Existing methods suffer\nfrom two main drawbacks. Firstly, they face challenges in estimating the\naccurate correction field due to the uniform velocity assumption, leading to\nsignificant image correction errors under complex motion. Secondly, the drastic\nocclusion in dynamic scenes prevents current solutions from achieving better\nimage quality because of the inherent difficulties in aligning and aggregating\nmultiple frames. To tackle these challenges, we model the curvilinear\ntrajectory of pixels analytically and propose a geometry-based Quadratic\nRolling Shutter (QRS) motion solver, which precisely estimates the high-order\ncorrection field of individual pixel. Besides, to reconstruct high-quality\nocclusion frames in dynamic scenes, we present a 3D video architecture that\neffectively Aligns and Aggregates multi-frame context, namely, RSA^2-Net. We\nevaluate our method across a broad range of cameras and video sequences,\ndemonstrating its significant superiority. Specifically, our method surpasses\nthe state-of-the-arts by +4.98, +0.77, and +4.33 of PSNR on Carla-RS,\nFastec-RS, and BS-RSC datasets, respectively.\n","authors":["Delin Qu","Yizhen Lao","Zhigang Wang","Dong Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2303.18125v2.pdf","comment":"8 pages, 11 figures"},{"id":"http://arxiv.org/abs/2304.01436v1","updated":"2023-04-04T01:10:04Z","published":"2023-04-04T01:10:04Z","title":"Learning Personalized High Quality Volumetric Head Avatars from\n  Monocular RGB Videos","summary":"  We propose a method to learn a high-quality implicit 3D head avatar from a\nmonocular RGB video captured in the wild. The learnt avatar is driven by a\nparametric face model to achieve user-controlled facial expressions and head\nposes. Our hybrid pipeline combines the geometry prior and dynamic tracking of\na 3DMM with a neural radiance field to achieve fine-grained control and\nphotorealism. To reduce over-smoothing and improve out-of-model expressions\nsynthesis, we propose to predict local features anchored on the 3DMM geometry.\nThese learnt features are driven by 3DMM deformation and interpolated in 3D\nspace to yield the volumetric radiance at a designated query point. We further\nshow that using a Convolutional Neural Network in the UV space is critical in\nincorporating spatial context and producing representative local features.\nExtensive experiments show that we are able to reconstruct high-quality\navatars, with more accurate expression-dependent details, good generalization\nto out-of-training expressions, and quantitatively superior renderings compared\nto other state-of-the-art approaches.\n","authors":["Ziqian Bai","Feitong Tan","Zeng Huang","Kripasindhu Sarkar","Danhang Tang","Di Qiu","Abhimitra Meka","Ruofei Du","Mingsong Dou","Sergio Orts-Escolano","Rohit Pandey","Ping Tan","Thabo Beeler","Sean Fanello","Yinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01436v1.pdf","comment":"In CVPR2023. Project page:\n  https://augmentedperception.github.io/monoavatar/"},{"id":"http://arxiv.org/abs/2210.12361v2","updated":"2023-04-04T01:05:39Z","published":"2022-10-22T05:51:52Z","title":"MS-DCANet: A MLP-based Multi-Scale Feature Framework For COVID-19\n  Infection Segmentation From Medical Images","summary":"  Coronavirus Disease 2019(COVID-19) spread rapidly around the world, causing a\nseries of severe health crises. Automated segmentation of lung infections based\non Deep Convolutional Neural Network(DCNN) from medical images such as CT,\nX-ray, etc, displayed a huge potential for accurate diagnosis and quantitative\nanalysis. Most COVID-19 medical images show blurred boundaries, dense noise\npoints, low contrast, and significant variation in the shape and size of\nlesions. Although various models based on UNet have been proposed, more\noptimisation is required to obtain accurate segmentation and meet complex\ncomputational needs. Furthermore, the existing COVID-19 infections segmentation\nDCNN based methods are only suitable for single modality medical images. To\nsolve these problems, this paper proposes a symmetric Encoder-Decoder\nsegmentation framework named MS-DCANet. We introduce Tokenized MLP block, a\nnovel attention scheme that uses a shift-window mechanism similar to the\nTransformer to acquire self-attention and achieve local-to-global semantic\ndependency. MS-DCANet also uses several Dual Channel blocks and a Res-ASPP\nblock to expand the receptive field and extract multi-scale features. In a\nlarge number of experiments on COVID-19 datasets using both X-ray and CT\nimages, MS-DCANet achieved state-of-the-art performance compared with other\nUNet models. MS-DCANet can also improve trade-off accuracy and complexity. To\nprove the proposed model's strong generalisability, we also apply MS-DCANet to\nthe segmentation of skin tumours from dermoscopy images and hand bone from\nX-ray images with satisfactory results.\n","authors":["Xiaoyu Pan","Huazheng Zhu","Jinglong Du","Guangtao Hu","Baoru Han","Yuanyuan Jia"],"pdf_url":"https://arxiv.org/pdf/2210.12361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01434v1","updated":"2023-04-04T01:03:32Z","published":"2023-04-04T01:03:32Z","title":"VNE: An Effective Method for Improving Deep Representation by\n  Manipulating Eigenvalue Distribution","summary":"  Since the introduction of deep learning, a wide scope of representation\nproperties, such as decorrelation, whitening, disentanglement, rank, isotropy,\nand mutual information, have been studied to improve the quality of\nrepresentation. However, manipulating such properties can be challenging in\nterms of implementational effectiveness and general applicability. To address\nthese limitations, we propose to regularize von Neumann entropy~(VNE) of\nrepresentation. First, we demonstrate that the mathematical formulation of VNE\nis superior in effectively manipulating the eigenvalues of the representation\nautocorrelation matrix. Then, we demonstrate that it is widely applicable in\nimproving state-of-the-art algorithms or popular benchmark algorithms by\ninvestigating domain-generalization, meta-learning, self-supervised learning,\nand generative models. In addition, we formally establish theoretical\nconnections with rank, disentanglement, and isotropy of representation.\nFinally, we provide discussions on the dimension control of VNE and the\nrelationship with Shannon entropy. Code is available at:\nhttps://github.com/jaeill/CVPR23-VNE.\n","authors":["Jaeill Kim","Suhyun Kang","Duhun Hwang","Jungwook Shin","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2304.01434v1.pdf","comment":"Accepted at CVPR 2023. Code is available at:\n  https://github.com/jaeill/CVPR23-VNE"},{"id":"http://arxiv.org/abs/2304.01430v1","updated":"2023-04-04T00:26:13Z","published":"2023-04-04T00:26:13Z","title":"Divided Attention: Unsupervised Multi-Object Discovery with Contextually\n  Separated Slots","summary":"  We introduce a method to segment the visual field into independently moving\nregions, trained with no ground truth or supervision. It consists of an\nadversarial conditional encoder-decoder architecture based on Slot Attention,\nmodified to use the image as context to decode optical flow without attempting\nto reconstruct the image itself. In the resulting multi-modal representation,\none modality (flow) feeds the encoder to produce separate latent codes (slots),\nwhereas the other modality (image) conditions the decoder to generate the first\n(flow) from the slots. This design frees the representation from having to\nencode complex nuisance variability in the image due to, for instance,\nillumination and reflectance properties of the scene. Since customary\nautoencoding based on minimizing the reconstruction error does not preclude the\nentire flow from being encoded into a single slot, we modify the loss to an\nadversarial criterion based on Contextual Information Separation. The resulting\nmin-max optimization fosters the separation of objects and their assignment to\ndifferent attention slots, leading to Divided Attention, or DivA. DivA\noutperforms recent unsupervised multi-object motion segmentation methods while\ntripling run-time speed up to 104FPS and reducing the performance gap from\nsupervised methods to 12% or less. DivA can handle different numbers of objects\nand different image sizes at training and test time, is invariant to\npermutation of object labels, and does not require explicit regularization.\n","authors":["Dong Lao","Zhengyang Hu","Francesco Locatello","Yanchao Yang","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2304.01430v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2304.01982v1","updated":"2023-04-04T17:37:06Z","published":"2023-04-04T17:37:06Z","title":"Rethinking the Role of Token Retrieval in Multi-Vector Retrieval","summary":"  Multi-vector retrieval models such as ColBERT [Khattab and Zaharia, 2020]\nallow token-level interactions between queries and documents, and hence achieve\nstate of the art on many information retrieval benchmarks. However, their\nnon-linear scoring function cannot be scaled to millions of documents,\nnecessitating a three-stage process for inference: retrieving initial\ncandidates via token retrieval, accessing all token vectors, and scoring the\ninitial candidate documents. The non-linear scoring function is applied over\nall token vectors of each candidate document, making the inference process\ncomplicated and slow. In this paper, we aim to simplify the multi-vector\nretrieval by rethinking the role of token retrieval. We present XTR,\nConteXtualized Token Retriever, which introduces a simple, yet novel, objective\nfunction that encourages the model to retrieve the most important document\ntokens first. The improvement to token retrieval allows XTR to rank candidates\nonly using the retrieved tokens rather than all tokens in the document, and\nenables a newly designed scoring stage that is two-to-three orders of magnitude\ncheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances the\nstate-of-the-art by 2.8 nDCG@10 without any distillation. Detailed analysis\nconfirms our decision to revisit the token retrieval stage, as XTR demonstrates\nmuch better recall of the token retrieval stage compared to ColBERT.\n","authors":["Jinhyuk Lee","Zhuyun Dai","Sai Meher Karthik Duddu","Tao Lei","Iftekhar Naim","Ming-Wei Chang","Vincent Y. Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.01982v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2304.01974v1","updated":"2023-04-04T17:31:32Z","published":"2023-04-04T17:31:32Z","title":"Dialogue-Contextualized Re-ranking for Medical History-Taking","summary":"  AI-driven medical history-taking is an important component in symptom\nchecking, automated patient intake, triage, and other AI virtual care\napplications. As history-taking is extremely varied, machine learning models\nrequire a significant amount of data to train. To overcome this challenge,\nexisting systems are developed using indirect data or expert knowledge. This\nleads to a training-inference gap as models are trained on different kinds of\ndata than what they observe at inference time. In this work, we present a\ntwo-stage re-ranking approach that helps close the training-inference gap by\nre-ranking the first-stage question candidates using a dialogue-contextualized\nmodel. For this, we propose a new model, global re-ranker, which cross-encodes\nthe dialogue with all questions simultaneously, and compare it with several\nexisting neural baselines. We test both transformer and S4-based language model\nbackbones. We find that relative to the expert system, the best performance is\nachieved by our proposed global re-ranker with a transformer backbone,\nresulting in a 30% higher normalized discount cumulative gain (nDCG) and a 77%\nhigher mean average precision (mAP).\n","authors":["Jian Zhu","Ilya Valmianski","Anitha Kannan"],"pdf_url":"https://arxiv.org/pdf/2304.01974v1.pdf","comment":"Code and pre-trained S4 checkpoints will be available after\n  publication"},{"id":"http://arxiv.org/abs/2304.01961v1","updated":"2023-04-04T17:11:34Z","published":"2023-04-04T17:11:34Z","title":"AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia\n  Content Creation","summary":"  This paper presents the AToMiC (Authoring Tools for Multimedia Content)\ndataset, designed to advance research in image/text cross-modal retrieval.\nWhile vision-language pretrained transformers have led to significant\nimprovements in retrieval effectiveness, existing research has relied on\nimage-caption datasets that feature only simplistic image-text relationships\nand underspecified user models of retrieval tasks. To address the gap between\nthese oversimplified settings and real-world applications for multimedia\ncontent creation, we introduce a new approach for building retrieval test\ncollections. We leverage hierarchical structures and diverse domains of texts,\nstyles, and types of images, as well as large-scale image-document associations\nembedded in Wikipedia. We formulate two tasks based on a realistic user model\nand validate our dataset through retrieval experiments using baseline models.\nAToMiC offers a testbed for scalable, diverse, and reproducible multimedia\nretrieval research. Finally, the dataset provides the basis for a dedicated\ntrack at the 2023 Text Retrieval Conference (TREC), and is publicly available\nat https://github.com/TREC-AToMiC/AToMiC.\n","authors":["Jheng-Hong Yang","Carlos Lassance","Rafael Sampaio de Rezende","Krishna Srinivasan","Miriam Redi","Stéphane Clinchant","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2304.01961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00686v2","updated":"2023-04-04T07:14:16Z","published":"2023-04-03T02:22:01Z","title":"DiffuRec: A Diffusion Model for Sequential Recommendation","summary":"  Mainstream solutions to Sequential Recommendation (SR) represent items with\nfixed vectors. These vectors have limited capability in capturing items' latent\naspects and users' diverse preferences. As a new generative paradigm, Diffusion\nmodels have achieved excellent performance in areas like computer vision and\nnatural language processing. To our understanding, its unique merit in\nrepresentation generation well fits the problem setting of sequential\nrecommendation. In this paper, we make the very first attempt to adapt\nDiffusion model to SR and propose DiffuRec, for item representation\nconstruction and uncertainty injection. Rather than modeling item\nrepresentations as fixed vectors, we represent them as distributions in\nDiffuRec, which reflect user's multiple interests and item's various aspects\nadaptively. In diffusion phase, DiffuRec corrupts the target item embedding\ninto a Gaussian distribution via noise adding, which is further applied for\nsequential item distribution representation generation and uncertainty\ninjection. Afterwards, the item representation is fed into an Approximator for\ntarget item representation reconstruction. In reversion phase, based on user's\nhistorical interaction behaviors, we reverse a Gaussian noise into the target\nitem representation, then apply rounding operation for target item prediction.\nExperiments over four datasets show that DiffuRec outperforms strong baselines\nby a large margin.\n","authors":["Zihao Li","Aixin Sun","Chenliang Li"],"pdf_url":"https://arxiv.org/pdf/2304.00686v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01577v1","updated":"2023-04-04T07:06:54Z","published":"2023-04-04T07:06:54Z","title":"Form-NLU: Dataset for the Form Language Understanding","summary":"  Compared to general document analysis tasks, form document structure\nunderstanding and retrieval are challenging. Form documents are typically made\nby two types of authors; A form designer, who develops the form structure and\nkeys, and a form user, who fills out form values based on the provided keys.\nHence, the form values may not be aligned with the form designer's intention\n(structure and keys) if a form user gets confused. In this paper, we introduce\nForm-NLU, the first novel dataset for form structure understanding and its key\nand value information extraction, interpreting the form designer's intent and\nthe alignment of user-written value on it. It consists of 857 form images, 6k\nform keys and values, and 4k table keys and values. Our dataset also includes\nthree form types: digital, printed, and handwritten, which cover diverse form\nappearances and layouts. We propose a robust positional and logical\nrelation-based form key-value information extraction framework. Using this\ndataset, Form-NLU, we first examine strong object detection models for the form\nlayout understanding, then evaluate the key information extraction task on the\ndataset, providing fine-grained results for different types of forms and keys.\nFurthermore, we examine it with the off-the-shelf pdf layout extraction tool\nand prove its feasibility in real-world cases.\n","authors":["Yihao Ding","Siqu Long","Jiabin Huang","Kaixuan Ren","Xingxiang Luo","Hyunsuk Chung","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2304.01577v1.pdf","comment":"Accepted by SIGIR 2023"},{"id":"http://arxiv.org/abs/2304.00353v2","updated":"2023-04-04T06:07:00Z","published":"2023-04-01T16:15:45Z","title":"Reviewer Assignment Problem: A Systematic Review of the Literature","summary":"  Appropriate reviewer assignment significantly impacts the quality of proposal\nevaluation, as accurate and fair reviews are contingent on their assignment to\nrelevant reviewers. The crucial task of assigning reviewers to submitted\nproposals is the starting point of the review process and is also known as the\nreviewer assignment problem (RAP). Due to the obvious restrictions of manual\nassignment, journal editors, conference organizers, and grant managers demand\nautomatic reviewer assignment approaches. Many studies have proposed assignment\nsolutions in response to the demand for automated procedures since 1992. The\nprimary objective of this survey paper is to provide scholars and practitioners\nwith a comprehensive overview of available research on the RAP. To achieve this\ngoal, this article presents an in-depth systematic review of 103 publications\nin the field of reviewer assignment published in the past three decades and\navailable in the Web of Science, Scopus, ScienceDirect, Google Scholar, and\nSemantic Scholar databases. This review paper classified and discussed the RAP\napproaches into two broad categories and numerous subcategories based on their\nunderlying techniques. Furthermore, potential future research directions for\neach category are presented. This survey shows that the research on the RAP is\nbecoming more significant and that more effort is required to develop new\napproaches and a framework.\n","authors":["Meltem Aksoy","Seda Yanik","Mehmet Fatih Amasyali"],"pdf_url":"https://arxiv.org/pdf/2304.00353v2.pdf","comment":"67 pages, 11 figures"},{"id":"http://arxiv.org/abs/2303.14524v2","updated":"2023-04-04T03:51:27Z","published":"2023-03-25T17:37:43Z","title":"Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender\n  System","summary":"  Large language models (LLMs) have demonstrated their significant potential to\nbe applied for addressing various application tasks. However, traditional\nrecommender systems continue to face great challenges such as poor\ninteractivity and explainability, which actually also hinder their broad\ndeployment in real-world systems. To address these limitations, this paper\nproposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender\nSystem) that innovatively augments LLMs for building conversational recommender\nsystems by converting user profiles and historical interactions into prompts.\nChat-Rec is demonstrated to be effective in learning user preferences and\nestablishing connections between users and products through in-context\nlearning, which also makes the recommendation process more interactive and\nexplainable. What's more, within the Chat-Rec framework, user's preferences can\ntransfer to different products for cross-domain recommendations, and\nprompt-based injection of information into LLMs can also handle the cold-start\nscenarios with new items. In our experiments, Chat-Rec effectively improve the\nresults of top-k recommendations and performs better in zero-shot rating\nprediction task. Chat-Rec offers a novel approach to improving recommender\nsystems and presents new practical scenarios for the implementation of AIGC (AI\ngenerated content) in recommender system studies.\n","authors":["Yunfan Gao","Tao Sheng","Youlin Xiang","Yun Xiong","Haofen Wang","Jiawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01446v1","updated":"2023-04-04T01:43:58Z","published":"2023-04-04T01:43:58Z","title":"Integrating Commercial and Social Determinants of Health: A Unified\n  Ontology for Non-Clinical Determinants of Health","summary":"  The objectives of this research are 1) to develop an ontology for CDoH by\nutilizing PubMed articles and ChatGPT; 2) to foster ontology reuse by\nintegrating CDoH with an existing SDoH ontology into a unified structure; 3) to\ndevise an overarching conception for all non-clinical determinants of health\nand to create an initial ontology, called N-CODH, for them; 4) and to validate\nthe degree of correspondence between concepts provided by ChatGPT with the\nexisting SDoH ontology\n","authors":["Navya Martin Kollapally","Vipina Kuttichi Keloth","Julia Xu","James Geller"],"pdf_url":"https://arxiv.org/pdf/2304.01446v1.pdf","comment":"Under review AMIA 2023"},{"id":"http://arxiv.org/abs/2304.02089v1","updated":"2023-04-04T19:35:43Z","published":"2023-04-04T19:35:43Z","title":"Hierarchically Fusing Long and Short-Term User Interests for\n  Click-Through Rate Prediction in Product Search","summary":"  Estimating Click-Through Rate (CTR) is a vital yet challenging task in\npersonalized product search. However, existing CTR methods still struggle in\nthe product search settings due to the following three challenges including how\nto more effectively extract users' short-term interests with respect to\nmultiple aspects, how to extract and fuse users' long-term interest with\nshort-term interests, how to address the entangling characteristic of long and\nshort-term interests. To resolve these challenges, in this paper, we propose a\nnew approach named Hierarchical Interests Fusing Network (HIFN), which consists\nof four basic modules namely Short-term Interests Extractor (SIE), Long-term\nInterests Extractor (LIE), Interests Fusion Module (IFM) and Interests\nDisentanglement Module (IDM). Specifically, SIE is proposed to extract user's\nshort-term interests by integrating three fundamental interests encoders within\nit namely query-dependent, target-dependent and causal-dependent interest\nencoder, respectively, followed by delivering the resultant representation to\nthe module LIE, where it can effectively capture user long-term interests by\ndevising an attention mechanism with respect to the short-term interests from\nSIE module. In IFM, the achieved long and short-term interests are further\nfused in an adaptive manner, followed by concatenating it with original raw\ncontext features for the final prediction result. Last but not least,\nconsidering the entangling characteristic of long and short-term interests, IDM\nfurther devises a self-supervised framework to disentangle long and short-term\ninterests. Extensive offline and online evaluations on a real-world e-commerce\nplatform demonstrate the superiority of HIFN over state-of-the-art methods.\n","authors":["Qijie Shen","Hong Wen","Jing Zhang","Qi Rao"],"pdf_url":"https://arxiv.org/pdf/2304.02089v1.pdf","comment":"accpeted by CIKM'22 as a Full Paper"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2304.02012v1","updated":"2023-04-04T17:59:14Z","published":"2023-04-04T17:59:14Z","title":"EGC: Image Generation and Classification via a Single Energy-Based Model","summary":"  Learning image classification and image generation using the same set of\nnetwork parameters is a challenging problem. Recent advanced approaches perform\nwell in one task often exhibit poor performance in the other. This work\nintroduces an energy-based classifier and generator, namely EGC, which can\nachieve superior performance in both tasks using a single neural network.\nUnlike a conventional classifier that outputs a label given an image (i.e., a\nconditional distribution $p(y|\\mathbf{x})$), the forward pass in EGC is a\nclassifier that outputs a joint distribution $p(\\mathbf{x},y)$, enabling an\nimage generator in its backward pass by marginalizing out the label $y$. This\nis done by estimating the energy and classification probability given a noisy\nimage in the forward pass, while denoising it using the score function\nestimated in the backward pass. EGC achieves competitive generation results\ncompared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN\nChurch, while achieving superior classification accuracy and robustness against\nadversarial attacks on CIFAR-10. This work represents the first successful\nattempt to simultaneously excel in both tasks using a single set of network\nparameters. We believe that EGC bridges the gap between discriminative and\ngenerative learning.\n","authors":["Qiushan Guo","Chuofan Ma","Yi Jiang","Zehuan Yuan","Yizhou Yu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2304.02012v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2304.02011v1","updated":"2023-04-04T17:59:09Z","published":"2023-04-04T17:59:09Z","title":"FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer","summary":"  Particle localization and -classification constitute two of the most\nfundamental problems in computational microscopy. In recent years, deep\nlearning based approaches have been introduced for these tasks with great\nsuccess. A key shortcoming of these supervised learning methods is their need\nfor large training data sets, typically generated from particle models in\nconjunction with complex numerical forward models simulating the physics of\ntransmission electron microscopes. Computer implementations of such forward\nmodels are computationally extremely demanding and limit the scope of their\napplicability. In this paper we propose a simple method for simulating the\nforward operator of an electron microscope based on additive noise and Neural\nStyle Transfer techniques. We evaluate the method on localization and\nclassification tasks using one of the established state-of-the-art\narchitectures showing performance on par with the benchmark. In contrast to\nprevious approaches, our method accelerates the data generation process by a\nfactor of 750 while using 33 times less memory and scales well to typical\ntransmission electron microscope detector sizes. It utilizes GPU acceleration\nand parallel processing. It can be used as a stand-alone method to adapt a\ntraining data set or as a data augmentation technique. The source code is\navailable at https://gitlab.com/deepet/faket.\n","authors":["Pavol Harar","Lukas Herrmann","Philipp Grohs","David Haselbach"],"pdf_url":"https://arxiv.org/pdf/2304.02011v1.pdf","comment":"1 table, 16 figures"},{"id":"http://arxiv.org/abs/2304.01996v1","updated":"2023-04-04T17:54:14Z","published":"2023-04-04T17:54:14Z","title":"Autoregressive Neural TensorNet: Bridging Neural Networks and Tensor\n  Networks for Quantum Many-Body Simulation","summary":"  Quantum many-body physics simulation has important impacts on understanding\nfundamental science and has applications to quantum materials design and\nquantum technology. However, due to the exponentially growing size of the\nHilbert space with respect to the particle number, a direct simulation is\nintractable. While representing quantum states with tensor networks and neural\nnetworks are the two state-of-the-art methods for approximate simulations, each\nhas its own limitations in terms of expressivity and optimization. To address\nthese challenges, we develop a novel architecture, Autoregressive Neural\nTensorNet (ANTN), which bridges tensor networks and autoregressive neural\nnetworks. We show that Autoregressive Neural TensorNet parameterizes normalized\nwavefunctions with exact sampling, generalizes the expressivity of tensor\nnetworks and autoregressive neural networks, and inherits a variety of\nsymmetries from autoregressive neural networks. We demonstrate our approach on\nthe 2D $J_1$-$J_2$ Heisenberg model with different systems sizes and coupling\nparameters, outperforming both tensor networks and autoregressive neural\nnetworks. Our work opens up new opportunities for both scientific simulations\nand machine learning applications.\n","authors":["Zhuo Chen","Laker Newhouse","Eddie Chen","Di Luo","Marin Soljačić"],"pdf_url":"https://arxiv.org/pdf/2304.01996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01994v1","updated":"2023-04-04T17:52:49Z","published":"2023-04-04T17:52:49Z","title":"DWA: Differential Wavelet Amplifier for Image Super-Resolution","summary":"  This work introduces Differential Wavelet Amplifier (DWA), a drop-in module\nfor wavelet-based image Super-Resolution (SR). DWA invigorates an approach\nrecently receiving less attention, namely Discrete Wavelet Transformation\n(DWT). DWT enables an efficient image representation for SR and reduces the\nspatial area of its input by a factor of 4, the overall model size, and\ncomputation cost, framing it as an attractive approach for sustainable ML. Our\nproposed DWA model improves wavelet-based SR models by leveraging the\ndifference between two convolutional filters to refine relevant feature\nextraction in the wavelet domain, emphasizing local contrasts and suppressing\ncommon noise in the input signals. We show its effectiveness by integrating it\ninto existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear\nimprovement in classical SR tasks. Moreover, DWA enables a direct application\nof DWSR and MWCNN to input image space, reducing the DWT representation\nchannel-wise since it omits traditional DWT.\n","authors":["Brian Moser","Stanislav Frolov","Federico Raue","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2304.01994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01990v1","updated":"2023-04-04T17:49:45Z","published":"2023-04-04T17:49:45Z","title":"Side Channel-Assisted Inference Leakage from Machine Learning-based ECG\n  Classification","summary":"  The Electrocardiogram (ECG) measures the electrical cardiac activity\ngenerated by the heart to detect abnormal heartbeat and heart attack. However,\nthe irregular occurrence of the abnormalities demands continuous monitoring of\nheartbeats. Machine learning techniques are leveraged to automate the task to\nreduce labor work needed during monitoring. In recent years, many companies\nhave launched products with ECG monitoring and irregular heartbeat alert. Among\nall classification algorithms, the time series-based algorithm dynamic time\nwarping (DTW) is widely adopted to undertake the ECG classification task.\nThough progress has been achieved, the DTW-based ECG classification also brings\na new attacking vector of leaking the patients' diagnosis results. This paper\nshows that the ECG input samples' labels can be stolen via a side-channel\nattack, Flush+Reload. In particular, we first identify the vulnerability of DTW\nfor ECG classification, i.e., the correlation between warping path choice and\nprediction results. Then we implement an attack that leverages Flush+Reload to\nmonitor the warping path selection with known ECG data and then build a\npredictor for constructing the relation between warping path selection and\nlabels of input ECG samples. Based on experiments, we find that the\nFlush+Reload-based inference leakage can achieve an 84.0\\% attacking success\nrate to identify the labels of the two samples in DTW.\n","authors":["Jialin Liu","Ning Miao","Chongzhou Fang","Houman Homayoun","Han Wang"],"pdf_url":"https://arxiv.org/pdf/2304.01990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01973v1","updated":"2023-04-04T17:31:15Z","published":"2023-04-04T17:31:15Z","title":"ERM++: An Improved Baseline for Domain Generalization","summary":"  Multi-source Domain Generalization (DG) measures a classifier's ability to\ngeneralize to new distributions of data it was not trained on, given several\ntraining domains. While several multi-source DG methods have been proposed,\nthey incur additional complexity during training by using domain labels. Recent\nwork has shown that a well-tuned Empirical Risk Minimization (ERM) training\nprocedure, that is simply minimizing the empirical risk on the source domains,\ncan outperform most existing DG methods. We identify several key candidate\ntechniques to further improve ERM performance, such as better utilization of\ntraining data, model parameter selection, and weight-space regularization. We\ncall the resulting method ERM++, and show it significantly improves the\nperformance of DG on five multi-source datasets by over 5% compared to standard\nERM, and beats state-of-the-art despite being less computationally expensive.\nAdditionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset, a\nchallenging DG benchmark. We hope that ERM++ becomes a strong baseline for\nfuture DG research. Code is released at\nhttps://github.com/piotr-teterwak/erm_plusplus.\n","authors":["Piotr Teterwak","Kuniaki Saito","Theodoros Tsiligkaridis","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2304.01973v1.pdf","comment":"An improved baseline for Domain Generalization"},{"id":"http://arxiv.org/abs/2304.01963v1","updated":"2023-04-04T17:13:22Z","published":"2023-04-04T17:13:22Z","title":"Model-corrected learned primal-dual models for fast limited-view\n  photoacoustic tomography","summary":"  Learned iterative reconstructions hold great promise to accelerate\ntomographic imaging with empirical robustness to model perturbations.\nNevertheless, an adoption for photoacoustic tomography is hindered by the need\nto repeatedly evaluate the computational expensive forward model. Computational\nfeasibility can be obtained by the use of fast approximate models, but a need\nto compensate model errors arises. In this work we advance the methodological\nand theoretical basis for model corrections in learned image reconstructions by\nembedding the model correction in a learned primal-dual framework. Here, the\nmodel correction is jointly learned in data space coupled with a learned\nupdating operator in image space within an unrolled end-to-end learned\niterative reconstruction approach. The proposed formulation allows an extension\nto a primal-dual deep equilibrium model providing fixed-point convergence as\nwell as reduced memory requirements for training. We provide theoretical and\nempirical insights into the proposed models with numerical validation in a\nrealistic 2D limited-view setting. The model-corrected learned primal-dual\nmethods show excellent reconstruction quality with fast inference times and\nthus providing a methodological basis for real-time capable and scalable\niterative reconstructions in photoacoustic tomography.\n","authors":["Andreas Hauptmann","Jenni Poimala"],"pdf_url":"https://arxiv.org/pdf/2304.01963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01959v1","updated":"2023-04-04T17:07:06Z","published":"2023-04-04T17:07:06Z","title":"Randomized Adversarial Style Perturbations for Domain Generalization","summary":"  We propose a novel domain generalization technique, referred to as Randomized\nAdversarial Style Perturbation (RASP), which is motivated by the observation\nthat the characteristics of each domain are captured by the feature statistics\ncorresponding to style. The proposed algorithm perturbs the style of a feature\nin an adversarial direction towards a randomly selected class, and makes the\nmodel learn against being misled by the unexpected styles observed in unseen\ntarget domains. While RASP is effective to handle domain shifts, its naive\nintegration into the training procedure might degrade the capability of\nlearning knowledge from source domains because it has no restriction on the\nperturbations of representations. This challenge is alleviated by Normalized\nFeature Mixup (NFM), which facilitates the learning of the original features\nwhile achieving robustness to perturbed representations via their mixup during\ntraining. We evaluate the proposed algorithm via extensive experiments on\nvarious benchmarks and show that our approach improves domain generalization\nperformance, especially in large-scale benchmarks.\n","authors":["Taehoon Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2304.01959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.05135v2","updated":"2023-04-04T16:30:00Z","published":"2022-09-12T10:42:26Z","title":"Signs of Language: Embodied Sign Language Fingerspelling Acquisition\n  from Demonstrations for Human-Robot Interaction","summary":"  Learning fine-grained movements is a challenging topic in robotics,\nparticularly in the context of robotic hands. One specific instance of this\nchallenge is the acquisition of fingerspelling sign language in robots. In this\npaper, we propose an approach for learning dexterous motor imitation from video\nexamples without additional information. To achieve this, we first build a URDF\nmodel of a robotic hand with a single actuator for each joint. We then leverage\npre-trained deep vision models to extract the 3D pose of the hand from RGB\nvideos. Next, using state-of-the-art reinforcement learning algorithms for\nmotion imitation (namely, proximal policy optimization and soft actor-critic),\nwe train a policy to reproduce the movement extracted from the demonstrations.\nWe identify the optimal set of hyperparameters for imitation based on a\nreference motion. Finally, we demonstrate the generalizability of our approach\nby testing it on six different tasks, corresponding to fingerspelled letters.\nOur results show that our approach is able to successfully imitate these\nfine-grained movements without additional information, highlighting its\npotential for real-world applications in robotics.\n","authors":["Federico Tavella","Aphrodite Galata","Angelo Cangelosi"],"pdf_url":"https://arxiv.org/pdf/2209.05135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.05711v3","updated":"2023-04-04T16:27:38Z","published":"2022-03-11T01:45:33Z","title":"Synopses of Movie Narratives: a Video-Language Dataset for Story\n  Understanding","summary":"  Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SYMON), containing\n5,193 video summaries of popular movies and TV series. SYMON captures\nnaturalistic story-telling videos for human audience made by human creators. As\na prototypical and naturalistic story dataset, SYMON features high coverage of\nmultimodal story events, abundant mental-state descriptions, and large semantic\ngaps between the visual and the textual modalities. We establish benchmarks on\nvideo-text retrieval and zero-shot alignment on movie summary videos, which\nshowcase the importance of in-domain data in story understanding. With SYMON,\nwe hope to lay the groundwork for progress in multimodal story understanding.\n","authors":["Yidan Sun","Qin Chao","Yangfeng Ji","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2203.05711v3.pdf","comment":"25 pages, 17 figures"},{"id":"http://arxiv.org/abs/2304.01926v1","updated":"2023-04-04T16:19:15Z","published":"2023-04-04T16:19:15Z","title":"High-Throughput Vector Similarity Search in Knowledge Graphs","summary":"  There is an increasing adoption of machine learning for encoding data into\nvectors to serve online recommendation and search use cases. As a result,\nrecent data management systems propose augmenting query processing with online\nvector similarity search. In this work, we explore vector similarity search in\nthe context of Knowledge Graphs (KGs). Motivated by the tasks of finding\nrelated KG queries and entities for past KG query workloads, we focus on hybrid\nvector similarity search (hybrid queries for short) where part of the query\ncorresponds to vector similarity search and part of the query corresponds to\npredicates over relational attributes associated with the underlying data\nvectors. For example, given past KG queries for a song entity, we want to\nconstruct new queries for new song entities whose vector representations are\nclose to the vector representation of the entity in the past KG query. But\nentities in a KG also have non-vector attributes such as a song associated with\nan artist, a genre, and a release date. Therefore, suggested entities must also\nsatisfy query predicates over non-vector attributes beyond a vector-based\nsimilarity predicate. While these tasks are central to KGs, our contributions\nare generally applicable to hybrid queries. In contrast to prior works that\noptimize online queries, we focus on enabling efficient batch processing of\npast hybrid query workloads. We present our system, HQI, for high-throughput\nbatch processing of hybrid queries. We introduce a workload-aware vector data\npartitioning scheme to tailor the vector index layout to the given workload and\ndescribe a multi-query optimization technique to reduce the overhead of vector\nsimilarity computations. We evaluate our methods on industrial workloads and\ndemonstrate that HQI yields a 31x improvement in throughput for finding related\nKG queries compared to existing hybrid query processing approaches.\n","authors":["Jason Mohoney","Anil Pacaci","Shihabur Rahman Chowdhury","Ali Mousavi","Ihab F. Ilyas","Umar Farooq Minhas","Jeffrey Pound","Theodoros Rekatsinas"],"pdf_url":"https://arxiv.org/pdf/2304.01926v1.pdf","comment":"13 pages, 7 figures, to be published in ACM SIGMOD 2023"},{"id":"http://arxiv.org/abs/2304.01910v1","updated":"2023-04-04T16:09:55Z","published":"2023-04-04T16:09:55Z","title":"Calibrated Chaos: Variance Between Runs of Neural Network Training is\n  Harmless and Inevitable","summary":"  Typical neural network trainings have substantial variance in test-set\nperformance between repeated runs, impeding hyperparameter comparison and\ntraining reproducibility. We present the following results towards\nunderstanding this variation. (1) Despite having significant variance on their\ntest-sets, we demonstrate that standard CIFAR-10 and ImageNet trainings have\nvery little variance in their performance on the test-distributions from which\nthose test-sets are sampled, suggesting that variance is less of a practical\nissue than previously thought. (2) We present a simplifying statistical\nassumption which closely approximates the structure of the test-set accuracy\ndistribution. (3) We argue that test-set variance is inevitable in the\nfollowing two senses. First, we show that variance is largely caused by high\nsensitivity of the training process to initial conditions, rather than by\nspecific sources of randomness like the data order and augmentations. Second,\nwe prove that variance is unavoidable given the observation that ensembles of\ntrained networks are well-calibrated. (4) We conduct preliminary studies of\ndistribution-shift, fine-tuning, data augmentation and learning rate through\nthe lens of variance between runs.\n","authors":["Keller Jordan"],"pdf_url":"https://arxiv.org/pdf/2304.01910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01908v1","updated":"2023-04-04T16:04:42Z","published":"2023-04-04T16:04:42Z","title":"Leveraging Deep Learning Approaches for Deepfake Detection: A Review","summary":"  Conspicuous progression in the field of machine learning and deep learning\nhave led the jump of highly realistic fake media, these media oftentimes\nreferred as deepfakes. Deepfakes are fabricated media which are generated by\nsophisticated AI that are at times very difficult to set apart from the real\nmedia. So far, this media can be uploaded to the various social media\nplatforms, hence advertising it to the world got easy, calling for an\nefficacious countermeasure. Thus, one of the optimistic counter steps against\ndeepfake would be deepfake detection. To undertake this threat, researchers in\nthe past have created models to detect deepfakes based on ML/DL techniques like\nConvolutional Neural Networks. This paper aims to explore different\nmethodologies with an intention to achieve a cost-effective model with a higher\naccuracy with different types of the datasets, which is to address the\ngeneralizability of the dataset.\n","authors":["Aniruddha Tiwari","Rushit Dave","Mounika Vanamala"],"pdf_url":"https://arxiv.org/pdf/2304.01908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01906v1","updated":"2023-04-04T16:00:48Z","published":"2023-04-04T16:00:48Z","title":"Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with\n  Python","summary":"  The $\\texttt{torch-choice}$ is an open-source library for flexible, fast\nchoice modeling with Python and PyTorch. $\\texttt{torch-choice}$ provides a\n$\\texttt{ChoiceDataset}$ data structure to manage databases flexibly and\nmemory-efficiently. The paper demonstrates constructing a\n$\\texttt{ChoiceDataset}$ from databases of various formats and functionalities\nof $\\texttt{ChoiceDataset}$. The package implements two widely used models,\nnamely the multinomial logit and nested logit models, and supports\nregularization during model estimation. The package incorporates the option to\ntake advantage of GPUs for estimation, allowing it to scale to massive datasets\nwhile being computationally efficient. Models can be initialized using either\nR-style formula strings or Python dictionaries. We conclude with a comparison\nof the computational efficiencies of $\\texttt{torch-choice}$ and\n$\\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the\nnumber of covariates increases, and (3) the expansion of item sets. Finally, we\ndemonstrate the scalability of $\\texttt{torch-choice}$ on large-scale datasets.\n","authors":["Tianyu Du","Ayush Kanodia","Susan Athey"],"pdf_url":"https://arxiv.org/pdf/2304.01906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12260v2","updated":"2023-04-04T16:00:35Z","published":"2023-02-23T16:08:39Z","title":"Solving differential equations using physics informed deep learning: a\n  hand-on tutorial with benchmark tests","summary":"  We revisit the original approach of using deep learning and neural networks\nto solve differential equations by incorporating the knowledge of the equation.\nThis is done by adding a dedicated term to the loss function during the\noptimization procedure in the training process. The so-called physics-informed\nneural networks (PINNs) are tested on a variety of academic ordinary\ndifferential equations in order to highlight the benefits and drawbacks of this\napproach with respect to standard integration methods. We focus on the\npossibility to use the least possible amount of data into the training process.\nThe principles of PINNs for solving differential equations by enforcing\nphysical laws via penalizing terms are reviewed. A tutorial on a simple\nequation model illustrates how to put into practice the method for ordinary\ndifferential equations. Benchmark tests show that a very small amount of\ntraining data is sufficient to predict the solution when the non linearity of\nthe problem is weak. However, this is not the case in strongly non linear\nproblems where a priori knowledge of training data over some partial or the\nwhole time integration interval is necessary.\n","authors":["Hubert Baty","Leo Baty"],"pdf_url":"https://arxiv.org/pdf/2302.12260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17612v2","updated":"2023-04-04T15:51:59Z","published":"2023-03-30T01:37:19Z","title":"oBERTa: Improving Sparse Transfer Learning via improved initialization,\n  distillation, and pruning regimes","summary":"  In this paper, we introduce the range of oBERTa language models, an\neasy-to-use set of language models which allows Natural Language Processing\n(NLP) practitioners to obtain between 3.8 and 24.3 times faster models without\nexpertise in model compression. Specifically, oBERTa extends existing work on\npruning, knowledge distillation, and quantization and leverages frozen\nembeddings improves distillation and model initialization to deliver higher\naccuracy on a broad range of transfer tasks. In generating oBERTa, we explore\nhow the highly optimized RoBERTa differs from the BERT for pruning during\npre-training and finetuning. We find it less amenable to compression during\nfine-tuning. We explore the use of oBERTa on seven representative NLP tasks and\nfind that the improved compression techniques allow a pruned oBERTa model to\nmatch the performance of BERTbase and exceed the performance of Prune OFA Large\non the SQUAD V1.1 Question Answering dataset, despite being 8x and 2x,\nrespectively faster in inference. We release our code, training regimes, and\nassociated model for broad usage to encourage usage and experimentation\n","authors":["Daniel Campos","Alexandre Marques","Mark Kurtz","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2303.17612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01899v1","updated":"2023-04-04T15:48:09Z","published":"2023-04-04T15:48:09Z","title":"Cross-Class Feature Augmentation for Class Incremental Learning","summary":"  We propose a novel class incremental learning approach by incorporating a\nfeature augmentation technique motivated by adversarial attacks. We employ a\nclassifier learned in the past to complement training examples rather than\nsimply play a role as a teacher for knowledge distillation towards subsequent\nmodels. The proposed approach has a unique perspective to utilize the previous\nknowledge in class incremental learning since it augments features of arbitrary\ntarget classes using examples in other classes via adversarial attacks on a\npreviously learned classifier. By allowing the cross-class feature\naugmentations, each class in the old tasks conveniently populates samples in\nthe feature space, which alleviates the collapse of the decision boundaries\ncaused by sample deficiency for the previous tasks, especially when the number\nof stored exemplars is small. This idea can be easily incorporated into\nexisting class incremental learning algorithms without any architecture\nmodification. Extensive experiments on the standard benchmarks show that our\nmethod consistently outperforms existing class incremental learning methods by\nsignificant margins in various scenarios, especially under an environment with\nan extremely limited memory budget.\n","authors":["Taehoon Kim","Jaeyoo Park","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2304.01899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01893v1","updated":"2023-04-04T15:46:42Z","published":"2023-04-04T15:46:42Z","title":"Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory\n  Diffusion","summary":"  We introduce a method for generating realistic pedestrian trajectories and\nfull-body animations that can be controlled to meet user-defined goals. We draw\non recent advances in guided diffusion modeling to achieve test-time\ncontrollability of trajectories, which is normally only associated with\nrule-based systems. Our guided diffusion model allows users to constrain\ntrajectories through target waypoints, speed, and specified social groups while\naccounting for the surrounding environment context. This trajectory diffusion\nmodel is integrated with a novel physics-based humanoid controller to form a\nclosed-loop, full-body pedestrian animation system capable of placing large\ncrowds in a simulated environment with varying terrains. We further propose\nutilizing the value function learned during RL training of the animation\ncontroller to guide diffusion to produce trajectories better suited for\nparticular scenarios such as collision avoidance and traversing uneven terrain.\nVideo results are available on the project page at\nhttps://nv-tlabs.github.io/trace-pace .\n","authors":["Davis Rempe","Zhengyi Luo","Xue Bin Peng","Ye Yuan","Kris Kitani","Karsten Kreis","Sanja Fidler","Or Litany"],"pdf_url":"https://arxiv.org/pdf/2304.01893v1.pdf","comment":"Conference on Computer Vision and Pattern Recognition (CVPR) 2023"},{"id":"http://arxiv.org/abs/2304.01890v1","updated":"2023-04-04T15:42:08Z","published":"2023-04-04T15:42:08Z","title":"Sociocultural knowledge is needed for selection of shots in hate speech\n  detection tasks","summary":"  We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for\nthe countries of Brazil, Germany, India and Kenya, to aid training and\ninterpretability of models. We demonstrate how our lexicon can be used to\ninterpret model predictions, showing that models developed to classify extreme\nspeech rely heavily on target words when making predictions. Further, we\npropose a method to aid shot selection for training in low-resource settings\nvia HATELEXICON. In few-shot learning, the selection of shots is of paramount\nimportance to model performance. In our work, we simulate a few-shot setting\nfor German and Hindi, using HASOC data for training and the Multilingual\nHateCheck (MHC) as a benchmark. We show that selecting shots based on our\nlexicon leads to models performing better on MHC than models trained on shots\nsampled randomly. Thus, when given only a few training examples, using our\nlexicon to select shots containing more sociocultural information leads to\nbetter few-shot performance.\n","authors":["Antonis Maronikolakis","Abdullatif Köksal","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2304.01890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01880v1","updated":"2023-04-04T15:34:53Z","published":"2023-04-04T15:34:53Z","title":"Measure theoretic results for approximation by neural networks with\n  limited weights","summary":"  In this paper, we study approximation properties of single hidden layer\nneural networks with weights varying on finitely many directions and thresholds\nfrom an open interval. We obtain a necessary and at the same time sufficient\nmeasure theoretic condition for density of such networks in the space of\ncontinuous functions. Further, we prove a density result for neural networks\nwith a specifically constructed activation function and a fixed number of\nneurons.\n","authors":["Vugar Ismailov","Ekrem Savas"],"pdf_url":"https://arxiv.org/pdf/2304.01880v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2304.01874v1","updated":"2023-04-04T15:28:22Z","published":"2023-04-04T15:28:22Z","title":"Incremental Verification of Neural Networks","summary":"  Complete verification of deep neural networks (DNNs) can exactly determine\nwhether the DNN satisfies a desired trustworthy property (e.g., robustness,\nfairness) on an infinite set of inputs or not. Despite the tremendous progress\nto improve the scalability of complete verifiers over the years on individual\nDNNs, they are inherently inefficient when a deployed DNN is updated to improve\nits inference speed or accuracy. The inefficiency is because the expensive\nverifier needs to be run from scratch on the updated DNN. To improve\nefficiency, we propose a new, general framework for incremental and complete\nDNN verification based on the design of novel theory, data structure, and\nalgorithms. Our contributions implemented in a tool named IVAN yield an overall\ngeometric mean speedup of 2.4x for verifying challenging MNIST and CIFAR10\nclassifiers and a geometric mean speedup of 3.8x for the ACAS-XU classifiers\nover the state-of-the-art baselines.\n","authors":["Shubham Ugare","Debangshu Banerjee","Sasa Misailovic","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2304.01874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.09410v3","updated":"2023-04-04T15:15:22Z","published":"2022-03-17T16:11:36Z","title":"A Framework and Benchmark for Deep Batch Active Learning for Regression","summary":"  The acquisition of labels for supervised learning can be expensive. In order\nto improve the sample-efficiency of neural network regression, we study active\nlearning methods that adaptively select batches of unlabeled data for labeling.\nWe present a framework for constructing such methods out of (network-dependent)\nbase kernels, kernel transformations and selection methods. Our framework\nencompasses many existing Bayesian methods based on Gaussian Process\napproximations of neural networks as well as non-Bayesian methods.\nAdditionally, we propose to replace the commonly used last-layer features with\nsketched finite-width Neural Tangent Kernels, and to combine them with a novel\nclustering method. To evaluate different methods, we introduce an open-source\nbenchmark consisting of 15 large tabular regression data sets. Our proposed\nmethod outperforms the state-of-the-art on our benchmark, scales to large data\nsets, and works out-of-the-box without adjusting the network architecture or\ntraining code. We provide open-source code that includes efficient\nimplementations of all kernels, kernel transformations, and selection methods,\nand can be used for reproducing our results.\n","authors":["David Holzmüller","Viktor Zaverkin","Johannes Kästner","Ingo Steinwart"],"pdf_url":"https://arxiv.org/pdf/2203.09410v3.pdf","comment":"Changes in v3: Improvements in writing and other minor changes.\n  Accompanying code can be found at https://github.com/dholzmueller/bmdal_reg"},{"id":"http://arxiv.org/abs/2302.00539v2","updated":"2023-04-04T15:07:57Z","published":"2023-02-01T16:04:48Z","title":"Analyzing Leakage of Personally Identifiable Information in Language\n  Models","summary":"  Language Models (LMs) have been shown to leak information about training data\nthrough sentence-level membership inference and reconstruction attacks.\nUnderstanding the risk of LMs leaking Personally Identifiable Information (PII)\nhas received less attention, which can be attributed to the false assumption\nthat dataset curation techniques such as scrubbing are sufficient to prevent\nPII leakage. Scrubbing techniques reduce but do not prevent the risk of PII\nleakage: in practice scrubbing is imperfect and must balance the trade-off\nbetween minimizing disclosure and preserving the utility of the dataset. On the\nother hand, it is unclear to which extent algorithmic defenses such as\ndifferential privacy, designed to guarantee sentence- or user-level privacy,\nprevent PII disclosure. In this work, we introduce rigorous game-based\ndefinitions for three types of PII leakage via black-box extraction, inference,\nand reconstruction attacks with only API access to an LM. We empirically\nevaluate the attacks against GPT-2 models fine-tuned with and without defenses\non three domains: case law, health care, and e-mails. Our main contributions\nare (i) novel attacks that can extract up to 10$\\times$ more PII sequences than\nexisting attacks, (ii) showing that sentence-level differential privacy reduces\nthe risk of PII disclosure but still leaks about 3% of PII sequences, and (iii)\na subtle connection between record-level membership inference and PII\nreconstruction.\n","authors":["Nils Lukas","Ahmed Salem","Robert Sim","Shruti Tople","Lukas Wutschitz","Santiago Zanella-Béguelin"],"pdf_url":"https://arxiv.org/pdf/2302.00539v2.pdf","comment":"IEEE Symposium on Security and Privacy (S&P) 2023"},{"id":"http://arxiv.org/abs/2211.02811v2","updated":"2023-04-04T15:03:52Z","published":"2022-11-05T04:24:20Z","title":"From Cubes to Networks: Fast Generic Model for Synthetic Networks\n  Generation","summary":"  Analytical explorations on complex networks and cubes (i.e.,\nmulti-dimensional datasets) are currently two separate research fields with\ndifferent strategies. To gain more insights into cube dynamics via unique\nnetwork-domain methodologies and to obtain abundant synthetic networks, we need\na transformation approach from cubes into associated networks. To this end, we\npropose FGM, a fast generic model converting cubes into interrelated networks,\nwhereby samples are remodeled into nodes and network dynamics are guided under\nthe concept of nearest-neighbor searching. Through comparison with previous\nmodels, we show that FGM can cost-efficiently generate networks exhibiting\ntypical patterns more closely aligned to factual networks, such as more\nauthentic degree distribution, power-law average nearest-neighbor degree\ndependency, and the influence decay phenomenon we consider vital for networks.\nFurthermore, we evaluate the networks that FGM generates through various cubes.\nResults show that FGM is resilient to input perturbations, producing networks\nwith consistent fine properties.\n","authors":["Shaojie Min","Ji Liu"],"pdf_url":"https://arxiv.org/pdf/2211.02811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.09474v3","updated":"2023-04-04T14:58:32Z","published":"2023-01-23T15:18:54Z","title":"DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained\n  Diffusion","summary":"  Real-world data generation often involves complex inter-dependencies among\ninstances, violating the IID-data hypothesis of standard learning paradigms and\nposing a challenge for uncovering the geometric structures for learning desired\ninstance representations. To this end, we introduce an energy constrained\ndiffusion model which encodes a batch of instances from a dataset into\nevolutionary states that progressively incorporate other instances' information\nby their interactions. The diffusion process is constrained by descent criteria\nw.r.t.~a principled energy function that characterizes the global consistency\nof instance representations over latent structures. We provide rigorous theory\nthat implies closed-form optimal estimates for the pairwise diffusion strength\namong arbitrary instance pairs, which gives rise to a new class of neural\nencoders, dubbed as DIFFormer (diffusion-based Transformers), with two\ninstantiations: a simple version with linear complexity for prohibitive\ninstance numbers, and an advanced version for learning complex structures.\nExperiments highlight the wide applicability of our model as a general-purpose\nencoder backbone with superior performance in various tasks, such as node\nclassification on large graphs, semi-supervised image/text classification, and\nspatial-temporal dynamics prediction.\n","authors":["Qitian Wu","Chenxiao Yang","Wentao Zhao","Yixuan He","David Wipf","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2301.09474v3.pdf","comment":"Published at ICLR 2023 as a spotlight presentation, the\n  implementation code is available at https://github.com/qitianwu/DIFFormer"},{"id":"http://arxiv.org/abs/2110.10745v4","updated":"2023-04-04T14:36:09Z","published":"2021-10-20T19:36:55Z","title":"Iterated Block Particle Filter for High-dimensional Parameter Learning:\n  Beating the Curse of Dimensionality","summary":"  Parameter learning for high-dimensional, partially observed, and nonlinear\nstochastic processes is a methodological challenge. Spatiotemporal disease\ntransmission systems provide examples of such processes giving rise to open\ninference problems. We propose the iterated block particle filter (IBPF)\nalgorithm for learning high-dimensional parameters over graphical state space\nmodels with general state spaces, measures, transition densities and graph\nstructure. Theoretical performance guarantees are obtained on beating the curse\nof dimensionality (COD), algorithm convergence, and likelihood maximization.\nExperiments on a highly nonlinear and non-Gaussian spatiotemporal model for\nmeasles transmission reveal that the iterated ensemble Kalman filter algorithm\n(Li et al. (2020)) is ineffective and the iterated filtering algorithm (Ionides\net al. (2015)) suffers from the COD, while our IBPF algorithm beats COD\nconsistently across various experiments with different metrics.\n","authors":["Ning Ning","Edward L. Ionides"],"pdf_url":"https://arxiv.org/pdf/2110.10745v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01829v1","updated":"2023-04-04T14:33:30Z","published":"2023-04-04T14:33:30Z","title":"A Survey on Vertical Federated Learning: From a Layered Perspective","summary":"  Vertical federated learning (VFL) is a promising category of federated\nlearning for the scenario where data is vertically partitioned and distributed\namong parties. VFL enriches the description of samples using features from\ndifferent parties to improve model capacity. Compared with horizontal federated\nlearning, in most cases, VFL is applied in the commercial cooperation scenario\nof companies. Therefore, VFL contains tremendous business values. In the past\nfew years, VFL has attracted more and more attention in both academia and\nindustry. In this paper, we systematically investigate the current work of VFL\nfrom a layered perspective. From the hardware layer to the vertical federated\nsystem layer, researchers contribute to various aspects of VFL. Moreover, the\napplication of VFL has covered a wide range of areas, e.g., finance,\nhealthcare, etc. At each layer, we categorize the existing work and explore the\nchallenges for the convenience of further research and development of VFL.\nEspecially, we design a novel MOSP tree taxonomy to analyze the core component\nof VFL, i.e., secure vertical federated machine learning algorithm. Our\ntaxonomy considers four dimensions, i.e., machine learning model (M),\nprotection object (O), security model (S), and privacy-preserving protocol (P),\nand provides a comprehensive investigation.\n","authors":["Liu Yang","Di Chai","Junxue Zhang","Yilun Jin","Leye Wang","Hao Liu","Han Tian","Qian Xu","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2304.01829v1.pdf","comment":"35 pages, 6 figures"},{"id":"http://arxiv.org/abs/2304.01828v1","updated":"2023-04-04T14:32:07Z","published":"2023-04-04T14:32:07Z","title":"Learning Stable and Robust Linear Parameter-Varying State-Space Models","summary":"  This paper presents two direct parameterizations of stable and robust linear\nparameter-varying state-space (LPV-SS) models. The model parametrizations\nguarantee a priori that for all parameter values during training, the allowed\nmodels are stable in the contraction sense or have their Lipschitz constant\nbounded by a user-defined value $\\gamma$. Furthermore, since the\nparametrizations are direct, the models can be trained using unconstrained\noptimization. The fact that the trained models are of the LPV-SS class makes\nthem useful for, e.g., further convex analysis or controller design. The\neffectiveness of the approach is demonstrated on an LPV identification problem.\n","authors":["Chris Verhoek","Ruigang Wang","Roland Tóth"],"pdf_url":"https://arxiv.org/pdf/2304.01828v1.pdf","comment":"Submitted to the 62nd IEEE Conference on Decision and Control\n  (CDC2023)"},{"id":"http://arxiv.org/abs/2304.01826v1","updated":"2023-04-04T14:31:18Z","published":"2023-04-04T14:31:18Z","title":"CGDTest: A Constrained Gradient Descent Algorithm for Testing Neural\n  Networks","summary":"  In this paper, we propose a new Deep Neural Network (DNN) testing algorithm\ncalled the Constrained Gradient Descent (CGD) method, and an implementation we\ncall CGDTest aimed at exposing security and robustness issues such as\nadversarial robustness and bias in DNNs. Our CGD algorithm is a\ngradient-descent (GD) method, with the twist that the user can also specify\nlogical properties that characterize the kinds of inputs that the user may\nwant. This functionality sets CGDTest apart from other similar DNN testing\ntools since it allows users to specify logical constraints to test DNNs not\nonly for $\\ell_p$ ball-based adversarial robustness but, more importantly,\nincludes richer properties such as disguised and flow adversarial constraints,\nas well as adversarial robustness in the NLP domain. We showcase the utility\nand power of CGDTest via extensive experimentation in the context of vision and\nNLP domains, comparing against 32 state-of-the-art methods over these diverse\ndomains. Our results indicate that CGDTest outperforms state-of-the-art testing\ntools for $\\ell_p$ ball-based adversarial robustness, and is significantly\nsuperior in testing for other adversarial robustness, with improvements in PAR2\nscores of over 1500% in some cases over the next best tool. Our evaluation\nshows that our CGD method outperforms competing methods we compared against in\nterms of expressibility (i.e., a rich constraint language and concomitant tool\nsupport to express a wide variety of properties), scalability (i.e., can be\napplied to very large real-world models with up to 138 million parameters), and\ngenerality (i.e., can be used to test a plethora of model architectures).\n","authors":["Vineel Nagisetty","Laura Graves","Guanting Pan","Piyush Jha","Vijay Ganesh"],"pdf_url":"https://arxiv.org/pdf/2304.01826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12798v2","updated":"2023-04-04T14:17:15Z","published":"2023-02-24T18:19:49Z","title":"3D Generative Model Latent Disentanglement via Local Eigenprojection","summary":"  Designing realistic digital humans is extremely complex. Most data-driven\ngenerative models used to simplify the creation of their underlying geometric\nshape do not offer control over the generation of local shape attributes. In\nthis paper, we overcome this limitation by introducing a novel loss function\ngrounded in spectral geometry and applicable to different neural-network-based\ngenerative models of 3D head and body meshes. Encouraging the latent variables\nof mesh variational autoencoders (VAEs) or generative adversarial networks\n(GANs) to follow the local eigenprojections of identity attributes, we improve\nlatent disentanglement and properly decouple the attribute creation.\nExperimental results show that our local eigenprojection disentangled (LED)\nmodels not only offer improved disentanglement with respect to the\nstate-of-the-art, but also maintain good generation capabilities with training\ntimes comparable to the vanilla implementations of the models.\n","authors":["Simone Foti","Bongjin Koo","Danail Stoyanov","Matthew J. Clarkson"],"pdf_url":"https://arxiv.org/pdf/2302.12798v2.pdf","comment":"Computer Graphics Forum 2023"},{"id":"http://arxiv.org/abs/2304.01814v1","updated":"2023-04-04T14:13:13Z","published":"2023-04-04T14:13:13Z","title":"CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for\n  Low-Dose CT Denoising and Generalization","summary":"  Low-dose computed tomography (CT) images suffer from noise and artifacts due\nto photon starvation and electronic noise. Recently, some works have attempted\nto use diffusion models to address the over-smoothness and training instability\nencountered by previous deep-learning-based denoising models. However,\ndiffusion models suffer from long inference times due to the large number of\nsampling steps involved. Very recently, cold diffusion model generalizes\nclassical diffusion models and has greater flexibility. Inspired by the cold\ndiffusion, this paper presents a novel COntextual eRror-modulated gEneralized\nDiffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First,\nCoreDiff utilizes LDCT images to displace the random Gaussian noise and employs\na novel mean-preserving degradation operator to mimic the physical process of\nCT degradation, significantly reducing sampling steps thanks to the informative\nLDCT images as the starting point of the sampling process. Second, to alleviate\nthe error accumulation problem caused by the imperfect restoration operator in\nthe sampling process, we propose a novel ContextuaL Error-modulAted Restoration\nNetwork (CLEAR-Net), which can leverage contextual information to constrain the\nsampling process from structural distortion and modulate time step embedding\nfeatures for better alignment with the input at the next time step. Third, to\nrapidly generalize to a new, unseen dose level with as few resources as\npossible, we devise a one-shot learning framework to make CoreDiff generalize\nfaster and better using only a single LDCT image (un)paired with NDCT.\nExtensive experimental results on two datasets demonstrate that our CoreDiff\noutperforms competing methods in denoising and generalization performance, with\na clinically acceptable inference time.\n","authors":["Qi Gao","Zilong Li","Junping Zhang","Yi Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2304.01814v1.pdf","comment":"11 pages, 12 figures"},{"id":"http://arxiv.org/abs/2304.01811v1","updated":"2023-04-04T14:08:42Z","published":"2023-04-04T14:08:42Z","title":"HarsanyiNet: Computing Accurate Shapley Values in a Single Forward\n  Propagation","summary":"  The Shapley value is widely regarded as a trustworthy attribution metric.\nHowever, when people use Shapley values to explain the attribution of input\nvariables of a deep neural network (DNN), it usually requires a very high\ncomputational cost to approximate relatively accurate Shapley values in\nreal-world applications. Therefore, we propose a novel network architecture,\nthe HarsanyiNet, which makes inferences on the input sample and simultaneously\ncomputes the exact Shapley values of the input variables in a single forward\npropagation. The HarsanyiNet is designed on the theoretical foundation that the\nShapley value can be reformulated as the redistribution of Harsanyi\ninteractions encoded by the network.\n","authors":["Lu Chen","Siyu Lou","Keyan Zhang","Jin Huang","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01802v1","updated":"2023-04-04T13:59:07Z","published":"2023-04-04T13:59:07Z","title":"Machine Learning Discovery of Optimal Quadrature Rules for Isogeometric\n  Analysis","summary":"  We propose the use of machine learning techniques to find optimal quadrature\nrules for the construction of stiffness and mass matrices in isogeometric\nanalysis (IGA). We initially consider 1D spline spaces of arbitrary degree\nspanned over uniform and non-uniform knot sequences, and then the generated\noptimal rules are used for integration over higher-dimensional spaces using\ntensor product sense. The quadrature rule search is posed as an optimization\nproblem and solved by a machine learning strategy based on gradient-descent.\nHowever, since the optimization space is highly non-convex, the success of the\nsearch strongly depends on the number of quadrature points and the parameter\ninitialization. Thus, we use a dynamic programming strategy that initializes\nthe parameters from the optimal solution over the spline space with a lower\nnumber of knots. With this method, we found optimal quadrature rules for spline\nspaces when using IGA discretizations with up to 50 uniform elements and\npolynomial degrees up to 8, showing the generality of the approach in this\nscenario. For non-uniform partitions, the method also finds an optimal rule in\na reasonable number of test cases. We also assess the generated optimal rules\nin two practical case studies, namely, the eigenvalue problem of the Laplace\noperator and the eigenfrequency analysis of freeform curved beams, where the\nlatter problem shows the applicability of the method to curved geometries. In\nparticular, the proposed method results in savings with respect to traditional\nGaussian integration of up to 44% in 1D, 68% in 2D, and 82% in 3D spaces.\n","authors":["Tomas Teijeiro","Jamie M. Taylor","Ali Hashemian","David Pardo"],"pdf_url":"https://arxiv.org/pdf/2304.01802v1.pdf","comment":"18 pages, 14 figures"},{"id":"http://arxiv.org/abs/2304.00423v2","updated":"2023-04-04T13:40:33Z","published":"2023-04-02T01:38:05Z","title":"Geometric constraints improve inference of sparsely observed stochastic\n  dynamics","summary":"  The dynamics of systems of many degrees of freedom evolving on multiple\nscales are often modeled in terms of stochastic differential equations. Usually\nthe structural form of these equations is unknown and the only manifestation of\nthe system's dynamics are observations at discrete points in time. Despite\ntheir widespread use, accurately inferring these systems from sparse-in-time\nobservations remains challenging. Conventional inference methods either focus\non the temporal structure of observations, neglecting the geometry of the\nsystem's invariant density, or use geometric approximations of the invariant\ndensity, which are limited to conservative driving forces. To address these\nlimitations, here, we introduce a novel approach that reconciles these two\nperspectives. We propose a path augmentation scheme that employs data-driven\ncontrol to account for the geometry of the invariant system's density.\nNon-parametric inference on the augmented paths, enables efficient\nidentification of the underlying deterministic forces of systems observed at\nlow sampling rates.\n","authors":["Dimitra Maoutsa"],"pdf_url":"https://arxiv.org/pdf/2304.00423v2.pdf","comment":"8+9 pages; 4 figures ; Accepted to ICLR 2023 workshop on Physics for\n  Machine Learning. An earlier account of this work has been previously\n  appeared in arXiv:2301.08102"},{"id":"http://arxiv.org/abs/2212.05867v3","updated":"2023-04-04T13:37:28Z","published":"2022-12-12T13:10:19Z","title":"ALSO: Automotive Lidar Self-supervision by Occupancy estimation","summary":"  We propose a new self-supervised method for pre-training the backbone of deep\nperception models operating on point clouds. The core idea is to train the\nmodel on a pretext task which is the reconstruction of the surface on which the\n3D points are sampled, and to use the underlying latent vectors as input to the\nperception head. The intuition is that if the network is able to reconstruct\nthe scene surface, given only sparse input points, then it probably also\ncaptures some fragments of semantic information, that can be used to boost an\nactual perception task. This principle has a very simple formulation, which\nmakes it both easy to implement and widely applicable to a large range of 3D\nsensors and deep networks performing semantic segmentation or object detection.\nIn fact, it supports a single-stream pipeline, as opposed to most contrastive\nlearning approaches, allowing training on limited resources. We conducted\nextensive experiments on various autonomous driving datasets, involving very\ndifferent kinds of lidars, for both semantic segmentation and object detection.\nThe results show the effectiveness of our method to learn useful\nrepresentations without any annotation, compared to existing approaches. Code\nis available at https://github.com/valeoai/ALSO\n","authors":["Alexandre Boulch","Corentin Sautier","Björn Michele","Gilles Puy","Renaud Marlet"],"pdf_url":"https://arxiv.org/pdf/2212.05867v3.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2212.08123v2","updated":"2023-04-04T13:26:22Z","published":"2022-12-15T20:23:09Z","title":"Bayesian posterior approximation with stochastic ensembles","summary":"  We introduce ensembles of stochastic neural networks to approximate the\nBayesian posterior, combining stochastic methods such as dropout with deep\nensembles. The stochastic ensembles are formulated as families of distributions\nand trained to approximate the Bayesian posterior with variational inference.\nWe implement stochastic ensembles based on Monte Carlo dropout, DropConnect and\na novel non-parametric version of dropout and evaluate them on a toy problem\nand CIFAR image classification. For both tasks, we test the quality of the\nposteriors directly against Hamiltonian Monte Carlo simulations. Our results\nshow that stochastic ensembles provide more accurate posterior estimates than\nother popular baselines for Bayesian inference.\n","authors":["Oleksandr Balabanov","Bernhard Mehlig","Hampus Linander"],"pdf_url":"https://arxiv.org/pdf/2212.08123v2.pdf","comment":"19 pages, CVPR 2023"},{"id":"http://arxiv.org/abs/2209.14233v3","updated":"2023-04-04T13:23:54Z","published":"2022-09-28T17:00:10Z","title":"Obstacle Identification and Ellipsoidal Decomposition for Fast Motion\n  Planning in Unknown Dynamic Environments","summary":"  Collision avoidance in the presence of dynamic obstacles in unknown\nenvironments is one of the most critical challenges for unmanned systems. In\nthis paper, we present a method that identifies obstacles in terms of\nellipsoids to estimate linear and angular obstacle velocities. Our proposed\nmethod is based on the idea of any object can be approximately expressed by\nellipsoids. To achieve this, we propose a method based on variational Bayesian\nestimation of Gaussian mixture model, the Kyachiyan algorithm, and a refinement\nalgorithm. Our proposed method does not require knowledge of the number of\nclusters and can operate in real-time, unlike existing optimization-based\nmethods. In addition, we define an ellipsoid-based feature vector to match\nobstacles given two timely close point frames. Our method can be applied to any\nenvironment with static and dynamic obstacles, including the ones with rotating\nobstacles. We compare our algorithm with other clustering methods and show that\nwhen coupled with a trajectory planner, the overall system can efficiently\ntraverse unknown environments in the presence of dynamic obstacles.\n","authors":["Mehmetcan Kaymaz","Nazim Kemal Ure"],"pdf_url":"https://arxiv.org/pdf/2209.14233v3.pdf","comment":"accepted to IEEE International Conference on Robotics and Automation\n  (ICRA), 2023, London, UK"},{"id":"http://arxiv.org/abs/2304.01781v1","updated":"2023-04-04T13:18:00Z","published":"2023-04-04T13:18:00Z","title":"Mixing predictions for online metric algorithms","summary":"  A major technique in learning-augmented online algorithms is combining\nmultiple algorithms or predictors. Since the performance of each predictor may\nvary over time, it is desirable to use not the single best predictor as a\nbenchmark, but rather a dynamic combination which follows different predictors\nat different times. We design algorithms that combine predictions and are\ncompetitive against such dynamic combinations for a wide class of online\nproblems, namely, metrical task systems. Against the best (in hindsight)\nunconstrained combination of $\\ell$ predictors, we obtain a competitive ratio\nof $O(\\ell^2)$, and show that this is best possible. However, for a benchmark\nwith slightly constrained number of switches between different predictors, we\ncan get a $(1+\\epsilon)$-competitive algorithm. Moreover, our algorithms can be\nadapted to access predictors in a bandit-like fashion, querying only one\npredictor at a time. An unexpected implication of one of our lower bounds is a\nnew structural insight about covering formulations for the $k$-server problem.\n","authors":["Antonios Antoniadis","Christian Coester","Marek Eliáš","Adam Polak","Bertrand Simon"],"pdf_url":"https://arxiv.org/pdf/2304.01781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01774v1","updated":"2023-04-04T13:05:10Z","published":"2023-04-04T13:05:10Z","title":"A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System","summary":"  Human-in-the-loop topic modelling incorporates users' knowledge into the\nmodelling process, enabling them to refine the model iteratively. Recent\nresearch has demonstrated the value of user feedback, but there are still\nissues to consider, such as the difficulty in tracking changes, comparing\ndifferent models and the lack of evaluation based on real-world examples of\nuse. We developed a novel, interactive human-in-the-loop topic modeling system\nwith a user-friendly interface that enables users compare and record every step\nthey take, and a novel topic words suggestion feature to help users provide\nfeedback that is faithful to the ground truth. Our system also supports not\nonly what traditional topic models can do, i.e., learning the topics from the\nwhole corpus, but also targeted topic modelling, i.e., learning topics for\nspecific aspects of the corpus. In this article, we provide an overview of the\nsystem and present the results of a series of user studies designed to assess\nthe value of the system in progressively more realistic applications of topic\nmodelling.\n","authors":["Zheng Fang","Lama Alqazlan","Du Liu","Yulan He","Rob Procter"],"pdf_url":"https://arxiv.org/pdf/2304.01774v1.pdf","comment":"The paper is accepted by the 17th Conference of the European Chapter\n  of the Association for Computational Linguistics (EACL)"},{"id":"http://arxiv.org/abs/2304.01772v1","updated":"2023-04-04T13:04:21Z","published":"2023-04-04T13:04:21Z","title":"A differentiable programming framework for spin models","summary":"  Spin systems are a powerful tool for modeling a wide range of physical\nsystems. In this paper, we propose a novel framework for modeling spin systems\nusing differentiable programming. Our approach enables us to efficiently\nsimulate spin systems, making it possible to model complex systems at scale.\nSpecifically, we demonstrate the effectiveness of our technique by applying it\nto three different spin systems: the Ising model, the Potts model, and the\nCellular Potts model. Our simulations show that our framework offers\nsignificant speedup compared to traditional simulation methods, thanks to its\nability to execute code efficiently across different hardware architectures,\nincluding Graphical Processing Units and Tensor Processing Units.\n","authors":["Tiago de Souza Farias","Vitor Vaz Schultz","José Carlos Merino Mombach","Jonas Maziero"],"pdf_url":"https://arxiv.org/pdf/2304.01772v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2304.01768v1","updated":"2023-04-04T12:58:47Z","published":"2023-04-04T12:58:47Z","title":"Convergence of alternating minimisation algorithms for dictionary\n  learning","summary":"  In this paper we derive sufficient conditions for the convergence of two\npopular alternating minimisation algorithms for dictionary learning - the\nMethod of Optimal Directions (MOD) and Online Dictionary Learning (ODL), which\ncan also be thought of as approximative K-SVD. We show that given a\nwell-behaved initialisation that is either within distance at most $1/\\log(K)$\nto the generating dictionary or has a special structure ensuring that each\nelement of the initialisation only points to one generating element, both\nalgorithms will converge with geometric convergence rate to the generating\ndictionary. This is done even for data models with non-uniform distributions on\nthe supports of the sparse coefficients. These allow the appearance frequency\nof the dictionary elements to vary heavily and thus model real data more\nclosely.\n","authors":["Simon Ruetz","Karin Schnass"],"pdf_url":"https://arxiv.org/pdf/2304.01768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05530v3","updated":"2023-04-04T12:52:44Z","published":"2022-06-11T13:40:37Z","title":"Memorization-Dilation: Modeling Neural Collapse Under Label Noise","summary":"  The notion of neural collapse refers to several emergent phenomena that have\nbeen empirically observed across various canonical classification problems.\nDuring the terminal phase of training a deep neural network, the feature\nembedding of all examples of the same class tend to collapse to a single\nrepresentation, and the features of different classes tend to separate as much\nas possible. Neural collapse is often studied through a simplified model,\ncalled the unconstrained feature representation, in which the model is assumed\nto have \"infinite expressivity\" and can map each data point to any arbitrary\nrepresentation. In this work, we propose a more realistic variant of the\nunconstrained feature representation that takes the limited expressivity of the\nnetwork into account. Empirical evidence suggests that the memorization of\nnoisy data points leads to a degradation (dilation) of the neural collapse.\nUsing a model of the memorization-dilation (M-D) phenomenon, we show one\nmechanism by which different losses lead to different performances of the\ntrained network on noisy data. Our proofs reveal why label smoothing, a\nmodification of cross-entropy empirically observed to produce a regularization\neffect, leads to improved generalization in classification tasks.\n","authors":["Duc Anh Nguyen","Ron Levie","Julian Lienen","Gitta Kutyniok","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2206.05530v3.pdf","comment":"to be published at ICLR 2023"},{"id":"http://arxiv.org/abs/2304.01762v1","updated":"2023-04-04T12:51:35Z","published":"2023-04-04T12:51:35Z","title":"Incorporating Unlabelled Data into Bayesian Neural Networks","summary":"  We develop a contrastive framework for learning better prior distributions\nfor Bayesian Neural Networks (BNNs) using unlabelled data. With this framework,\nwe propose a practical BNN algorithm that offers the label-efficiency of\nself-supervised learning and the principled uncertainty estimates of Bayesian\nmethods. Finally, we demonstrate the advantages of our approach for\ndata-efficient learning in semi-supervised and low-budget active learning\nproblems.\n","authors":["Mrinank Sharma","Tom Rainforth","Yee Whye Teh","Vincent Fortuin"],"pdf_url":"https://arxiv.org/pdf/2304.01762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01752v1","updated":"2023-04-04T12:42:29Z","published":"2023-04-04T12:42:29Z","title":"Black Box Few-Shot Adaptation for Vision-Language models","summary":"  Vision-Language (V-L) models trained with contrastive learning to align the\nvisual and language modalities have been shown to be strong few-shot learners.\nSoft prompt learning is the method of choice for few-shot downstream adaption\naiming to bridge the modality gap caused by the distribution shift induced by\nthe new domain. While parameter-efficient, prompt learning still requires\naccess to the model weights and can be computationally infeasible for large\nmodels with billions of parameters. To address these shortcomings, in this\nwork, we describe a black-box method for V-L few-shot adaptation that (a)\noperates on pre-computed image and text features and hence works without access\nto the model's weights, (b) it is orders of magnitude faster at training time,\n(c) it is amenable to both supervised and unsupervised training, and (d) it can\nbe even used to align image and text features computed from uni-modal models.\nTo achieve this, we propose Linear Feature Alignment (LFA), a simple linear\napproach for V-L re-alignment in the target domain. LFA is initialized from a\nclosed-form solution to a least-squares problem and then it is iteratively\nupdated by minimizing a re-ranking loss. Despite its simplicity, our approach\ncan even surpass soft-prompt learning methods as shown by extensive experiments\non 11 image and 2 video datasets.\n","authors":["Yassine Ouali","Adrian Bulat","Brais Martinez","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2304.01752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00388v2","updated":"2023-04-04T12:42:26Z","published":"2023-04-01T21:11:05Z","title":"Multilevel CNNs for Parametric PDEs","summary":"  We combine concepts from multilevel solvers for partial differential\nequations (PDEs) with neural network based deep learning and propose a new\nmethodology for the efficient numerical solution of high-dimensional parametric\nPDEs. An in-depth theoretical analysis shows that the proposed architecture is\nable to approximate multigrid V-cycles to arbitrary precision with the number\nof weights only depending logarithmically on the resolution of the finest mesh.\nAs a consequence, approximation bounds for the solution of parametric PDEs by\nneural networks that are independent on the (stochastic) parameter dimension\ncan be derived. The performance of the proposed method is illustrated on\nhigh-dimensional parametric linear elliptic PDEs that are common benchmark\nproblems in uncertainty quantification. We find substantial improvements over\nstate-of-the-art deep learning-based solvers. As particularly challenging\nexamples, random conductivity with high-dimensional non-affine Gaussian fields\nin 100 parameter dimensions and a random cookie problem are examined. Due to\nthe multilevel structure of our method, the amount of training samples can be\nreduced on finer levels, hence significantly lowering the generation time for\ntraining data and the training time of our method.\n","authors":["Cosmas Heiß","Ingo Gühring","Martin Eigel"],"pdf_url":"https://arxiv.org/pdf/2304.00388v2.pdf","comment":"42 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2110.04829v3","updated":"2023-04-04T12:22:20Z","published":"2021-10-10T15:51:01Z","title":"Adaptive joint distribution learning","summary":"  We develop a new framework for embedding joint probability distributions in\ntensor product reproducing kernel Hilbert spaces (RKHS). Our framework\naccommodates a low-dimensional, normalized and positive model of a\nRadon-Nikodym derivative, which we estimate from sample sizes of up to several\nmillion data points, alleviating the inherent limitations of RKHS modeling.\nWell-defined normalized and positive conditional distributions are natural\nby-products to our approach. The embedding is fast to compute and accommodates\nlearning problems ranging from prediction to classification. Our theoretical\nfindings are supplemented by favorable numerical results.\n","authors":["Damir Filipovic","Michael Multerer","Paul Schneider"],"pdf_url":"https://arxiv.org/pdf/2110.04829v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01732v1","updated":"2023-04-04T12:05:51Z","published":"2023-04-04T12:05:51Z","title":"Adaptive learning of effective dynamics: Adaptive real-time, online\n  modeling for complex systems","summary":"  Predictive simulations are essential for applications ranging from weather\nforecasting to material design. The veracity of these simulations hinges on\ntheir capacity to capture the effective system dynamics. Massively parallel\nsimulations predict the systems dynamics by resolving all spatiotemporal\nscales, often at a cost that prevents experimentation. On the other hand,\nreduced order models are fast but often limited by the linearization of the\nsystem dynamics and the adopted heuristic closures. We propose a novel\nsystematic framework that bridges large scale simulations and reduced order\nmodels to extract and forecast adaptively the effective dynamics (AdaLED) of\nmultiscale systems. AdaLED employs an autoencoder to identify reduced-order\nrepresentations of the system dynamics and an ensemble of probabilistic\nrecurrent neural networks (RNNs) as the latent time-stepper. The framework\nalternates between the computational solver and the surrogate, accelerating\nlearned dynamics while leaving yet-to-be-learned dynamics regimes to the\noriginal solver. AdaLED continuously adapts the surrogate to the new dynamics\nthrough online training. The transitions between the surrogate and the\ncomputational solver are determined by monitoring the prediction accuracy and\nuncertainty of the surrogate. The effectiveness of AdaLED is demonstrated on\nthree different systems - a Van der Pol oscillator, a 2D reaction-diffusion\nequation, and a 2D Navier-Stokes flow past a cylinder for varying Reynolds\nnumbers (400 up to 1200), showcasing its ability to learn effective dynamics\nonline, detect unseen dynamics regimes, and provide net speed-ups. To the best\nof our knowledge, AdaLED is the first framework that couples a surrogate model\nwith a computational solver to achieve online adaptive learning of effective\ndynamics. It constitutes a potent tool for applications requiring many\nexpensive simulations.\n","authors":["Ivica Kičić","Pantelis R. Vlachas","Georgios Arampatzis","Michail Chatzimanolakis","Leonidas Guibas","Petros Koumoutsakos"],"pdf_url":"https://arxiv.org/pdf/2304.01732v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2304.01731v1","updated":"2023-04-04T12:04:19Z","published":"2023-04-04T12:04:19Z","title":"Selective Knowledge Sharing for Privacy-Preserving Federated\n  Distillation without A Good Teacher","summary":"  While federated learning is promising for privacy-preserving collaborative\nlearning without revealing local data, it remains vulnerable to white-box\nattacks and struggles to adapt to heterogeneous clients. Federated distillation\n(FD), built upon knowledge distillation--an effective technique for\ntransferring knowledge from a teacher model to student models--emerges as an\nalternative paradigm, which provides enhanced privacy guarantees and addresses\nmodel heterogeneity. Nevertheless, challenges arise due to variations in local\ndata distributions and the absence of a well-trained teacher model, which leads\nto misleading and ambiguous knowledge sharing that significantly degrades model\nperformance. To address these issues, this paper proposes a selective knowledge\nsharing mechanism for FD, termed Selective-FD. It includes client-side\nselectors and a server-side selector to accurately and precisely identify\nknowledge from local and ensemble predictions, respectively. Empirical studies,\nbacked by theoretical insights, demonstrate that our approach enhances the\ngeneralization capabilities of the FD framework and consistently outperforms\nbaseline methods. This study presents a promising direction for effective\nknowledge transfer in privacy-preserving collaborative learning.\n","authors":["Jiawei Shao","Fangzhao Wu","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.12380v4","updated":"2023-04-04T11:56:27Z","published":"2022-12-23T14:53:09Z","title":"Towards Scalable Physically Consistent Neural Networks: an Application\n  to Data-driven Multi-zone Thermal Building Models","summary":"  With more and more data being collected, data-driven modeling methods have\nbeen gaining in popularity in recent years. While physically sound, classical\ngray-box models are often cumbersome to identify and scale, and their accuracy\nmight be hindered by their limited expressiveness. On the other hand, classical\nblack-box methods, typically relying on Neural Networks (NNs) nowadays, often\nachieve impressive performance, even at scale, by deriving statistical patterns\nfrom data. However, they remain completely oblivious to the underlying physical\nlaws, which may lead to potentially catastrophic failures if decisions for\nreal-world physical systems are based on them. Physically Consistent Neural\nNetworks (PCNNs) were recently developed to address these aforementioned\nissues, ensuring physical consistency while still leveraging NNs to attain\nstate-of-the-art accuracy.\n  In this work, we scale PCNNs to model building temperature dynamics and\npropose a thorough comparison with classical gray-box and black-box methods.\nMore precisely, we design three distinct PCNN extensions, thereby exemplifying\nthe modularity and flexibility of the architecture, and formally prove their\nphysical consistency. In the presented case study, PCNNs are shown to achieve\nstate-of-the-art accuracy, even outperforming classical NN-based models despite\ntheir constrained structure. Our investigations furthermore provide a clear\nillustration of NNs achieving seemingly good performance while remaining\ncompletely physics-agnostic, which can be misleading in practice. While this\nperformance comes at the cost of computational complexity, PCNNs on the other\nhand show accuracy improvements of 17-35% compared to all other physically\nconsistent methods, paving the way for scalable physically consistent models\nwith state-of-the-art performance.\n","authors":["Loris Di Natale","Bratislav Svetozarevic","Philipp Heer","Colin Neil Jones"],"pdf_url":"https://arxiv.org/pdf/2212.12380v4.pdf","comment":"Accepted in Applied Energy"},{"id":"http://arxiv.org/abs/2006.10859v3","updated":"2023-04-04T11:37:44Z","published":"2020-06-18T21:22:16Z","title":"MARS: Masked Automatic Ranks Selection in Tensor Decompositions","summary":"  Tensor decomposition methods have proven effective in various applications,\nincluding compression and acceleration of neural networks. At the same time,\nthe problem of determining optimal decomposition ranks, which present the\ncrucial parameter controlling the compression-accuracy trade-off, is still\nacute. In this paper, we introduce MARS -- a new efficient method for the\nautomatic selection of ranks in general tensor decompositions. During training,\nthe procedure learns binary masks over decomposition cores that \"select\" the\noptimal tensor structure. The learning is performed via relaxed maximum a\nposteriori (MAP) estimation in a specific Bayesian model and can be naturally\nembedded into the standard neural network training routine. Diverse experiments\ndemonstrate that MARS achieves better results compared to previous works in\nvarious tasks.\n","authors":["Maxim Kodryan","Dmitry Kropotov","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2006.10859v3.pdf","comment":"AISTATS 2023"},{"id":"http://arxiv.org/abs/2211.15136v2","updated":"2023-04-04T11:36:21Z","published":"2022-11-28T08:48:58Z","title":"Collective Intelligence for 2D Push Manipulation with Mobile Robots","summary":"  While natural systems often present collective intelligence that allows them\nto self-organize and adapt to changes, the equivalent is missing in most\nartificial systems. We explore the possibility of such a system in the context\nof cooperative 2D push manipulations using mobile robots. Although conventional\nworks demonstrate potential solutions for the problem in restricted settings,\nthey have computational and learning difficulties. More importantly, these\nsystems do not possess the ability to adapt when facing environmental changes.\nIn this work, we show that by distilling a planner derived from a\ndifferentiable soft-body physics simulator into an attention-based neural\nnetwork, our multi-robot push manipulation system achieves better performance\nthan baselines. In addition, our system also generalizes to configurations not\nseen during training and is able to adapt toward task completions when external\nturbulence and environmental changes are applied. Supplementary videos can be\nfound on our project website:\n  \\url{https://sites.google.com/view/ciom/home}.\n","authors":["So Kuroki","Tatsuya Matsushima","Jumpei Arima","Hiroki Furuta","Yutaka Matsuo","Shixiang Shane Gu","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2211.15136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01717v1","updated":"2023-04-04T11:25:57Z","published":"2023-04-04T11:25:57Z","title":"Characterizing the contribution of dependent features in XAI methods","summary":"  Explainable Artificial Intelligence (XAI) provides tools to help\nunderstanding how the machine learning models work and reach a specific\noutcome. It helps to increase the interpretability of models and makes the\nmodels more trustworthy and transparent. In this context, many XAI methods were\nproposed being SHAP and LIME the most popular. However, the proposed methods\nassume that used predictors in the machine learning models are independent\nwhich in general is not necessarily true. Such assumption casts shadows on the\nrobustness of the XAI outcomes such as the list of informative predictors.\nHere, we propose a simple, yet useful proxy that modifies the outcome of any\nXAI feature ranking method allowing to account for the dependency among the\npredictors. The proposed approach has the advantage of being model-agnostic as\nwell as simple to calculate the impact of each predictor in the model in\npresence of collinearity.\n","authors":["Ahmed Salih","Ilaria Boscolo Galazzo","Zahra Raisi-Estabragh","Steffen E. Petersen","Gloria Menegaz","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2304.01717v1.pdf","comment":"17 pages, 5 tables"},{"id":"http://arxiv.org/abs/2304.01708v1","updated":"2023-04-04T11:11:26Z","published":"2023-04-04T11:11:26Z","title":"Learning and Concentration for High Dimensional Linear Gaussians: an\n  Invariant Subspace Approach","summary":"  In this work, we study non-asymptotic bounds on correlation between two time\nrealizations of stable linear systems with isotropic Gaussian noise.\nConsequently, via sampling from a sub-trajectory and using \\emph{Talagrands'}\ninequality, we show that empirical averages of reward concentrate around steady\nstate (dynamical system mixes to when closed loop system is stable under linear\nfeedback policy ) reward , with high-probability. As opposed to common belief\nof larger the spectral radius stronger the correlation between samples,\n\\emph{large discrepancy between algebraic and geometric multiplicity of system\neigenvalues leads to large invariant subspaces related to system-transition\nmatrix}; once the system enters the large invariant subspace it will travel\naway from origin for a while before coming close to a unit ball centered at\norigin where an isotropic Gaussian noise can with high probability allow it to\nescape the current invariant subspace it resides in, leading to\n\\emph{bottlenecks} between different invariant subspaces that span\n$\\mathbb{R}^{n}$, to be precise : system initiated in a large invariant\nsubspace will be stuck there for a long-time: log-linear in dimension of the\ninvariant subspace and inversely to log of inverse of magnitude of the\neigenvalue. In the problem of Ordinary Least Squares estimate of system\ntransition matrix via a single trajectory, this phenomenon is even more evident\nif spectrum of transition matrix associated to large invariant subspace is\nexplosive and small invariant subspaces correspond to stable eigenvalues. Our\nanalysis provide first interpretable and geometric explanation into intricacies\nof learning and concentration for random dynamical systems on continuous, high\ndimensional state space; exposing us to surprises in high dimensions\n","authors":["Muhammad Abdullah Naeem"],"pdf_url":"https://arxiv.org/pdf/2304.01708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01701v1","updated":"2023-04-04T10:55:32Z","published":"2023-04-04T10:55:32Z","title":"Optimal Transport for Correctional Learning","summary":"  The contribution of this paper is a generalized formulation of correctional\nlearning using optimal transport, which is about how to optimally transport one\nmass distribution to another. Correctional learning is a framework developed to\nenhance the accuracy of parameter estimation processes by means of a\nteacher-student approach. In this framework, an expert agent, referred to as\nthe teacher, modifies the data used by a learning agent, known as the student,\nto improve its estimation process. The objective of the teacher is to alter the\ndata such that the student's estimation error is minimized, subject to a fixed\nintervention budget. Compared to existing formulations of correctional\nlearning, our novel optimal transport approach provides several benefits. It\nallows for the estimation of more complex characteristics as well as the\nconsideration of multiple intervention policies for the teacher. We evaluate\nour approach on two theoretical examples, and on a human-robot interaction\napplication in which the teacher's role is to improve the robots performance in\nan inverse reinforcement learning setting.\n","authors":["Rebecka Winqvist","Inês Lourenco","Francesco Quinzan","Cristian R. Rojas","Bo Wahlberg"],"pdf_url":"https://arxiv.org/pdf/2304.01701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09812v2","updated":"2023-04-04T10:50:48Z","published":"2022-11-16T06:24:26Z","title":"GAMMT: Generative Ambiguity Modeling Using Multiple Transformers","summary":"  We introduce a novel model called GAMMT (Generative Ambiguity Models using\nMultiple Transformers) for sequential data that is based on sets of\nprobabilities. Unlike conventional models, our approach acknowledges that the\ndata generation process of a sequence is not deterministic, but rather\nambiguous and influenced by a set of probabilities. To capture this ambiguity,\nGAMMT employs multiple parallel transformers that are linked by a selection\nmechanism, allowing for the approximation of ambiguous probabilities. The\ngenerative nature of our approach also enables multiple representations of\ninput tokens and sequences. While our models have not yet undergone\nexperimental validation, we believe that our model has great potential to\nachieve high quality and diversity in modeling sequences with uncertain data\ngeneration processes.\n","authors":["Xingcheng Xu"],"pdf_url":"https://arxiv.org/pdf/2211.09812v2.pdf","comment":"14 pages, 1 figure, 3 algorithms"},{"id":"http://arxiv.org/abs/2212.11726v2","updated":"2023-04-04T10:46:54Z","published":"2022-12-22T14:19:35Z","title":"Reusable Options through Gradient-based Meta Learning","summary":"  Hierarchical methods in reinforcement learning have the potential to reduce\nthe amount of decisions that the agent needs to perform when learning new\ntasks. However, finding reusable useful temporal abstractions that facilitate\nfast learning remains a challenging problem. Recently, several deep learning\napproaches were proposed to learn such temporal abstractions in the form of\noptions in an end-to-end manner. In this work, we point out several\nshortcomings of these methods and discuss their potential negative\nconsequences. Subsequently, we formulate the desiderata for reusable options\nand use these to frame the problem of learning options as a gradient-based\nmeta-learning problem. This allows us to formulate an objective that explicitly\nincentivizes options which allow a higher-level decision maker to adjust in few\nsteps to different tasks. Experimentally, we show that our method is able to\nlearn transferable components which accelerate learning and performs better\nthan existing prior methods developed for this setting. Additionally, we\nperform ablations to quantify the impact of using gradient-based meta-learning\nas well as other proposed changes.\n","authors":["David Kuric","Herke van Hoof"],"pdf_url":"https://arxiv.org/pdf/2212.11726v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2207.07338v6","updated":"2023-04-04T10:44:38Z","published":"2022-07-15T08:30:33Z","title":"Context-sensitive neocortical neurons transform the effectiveness and\n  efficiency of neural information processing","summary":"  Deep learning (DL) has big-data processing capabilities that are as good, or\neven better, than those of humans in many real-world domains, but at the cost\nof high energy requirements that may be unsustainable in some applications and\nof errors, that, though infrequent, can be large. We hypothesise that a\nfundamental weakness of DL lies in its intrinsic dependence on\nintegrate-and-fire point neurons that maximise information transmission\nirrespective of whether it is relevant in the current context or not. This\nleads to unnecessary neural firing and to the feedforward transmission of\nconflicting messages, which makes learning difficult and processing energy\ninefficient. Here we show how to circumvent these limitations by mimicking the\ncapabilities of context-sensitive neocortical neurons that receive input from\ndiverse sources as a context to amplify and attenuate the transmission of\nrelevant and irrelevant information, respectively. We demonstrate that a deep\nnetwork composed of such local processors seeks to maximise agreement between\nthe active neurons, thus restricting the transmission of conflicting\ninformation to higher levels and reducing the neural activity required to\nprocess large amounts of heterogeneous real-world data. As shown to be far more\neffective and efficient than current forms of DL, this two-point neuron study\noffers a possible step-change in transforming the cellular foundations of deep\nnetwork architectures.\n","authors":["Ahsan Adeel","Mario Franco","Mohsin Raza","Khubaib Ahmed"],"pdf_url":"https://arxiv.org/pdf/2207.07338v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17595v2","updated":"2023-04-04T10:32:52Z","published":"2023-03-30T17:59:02Z","title":"Neglected Free Lunch; Learning Image Classifiers Using Annotation\n  Byproducts","summary":"  Supervised learning of image classifiers distills human knowledge into a\nparametric model through pairs of images and corresponding labels (X,Y). We\nargue that this simple and widely used representation of human knowledge\nneglects rich auxiliary information from the annotation procedure, such as the\ntime-series of mouse traces and clicks left after image selection. Our insight\nis that such annotation byproducts Z provide approximate human attention that\nweakly guides the model to focus on the foreground cues, reducing spurious\ncorrelations and discouraging shortcut learning. To verify this, we create\nImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with\nsample-wise annotation byproducts, collected by replicating the respective\noriginal annotation tasks. We refer to the new paradigm of training models with\nannotation byproducts as learning using annotation byproducts (LUAB). We show\nthat a simple multitask loss for regressing Z together with Y already improves\nthe generalisability and robustness of the learned models. Compared to the\noriginal supervised learning, LUAB does not require extra annotation costs.\nImageNet-AB and COCO-AB are at https://github.com/naver-ai/NeglectedFreeLunch.\n","authors":["Dongyoon Han","Junsuk Choe","Seonghyeok Chun","John Joon Young Chung","Minsuk Chang","Sangdoo Yun","Jean Y. Song","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2303.17595v2.pdf","comment":"Code at https://github.com/naver-ai/NeglectedFreeLunch"},{"id":"http://arxiv.org/abs/2208.07316v3","updated":"2023-04-04T10:23:20Z","published":"2022-08-15T16:30:14Z","title":"MENLI: Robust Evaluation Metrics from Natural Language Inference","summary":"  Recently proposed BERT-based evaluation metrics for text generation perform\nwell on standard benchmarks but are vulnerable to adversarial attacks, e.g.,\nrelating to information correctness. We argue that this stems (in part) from\nthe fact that they are models of semantic similarity. In contrast, we develop\nevaluation metrics based on Natural Language Inference (NLI), which we deem a\nmore appropriate modeling. We design a preference-based adversarial attack\nframework and show that our NLI based metrics are much more robust to the\nattacks than the recent BERT-based metrics. On standard benchmarks, our NLI\nbased metrics outperform existing summarization metrics, but perform below SOTA\nMT metrics. However, when combining existing metrics with our NLI metrics, we\nobtain both higher adversarial robustness (15%-30%) and higher quality metrics\nas measured on standard benchmarks (+5% to 30%).\n","authors":["Yanran Chen","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2208.07316v3.pdf","comment":"TACL 2023 Camera-ready github link fixed"},{"id":"http://arxiv.org/abs/2303.15383v2","updated":"2023-04-04T10:05:42Z","published":"2023-03-27T16:56:57Z","title":"List Online Classification","summary":"  We study multiclass online prediction where the learner can predict using a\nlist of multiple labels (as opposed to just one label in the traditional\nsetting). We characterize learnability in this model using the $b$-ary\nLittlestone dimension. This dimension is a variation of the classical\nLittlestone dimension with the difference that binary mistake trees are\nreplaced with $(k+1)$-ary mistake trees, where $k$ is the number of labels in\nthe list. In the agnostic setting, we explore different scenarios depending on\nwhether the comparator class consists of single-labeled or multi-labeled\nfunctions and its tradeoff with the size of the lists the algorithm uses. We\nfind that it is possible to achieve negative regret in some cases and provide a\ncomplete characterization of when this is possible. As part of our work, we\nadapt classical algorithms such as Littlestone's SOA and Rosenblatt's\nPerceptron to predict using lists of labels. We also establish combinatorial\nresults for list-learnable classes, including an list online version of the\nSauer-Shelah-Perles Lemma. We state our results within the framework of pattern\nclasses -- a generalization of hypothesis classes which can represent adaptive\nhypotheses (i.e. functions with memory), and model data-dependent assumptions\nsuch as linear classification with margin.\n","authors":["Shay Moran","Ohad Sharon","Iska Tsubari"],"pdf_url":"https://arxiv.org/pdf/2303.15383v2.pdf","comment":"Fixed typos"},{"id":"http://arxiv.org/abs/2304.01670v1","updated":"2023-04-04T09:58:22Z","published":"2023-04-04T09:58:22Z","title":"Denoising Diffusion Probabilistic Models to Predict the Density of\n  Molecular Clouds","summary":"  We introduce the state-of-the-art deep learning Denoising Diffusion\nProbabilistic Model (DDPM) as a method to infer the volume or number density of\ngiant molecular clouds (GMCs) from projected mass surface density maps. We\nadopt magnetohydrodynamic simulations with different global magnetic field\nstrengths and large-scale dynamics, i.e., noncolliding and colliding GMCs. We\ntrain a diffusion model on both mass surface density maps and their\ncorresponding mass-weighted number density maps from different viewing angles\nfor all the simulations. We compare the diffusion model performance with a more\ntraditional empirical two-component and three-component power-law fitting\nmethod and with a more traditional neural network machine learning approach\n(CASI-2D). We conclude that the diffusion model achieves an order of magnitude\nimprovement on the accuracy of predicting number density compared to that by\nother methods. We apply the diffusion method to some example astronomical\ncolumn density maps of Taurus and the Infrared Dark Clouds (IRDCs) G28.37+0.07\nand G35.39-0.33 to produce maps of their mean volume densities.\n","authors":["Duo Xu","Jonathan C. Tan","Chia-Jung Hsu","Ye Zhu"],"pdf_url":"https://arxiv.org/pdf/2304.01670v1.pdf","comment":"ApJ accepted"},{"id":"http://arxiv.org/abs/2304.01669v1","updated":"2023-04-04T09:58:07Z","published":"2023-04-04T09:58:07Z","title":"Re-thinking Model Inversion Attacks Against Deep Neural Networks","summary":"  Model inversion (MI) attacks aim to infer and reconstruct private training\ndata by abusing access to a model. MI attacks have raised concerns about the\nleaking of sensitive information (e.g. private face images used in training a\nface recognition system). Recently, several algorithms for MI have been\nproposed to improve the attack performance. In this work, we revisit MI, study\ntwo fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms,\nand propose solutions to these issues which lead to a significant boost in\nattack performance for all SOTA MI. In particular, our contributions are\ntwo-fold: 1) We analyze the optimization objective of SOTA MI algorithms, argue\nthat the objective is sub-optimal for achieving MI, and propose an improved\noptimization objective that boosts attack performance significantly. 2) We\nanalyze \"MI overfitting\", show that it would prevent reconstructed images from\nlearning semantics of training data, and propose a novel \"model augmentation\"\nidea to overcome this issue. Our proposed solutions are simple and improve all\nSOTA MI attack accuracy significantly. E.g., in the standard CelebA benchmark,\nour solutions improve accuracy by 11.8% and achieve for the first time over 90%\nattack accuracy. Our findings demonstrate that there is a clear risk of leaking\nsensitive information from deep learning models. We urge serious consideration\nto be given to the privacy implications. Our code, demo, and models are\navailable at\nhttps://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/\n","authors":["Ngoc-Bao Nguyen","Keshigeyan Chandrasegaran","Milad Abdollahzadeh","Ngai-Man Cheung"],"pdf_url":"https://arxiv.org/pdf/2304.01669v1.pdf","comment":"Accepted to CVPR 2023. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2304.01663v1","updated":"2023-04-04T09:34:14Z","published":"2023-04-04T09:34:14Z","title":"On the Stability-Plasticity Dilemma of Class-Incremental Learning","summary":"  A primary goal of class-incremental learning is to strike a balance between\nstability and plasticity, where models should be both stable enough to retain\nknowledge learned from previously seen classes, and plastic enough to learn\nconcepts from new classes. While previous works demonstrate strong performance\non class-incremental benchmarks, it is not clear whether their success comes\nfrom the models being stable, plastic, or a mixture of both. This paper aims to\nshed light on how effectively recent class-incremental learning algorithms\naddress the stability-plasticity trade-off. We establish analytical tools that\nmeasure the stability and plasticity of feature representations, and employ\nsuch tools to investigate models trained with various algorithms on large-scale\nclass-incremental benchmarks. Surprisingly, we find that the majority of\nclass-incremental learning algorithms heavily favor stability over plasticity,\nto the extent that the feature extractor of a model trained on the initial set\nof classes is no less effective than that of the final incremental model. Our\nobservations not only inspire two simple algorithms that highlight the\nimportance of feature representation analysis, but also suggest that\nclass-incremental learning approaches, in general, should strive for better\nfeature representation learning.\n","authors":["Dongwan Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2304.01663v1.pdf","comment":"CVPR 2023"},{"id":"http://arxiv.org/abs/2212.04508v2","updated":"2023-04-04T09:28:14Z","published":"2022-12-08T19:00:01Z","title":"Compiler Optimization for Quantum Computing Using Reinforcement Learning","summary":"  Any quantum computing application, once encoded as a quantum circuit, must be\ncompiled before being executable on a quantum computer. Similar to classical\ncompilation, quantum compilation is a sequential process with many compilation\nsteps and numerous possible optimization passes. Despite the similarities, the\ndevelopment of compilers for quantum computing is still in its infancy --\nlacking mutual consolidation on the best sequence of passes, compatibility,\nadaptability, and flexibility. In this work, we take advantage of decades of\nclassical compiler optimization and propose a reinforcement learning framework\nfor developing optimized quantum circuit compilation flows. Through distinct\nconstraints and a unifying interface, the framework supports the combination of\ntechniques from different compilers and optimization tools in a single\ncompilation flow. Experimental evaluations show that the proposed framework --\nset up with a selection of compilation passes from IBM's Qiskit and\nQuantinuum's TKET -- significantly outperforms both individual compilers in 73%\nof cases regarding the expected fidelity. The framework is available on GitHub\n(https://github.com/cda-tum/MQTPredictor) as part of the Munich Quantum Toolkit\n(MQT).\n","authors":["Nils Quetschlich","Lukas Burgholzer","Robert Wille"],"pdf_url":"https://arxiv.org/pdf/2212.04508v2.pdf","comment":"6 pages, 3 figures, minor changes, to be published at Design\n  Automation Conference (DAC), 2023"},{"id":"http://arxiv.org/abs/2302.06223v2","updated":"2023-04-04T09:25:10Z","published":"2023-02-13T09:54:50Z","title":"Variational Mixture of HyperGenerators for Learning Distributions Over\n  Functions","summary":"  Recent approaches build on implicit neural representations (INRs) to propose\ngenerative models over function spaces. However, they are computationally\ncostly when dealing with inference tasks, such as missing data imputation, or\ndirectly cannot tackle them. In this work, we propose a novel deep generative\nmodel, named VAMoH. VAMoH combines the capabilities of modeling continuous\nfunctions using INRs and the inference capabilities of Variational Autoencoders\n(VAEs). In addition, VAMoH relies on a normalizing flow to define the prior,\nand a mixture of hypernetworks to parametrize the data log-likelihood. This\ngives VAMoH a high expressive capability and interpretability. Through\nexperiments on a diverse range of data types, such as images, voxels, and\nclimate data, we show that VAMoH can effectively learn rich distributions over\ncontinuous functions. Furthermore, it can perform inference-related tasks, such\nas conditional super-resolution generation and in-painting, as well or better\nthan previous approaches, while being less computationally demanding.\n","authors":["Batuhan Koyuncu","Pablo Sanchez-Martin","Ignacio Peis","Pablo M. Olmos","Isabel Valera"],"pdf_url":"https://arxiv.org/pdf/2302.06223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00553v2","updated":"2023-04-04T09:04:27Z","published":"2023-04-02T15:04:43Z","title":"From Isolated Islands to Pangea: Unifying Semantic Space for Human\n  Action Understanding","summary":"  Action understanding matters and attracts attention. It can be formed as the\nmapping from the action physical space to the semantic space. Typically,\nresearchers built action datasets according to idiosyncratic choices to define\nclasses and push the envelope of benchmarks respectively. Thus, datasets are\nincompatible with each other like \"Isolated Islands\" due to semantic gaps and\nvarious class granularities, e.g., do housework in dataset A and wash plate in\ndataset B. We argue that a more principled semantic space is an urgent need to\nconcentrate the community efforts and enable us to use all datasets together to\npursue generalizable action learning. To this end, we design a Poincare action\nsemantic space given verb taxonomy hierarchy and covering massive actions. By\naligning the classes of previous datasets to our semantic space, we gather\n(image/video/skeleton/MoCap) datasets into a unified database in a unified\nlabel system, i.e., bridging \"isolated islands\" into a \"Pangea\". Accordingly,\nwe propose a bidirectional mapping model between physical and semantic space to\nfully use Pangea. In extensive experiments, our system shows significant\nsuperiority, especially in transfer learning. Code and data will be made\npublicly available.\n","authors":["Yong-Lu Li","Xiaoqian Wu","Xinpeng Liu","Yiming Dou","Yikun Ji","Junyi Zhang","Yixing Li","Jingru Tan","Xudong Lu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2304.00553v2.pdf","comment":"Project Webpage: https://mvig-rhos.com/pangea"},{"id":"http://arxiv.org/abs/2302.00873v2","updated":"2023-04-04T08:50:05Z","published":"2023-02-02T04:46:21Z","title":"Predicting the Silent Majority on Graphs: Knowledge Transferable Graph\n  Neural Network","summary":"  Graphs consisting of vocal nodes (\"the vocal minority\") and silent nodes\n(\"the silent majority\"), namely VS-Graph, are ubiquitous in the real world. The\nvocal nodes tend to have abundant features and labels. In contrast, silent\nnodes only have incomplete features and rare labels, e.g., the description and\npolitical tendency of politicians (vocal) are abundant while not for ordinary\npeople (silent) on the twitter's social network. Predicting the silent majority\nremains a crucial yet challenging problem. However, most existing\nmessage-passing based GNNs assume that all nodes belong to the same domain,\nwithout considering the missing features and distribution-shift between\ndomains, leading to poor ability to deal with VS-Graph. To combat the above\nchallenges, we propose Knowledge Transferable Graph Neural Network (KT-GNN),\nwhich models distribution shifts during message passing and representation\nlearning by transferring knowledge from vocal nodes to silent nodes.\nSpecifically, we design the domain-adapted \"feature completion and message\npassing mechanism\" for node representation learning while preserving domain\ndifference. And a knowledge transferable classifier based on KL-divergence is\nfollowed. Comprehensive experiments on real-world scenarios (i.e., company\nfinancial risk assessment and political elections) demonstrate the superior\nperformance of our method. Our source code has been open sourced.\n","authors":["Wendong Bi","Bingbing Xu","Xiaoqian Sun","Li Xu","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2302.00873v2.pdf","comment":"Paper was accepted by WWW2023"},{"id":"http://arxiv.org/abs/2304.01634v1","updated":"2023-04-04T08:46:00Z","published":"2023-04-04T08:46:00Z","title":"De-novo Identification of Small Molecules from Their GC-EI-MS Spectra","summary":"  Identification of experimentally acquired mass spectra of unknown compounds\npresents a~particular challenge because reliable spectral databases do not\ncover the potential chemical space with sufficient density. Therefore machine\nlearning based \\emph{de-novo} methods, which derive molecular structure\ndirectly from its mass spectrum gained attention recently. We present a~novel\nmethod in this family, addressing a~specific usecase of GC-EI-MS spectra, which\nis particularly hard due to lack of additional information from the first stage\nof MS/MS experiments, on which the previously published methods rely. We\nanalyze strengths and drawbacks or our approach and discuss future directions.\n","authors":["Adam Hájek","Michal Starý","Filip Jozefov","Helge Hecht","Elliott Price","Aleš Křenek"],"pdf_url":"https://arxiv.org/pdf/2304.01634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04260v2","updated":"2023-04-04T08:41:47Z","published":"2022-10-09T13:24:54Z","title":"Coresets for Wasserstein Distributionally Robust Optimization Problems","summary":"  Wasserstein distributionally robust optimization (\\textsf{WDRO}) is a popular\nmodel to enhance the robustness of machine learning with ambiguous data.\nHowever, the complexity of \\textsf{WDRO} can be prohibitive in practice since\nsolving its ``minimax'' formulation requires a great amount of computation.\nRecently, several fast \\textsf{WDRO} training algorithms for some specific\nmachine learning tasks (e.g., logistic regression) have been developed.\nHowever, the research on designing efficient algorithms for general large-scale\n\\textsf{WDRO}s is still quite limited, to the best of our knowledge.\n\\textit{Coreset} is an important tool for compressing large dataset, and thus\nit has been widely applied to reduce the computational complexities for many\noptimization problems. In this paper, we introduce a unified framework to\nconstruct the $\\epsilon$-coreset for the general \\textsf{WDRO} problems. Though\nit is challenging to obtain a conventional coreset for \\textsf{WDRO} due to the\nuncertainty issue of ambiguous data, we show that we can compute a ``dual\ncoreset'' by using the strong duality property of \\textsf{WDRO}. Also, the\nerror introduced by the dual coreset can be theoretically guaranteed for the\noriginal \\textsf{WDRO} objective. To construct the dual coreset, we propose a\nnovel grid sampling approach that is particularly suitable for the dual\nformulation of \\textsf{WDRO}. Finally, we implement our coreset approach and\nillustrate its effectiveness for several \\textsf{WDRO} problems in the\nexperiments.\n","authors":["Ruomin Huang","Jiawei Huang","Wenjie Liu","Hu Ding"],"pdf_url":"https://arxiv.org/pdf/2210.04260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01628v1","updated":"2023-04-04T08:33:13Z","published":"2023-04-04T08:33:13Z","title":"Equivariant Networks for Porous Crystalline Materials","summary":"  Efficiently predicting properties of porous crystalline materials has great\npotential to accelerate the high throughput screening process for developing\nnew materials, as simulations carried out using first principles model are\noften computationally expensive. To effectively make use of Deep Learning\nmethods to model these materials, we need to utilize the symmetries present in\nthe crystals, which are defined by their space group. Existing methods for\ncrystal property prediction either have symmetry constraints that are too\nrestrictive or only incorporate symmetries between unit cells. In addition,\nthese models do not explicitly model the porous structure of the crystal. In\nthis paper, we develop a model which incorporates the symmetries of the unit\ncell of a crystal in its architecture and explicitly models the porous\nstructure. We evaluate our model by predicting the heat of adsorption of CO$_2$\nfor different configurations of the mordenite zeolite. Our results confirm that\nour method performs better than existing methods for crystal property\nprediction and that the inclusion of pores results in a more efficient model.\n","authors":["Marko Petković","Pablo Romero-Marimon","Vlado Menkovski","Sofia Calero"],"pdf_url":"https://arxiv.org/pdf/2304.01628v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.17229v2","updated":"2023-04-04T08:33:05Z","published":"2023-03-30T08:56:28Z","title":"The Graphical Nadaraya-Watson Estimator on Latent Position Models","summary":"  Given a graph with a subset of labeled nodes, we are interested in the\nquality of the averaging estimator which for an unlabeled node predicts the\naverage of the observations of its labeled neighbors. We rigorously study\nconcentration properties, variance bounds and risk bounds in this context.\nWhile the estimator itself is very simple we believe that our results will\ncontribute towards the theoretical understanding of learning on graphs through\nmore sophisticated methods such as Graph Neural Networks.\n","authors":["M. Gjorgjevski"],"pdf_url":"https://arxiv.org/pdf/2303.17229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00252v2","updated":"2023-04-04T08:17:55Z","published":"2023-04-01T08:00:32Z","title":"Recover Triggered States: Protect Model Against Backdoor Attack in\n  Reinforcement Learning","summary":"  A backdoor attack allows a malicious user to manipulate the environment or\ncorrupt the training data, thus inserting a backdoor into the trained agent.\nSuch attacks compromise the RL system's reliability, leading to potentially\ncatastrophic results in various key fields. In contrast, relatively limited\nresearch has investigated effective defenses against backdoor attacks in RL.\nThis paper proposes the Recovery Triggered States (RTS) method, a novel\napproach that effectively protects the victim agents from backdoor attacks. RTS\ninvolves building a surrogate network to approximate the dynamics model.\nDevelopers can then recover the environment from the triggered state to a clean\nstate, thereby preventing attackers from activating backdoors hidden in the\nagent by presenting the trigger. When training the surrogate to predict states,\nwe incorporate agent action information to reduce the discrepancy between the\nactions taken by the agent on predicted states and the actions taken on real\nstates. RTS is the first approach to defend against backdoor attacks in a\nsingle-agent setting. Our results show that using RTS, the cumulative reward\nonly decreased by 1.41% under the backdoor attack.\n","authors":["Hao Chen","Chen Gong","Yizhe Wang","Xinwen Hou"],"pdf_url":"https://arxiv.org/pdf/2304.00252v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14318v2","updated":"2023-04-04T07:56:03Z","published":"2022-08-30T14:58:44Z","title":"Convergence Rates of Training Deep Neural Networks via Alternating\n  Minimization Methods","summary":"  Training deep neural networks (DNNs) is an important and challenging\noptimization problem in machine learning due to its non-convexity and\nnon-separable structure. The alternating minimization (AM) approaches split the\ncomposition structure of DNNs and have drawn great interest in the deep\nlearning and optimization communities. In this paper, we propose a unified\nframework for analyzing the convergence rate of AM-type network training\nmethods. Our analysis is based on the non-monotone $j$-step sufficient decrease\nconditions and the Kurdyka-Lojasiewicz (KL) property, which relaxes the\nrequirement of designing descent algorithms. We show the detailed local\nconvergence rate if the KL exponent $\\theta$ varies in $[0,1)$. Moreover, the\nlocal R-linear convergence is discussed under a stronger $j$-step sufficient\ndecrease condition.\n","authors":["Jintao Xu","Chenglong Bao","Wenxun Xing"],"pdf_url":"https://arxiv.org/pdf/2208.14318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09713v2","updated":"2023-04-04T07:52:40Z","published":"2022-12-19T18:42:19Z","title":"A Probabilistic Framework for Lifelong Test-Time Adaptation","summary":"  Test-time adaptation (TTA) is the problem of updating a pre-trained source\nmodel at inference time given test input(s) from a different target domain.\nMost existing TTA approaches assume the setting in which the target domain is\nstationary, i.e., all the test inputs come from a single target domain.\nHowever, in many practical settings, the test input distribution might exhibit\na lifelong/continual shift over time. Moreover, existing TTA approaches also\nlack the ability to provide reliable uncertainty estimates, which is crucial\nwhen distribution shifts occur between the source and target domain. To address\nthese issues, we present PETAL (Probabilistic lifElong Test-time Adaptation\nwith seLf-training prior), which solves lifelong TTA using a probabilistic\napproach, and naturally results in (1) a student-teacher framework, where the\nteacher model is an exponential moving average of the student model, and (2)\nregularizing the model updates at inference time using the source model as a\nregularizer. To prevent model drift in the lifelong/continual TTA setting, we\nalso propose a data-driven parameter restoration technique which contributes to\nreducing the error accumulation and maintaining the knowledge of recent domains\nby restoring only the irrelevant parameters. In terms of predictive error rate\nas well as uncertainty based metrics such as Brier score and negative\nlog-likelihood, our method achieves better results than the current\nstate-of-the-art for online lifelong test-time adaptation across various\nbenchmarks, such as CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC\ndatasets. The source code for our approach is accessible at\nhttps://github.com/dhanajitb/petal.\n","authors":["Dhanajit Brahma","Piyush Rai"],"pdf_url":"https://arxiv.org/pdf/2212.09713v2.pdf","comment":"Accepted in CVPR 2023"},{"id":"http://arxiv.org/abs/2112.07837v4","updated":"2023-04-04T07:49:03Z","published":"2021-12-15T02:17:03Z","title":"Central-Smoothing Hypergraph Neural Networks for Predicting Drug-Drug\n  Interactions","summary":"  Predicting drug-drug interactions (DDI) is the problem of predicting side\neffects (unwanted outcomes) of a pair of drugs using drug information and known\nside effects of many pairs. This problem can be formulated as predicting labels\n(i.e. side effects) for each pair of nodes in a DDI graph, of which nodes are\ndrugs and edges are interacting drugs with known labels. State-of-the-art\nmethods for this problem are graph neural networks (GNNs), which leverage\nneighborhood information in the graph to learn node representations. For DDI,\nhowever, there are many labels with complicated relationships due to the nature\nof side effects. Usual GNNs often fix labels as one-hot vectors that do not\nreflect label relationships and potentially do not obtain the highest\nperformance in the difficult cases of infrequent labels. In this paper, we\nformulate DDI as a hypergraph where each hyperedge is a triple: two nodes for\ndrugs and one node for a label. We then present CentSmoothie, a hypergraph\nneural network that learns representations of nodes and labels altogether with\na novel central-smoothing formulation. We empirically demonstrate the\nperformance advantages of CentSmoothie in simulations as well as real datasets.\n","authors":["Duc Anh Nguyen","Canh Hao Nguyen","Hiroshi Mamitsuka"],"pdf_url":"https://arxiv.org/pdf/2112.07837v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04341v2","updated":"2023-04-04T07:28:30Z","published":"2023-01-11T07:31:47Z","title":"A Meta Path-based Approach for Rumor Detection on Social Media","summary":"  The prominent role of social media in people's daily lives has made them more\ninclined to receive news through social networks than traditional sources. This\nshift in public behavior has opened doors for some to diffuse fake news on\nsocial media; and subsequently cause negative economic, political, and social\nconsequences as well as distrust among the public.\n  There are many proposed methods to solve the rumor detection problem, most of\nwhich do not take full advantage of the heterogeneous nature of news\npropagation networks. With this intention, we considered a previously proposed\narchitecture as our baseline and performed the idea of structural feature\nextraction from the heterogeneous rumor propagation over its architecture using\nthe concept of meta path-based embeddings. We named our model Meta Path-based\nGlobal Local Attention Network (MGLAN). Extensive experimental analysis on\nthree state-of-the-art datasets has demonstrated that MGLAN outperforms other\nmodels by capturing node-level discrimination to different node types.\n","authors":["Bita Azarijoo","Mostafa Salehi","Shaghayegh Najari"],"pdf_url":"https://arxiv.org/pdf/2301.04341v2.pdf","comment":"This paper has been accepted to 2023 28th International Computer\n  Conference, Computer Society of Iran (CSICC)"},{"id":"http://arxiv.org/abs/2203.12459v3","updated":"2023-04-04T07:24:58Z","published":"2022-03-23T14:54:29Z","title":"Importance Sampling CAMs for Weakly-Supervised Segmentation","summary":"  Classification networks can be used to localize and segment objects in images\nby means of class activation maps (CAMs). However, without pixel-level\nannotations, classification networks are known to (1) mainly focus on\ndiscriminative regions, and (2) to produce diffuse CAMs without well-defined\nprediction contours. In this work, we approach both problems with two\ncontributions for improving CAM learning. First, we incorporate importance\nsampling based on the class-wise probability mass function induced by the CAMs\nto produce stochastic image-level class predictions. This results in CAMs which\nactivate over a larger extent of objects. Second, we formulate a feature\nsimilarity loss term which aims to match the prediction contours with edges in\nthe image. As a third contribution, we conduct experiments on the PASCAL VOC\n2012 benchmark dataset to demonstrate that these modifications significantly\nincrease the performance in terms of contour accuracy, while being comparable\nto current state-of-the-art methods in terms of region similarity.\n","authors":["Arvi Jonnarth","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2203.12459v3.pdf","comment":"Updated to the version published at ICASSP2022"},{"id":"http://arxiv.org/abs/2304.01576v1","updated":"2023-04-04T07:05:15Z","published":"2023-04-04T07:05:15Z","title":"MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network\n  with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan","summary":"  Accurate lung nodule segmentation is crucial for early-stage lung cancer\ndiagnosis, as it can substantially enhance patient survival rates. Computed\ntomography (CT) images are widely employed for early diagnosis in lung nodule\nanalysis. However, the heterogeneity of lung nodules, size diversity, and the\ncomplexity of the surrounding environment pose challenges for developing robust\nnodule segmentation methods. In this study, we propose an efficient end-to-end\nframework, the multi-encoder-based self-adaptive hard attention network\n(MESAHA-Net), for precise lung nodule segmentation in CT scans. MESAHA-Net\ncomprises three encoding paths, an attention block, and a decoder block,\nfacilitating the integration of three types of inputs: CT slice patches,\nforward and backward maximum intensity projection (MIP) images, and region of\ninterest (ROI) masks encompassing the nodule. By employing a novel adaptive\nhard attention mechanism, MESAHA-Net iteratively performs slice-by-slice 2D\nsegmentation of lung nodules, focusing on the nodule region in each slice to\ngenerate 3D volumetric segmentation of lung nodules. The proposed framework has\nbeen comprehensively evaluated on the LIDC-IDRI dataset, the largest publicly\navailable dataset for lung nodule segmentation. The results demonstrate that\nour approach is highly robust for various lung nodule types, outperforming\nprevious state-of-the-art techniques in terms of segmentation accuracy and\ncomputational complexity, rendering it suitable for real-time clinical\nimplementation.\n","authors":["Muhammad Usman","Azka Rehman","Abdullah Shahid","Siddique Latif","Shi Sub Byon","Sung Hyun Kim","Tariq Mahmood Khan","Yeong Gil Shin"],"pdf_url":"https://arxiv.org/pdf/2304.01576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01575v1","updated":"2023-04-04T07:03:08Z","published":"2023-04-04T07:03:08Z","title":"The expressive power of pooling in Graph Neural Networks","summary":"  In Graph Neural Networks (GNNs), hierarchical pooling operators generate a\ncoarser representation of the input data by creating local summaries of the\ngraph structure and its vertex features. Considerable attention has been\ndevoted to studying the expressive power of message-passing (MP) layers in\nGNNs, while a study on how pooling operators affect the expressivity of a GNN\nis still lacking. Additionally, despite the recent advances in the design of\neffective pooling operators, there is not a principled criterion to compare\nthem. Our work aims to fill this gap by providing sufficient conditions for a\npooling operator to fully preserve the expressive power of the MP layers before\nit. These conditions serve as a universal and theoretically-grounded criterion\nfor choosing among existing pooling operators or designing new ones. Based on\nour theoretical findings, we reviewed several existing pooling operators and\nidentified those that fail to satisfy the expressiveness assumptions. Finally,\nwe introduced an experimental setup to empirically measure the expressive power\nof a GNN equipped with pooling layers, in terms of its capability to perform a\ngraph isomorphism test.\n","authors":["Filippo Maria Bianchi","Veronica Lachi"],"pdf_url":"https://arxiv.org/pdf/2304.01575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11543v2","updated":"2023-04-04T06:56:16Z","published":"2022-08-24T13:41:37Z","title":"A methodology for identifying resiliency in renewable electrical\n  distribution system using complex network","summary":"  Recently, Electrical Distribution Systems are extensively penetrated with the\nDistributed Energy Resources (DERs) to cater the energy demands with general\nperception that it enhances the system resiliency. However, it may be adverse\nfor the grid operation due to various factors like its intermittent\navailability, dynamics in weather condition, introduction of nonlinearity,\ncomplexity etc. This needs a detailed understanding of system resiliency that\nour method proposes here. We introduce a methodology using complex network\ntheory to identify the resiliency of distribution system when incorporated with\nSolar PV generation under various undesirable configurations. Complex\ncorrelated networks for different conditions were obtained and various network\nparameters were computed for identifying the resiliency of those networks. The\nproposed methodology identifies the hosting capacity of solar panels in the\nsystem while maintaining the resiliency under different unwanted conditions\nhence helps to obtain an optimal allocation topology for solar panels in the\nsystem. The proposed method also identifies the critical nodes that are highly\nsensitive to the changes and could drive the system into non-resiliency. This\nframework was demonstrated on IEEE-123 Test Feeder system with time-series data\ngenerated using GridLAB-D and variety of analysis were performed using complex\nnetwork and machine learning models.\n","authors":["Divyanshi Dwivedi","Pradeep Kumar Yemula","Mayukha Pal"],"pdf_url":"https://arxiv.org/pdf/2208.11543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03157v2","updated":"2023-04-04T06:49:50Z","published":"2023-03-06T14:21:42Z","title":"Data-Driven Control with Inherent Lyapunov Stability","summary":"  Recent advances in learning-based control leverage deep function\napproximators, such as neural networks, to model the evolution of controlled\ndynamical systems over time. However, the problem of learning a dynamics model\nand a stabilizing controller persists, since the synthesis of a stabilizing\nfeedback law for known nonlinear systems is a difficult task, let alone for\ncomplex parametric representations that must be fit to data. To this end, we\npropose Control with Inherent Lyapunov Stability (CoILS), a method for jointly\nlearning parametric representations of a nonlinear dynamics model and a\nstabilizing controller from data. To do this, our approach simultaneously\nlearns a parametric Lyapunov function which intrinsically constrains the\ndynamics model to be stabilizable by the learned controller. In addition to the\nstabilizability of the learned dynamics guaranteed by our novel construction,\nwe show that the learned controller stabilizes the true dynamics under certain\nassumptions on the fidelity of the learned dynamics. Finally, we demonstrate\nthe efficacy of CoILS on a variety of simulated nonlinear dynamical systems.\n","authors":["Youngjae Min","Spencer M. Richards","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2303.03157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01569v1","updated":"2023-04-04T06:48:07Z","published":"2023-04-04T06:48:07Z","title":"Spatiotemporal and Semantic Zero-inflated Urban Anomaly Prediction","summary":"  Urban anomaly predictions, such as traffic accident prediction and crime\nprediction, are of vital importance to smart city security and maintenance.\nExisting methods typically use deep learning to capture the intra-dependencies\nin spatial and temporal dimensions. However, numerous key challenges remain\nunsolved, for instance, sparse zero-inflated data due to urban anomalies\noccurring with low frequency (which can lead to poor performance on real-world\ndatasets), and both intra- and inter-dependencies of abnormal patterns across\nspatial, temporal, and semantic dimensions. Moreover, a unified approach to\npredict multiple kinds of anomaly is left to explore. In this paper, we propose\nSTS to jointly capture the intra- and inter-dependencies between the patterns\nand the influential factors in three dimensions. Further, we use a multi-task\nprediction module with a customized loss function to solve the zero-inflated\nissue. To verify the effectiveness of the model, we apply it to two urban\nanomaly prediction tasks, crime prediction and traffic accident risk\nprediction, respectively. Experiments on two application scenarios with four\nreal-world datasets demonstrate the superiority of STS, which outperforms\nstate-of-the-art methods in the mean absolute error and the root mean square\nerror by 37.88% and 18.10% on zero-inflated datasets, and, 60.32% and 37.28% on\nnon-zero datasets, respectively.\n","authors":["Yao Lu","Pengyuan Zhou","Yong Liao","Haiyong Xie"],"pdf_url":"https://arxiv.org/pdf/2304.01569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01568v1","updated":"2023-04-04T06:47:54Z","published":"2023-04-04T06:47:54Z","title":"Arrhythmia Classifier Based on Ultra-Lightweight Binary Neural Network","summary":"  Reasonably and effectively monitoring arrhythmias through ECG signals has\nsignificant implications for human health. With the development of deep\nlearning, numerous ECG classification algorithms based on deep learning have\nemerged. However, most existing algorithms trade off high accuracy for complex\nmodels, resulting in high storage usage and power consumption. This also\ninevitably increases the difficulty of implementation on wearable Artificial\nIntelligence-of-Things (AIoT) devices with limited resources. In this study, we\nproposed a universally applicable ultra-lightweight binary neural network(BNN)\nthat is capable of 5-class and 17-class arrhythmia classification based on ECG\nsignals. Our BNN achieves 96.90% (full precision 97.09%) and 97.50% (full\nprecision 98.00%) accuracy for 5-class and 17-class classification,\nrespectively, with state-of-the-art storage usage (3.76 KB and 4.45 KB).\nCompared to other binarization works, our approach excels in supporting two\nmulti-classification modes while achieving the smallest known storage space.\nMoreover, our model achieves optimal accuracy in 17-class classification and\nboasts an elegantly simple network architecture. The algorithm we use is\noptimized specifically for hardware implementation. Our research showcases the\npotential of lightweight deep learning models in the healthcare industry,\nspecifically in wearable medical devices, which hold great promise for\nimproving patient outcomes and quality of life. Code is available on:\nhttps://github.com/xpww/ECG_BNN_Net\n","authors":["Ninghao Pu","Zhongxing Wu","Ao Wang","Hanshi Sun","Zijin Liu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2304.01568v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2103.15949v2","updated":"2023-04-04T06:43:19Z","published":"2021-03-29T20:51:33Z","title":"Transformer visualization via dictionary learning: contextualized\n  embedding as a linear superposition of transformer factors","summary":"  Transformer networks have revolutionized NLP representation learning since\nthey were introduced. Though a great effort has been made to explain the\nrepresentation in transformers, it is widely recognized that our understanding\nis not sufficient. One important reason is that there lack enough visualization\ntools for detailed analysis. In this paper, we propose to use dictionary\nlearning to open up these \"black boxes\" as linear superpositions of transformer\nfactors. Through visualization, we demonstrate the hierarchical semantic\nstructures captured by the transformer factors, e.g., word-level polysemy\ndisambiguation, sentence-level pattern formation, and long-range dependency.\nWhile some of these patterns confirm the conventional prior linguistic\nknowledge, the rest are relatively unexpected, which may provide new insights.\nWe hope this visualization tool can bring further knowledge and a better\nunderstanding of how transformer networks work. The code is available at\nhttps://github.com/zeyuyun1/TransformerVis\n","authors":["Zeyu Yun","Yubei Chen","Bruno A Olshausen","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2103.15949v2.pdf","comment":"This paper is published at DeeLIO Workshop@NAACL 2021"},{"id":"http://arxiv.org/abs/2304.01565v1","updated":"2023-04-04T06:41:15Z","published":"2023-04-04T06:41:15Z","title":"A Survey on Graph Diffusion Models: Generative AI in Science for\n  Molecule, Protein and Material","summary":"  Diffusion models have become a new SOTA generative modeling method in various\nfields, for which there are multiple survey works that provide an overall\nsurvey. With the number of articles on diffusion models increasing\nexponentially in the past few years, there is an increasing need for surveys of\ndiffusion models on specific fields. In this work, we are committed to\nconducting a survey on the graph diffusion models. Even though our focus is to\ncover the progress of diffusion models in graphs, we first briefly summarize\nhow other generative modeling methods are used for graphs. After that, we\nintroduce the mechanism of diffusion models in various forms, which facilitates\nthe discussion on the graph diffusion models. The applications of graph\ndiffusion models mainly fall into the category of AI-generated content (AIGC)\nin science, for which we mainly focus on how graph diffusion models are\nutilized for generating molecules and proteins but also cover other cases,\nincluding materials design. Moreover, we discuss the issue of evaluating\ndiffusion models in the graph domain and the existing challenges.\n","authors":["Mengchun Zhang","Maryam Qamar","Taegoo Kang","Yuna Jung","Chenshuang Zhang","Sung-Ho Bae","Chaoning Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01561v1","updated":"2023-04-04T06:35:02Z","published":"2023-04-04T06:35:02Z","title":"Optimal rates of approximation by shallow ReLU$^k$ neural networks and\n  applications to nonparametric regression","summary":"  We study the approximation capacity of some variation spaces corresponding to\nshallow ReLU$^k$ neural networks. It is shown that sufficiently smooth\nfunctions are contained in these spaces with finite variation norms. For\nfunctions with less smoothness, the approximation rates in terms of the\nvariation norm are established. Using these results, we are able to prove the\noptimal approximation rates in terms of the number of neurons for shallow\nReLU$^k$ neural networks. It is also shown how these results can be used to\nderive approximation bounds for deep neural networks and convolutional neural\nnetworks (CNNs). As applications, we study convergence rates for nonparametric\nregression using three ReLU neural network models: shallow neural network,\nover-parameterized neural network, and CNN. In particular, we show that shallow\nneural networks can achieve the minimax optimal rates for learning H\\\"older\nfunctions, which complements recent results for deep neural networks. It is\nalso proven that over-parameterized (deep or shallow) neural networks can\nachieve nearly optimal rates for nonparametric regression.\n","authors":["Yunfei Yang","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.01561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07941v3","updated":"2023-04-04T06:29:36Z","published":"2022-03-15T14:25:44Z","title":"Reachability In Simple Neural Networks","summary":"  We investigate the complexity of the reachability problem for (deep) neural\nnetworks: does it compute valid output given some valid input? It was recently\nclaimed that the problem is NP-complete for general neural networks and\nspecifications over the input/output dimension given by conjunctions of linear\ninequalities. We recapitulate the proof and repair some flaws in the original\nupper and lower bound proofs. Motivated by the general result, we show that\nNP-hardness already holds for restricted classes of simple specifications and\nneural networks. Allowing for a single hidden layer and an output dimension of\none as well as neural networks with just one negative, zero and one positive\nweight or bias is sufficient to ensure NP-hardness. Additionally, we give a\nthorough discussion and outlook of possible extensions for this direction of\nresearch on neural network verification.\n","authors":["Marco Sälzer","Martin Lange"],"pdf_url":"https://arxiv.org/pdf/2203.07941v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2108.13179"},{"id":"http://arxiv.org/abs/2301.06646v3","updated":"2023-04-04T06:26:18Z","published":"2023-01-17T00:43:34Z","title":"Async-HFL: Efficient and Robust Asynchronous Federated Learning in\n  Hierarchical IoT Networks","summary":"  Federated Learning (FL) has gained increasing interest in recent years as a\ndistributed on-device learning paradigm. However, multiple challenges remain to\nbe addressed for deploying FL in real-world Internet-of-Things (IoT) networks\nwith hierarchies. Although existing works have proposed various approaches to\naccount data heterogeneity, system heterogeneity, unexpected stragglers and\nscalibility, none of them provides a systematic solution to address all of the\nchallenges in a hierarchical and unreliable IoT network. In this paper, we\npropose an asynchronous and hierarchical framework (Async-HFL) for performing\nFL in a common three-tier IoT network architecture. In response to the largely\nvaried delays, Async-HFL employs asynchronous aggregations at both the gateway\nand the cloud levels thus avoids long waiting time. To fully unleash the\npotential of Async-HFL in converging speed under system heterogeneities and\nstragglers, we design device selection at the gateway level and device-gateway\nassociation at the cloud level. Device selection chooses edge devices to\ntrigger local training in real-time while device-gateway association determines\nthe network topology periodically after several cloud epochs, both satisfying\nbandwidth limitation. We evaluate Async-HFL's convergence speedup using\nlarge-scale simulations based on ns-3 and a network topology from NYCMesh. Our\nresults show that Async-HFL converges 1.08-1.31x faster in wall-clock time and\nsaves up to 21.6% total communication cost compared to state-of-the-art\nasynchronous FL algorithms (with client selection). We further validate\nAsync-HFL on a physical deployment and observe robust convergence under\nunexpected stragglers.\n","authors":["Xiaofan Yu","Ludmila Cherkasova","Harsh Vardhan","Quanling Zhao","Emily Ekaireb","Xiyuan Zhang","Arya Mazumdar","Tajana Rosing"],"pdf_url":"https://arxiv.org/pdf/2301.06646v3.pdf","comment":"Accepted by IoTDI'23"},{"id":"http://arxiv.org/abs/2303.16796v2","updated":"2023-04-04T06:26:06Z","published":"2023-03-29T15:38:25Z","title":"Module-based regularization improves Gaussian graphical models when\n  observing noisy data","summary":"  Researchers often represent relations in multi-variate correlational data\nusing Gaussian graphical models, which require regularization to sparsify the\nmodels. Acknowledging that they often study the modular structure of the\ninferred network, we suggest integrating it in the cross-validation of the\nregularization strength to balance under- and overfitting. Using synthetic and\nreal data, we show that this approach allows us to better recover and infer\nmodular structure in noisy data compared with the graphical lasso, a standard\napproach using the Gaussian log-likelihood when cross-validating the\nregularization strength.\n","authors":["Magnus Neuman","Joaquín Calatayud","Viktor Tasselius","Martin Rosvall"],"pdf_url":"https://arxiv.org/pdf/2303.16796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01555v1","updated":"2023-04-04T06:15:53Z","published":"2023-04-04T06:15:53Z","title":"Real-time Driver Monitoring Systems on Edge AI Device","summary":"  As road accident cases are increasing due to the inattention of the driver,\nautomated driver monitoring systems (DMS) have gained an increase in\nacceptance. In this report, we present a real-time DMS system that runs on a\nhardware-accelerator-based edge device. The system consists of an InfraRed\ncamera to record the driver footage and an edge device to process the data. To\nsuccessfully port the deep learning models to run on the edge device taking\nfull advantage of the hardware accelerators, model surgery was performed. The\nfinal DMS system achieves 63 frames per second (FPS) on the TI-TDA4VM edge\ndevice.\n","authors":["Jyothi Hariharan","Rahul Rama Varior","Sunil Karunakaran"],"pdf_url":"https://arxiv.org/pdf/2304.01555v1.pdf","comment":"Driver Monitoring System tech report - Ignitarium Technology\n  Solutions Private Limited"},{"id":"http://arxiv.org/abs/2304.01553v1","updated":"2023-04-04T06:13:07Z","published":"2023-04-04T06:13:07Z","title":"Heating and dynamics of the Solar atmosphere","summary":"  The solar atmosphere shows anomalous variation in temperature, starting from\nthe 5500 K photosphere to the million-degree Kelvin corona. The corona itself\nexpands into the interstellar medium as the free streaming solar wind, which\nmodulates and impacts the near-Earth space weather. The precise source regions\nof different structures in the solar wind, their formation height, and the\nheating of the solar atmosphere are inextricably linked and unsolved problems\nin astrophysics. Observations suggest correlations between Coronal holes (CHs),\nwhich are cool, intensity deficit structures in the solar corona, with\nstructures in the solar wind. Observations also suggest the local plasma\nheating in the corona through power-law distributed impulsive events. In this\nthesis, we use narrowband photometric, spectroscopic, and disc-integrated\nemission of the solar atmosphere ranging from Near Ultraviolet to X-rays along\nwith in-situ solar wind measurements to understand (i). the source regions of\nthe solar wind, (ii). the underlying mechanism of solar coronal heating, and\n(iii). the differentiation in dynamics of CHs with the background Quiet Sun\n(QS) regions, which do not show any significant signature of the solar wind. We\nleverage machine learning and numerical modeling tools to develop solar wind\nforecasting codes using interpretable AI, inversion codes to infer the\nproperties of impulsive events and to understand the differences in the\nthermodynamics of CHs and QS regions. We finally present a unified scenario of\nsolar wind emergence and heating in the solar atmosphere and discuss the\nimplications of inferences from this thesis.\n","authors":["Vishal Upendran"],"pdf_url":"https://arxiv.org/pdf/2304.01553v1.pdf","comment":"PhD thesis presented to IUCAA and JNU. Refer to the thesis for list\n  of papers"},{"id":"http://arxiv.org/abs/2304.01552v1","updated":"2023-04-04T06:06:59Z","published":"2023-04-04T06:06:59Z","title":"Meta-Learning with a Geometry-Adaptive Preconditioner","summary":"  Model-agnostic meta-learning (MAML) is one of the most successful\nmeta-learning algorithms. It has a bi-level optimization structure where the\nouter-loop process learns a shared initialization and the inner-loop process\noptimizes task-specific weights. Although MAML relies on the standard gradient\ndescent in the inner-loop, recent studies have shown that controlling the\ninner-loop's gradient descent with a meta-learned preconditioner can be\nbeneficial. Existing preconditioners, however, cannot simultaneously adapt in a\ntask-specific and path-dependent way. Additionally, they do not satisfy the\nRiemannian metric condition, which can enable the steepest descent learning\nwith preconditioned gradient. In this study, we propose Geometry-Adaptive\nPreconditioned gradient descent (GAP) that can overcome the limitations in\nMAML; GAP can efficiently meta-learn a preconditioner that is dependent on\ntask-specific parameters, and its preconditioner can be shown to be a\nRiemannian metric. Thanks to the two properties, the geometry-adaptive\npreconditioner is effective for improving the inner-loop optimization.\nExperiment results show that GAP outperforms the state-of-the-art MAML family\nand preconditioned gradient descent-MAML (PGD-MAML) family in a variety of\nfew-shot learning tasks. Code is available at:\nhttps://github.com/Suhyun777/CVPR23-GAP.\n","authors":["Suhyun Kang","Duhun Hwang","Moonjung Eo","Taesup Kim","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2304.01552v1.pdf","comment":"Accepted at CVPR 2023. Code is available at:\n  https://github.com/Suhyun777/CVPR23-GAP"},{"id":"http://arxiv.org/abs/2304.01545v1","updated":"2023-04-04T05:44:12Z","published":"2023-04-04T05:44:12Z","title":"How Regional Wind Characteristics Affect CNN-based wind predictions:\n  Insights from Spatiotemporal Correlation Analysis","summary":"  This study investigates the impact of spatiotemporal data dimensions on the\nprecision of a wind forecasting model developed using an artificial neural\nnetwork. Although previous studies have shown that incorporating spatial data\ncan enhance the accuracy of wind forecasting models, few investigations have\nexplored the extent of the improvement owing to different spatial scales in\nneural network-based predictive models. Additionally, there are limited studies\non the optimal temporal length of the input data for these models. To address\nthis gap, this study employs data with various spatiotemporal dimensions as\ninputs when forecasting wind using 3D-Convolutional Neural Networks (3D-CNN)\nand assesses their predictive performance. The results indicate that using\nspatial data of the surrounding area for 3D-CNN training can achieve better\npredictive performance than using only single-point information. Additionally,\nmulti-time data had a more positive effect on the predictive performance than\nsingle-time data. To determine the reasons for this, correlation analyses were\nused to determine the impact of the spatial and temporal sizes of the training\ndata on the prediction performance. The study found that as the autocorrelation\ncoefficient (ACC) decreased, meaning that there was less similarity over time,\nthe prediction performance decreased. Furthermore, the spatial standard\ndeviation of the ACC also affects the prediction performance. A Pearson\ncorrelation coefficient (PCC) analysis was conducted to examine the effect of\nspace on the prediction performance. Through the PCC analysis, we show that\nlocal geometric and seasonal wind conditions can influence the forecast\ncapability of a predictive model.\n","authors":["Heesoo Shin","Mario Rüttgers","Sangseung Lee"],"pdf_url":"https://arxiv.org/pdf/2304.01545v1.pdf","comment":"22 pages, 16 figures"},{"id":"http://arxiv.org/abs/2304.01541v1","updated":"2023-04-04T05:37:17Z","published":"2023-04-04T05:37:17Z","title":"Privacy Amplification via Compression: Achieving the Optimal\n  Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation","summary":"  Privacy and communication constraints are two major bottlenecks in federated\nlearning (FL) and analytics (FA). We study the optimal accuracy of mean and\nfrequency estimation (canonical models for FL and FA respectively) under joint\ncommunication and $(\\varepsilon, \\delta)$-differential privacy (DP)\nconstraints. We show that in order to achieve the optimal error under\n$(\\varepsilon, \\delta)$-DP, it is sufficient for each client to send\n$\\Theta\\left( n \\min\\left(\\varepsilon, \\varepsilon^2\\right)\\right)$ bits for FL\nand $\\Theta\\left(\\log\\left( n\\min\\left(\\varepsilon, \\varepsilon^2\\right)\n\\right)\\right)$ bits for FA to the server, where $n$ is the number of\nparticipating clients. Without compression, each client needs $O(d)$ bits and\n$\\log d$ bits for the mean and frequency estimation problems respectively\n(where $d$ corresponds to the number of trainable parameters in FL or the\ndomain size in FA), which means that we can get significant savings in the\nregime $ n \\min\\left(\\varepsilon, \\varepsilon^2\\right) = o(d)$, which is often\nthe relevant regime in practice. Our algorithms leverage compression for\nprivacy amplification: when each client communicates only partial information\nabout its sample, we show that privacy can be amplified by randomly selecting\nthe part contributed by each client.\n","authors":["Wei-Ning Chen","Dan Song","Ayfer Ozgur","Peter Kairouz"],"pdf_url":"https://arxiv.org/pdf/2304.01541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01525v1","updated":"2023-04-04T04:32:29Z","published":"2023-04-04T04:32:29Z","title":"Online Learning with Adversaries: A Differential Inclusion Analysis","summary":"  We consider the measurement model $Y = AX,$ where $X$ and, hence, $Y$ are\nrandom variables and $A$ is an a priori known tall matrix. At each time\ninstance, a sample of one of $Y$'s coordinates is available, and the goal is to\nestimate $\\mu := \\mathbb{E}[X]$ via these samples. However, the challenge is\nthat a small but unknown subset of $Y$'s coordinates are controlled by\nadversaries with infinite power: they can return any real number each time they\nare queried for a sample. For such an adversarial setting, we propose the first\nasynchronous online algorithm that converges to $\\mu$ almost surely. We prove\nthis result using a novel differential inclusion based two-timescale analysis.\nTwo key highlights of our proof include: (a) the use of a novel Lyapunov\nfunction for showing that $\\mu$ is the unique global attractor for our\nalgorithm's limiting dynamics, and (b) the use of martingale and stopping time\ntheory to show that our algorithm's iterates are almost surely bounded.\n","authors":["Swetha Ganesh","Alexandre Reiffers-Masson","Gugan Thoppe"],"pdf_url":"https://arxiv.org/pdf/2304.01525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15072v2","updated":"2023-04-04T04:29:13Z","published":"2022-11-28T05:16:20Z","title":"FaiREE: Fair Classification with Finite-Sample and Distribution-Free\n  Guarantee","summary":"  Algorithmic fairness plays an increasingly critical role in machine learning\nresearch. Several group fairness notions and algorithms have been proposed.\nHowever, the fairness guarantee of existing fair classification methods mainly\ndepends on specific data distributional assumptions, often requiring large\nsample sizes, and fairness could be violated when there is a modest number of\nsamples, which is often the case in practice. In this paper, we propose FaiREE,\na fair classification algorithm that can satisfy group fairness constraints\nwith finite-sample and distribution-free theoretical guarantees. FaiREE can be\nadapted to satisfy various group fairness notions (e.g., Equality of\nOpportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal\naccuracy. These theoretical guarantees are further supported by experiments on\nboth synthetic and real data. FaiREE is shown to have favorable performance\nover state-of-the-art algorithms.\n","authors":["Puheng Li","James Zou","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.15072v2.pdf","comment":"46 pages, 9 figures"},{"id":"http://arxiv.org/abs/2304.01518v1","updated":"2023-04-04T04:03:48Z","published":"2023-04-04T04:03:48Z","title":"Multimodal Neural Processes for Uncertainty Estimation","summary":"  Neural processes (NPs) have brought the representation power of parametric\ndeep neural networks and the reliable uncertainty estimation of non-parametric\nGaussian processes together. Although recent development of NPs has shown\nsuccess in both regression and classification, how to adapt NPs to multimodal\ndata has not be carefully studied. For the first time, we propose a new model\nof NP family for multimodal uncertainty estimation, namely Multimodal Neural\nProcesses. In a holistic and principled way, we develop a dynamic context\nmemory updated by the classification error, a multimodal Bayesian aggregation\nmechanism to aggregate multimodal representations, and a new attention\nmechanism for calibrated predictions. In extensive empirical evaluation, our\nmethod achieves the state-of-the-art multimodal uncertainty estimation\nperformance, showing its appealing ability of being robust against noisy\nsamples and reliable in out-of-domain detection.\n","authors":["Myong Chol Jung","He Zhao","Joanna Dipnall","Belinda Gabbe","Lan Du"],"pdf_url":"https://arxiv.org/pdf/2304.01518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01515v1","updated":"2023-04-04T03:52:49Z","published":"2023-04-04T03:52:49Z","title":"Text-Conditioned Sampling Framework for Text-to-Image Generation with\n  Masked Generative Models","summary":"  Token-based masked generative models are gaining popularity for their fast\ninference time with parallel decoding. While recent token-based approaches\nachieve competitive performance to diffusion-based models, their generation\nperformance is still suboptimal as they sample multiple tokens simultaneously\nwithout considering the dependence among them. We empirically investigate this\nproblem and propose a learnable sampling model, Text-Conditioned Token\nSelection (TCTS), to select optimal tokens via localized supervision with text\ninformation. TCTS improves not only the image quality but also the semantic\nalignment of the generated images with the given texts. To further improve the\nimage quality, we introduce a cohesive sampling strategy, Frequency Adaptive\nSampling (FAS), to each group of tokens divided according to the self-attention\nmaps. We validate the efficacy of TCTS combined with FAS with various\ngenerative tasks, demonstrating that it significantly outperforms the baselines\nin image-text alignment and image quality. Our text-conditioned sampling\nframework further reduces the original inference time by more than 50% without\nmodifying the original generative model.\n","authors":["Jaewoong Lee","Sangwon Jang","Jaehyeong Jo","Jaehong Yoon","Yunji Kim","Jin-Hwa Kim","Jung-Woo Ha","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2304.01515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14524v2","updated":"2023-04-04T03:51:27Z","published":"2023-03-25T17:37:43Z","title":"Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender\n  System","summary":"  Large language models (LLMs) have demonstrated their significant potential to\nbe applied for addressing various application tasks. However, traditional\nrecommender systems continue to face great challenges such as poor\ninteractivity and explainability, which actually also hinder their broad\ndeployment in real-world systems. To address these limitations, this paper\nproposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender\nSystem) that innovatively augments LLMs for building conversational recommender\nsystems by converting user profiles and historical interactions into prompts.\nChat-Rec is demonstrated to be effective in learning user preferences and\nestablishing connections between users and products through in-context\nlearning, which also makes the recommendation process more interactive and\nexplainable. What's more, within the Chat-Rec framework, user's preferences can\ntransfer to different products for cross-domain recommendations, and\nprompt-based injection of information into LLMs can also handle the cold-start\nscenarios with new items. In our experiments, Chat-Rec effectively improve the\nresults of top-k recommendations and performs better in zero-shot rating\nprediction task. Chat-Rec offers a novel approach to improving recommender\nsystems and presents new practical scenarios for the implementation of AIGC (AI\ngenerated content) in recommender system studies.\n","authors":["Yunfan Gao","Tao Sheng","Youlin Xiang","Yun Xiong","Haofen Wang","Jiawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01512v1","updated":"2023-04-04T03:46:25Z","published":"2023-04-04T03:46:25Z","title":"Handling Concept Drift in Global Time Series Forecasting","summary":"  Machine learning (ML) based time series forecasting models often require and\nassume certain degrees of stationarity in the data when producing forecasts.\nHowever, in many real-world situations, the data distributions are not\nstationary and they can change over time while reducing the accuracy of the\nforecasting models, which in the ML literature is known as concept drift.\nHandling concept drift in forecasting is essential for many ML methods in use\nnowadays, however, the prior work only proposes methods to handle concept drift\nin the classification domain. To fill this gap, we explore concept drift\nhandling methods in particular for Global Forecasting Models (GFM) which\nrecently have gained popularity in the forecasting domain. We propose two new\nconcept drift handling methods, namely: Error Contribution Weighting (ECW) and\nGradient Descent Weighting (GDW), based on a continuous adaptive weighting\nconcept. These methods use two forecasting models which are separately trained\nwith the most recent series and all series, and finally, the weighted average\nof the forecasts provided by the two models are considered as the final\nforecasts. Using LightGBM as the underlying base learner, in our evaluation on\nthree simulated datasets, the proposed models achieve significantly higher\naccuracy than a set of statistical benchmarks and LightGBM baselines across\nfour evaluation metrics.\n","authors":["Ziyi Liu","Rakshitha Godahewa","Kasun Bandara","Christoph Bergmeir"],"pdf_url":"https://arxiv.org/pdf/2304.01512v1.pdf","comment":"23 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2304.01508v1","updated":"2023-04-04T03:36:14Z","published":"2023-04-04T03:36:14Z","title":"EPVT: Environment-aware Prompt Vision Transformer for Domain\n  Generalization in Skin Lesion Recognition","summary":"  Skin lesion recognition using deep learning has made remarkable progress, and\nthere is an increasing need for deploying these systems in real-world\nscenarios. However, recent research has revealed that deep neural networks for\nskin lesion recognition may overly depend on disease-irrelevant image artifacts\n(i.e. dark corners, dense hairs), leading to poor generalization in unseen\nenvironments. To address this issue, we propose a novel domain generalization\nmethod called EPVT, which involves embedding prompts into the vision\ntransformer to collaboratively learn knowledge from diverse domains.\nConcretely, EPVT leverages a set of domain prompts, each of which plays as a\ndomain expert, to capture domain-specific knowledge; and a shared prompt for\ngeneral knowledge over the entire dataset. To facilitate knowledge sharing and\nthe interaction of different prompts, we introduce a domain prompt generator\nthat enables low-rank multiplicative updates between domain prompts and the\nshared prompt. A domain mixup strategy is additionally devised to reduce the\nco-occurring artifacts in each domain, which allows for more flexible decision\nmargins and mitigates the issue of incorrectly assigned domain labels.\nExperiments on four out-of-distribution datasets and six different biased ISIC\ndatasets demonstrate the superior generalization ability of EPVT in skin lesion\nrecognition across various environments. Our code and dataset will be released\nat https://github.com/SiyuanYan1/EPVT.\n","authors":["Siyuan Yan","Chi Liu","Zhen Yu","Lie Ju","Dwarikanath Mahapatrainst","Victoria Mar","Monika Janda","Peter Soyer","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2304.01508v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2304.01507v1","updated":"2023-04-04T03:35:29Z","published":"2023-04-04T03:35:29Z","title":"RARE: Robust Masked Graph Autoencoder","summary":"  Masked graph autoencoder (MGAE) has emerged as a promising self-supervised\ngraph pre-training (SGP) paradigm due to its simplicity and effectiveness.\nHowever, existing efforts perform the mask-then-reconstruct operation in the\nraw data space as is done in computer vision (CV) and natural language\nprocessing (NLP) areas, while neglecting the important non-Euclidean property\nof graph data. As a result, the highly unstable local connection structures\nlargely increase the uncertainty in inferring masked data and decrease the\nreliability of the exploited self-supervision signals, leading to inferior\nrepresentations for downstream evaluations. To address this issue, we propose a\nnovel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve the\ncertainty in inferring masked data and the reliability of the self-supervision\nmechanism by further masking and reconstructing node samples in the high-order\nlatent feature space. Through both theoretical and empirical analyses, we have\ndiscovered that performing a joint mask-then-reconstruct strategy in both\nlatent feature and raw data spaces could yield improved stability and\nperformance. To this end, we elaborately design a masked latent feature\ncompletion scheme, which predicts latent features of masked nodes under the\nguidance of high-order sample correlations that are hard to be observed from\nthe raw data perspective. Specifically, we first adopt a latent feature\npredictor to predict the masked latent features from the visible ones. Next, we\nencode the raw data of masked samples with a momentum graph encoder and\nsubsequently employ the resulting representations to improve predicted results\nthrough latent feature matching. Extensive experiments on seventeen datasets\nhave demonstrated the effectiveness and robustness of RARE against\nstate-of-the-art (SOTA) competitors across three downstream tasks.\n","authors":["Wenxuan Tu","Qing Liao","Sihang Zhou","Xin Peng","Chuan Ma","Zhe Liu","Xinwang Liu","Zhiping Cai"],"pdf_url":"https://arxiv.org/pdf/2304.01507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01506v1","updated":"2023-04-04T03:35:14Z","published":"2023-04-04T03:35:14Z","title":"OneShotSTL: One-Shot Seasonal-Trend Decomposition For Online Time Series\n  Anomaly Detection And Forecasting","summary":"  Seasonal-trend decomposition is one of the most fundamental concepts in time\nseries analysis that supports various downstream tasks, including time series\nanomaly detection and forecasting. However, existing decomposition methods rely\non batch processing with a time complexity of O(W), where W is the number of\ndata points within a time window. Therefore, they cannot always efficiently\nsupport real-time analysis that demands low processing delay. To address this\nchallenge, we propose OneShotSTL, an efficient and accurate algorithm that can\ndecompose time series online with an update time complexity of O(1). OneShotSTL\nis more than $1,000$ times faster than the batch methods, with accuracy\ncomparable to the best counterparts. Extensive experiments on real-world\nbenchmark datasets for downstream time series anomaly detection and forecasting\ntasks demonstrate that OneShotSTL is from 10 to over 1,000 times faster than\nthe state-of-the-art methods, while still providing comparable or even better\naccuracy.\n","authors":["Xiao He","Ye Li","Jian Tan","Bin Wu","Feifei Li"],"pdf_url":"https://arxiv.org/pdf/2304.01506v1.pdf","comment":"PVLDB 2023"},{"id":"http://arxiv.org/abs/2304.01502v1","updated":"2023-04-04T03:27:54Z","published":"2023-04-04T03:27:54Z","title":"SLPerf: a Unified Framework for Benchmarking Split Learning","summary":"  Data privacy concerns has made centralized training of data, which is\nscattered across silos, infeasible, leading to the need for collaborative\nlearning frameworks. To address that, two prominent frameworks emerged, i.e.,\nfederated learning (FL) and split learning (SL). While FL has established\nvarious benchmark frameworks and research libraries, SL currently lacks a\nunified library despite its diversity in terms of label sharing, model\naggregation, and cut layer choice. This lack of standardization makes comparing\nSL paradigms difficult. To address this, we propose SLPerf, a unified research\nframework and open research library for SL, and conduct extensive experiments\non four widely-used datasets under both IID and Non-IID data settings. Our\ncontributions include a comprehensive survey of recently proposed SL paradigms,\na detailed benchmark comparison of different SL paradigms in different\nsituations, and rich engineering take-away messages and research insights for\nimproving SL paradigms. SLPerf can facilitate SL algorithm development and fair\nperformance comparisons.\n","authors":["Tianchen Zhou","Zhanyi Hu","Bingzhe Wu","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2304.01502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01500v1","updated":"2023-04-04T03:19:36Z","published":"2023-04-04T03:19:36Z","title":"Physics-aware Roughness Optimization for Diffractive Optical Neural\n  Networks","summary":"  As a representative next-generation device/circuit technology beyond CMOS,\ndiffractive optical neural networks (DONNs) have shown promising advantages\nover conventional deep neural networks due to extreme fast computation speed\n(light speed) and low energy consumption. However, there is a mismatch, i.e.,\nsignificant prediction accuracy loss, between the DONN numerical modelling and\nphysical optical device deployment, because of the interpixel interaction\nwithin the diffractive layers. In this work, we propose a physics-aware\ndiffractive optical neural network training framework to reduce the performance\ndifference between numerical modeling and practical deployment. Specifically,\nwe propose the roughness modeling regularization in the training process and\nintegrate the physics-aware sparsification method to introduce sparsity to the\nphase masks to reduce sharp phase changes between adjacent pixels in\ndiffractive layers. We further develop $2\\pi$ periodic optimization to reduce\nthe roughness of the phase masks to preserve the performance of DONN.\nExperiment results demonstrate that, compared to state-of-the-arts, our\nphysics-aware optimization can provide $35.7\\%$, $34.2\\%$, $28.1\\%$, and\n$27.3\\%$ reduction in roughness with only accuracy loss on MNIST, FMNIST,\nKMNIST, and EMNIST, respectively.\n","authors":["Shanglin Zhou","Yingjie Li","Minhan Lou","Weilu Gao","Zhijie Shi","Cunxi Yu","Caiwen Ding"],"pdf_url":"https://arxiv.org/pdf/2304.01500v1.pdf","comment":"This paper is accepted by the Design Automation Conference (DAC),\n  2023"},{"id":"http://arxiv.org/abs/2304.01491v1","updated":"2023-04-04T03:11:49Z","published":"2023-04-04T03:11:49Z","title":"Multi model LSTM architecture for Track Association based on Automatic\n  Identification System Data","summary":"  For decades, track association has been a challenging problem in marine\nsurveillance, which involves the identification and association of vessel\nobservations over time. However, the Automatic Identification System (AIS) has\nprovided a new opportunity for researchers to tackle this problem by offering a\nlarge database of dynamic and geo-spatial information of marine vessels. With\nthe availability of such large databases, researchers can now develop\nsophisticated models and algorithms that leverage the increased availability of\ndata to address the track association challenge effectively. Furthermore, with\nthe advent of deep learning, track association can now be approached as a\ndata-intensive problem. In this study, we propose a Long Short-Term Memory\n(LSTM) based multi-model framework for track association. LSTM is a recurrent\nneural network architecture that is capable of processing multivariate temporal\ndata collected over time in a sequential manner, enabling it to predict current\nvessel locations from historical observations. Based on these predictions, a\ngeodesic distance based similarity metric is then utilized to associate the\nunclassified observations to their true tracks (vessels). We evaluate the\nperformance of our approach using standard performance metrics, such as\nprecision, recall, and F1 score, which provide a comprehensive summary of the\naccuracy of the proposed framework.\n","authors":["Md Asif Bin Syed","Imtiaz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2304.01491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01487v1","updated":"2023-04-04T03:04:28Z","published":"2023-04-04T03:04:28Z","title":"To ChatGPT, or not to ChatGPT: That is the question!","summary":"  ChatGPT has become a global sensation. As ChatGPT and other Large Language\nModels (LLMs) emerge, concerns of misusing them in various ways increase, such\nas disseminating fake news, plagiarism, manipulating public opinion, cheating,\nand fraud. Hence, distinguishing AI-generated from human-generated becomes\nincreasingly essential. Researchers have proposed various detection\nmethodologies, ranging from basic binary classifiers to more complex\ndeep-learning models. Some detection techniques rely on statistical\ncharacteristics or syntactic patterns, while others incorporate semantic or\ncontextual information to improve accuracy. The primary objective of this study\nis to provide a comprehensive and contemporary assessment of the most recent\ntechniques in ChatGPT detection. Additionally, we evaluated other AI-generated\ntext detection tools that do not specifically claim to detect ChatGPT-generated\ncontent to assess their performance in detecting ChatGPT-generated content. For\nour evaluation, we have curated a benchmark dataset consisting of prompts from\nChatGPT and humans, including diverse questions from medical, open Q&A, and\nfinance domains and user-generated responses from popular social networking\nplatforms. The dataset serves as a reference to assess the performance of\nvarious techniques in detecting ChatGPT-generated content. Our evaluation\nresults demonstrate that none of the existing methods can effectively detect\nChatGPT-generated content.\n","authors":["Alessandro Pegoraro","Kavita Kumari","Hossein Fereidooni","Ahmad-Reza Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2304.01487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01483v1","updated":"2023-04-04T02:55:40Z","published":"2023-04-04T02:55:40Z","title":"Blockwise Compression of Transformer-based Models without Retraining","summary":"  Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have\nrecently attracted increasing interest, research enthusiasm, and business\ndemand. However, their massive computation resources and huge memory footprint\nare inevitable challenges. To tackle this issue, we propose BCT, a framework of\nblockwise compression for transformers without retraining, to lower deployment\nthresholds. BCT achieves more fine-grained compression of the whole\ntransformer, including embedding, matrix multiplication, GELU, Softmax, layer\nnormalization, and all the intermediate results. As a case, we compress an\nefficient model with BCT and evaluate it on several General Language\nUnderstanding Evaluation (GLUE) datasets. The results show that BCT can achieve\na less than 0.90% accuracy drop in most tasks.\n","authors":["Gaochen Dong","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2304.01483v1.pdf","comment":"6 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:2303.09184"},{"id":"http://arxiv.org/abs/2304.01461v1","updated":"2023-04-04T02:01:48Z","published":"2023-04-04T02:01:48Z","title":"Time-space-frequency feature Fusion for 3-channel motor imagery\n  classification","summary":"  Low-channel EEG devices are crucial for portable and entertainment\napplications. However, the low spatial resolution of EEG presents challenges in\ndecoding low-channel motor imagery. This study introduces TSFF-Net, a novel\nnetwork architecture that integrates time-space-frequency features, effectively\ncompensating for the limitations of single-mode feature extraction networks\nbased on time-series or time-frequency modalities. TSFF-Net comprises four main\ncomponents: time-frequency representation, time-frequency feature extraction,\ntime-space feature extraction, and feature fusion and classification.\nTime-frequency representation and feature extraction transform raw EEG signals\ninto time-frequency spectrograms and extract relevant features. The time-space\nnetwork processes time-series EEG trials as input and extracts temporal-spatial\nfeatures. Feature fusion employs MMD loss to constrain the distribution of\ntime-frequency and time-space features in the Reproducing Kernel Hilbert Space,\nsubsequently combining these features using a weighted fusion approach to\nobtain effective time-space-frequency features. Moreover, few studies have\nexplored the decoding of three-channel motor imagery based on time-frequency\nspectrograms. This study proposes a shallow, lightweight decoding architecture\n(TSFF-img) based on time-frequency spectrograms and compares its classification\nperformance in low-channel motor imagery with other methods using two publicly\navailable datasets. Experimental results demonstrate that TSFF-Net not only\ncompensates for the shortcomings of single-mode feature extraction networks in\nEEG decoding, but also outperforms other state-of-the-art methods. Overall,\nTSFF-Net offers considerable advantages in decoding low-channel motor imagery\nand provides valuable insights for algorithmically enhancing low-channel EEG\ndecoding.\n","authors":["Zhengqing Miao","Meirong Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.01461v1.pdf","comment":"15 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2304.01457v1","updated":"2023-04-04T01:56:16Z","published":"2023-04-04T01:56:16Z","title":"Exploring Vision-Language Models for Imbalanced Learning","summary":"  Vision-Language models (VLMs) that use contrastive language-image\npre-training have shown promising zero-shot classification performance.\nHowever, their performance on imbalanced dataset is relatively poor, where the\ndistribution of classes in the training dataset is skewed, leading to poor\nperformance in predicting minority classes. For instance, CLIP achieved only 5%\naccuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder\nto VLMs to avoid OOM (out of memory) problem caused by large number of classes\nand capture nuanced features for tail classes. Then, we explore improvements of\nVLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms\nsuch as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments\ndemonstrate that the performance of VLMs can be further boosted when used with\ndecoder and imbalanced methods. Specifically, our improved VLMs significantly\noutperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,\nand 6.17%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. We\nfurther analyze the influence of pre-training data size, backbones, and\ntraining cost. Our study highlights the significance of imbalanced learning\nalgorithms in face of VLMs pre-trained by huge data. We release our code at\nhttps://github.com/Imbalance-VLM/Imbalance-VLM.\n","authors":["Yidong Wang","Zhuohao Yu","Jindong Wang","Qiang Heng","Hao Chen","Wei Ye","Rui Xie","Xing Xie","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01457v1.pdf","comment":"Technical report; 14 pages; code:\n  https://github.com/Imbalance-VLM/Imbalance-VLM"},{"id":"http://arxiv.org/abs/2304.01450v1","updated":"2023-04-04T01:49:57Z","published":"2023-04-04T01:49:57Z","title":"Clustering Validation with The Area Under Precision-Recall Curves","summary":"  Confusion matrices and derived metrics provide a comprehensive framework for\nthe evaluation of model performance in machine learning. These are well-known\nand extensively employed in the supervised learning domain, particularly\nclassification. Surprisingly, such a framework has not been fully explored in\nthe context of clustering validation. Indeed, just recently such a gap has been\nbridged with the introduction of the Area Under the ROC Curve for Clustering\n(AUCC), an internal/relative Clustering Validation Index (CVI) that allows for\nclustering validation in real application scenarios. In this work we explore\nthe Area Under Precision-Recall Curve (and related metrics) in the context of\nclustering validation. We show that these are not only appropriate as CVIs, but\nshould also be preferred in the presence of cluster imbalance. We perform a\ncomprehensive evaluation of proposed and state-of-art CVIs on real and\nsimulated data sets. Our observations corroborate towards an unified validation\nframework for supervised and unsupervised learning, given that they are\nconsistent with existing guidelines established for the evaluation of\nsupervised learning models.\n","authors":["Pablo Andretta Jaskowiak","Ivan Gesteira Costa"],"pdf_url":"https://arxiv.org/pdf/2304.01450v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2304.01447v1","updated":"2023-04-04T01:44:19Z","published":"2023-04-04T01:44:19Z","title":"Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning","summary":"  Learning anticipation in Multi-Agent Reinforcement Learning (MARL) is a\nreasoning paradigm where agents anticipate the learning steps of other agents\nto improve cooperation among themselves. As MARL uses gradient-based\noptimization, learning anticipation requires using Higher-Order Gradients\n(HOG), with so-called HOG methods. Existing HOG methods are based on policy\nparameter anticipation, i.e., agents anticipate the changes in policy\nparameters of other agents. Currently, however, these existing HOG methods have\nonly been applied to differentiable games or games with small state spaces. In\nthis work, we demonstrate that in the case of non-differentiable games with\nlarge state spaces, existing HOG methods do not perform well and are\ninefficient due to their inherent limitations related to policy parameter\nanticipation and multiple sampling stages. To overcome these problems, we\npropose Off-Policy Action Anticipation (OffPA2), a novel framework that\napproaches learning anticipation through action anticipation, i.e., agents\nanticipate the changes in actions of other agents, via off-policy sampling. We\ntheoretically analyze our proposed OffPA2 and employ it to develop multiple HOG\nmethods that are applicable to non-differentiable games with large state\nspaces. We conduct a large set of experiments and illustrate that our proposed\nHOG methods outperform the existing ones regarding efficiency and performance.\n","authors":["Ariyan Bighashdel","Daan de Geus","Pavol Jancura","Gijs Dubbelman"],"pdf_url":"https://arxiv.org/pdf/2304.01447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01440v1","updated":"2023-04-04T01:27:21Z","published":"2023-04-04T01:27:21Z","title":"A Deep Multi-Modal Cyber-Attack Detection in Industrial Control Systems","summary":"  The growing number of cyber-attacks against Industrial Control Systems (ICS)\nin recent years has elevated security concerns due to the potential\ncatastrophic impact. Considering the complex nature of ICS, detecting a\ncyber-attack in them is extremely challenging and requires advanced methods\nthat can harness multiple data modalities. This research utilizes network and\nsensor modality data from ICS processed with a deep multi-modal cyber-attack\ndetection model for ICS. Results using the Secure Water Treatment (SWaT) system\nshow that the proposed model can outperform existing single modality models and\nrecent works in the literature by achieving 0.99 precision, 0.98 recall, and\n0.98 f-measure, which shows the effectiveness of using both modalities in a\ncombined model for detecting cyber-attacks.\n","authors":["Sepideh Bahadoripour","Ethan MacDonald","Hadis Karimipour"],"pdf_url":"https://arxiv.org/pdf/2304.01440v1.pdf","comment":"This paper is accepted by the 24th IEEE International Conference on\n  Industrial Technology (ICIT2023)"},{"id":"http://arxiv.org/abs/2304.01435v1","updated":"2023-04-04T01:04:53Z","published":"2023-04-04T01:04:53Z","title":"Optimizing Irrigation Efficiency using Deep Reinforcement Learning in\n  the Field","summary":"  Agricultural irrigation is a significant contributor to freshwater\nconsumption. However, the current irrigation systems used in the field are not\nefficient. They rely mainly on soil moisture sensors and the experience of\ngrowers, but do not account for future soil moisture loss. Predicting soil\nmoisture loss is challenging because it is influenced by numerous factors,\nincluding soil texture, weather conditions, and plant characteristics. This\npaper proposes a solution to improve irrigation efficiency, which is called\nDRLIC. DRLIC is a sophisticated irrigation system that uses deep reinforcement\nlearning (DRL) to optimize its performance. The system employs a neural\nnetwork, known as the DRL control agent, which learns an optimal control policy\nthat considers both the current soil moisture measurement and the future soil\nmoisture loss. We introduce an irrigation reward function that enables our\ncontrol agent to learn from previous experiences. However, there may be\ninstances where the output of our DRL control agent is unsafe, such as\nirrigating too much or too little water. To avoid damaging the health of the\nplants, we implement a safety mechanism that employs a soil moisture predictor\nto estimate the performance of each action. If the predicted outcome is deemed\nunsafe, we perform a relatively-conservative action instead. To demonstrate the\nreal-world application of our approach, we developed an irrigation system that\ncomprises sprinklers, sensing and control nodes, and a wireless network. We\nevaluate the performance of DRLIC by deploying it in a testbed consisting of\nsix almond trees. During a 15-day in-field experiment, we compared the water\nconsumption of DRLIC with a widely-used irrigation scheme. Our results indicate\nthat DRLIC outperformed the traditional irrigation method by achieving a water\nsavings of up to 9.52%.\n","authors":["Xianzhong Ding","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2304.01435v1.pdf","comment":"15 pages, 19 figures"},{"id":"http://arxiv.org/abs/2304.01434v1","updated":"2023-04-04T01:03:32Z","published":"2023-04-04T01:03:32Z","title":"VNE: An Effective Method for Improving Deep Representation by\n  Manipulating Eigenvalue Distribution","summary":"  Since the introduction of deep learning, a wide scope of representation\nproperties, such as decorrelation, whitening, disentanglement, rank, isotropy,\nand mutual information, have been studied to improve the quality of\nrepresentation. However, manipulating such properties can be challenging in\nterms of implementational effectiveness and general applicability. To address\nthese limitations, we propose to regularize von Neumann entropy~(VNE) of\nrepresentation. First, we demonstrate that the mathematical formulation of VNE\nis superior in effectively manipulating the eigenvalues of the representation\nautocorrelation matrix. Then, we demonstrate that it is widely applicable in\nimproving state-of-the-art algorithms or popular benchmark algorithms by\ninvestigating domain-generalization, meta-learning, self-supervised learning,\nand generative models. In addition, we formally establish theoretical\nconnections with rank, disentanglement, and isotropy of representation.\nFinally, we provide discussions on the dimension control of VNE and the\nrelationship with Shannon entropy. Code is available at:\nhttps://github.com/jaeill/CVPR23-VNE.\n","authors":["Jaeill Kim","Suhyun Kang","Duhun Hwang","Jungwook Shin","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2304.01434v1.pdf","comment":"Accepted at CVPR 2023. Code is available at:\n  https://github.com/jaeill/CVPR23-VNE"},{"id":"http://arxiv.org/abs/2304.01433v1","updated":"2023-04-04T00:52:46Z","published":"2023-04-04T00:52:46Z","title":"TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning\n  with Hardware Support for Embeddings","summary":"  In response to innovations in machine learning (ML) models, production\nworkloads changed radically and rapidly. TPU v4 is the fifth Google domain\nspecific architecture (DSA) and its third supercomputer for such ML models.\nOptical circuit switches (OCSes) dynamically reconfigure its interconnect\ntopology to improve scale, availability, utilization, modularity, deployment,\nsecurity, power, and performance; users can pick a twisted 3D torus topology if\ndesired. Much cheaper, lower power, and faster than Infiniband, OCSes and\nunderlying optical components are <5% of system cost and <3% of system power.\nEach TPU v4 includes SparseCores, dataflow processors that accelerate models\nthat rely on embeddings by 5x-7x yet use only 5% of die area and power.\nDeployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves\nperformance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips\nand thus ~10x faster overall, which along with OCS flexibility helps large\nlanguage models. For similar sized systems, it is ~4.3x-4.5x faster than the\nGraphcore IPU Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power than\nthe Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers\nof Google Cloud use ~3x less energy and produce ~20x less CO2e than\ncontemporary DSAs in a typical on-premise data center.\n","authors":["Norman P. Jouppi","George Kurian","Sheng Li","Peter Ma","Rahul Nagarajan","Lifeng Nai","Nishant Patil","Suvinay Subramanian","Andy Swing","Brian Towles","Cliff Young","Xiang Zhou","Zongwei Zhou","David Patterson"],"pdf_url":"https://arxiv.org/pdf/2304.01433v1.pdf","comment":"15 pages; 16 figures; to be published at ISCA 2023 (the International\n  Symposium on Computer Architecture)"},{"id":"http://arxiv.org/abs/2304.01432v1","updated":"2023-04-04T00:43:05Z","published":"2023-04-04T00:43:05Z","title":"Reducing Discretization Error in the Frank-Wolfe Method","summary":"  The Frank-Wolfe algorithm is a popular method in structurally constrained\nmachine learning applications, due to its fast per-iteration complexity.\nHowever, one major limitation of the method is a slow rate of convergence that\nis difficult to accelerate due to erratic, zig-zagging step directions, even\nasymptotically close to the solution. We view this as an artifact of\ndiscretization; that is to say, the Frank-Wolfe \\emph{flow}, which is its\ntrajectory at asymptotically small step sizes, does not zig-zag, and reducing\ndiscretization error will go hand-in-hand in producing a more stabilized\nmethod, with better convergence properties. We propose two improvements: a\nmultistep Frank-Wolfe method that directly applies optimized higher-order\ndiscretization schemes; and an LMO-averaging scheme with reduced discretization\nerror, and whose local convergence rate over general convex sets accelerates\nfrom a rate of $O(1/k)$ to up to $O(1/k^{3/2})$.\n","authors":["Zhaoyue Chen","Yifan Sun"],"pdf_url":"https://arxiv.org/pdf/2304.01432v1.pdf","comment":"International Conference on Artificial Intelligence and Statistics\n  (AISTATS) 2023. arXiv admin note: text overlap with arXiv:2205.11794"},{"id":"http://arxiv.org/abs/2304.01430v1","updated":"2023-04-04T00:26:13Z","published":"2023-04-04T00:26:13Z","title":"Divided Attention: Unsupervised Multi-Object Discovery with Contextually\n  Separated Slots","summary":"  We introduce a method to segment the visual field into independently moving\nregions, trained with no ground truth or supervision. It consists of an\nadversarial conditional encoder-decoder architecture based on Slot Attention,\nmodified to use the image as context to decode optical flow without attempting\nto reconstruct the image itself. In the resulting multi-modal representation,\none modality (flow) feeds the encoder to produce separate latent codes (slots),\nwhereas the other modality (image) conditions the decoder to generate the first\n(flow) from the slots. This design frees the representation from having to\nencode complex nuisance variability in the image due to, for instance,\nillumination and reflectance properties of the scene. Since customary\nautoencoding based on minimizing the reconstruction error does not preclude the\nentire flow from being encoded into a single slot, we modify the loss to an\nadversarial criterion based on Contextual Information Separation. The resulting\nmin-max optimization fosters the separation of objects and their assignment to\ndifferent attention slots, leading to Divided Attention, or DivA. DivA\noutperforms recent unsupervised multi-object motion segmentation methods while\ntripling run-time speed up to 104FPS and reducing the performance gap from\nsupervised methods to 12% or less. DivA can handle different numbers of objects\nand different image sizes at training and test time, is invariant to\npermutation of object labels, and does not require explicit regularization.\n","authors":["Dong Lao","Zhengyang Hu","Francesco Locatello","Yanchao Yang","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2304.01430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01429v1","updated":"2023-04-04T00:24:40Z","published":"2023-04-04T00:24:40Z","title":"Learning from data with structured missingness","summary":"  Missing data are an unavoidable complication in many machine learning tasks.\nWhen data are `missing at random' there exist a range of tools and techniques\nto deal with the issue. However, as machine learning studies become more\nambitious, and seek to learn from ever-larger volumes of heterogeneous data, an\nincreasingly encountered problem arises in which missing values exhibit an\nassociation or structure, either explicitly or implicitly. Such `structured\nmissingness' raises a range of challenges that have not yet been systematically\naddressed, and presents a fundamental hindrance to machine learning at scale.\nHere, we outline the current literature and propose a set of grand challenges\nin learning from data with structured missingness.\n","authors":["Robin Mitra","Sarah F. McGough","Tapabrata Chakraborti","Chris Holmes","Ryan Copping","Niels Hagenbuch","Stefanie Biedermann","Jack Noonan","Brieuc Lehmann","Aditi Shenvi","Xuan Vinh Doan","David Leslie","Ginestra Bianconi","Ruben Sanchez-Garcia","Alisha Davies","Maxine Mackintosh","Eleni-Rosalina Andrinopoulou","Anahid Basiri","Chris Harbron","Ben D. MacArthur"],"pdf_url":"https://arxiv.org/pdf/2304.01429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01428v1","updated":"2023-04-04T00:22:17Z","published":"2023-04-04T00:22:17Z","title":"Learned Tree Search for Long-Horizon Social Robot Navigation in Shared\n  Airspace","summary":"  The fast-growing demand for fully autonomous aerial operations in shared\nspaces necessitates developing trustworthy agents that can safely and\nseamlessly navigate in crowded, dynamic spaces. In this work, we propose Social\nRobot Tree Search (SoRTS), an algorithm for the safe navigation of mobile\nrobots in social domains. SoRTS aims to augment existing socially-aware\ntrajectory prediction policies with a Monte Carlo Tree Search planner for\nimproved downstream navigation of mobile robots. To evaluate the performance of\nour method, we choose the use case of social navigation for general aviation.\nTo aid this evaluation, within this work, we also introduce X-PlaneROS, a\nhigh-fidelity aerial simulator, to enable more research in full-scale aerial\nautonomy. By conducting a user study based on the assessments of 26 FAA\ncertified pilots, we show that SoRTS performs comparably to a competent human\npilot, significantly outperforming our baseline algorithm. We further\ncomplement these results with self-play experiments in scenarios with\nincreasing complexity.\n","authors":["Ingrid Navarro","Jay Patrikar","Joao P. A. Dantas","Rohan Baijal","Ian Higgins","Sebastian Scherer","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2304.01428v1.pdf","comment":"8 Pages, 3 Figs, 4 Tables"},{"id":"http://arxiv.org/abs/2304.01426v1","updated":"2023-04-04T00:20:26Z","published":"2023-04-04T00:20:26Z","title":"Conformalized Unconditional Quantile Regression","summary":"  We develop a predictive inference procedure that combines conformal\nprediction (CP) with unconditional quantile regression (QR) -- a commonly used\ntool in econometrics that involves regressing the recentered influence function\n(RIF) of the quantile functional over input covariates. Unlike the more\nwidely-known conditional QR, unconditional QR explicitly captures the impact of\nchanges in covariate distribution on the quantiles of the marginal distribution\nof outcomes. Leveraging this property, our procedure issues adaptive predictive\nintervals with localized frequentist coverage guarantees. It operates by\nfitting a machine learning model for the RIFs using training data, and then\napplying the CP procedure for any test covariate with respect to a\n``hypothetical'' covariate distribution localized around the new instance.\nExperiments show that our procedure is adaptive to heteroscedasticity, provides\ntransparent coverage guarantees that are relevant to the test instance at hand,\nand performs competitively with existing methods in terms of efficiency.\n","authors":["Ahmed M. Alaa","Zeshan Hussain","David Sontag"],"pdf_url":"https://arxiv.org/pdf/2304.01426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.10842v4","updated":"2023-04-04T00:16:09Z","published":"2022-08-23T09:50:55Z","title":"Lottery Pools: Winning More by Interpolating Tickets without Increasing\n  Training or Inference Cost","summary":"  Lottery tickets (LTs) is able to discover accurate and sparse subnetworks\nthat could be trained in isolation to match the performance of dense networks.\nEnsemble, in parallel, is one of the oldest time-proven tricks in machine\nlearning to improve performance by combining the output of multiple independent\nmodels. However, the benefits of ensemble in the context of LTs will be diluted\nsince ensemble does not directly lead to stronger sparse subnetworks, but\nleverages their predictions for a better decision. In this work, we first\nobserve that directly averaging the weights of the adjacent learned subnetworks\nsignificantly boosts the performance of LTs. Encouraged by this observation, we\nfurther propose an alternative way to perform an 'ensemble' over the\nsubnetworks identified by iterative magnitude pruning via a simple\ninterpolating strategy. We call our method Lottery Pools. In contrast to the\nnaive ensemble which brings no performance gains to each single subnetwork,\nLottery Pools yields much stronger sparse subnetworks than the original LTs\nwithout requiring any extra training or inference cost. Across various modern\narchitectures on CIFAR-10/100 and ImageNet, we show that our method achieves\nsignificant performance gains in both, in-distribution and out-of-distribution\nscenarios. Impressively, evaluated with VGG-16 and ResNet-18, the produced\nsparse subnetworks outperform the original LTs by up to 1.88% on CIFAR-100 and\n2.36% on CIFAR-100-C; the resulting dense network surpasses the pre-trained\ndense-model up to 2.22% on CIFAR-100 and 2.38% on CIFAR-100-C.\n","authors":["Lu Yin","Shiwei Liu","Meng Fang","Tianjin Huang","Vlado Menkovski","Mykola Pechenizkiy"],"pdf_url":"https://arxiv.org/pdf/2208.10842v4.pdf","comment":"Published in AAAI 2023. Code can be found at\n  https://github.com/luuyin/Lottery-pools"},{"id":"http://arxiv.org/abs/2304.02169v1","updated":"2023-04-04T23:53:34Z","published":"2023-04-04T23:53:34Z","title":"Synthesize Extremely High-dimensional Longitudinal Electronic Health\n  Records via Hierarchical Autoregressive Language Model","summary":"  Synthetic electronic health records (EHRs) that are both realistic and\npreserve privacy can serve as an alternative to real EHRs for machine learning\n(ML) modeling and statistical analysis. However, generating high-fidelity and\ngranular electronic health record (EHR) data in its original,\nhighly-dimensional form poses challenges for existing methods due to the\ncomplexities inherent in high-dimensional data. In this paper, we propose\nHierarchical Autoregressive Language mOdel (HALO) for generating longitudinal\nhigh-dimensional EHR, which preserve the statistical properties of real EHR and\ncan be used to train accurate ML models without privacy concerns. Our HALO\nmethod, designed as a hierarchical autoregressive model, generates a\nprobability density function of medical codes, clinical visits, and patient\nrecords, allowing for the generation of realistic EHR data in its original,\nunaggregated form without the need for variable selection or aggregation.\nAdditionally, our model also produces high-quality continuous variables in a\nlongitudinal and probabilistic manner. We conducted extensive experiments and\ndemonstrate that HALO can generate high-fidelity EHR data with high-dimensional\ndisease code probabilities (d > 10,000), disease co-occurrence probabilities\nwithin visits (d > 1,000,000), and conditional probabilities across consecutive\nvisits (d > 5,000,000) and achieve above 0.9 R2 correlation in comparison to\nreal EHR data. This performance then enables downstream ML models trained on\nits synthetic data to achieve comparable accuracy to models trained on real\ndata (0.938 AUROC with HALO data vs. 0.943 with real data). Finally, using a\ncombination of real and synthetic data enhances the accuracy of ML models\nbeyond that achieved by using only real EHR data.\n","authors":["Brandon Theodorou","Cao Xiao","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2304.02169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02168v1","updated":"2023-04-04T23:51:48Z","published":"2023-04-04T23:51:48Z","title":"I2I: Initializing Adapters with Improvised Knowledge","summary":"  Adapters present a promising solution to the catastrophic forgetting problem\nin continual learning. However, training independent Adapter modules for every\nnew task misses an opportunity for cross-task knowledge transfer. We propose\nImprovise to Initialize (I2I), a continual learning algorithm that initializes\nAdapters for incoming tasks by distilling knowledge from previously-learned\ntasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning\nbenchmark, by conducting experiments on sequences of visual question answering\ntasks. Adapters trained with I2I consistently achieve better task accuracy than\nindependently-trained Adapters, demonstrating that our algorithm facilitates\nknowledge transfer between task Adapters. I2I also results in better cross-task\nknowledge transfer than the state-of-the-art AdapterFusion without incurring\nthe associated parametric cost.\n","authors":["Tejas Srinivasan","Furong Jia","Mohammad Rostami","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2304.02168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02160v1","updated":"2023-04-04T23:19:53Z","published":"2023-04-04T23:19:53Z","title":"Pac-HuBERT: Self-Supervised Music Source Separation via Primitive\n  Auditory Clustering and Hidden-Unit BERT","summary":"  In spite of the progress in music source separation research, the small\namount of publicly-available clean source data remains a constant limiting\nfactor for performance. Thus, recent advances in self-supervised learning\npresent a largely-unexplored opportunity for improving separation models by\nleveraging unlabelled music data. In this paper, we propose a self-supervised\nlearning framework for music source separation inspired by the HuBERT speech\nrepresentation model. We first investigate the potential impact of the original\nHuBERT model by inserting an adapted version of it into the well-known Demucs\nV2 time-domain separation model architecture. We then propose a\ntime-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory\nclustering HuBERT), that we later use in combination with a Res-U-Net decoder\nfor source separation. Pac-HuBERT uses primitive auditory features of music as\nunsupervised clustering labels to initialize the self-supervised pretraining\nprocess using the Free Music Archive (FMA) dataset. The resulting framework\nachieves better source-to-distortion ratio (SDR) performance on the MusDB18\ntest set than the original Demucs V2 and Res-U-Net models. We further\ndemonstrate that it can boost performance with small amounts of supervised\ndata. Ultimately, our proposed framework is an effective solution to the\nchallenge of limited clean source data for music source separation.\n","authors":["Ke Chen","Gordon Wichern","François G. Germain","Jonathan Le Roux"],"pdf_url":"https://arxiv.org/pdf/2304.02160v1.pdf","comment":"5 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2112.03222v2","updated":"2023-04-04T22:41:24Z","published":"2021-12-06T18:25:11Z","title":"On Complexity of 1-Center in Various Metrics","summary":"  We consider the classic 1-center problem: Given a set $P$ of $n$ points in a\nmetric space find the point in $P$ that minimizes the maximum distance to the\nother points of $P$. We study the complexity of this problem in $d$-dimensional\n$\\ell_p$-metrics and in edit and Ulam metrics over strings of length $d$. Our\nresults for the 1-center problem may be classified based on $d$ as follows.\n  $\\bullet$ Small $d$: Assuming the hitting set conjecture (HSC), we show that\nwhen $d=\\omega(\\log n)$, no subquadratic algorithm can solve 1-center problem\nin any of the $\\ell_p$-metrics, or in edit or Ulam metrics.\n  $\\bullet$ Large $d$: When $d=\\Omega(n)$, we extend our conditional lower\nbound to rule out subquartic algorithms for 1-center problem in edit metric\n(assuming Quantified SETH). On the other hand, we give a\n$(1+\\epsilon)$-approximation for 1-center in Ulam metric with running time\n$\\tilde{O_{\\varepsilon}}(nd+n^2\\sqrt{d})$.\n  We also strengthen some of the above lower bounds by allowing approximations\nor by reducing the dimension $d$, but only against a weaker class of algorithms\nwhich list all requisite solutions. Moreover, we extend one of our hardness\nresults to rule out subquartic algorithms for the well-studied 1-median problem\nin the edit metric, where given a set of $n$ strings each of length $n$, the\ngoal is to find a string in the set that minimizes the sum of the edit\ndistances to the rest of the strings in the set.\n","authors":["Amir Abboud","Mohammad Hossein Bateni","Vincent Cohen-Addad","Karthik C. S.","Saeed Seddighin"],"pdf_url":"https://arxiv.org/pdf/2112.03222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02147v1","updated":"2023-04-04T22:23:50Z","published":"2023-04-04T22:23:50Z","title":"ConvFormer: Parameter Reduction in Transformer Models for 3D Human Pose\n  Estimation by Leveraging Dynamic Multi-Headed Convolutional Attention","summary":"  Recently, fully-transformer architectures have replaced the defacto\nconvolutional architecture for the 3D human pose estimation task. In this paper\nwe propose \\textbf{\\textit{ConvFormer}}, a novel convolutional transformer that\nleverages a new \\textbf{\\textit{dynamic multi-headed convolutional\nself-attention}} mechanism for monocular 3D human pose estimation. We designed\na spatial and temporal convolutional transformer to comprehensively model human\njoint relations within individual frames and globally across the motion\nsequence. Moreover, we introduce a novel notion of \\textbf{\\textit{temporal\njoints profile}} for our temporal ConvFormer that fuses complete temporal\ninformation immediately for a local neighborhood of joint features. We have\nquantitatively and qualitatively validated our method on three common benchmark\ndatasets: Human3.6M, MPI-INF-3DHP, and HumanEva. Extensive experiments have\nbeen conducted to identify the optimal hyper-parameter set. These experiments\ndemonstrated that we achieved a \\textbf{significant parameter reduction\nrelative to prior transformer models} while attaining State-of-the-Art (SOTA)\nor near SOTA on all three datasets. Additionally, we achieved SOTA for Protocol\nIII on H36M for both GT and CPN detection inputs. Finally, we obtained SOTA on\nall three metrics for the MPI-INF-3DHP dataset and for all three subjects on\nHumanEva under Protocol II.\n","authors":["Alec Diaz-Arias","Dmitriy Shin"],"pdf_url":"https://arxiv.org/pdf/2304.02147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02146v1","updated":"2023-04-04T22:10:40Z","published":"2023-04-04T22:10:40Z","title":"Structure Learning with Continuous Optimization: A Sober Look and Beyond","summary":"  This paper investigates in which cases continuous optimization for directed\nacyclic graph (DAG) structure learning can and cannot perform well and why this\nhappens, and suggests possible directions to make the search procedure more\nreliable. Reisach et al. (2021) suggested that the remarkable performance of\nseveral continuous structure learning approaches is primarily driven by a high\nagreement between the order of increasing marginal variances and the\ntopological order, and demonstrated that these approaches do not perform well\nafter data standardization. We analyze this phenomenon for continuous\napproaches assuming equal and non-equal noise variances, and show that the\nstatement may not hold in either case by providing counterexamples,\njustifications, and possible alternative explanations. We further demonstrate\nthat nonconvexity may be a main concern especially for the non-equal noise\nvariances formulation, while recent advances in continuous structure learning\nfail to achieve improvement in this case. Our findings suggest that future\nworks should take into account the non-equal noise variances formulation to\nhandle more general settings and for a more comprehensive empirical evaluation.\nLastly, we provide insights into other aspects of the search procedure,\nincluding thresholding and sparsity, and show that they play an important role\nin the final solutions.\n","authors":["Ignavier Ng","Biwei Huang","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.02146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02141v1","updated":"2023-04-04T22:00:40Z","published":"2023-04-04T22:00:40Z","title":"Sequential Linearithmic Time Optimal Unimodal Fitting When Minimizing\n  Univariate Linear Losses","summary":"  This paper focuses on optimal unimodal transformation of the score outputs of\na univariate learning model under linear loss functions. We demonstrate that\nthe optimal mapping between score values and the target region is a rectangular\nfunction. To produce this optimal rectangular fit for the observed samples, we\npropose a sequential approach that can its estimation with each incoming new\nsample. Our approach has logarithmic time complexity per iteration and is\noptimally efficient.\n","authors":["Kaan Gokcesu","Hakan Gokcesu"],"pdf_url":"https://arxiv.org/pdf/2304.02141v1.pdf","comment":"this work draws from arXiv:2108.08780"},{"id":"http://arxiv.org/abs/2011.06528v5","updated":"2023-04-04T21:44:16Z","published":"2020-11-12T17:40:53Z","title":"Treatment Allocation with Strategic Agents","summary":"  There is increasing interest in allocating treatments based on observed\nindividual characteristics: examples include targeted marketing, individualized\ncredit offers, and heterogeneous pricing. Treatment personalization introduces\nincentives for individuals to modify their behavior to obtain a better\ntreatment. Strategic behavior shifts the joint distribution of covariates and\npotential outcomes. The optimal rule without strategic behavior allocates\ntreatments only to those with a positive Conditional Average Treatment Effect.\nWith strategic behavior, we show that the optimal rule can involve\nrandomization, allocating treatments with less than 100% probability even to\nthose who respond positively on average to the treatment. We propose a\nsequential experiment based on Bayesian Optimization that converges to the\noptimal treatment rule without parametric assumptions on individual strategic\nbehavior.\n","authors":["Evan Munro"],"pdf_url":"https://arxiv.org/pdf/2011.06528v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.02729v4","updated":"2023-04-04T21:40:03Z","published":"2023-01-06T22:22:04Z","title":"A Characterization of Multioutput Learnability","summary":"  We consider the problem of learning multioutput function classes in batch and\nonline settings. In both settings, we show that a multioutput function class is\nlearnable if and only if each single-output restriction of the function class\nis learnable. This provides a complete characterization of the learnability of\nmultilabel classification and multioutput regression in both batch and online\nsettings. As an extension, we also consider multilabel learnability in the\nbandit feedback setting and show a similar characterization as in the\nfull-feedback setting.\n","authors":["Vinod Raman","Unique Subedi","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2301.02729v4.pdf","comment":"37, Updated Online Section"},{"id":"http://arxiv.org/abs/2209.03487v2","updated":"2023-04-04T21:04:26Z","published":"2022-09-07T22:36:56Z","title":"A simple approach for quantizing neural networks","summary":"  In this short note, we propose a new method for quantizing the weights of a\nfully trained neural network. A simple deterministic pre-processing step allows\nus to quantize network layers via memoryless scalar quantization while\npreserving the network performance on given training data. On one hand, the\ncomputational complexity of this pre-processing slightly exceeds that of\nstate-of-the-art algorithms in the literature. On the other hand, our approach\ndoes not require any hyper-parameter tuning and, in contrast to previous\nmethods, allows a plain analysis. We provide rigorous theoretical guarantees in\nthe case of quantizing single network layers and show that the relative error\ndecays with the number of parameters in the network if the training data\nbehaves well, e.g., if it is sampled from suitable random distributions. The\ndeveloped method also readily allows the quantization of deep networks by\nconsecutive application to single layers.\n","authors":["Johannes Maly","Rayan Saab"],"pdf_url":"https://arxiv.org/pdf/2209.03487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02119v1","updated":"2023-04-04T20:57:34Z","published":"2023-04-04T20:57:34Z","title":"Initialization Approach for Nonlinear State-Space Identification via the\n  Subspace Encoder Approach","summary":"  The SUBNET neural network architecture has been developed to identify\nnonlinear state-space models from input-output data. To achieve this, it\ncombines the rolled-out nonlinear state-space equations and a state encoder\nfunction, both parameterised as a neural network. The encoder function is\nintroduced to reconstruct the current state from past input-output data. Hence\nit enables the forward simulation of the rolled-out state-space model. While\nthis approach has shown to provide high-accuracy and consistent model\nestimation, its convergence can be significantly improved by efficient\ninitialization of the training process. This paper focuses on such an\ninitialisation of the subspace encoder approach using the Best Linear\nApproximation (BLA). Using the BLA provided state-space matrices and its\nassociated reconstructability map both the state-transition part of the network\nand the encoder are initialized. The performance of the improved initialisation\nscheme is evaluated on a Wiener-Hammerstein simulation example and a benchmark\ndataset. The results show that for a weakly nonlinear system, the proposed\ninitialisation based on the linear reconstructability map results in a faster\nconvergence and a better model quality.\n","authors":["Rishi Ramkannan","Gerben I. Beintema","Roland Tóth","Maarten Schoukens"],"pdf_url":"https://arxiv.org/pdf/2304.02119v1.pdf","comment":"Accepted for presentation at the IFAC World Congress 2023"},{"id":"http://arxiv.org/abs/2304.02104v1","updated":"2023-04-04T20:17:44Z","published":"2023-04-04T20:17:44Z","title":"Deep learning for diffusion in porous media","summary":"  We adopt convolutional neural networks (CNN) to predict the basic properties\nof the porous media. Two different media types are considered: one mimics the\nsandstone, and the other mimics the systems derived from the extracellular\nspace of biological tissues. The Lattice Boltzmann Method is used to obtain the\nlabeled data necessary for performing supervised learning. We distinguish two\ntasks. In the first, networks based on the analysis of the system's geometry\npredict porosity and effective diffusion coefficient. In the second, networks\nreconstruct the system's geometry and concentration map. In the first task, we\npropose two types of CNN models: the C-Net and the encoder part of the U-Net.\nBoth networks are modified by adding a self-normalization module. The models\npredict with reasonable accuracy but only within the data type, they are\ntrained on. For instance, the model trained on sandstone-like samples\novershoots or undershoots for biological-like samples. In the second task, we\npropose the usage of the U-Net architecture. It accurately reconstructs the\nconcentration fields. Moreover, the network trained on one data type works well\nfor the other. For instance, the model trained on sandstone-like samples works\nperfectly on biological-like samples.\n","authors":["Krzysztof M. Graczyk","Dawid Strzelczyk","Maciej Matyka"],"pdf_url":"https://arxiv.org/pdf/2304.02104v1.pdf","comment":"17 pages, 19 figures"},{"id":"http://arxiv.org/abs/2303.00280v2","updated":"2023-04-04T20:07:36Z","published":"2023-03-01T07:02:09Z","title":"Label Attention Network for sequential multi-label classification: you\n  were looking at a wrong self-attention","summary":"  Most of the available user information can be represented as a sequence of\ntimestamped events. Each event is assigned a set of categorical labels whose\nfuture structure is of great interest. For instance, our goal is to predict a\ngroup of items in the next customer's purchase or tomorrow's client\ntransactions. This is a multi-label classification problem for sequential data.\nModern approaches focus on transformer architecture for sequential data\nintroducing self-attention for the elements in a sequence. In that case, we\ntake into account events' time interactions but lose information on label\ninter-dependencies. Motivated by this shortcoming, we propose leveraging a\nself-attention mechanism over labels preceding the predicted step. As our\napproach is a Label-Attention NETwork, we call it LANET. Experimental evidence\nsuggests that LANET outperforms the established models' performance and greatly\ncaptures interconnections between labels. For example, the micro-AUC of our\napproach is $0.9536$ compared to $0.7501$ for a vanilla transformer. We provide\nan implementation of LANET to facilitate its wider usage.\n","authors":["Elizaveta Kovtun","Galina Boeva","Artem Zabolotnyi","Evgeny Burnaev","Martin Spindler","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2303.00280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12126v3","updated":"2023-04-04T20:03:36Z","published":"2023-02-23T16:09:42Z","title":"KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate\n  Political Stance Prediction","summary":"  The political stance prediction for news articles has been widely studied to\nmitigate the echo chamber effect -- people fall into their thoughts and\nreinforce their pre-existing beliefs. The previous works for the political\nstance problem focus on (1) identifying political factors that could reflect\nthe political stance of a news article and (2) capturing those factors\neffectively. Despite their empirical successes, they are not sufficiently\njustified in terms of how effective their identified factors are in the\npolitical stance prediction. Motivated by this, in this work, we conduct a user\nstudy to investigate important factors in political stance prediction, and\nobserve that the context and tone of a news article (implicit) and external\nknowledge for real-world entities appearing in the article (explicit) are\nimportant in determining its political stance. Based on this observation, we\npropose a novel knowledge-aware approach to political stance prediction (KHAN),\nemploying (1) hierarchical attention networks (HAN) to learn the relationships\namong words and sentences in three different levels and (2) knowledge encoding\n(KE) to incorporate external knowledge for real-world entities into the process\nof political stance prediction. Also, to take into account the subtle and\nimportant difference between opposite political stances, we build two\nindependent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by\nourselves and learn to fuse the different political knowledge. Through\nextensive evaluations on three real-world datasets, we demonstrate the\nsuperiority of DASH in terms of (1) accuracy, (2) efficiency, and (3)\neffectiveness.\n","authors":["Yunyong Ko","Seongeun Ryu","Soeun Han","Youngseung Jeon","Jaehoon Kim","Sohyun Park","Kyungsik Han","Hanghang Tong","Sang-Wook Kim"],"pdf_url":"https://arxiv.org/pdf/2302.12126v3.pdf","comment":"12 pages, 5 figures, 10 tables, the Web Conference 2023 (WWW)"},{"id":"http://arxiv.org/abs/2302.12252v2","updated":"2023-04-04T19:46:08Z","published":"2023-02-23T18:59:56Z","title":"Boosting Adversarial Transferability using Dynamic Cues","summary":"  The transferability of adversarial perturbations between image models has\nbeen extensively studied. In this case, an attack is generated from a known\nsurrogate \\eg, the ImageNet trained model, and transferred to change the\ndecision of an unknown (black-box) model trained on an image dataset. However,\nattacks generated from image models do not capture the dynamic nature of a\nmoving object or a changing scene due to a lack of temporal cues within image\nmodels. This leads to reduced transferability of adversarial attacks from\nrepresentation-enriched \\emph{image} models such as Supervised Vision\nTransformers (ViTs), Self-supervised ViTs (\\eg, DINO), and Vision-language\nmodels (\\eg, CLIP) to black-box \\emph{video} models. In this work, we induce\ndynamic cues within the image models without sacrificing their original\nperformance on images. To this end, we optimize \\emph{temporal prompts} through\nfrozen image models to capture motion dynamics. Our temporal prompts are the\nresult of a learnable transformation that allows optimizing for temporal\ngradients during an adversarial attack to fool the motion dynamics.\nSpecifically, we introduce spatial (image) and temporal (video) cues within the\nsame source model through task-specific prompts. Attacking such prompts\nmaximizes the adversarial transferability from image-to-video and\nimage-to-image models using the attacks designed for image models. Our attack\nresults indicate that the attacker does not need specialized architectures,\n\\eg, divided space-time attention, 3D convolutions, or multi-view convolution\nnetworks for different data modalities. Image models are effective surrogates\nto optimize an adversarial attack to fool black-box models in a changing\nenvironment over time. Code is available at https://bit.ly/3Xd9gRQ\n","authors":["Muzammal Naseer","Ahmad Mahmood","Salman Khan","Fahad Khan"],"pdf_url":"https://arxiv.org/pdf/2302.12252v2.pdf","comment":"International Conference on Learning Representations (ICLR'23),\n  Code:https://bit.ly/3Xd9gRQ"},{"id":"http://arxiv.org/abs/2304.02096v1","updated":"2023-04-04T19:45:04Z","published":"2023-04-04T19:45:04Z","title":"The CAMELS project: Expanding the galaxy formation model space with new\n  ASTRID and 28-parameter TNG and SIMBA suites","summary":"  We present CAMELS-ASTRID, the third suite of hydrodynamical simulations in\nthe Cosmology and Astrophysics with MachinE Learning (CAMELS) project, along\nwith new simulation sets that extend the model parameter space based on the\nprevious frameworks of CAMELS-TNG and CAMELS-SIMBA, to provide broader training\nsets and testing grounds for machine-learning algorithms designed for\ncosmological studies. CAMELS-ASTRID employs the galaxy formation model\nfollowing the ASTRID simulation and contains 2,124 hydrodynamic simulation runs\nthat vary 3 cosmological parameters ($\\Omega_m$, $\\sigma_8$, $\\Omega_b$) and 4\nparameters controlling stellar and AGN feedback. Compared to the existing TNG\nand SIMBA simulation suites in CAMELS, the fiducial model of ASTRID features\nthe mildest AGN feedback and predicts the least baryonic effect on the matter\npower spectrum. The training set of ASTRID covers a broader variation in the\ngalaxy populations and the baryonic impact on the matter power spectrum\ncompared to its TNG and SIMBA counterparts, which can make machine-learning\nmodels trained on the ASTRID suite exhibit better extrapolation performance\nwhen tested on other hydrodynamic simulation sets. We also introduce extension\nsimulation sets in CAMELS that widely explore 28 parameters in the TNG and\nSIMBA models, demonstrating the enormity of the overall galaxy formation model\nparameter space and the complex non-linear interplay between cosmology and\nastrophysical processes. With the new simulation suites, we show that building\nrobust machine-learning models favors training and testing on the largest\npossible diversity of galaxy formation models. We also demonstrate that it is\npossible to train accurate neural networks to infer cosmological parameters\nusing the high-dimensional TNG-SB28 simulation set.\n","authors":["Yueying Ni","Shy Genel","Daniel Anglés-Alcázar","Francisco Villaescusa-Navarro","Yongseok Jo","Simeon Bird","Tiziana Di Matteo","Rupert Croft","Nianyi Chen","Natalí S. M. de Santi","Matthew Gebhardt","Helen Shao","Shivam Pandey","Lars Hernquist","Romeel Dave"],"pdf_url":"https://arxiv.org/pdf/2304.02096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02089v1","updated":"2023-04-04T19:35:43Z","published":"2023-04-04T19:35:43Z","title":"Hierarchically Fusing Long and Short-Term User Interests for\n  Click-Through Rate Prediction in Product Search","summary":"  Estimating Click-Through Rate (CTR) is a vital yet challenging task in\npersonalized product search. However, existing CTR methods still struggle in\nthe product search settings due to the following three challenges including how\nto more effectively extract users' short-term interests with respect to\nmultiple aspects, how to extract and fuse users' long-term interest with\nshort-term interests, how to address the entangling characteristic of long and\nshort-term interests. To resolve these challenges, in this paper, we propose a\nnew approach named Hierarchical Interests Fusing Network (HIFN), which consists\nof four basic modules namely Short-term Interests Extractor (SIE), Long-term\nInterests Extractor (LIE), Interests Fusion Module (IFM) and Interests\nDisentanglement Module (IDM). Specifically, SIE is proposed to extract user's\nshort-term interests by integrating three fundamental interests encoders within\nit namely query-dependent, target-dependent and causal-dependent interest\nencoder, respectively, followed by delivering the resultant representation to\nthe module LIE, where it can effectively capture user long-term interests by\ndevising an attention mechanism with respect to the short-term interests from\nSIE module. In IFM, the achieved long and short-term interests are further\nfused in an adaptive manner, followed by concatenating it with original raw\ncontext features for the final prediction result. Last but not least,\nconsidering the entangling characteristic of long and short-term interests, IDM\nfurther devises a self-supervised framework to disentangle long and short-term\ninterests. Extensive offline and online evaluations on a real-world e-commerce\nplatform demonstrate the superiority of HIFN over state-of-the-art methods.\n","authors":["Qijie Shen","Hong Wen","Jing Zhang","Qi Rao"],"pdf_url":"https://arxiv.org/pdf/2304.02089v1.pdf","comment":"accpeted by CIKM'22 as a Full Paper"},{"id":"http://arxiv.org/abs/2304.02086v1","updated":"2023-04-04T19:33:00Z","published":"2023-04-04T19:33:00Z","title":"Scalable Online Learning of Approximate Stackelberg Solutions in Energy\n  Trading Games with Demand Response Aggregators","summary":"  In this work, a Stackelberg game theoretic framework is proposed for trading\nenergy bidirectionally between the demand-response (DR) aggregator and the\nprosumers. This formulation allows for flexible energy arbitrage and additional\nmonetary rewards while ensuring that the prosumers' desired daily energy demand\nis met. Then, a scalable (with the number of prosumers) approach is proposed to\nfind approximate equilibria based on online sampling and learning of the\nprosumers' cumulative best response. Moreover, bounds are provided on the\nquality of the approximate equilibrium solution. Last, real-world data from the\nCalifornia day-ahead energy market and the University of California at Davis\nbuilding energy demands are utilized to demonstrate the efficacy of the\nproposed framework and the online scalable solution.\n","authors":["Styliani I. Kampezidou","Justin Romberg","Kyriakos G. Vamvoudakis","Dimitri N. Mavris"],"pdf_url":"https://arxiv.org/pdf/2304.02086v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2304.02084v1","updated":"2023-04-04T19:28:51Z","published":"2023-04-04T19:28:51Z","title":"EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri\n  using X-ray CT","summary":"  We present a complete software pipeline for revealing the hidden texts of the\nHerculaneum papyri using X-ray CT images. This enhanced virtual unwrapping\npipeline combines machine learning with a novel geometric framework linking 3D\nand 2D images. We also present EduceLab-Scrolls, a comprehensive open dataset\nrepresenting two decades of research effort on this problem. EduceLab-Scrolls\ncontains a set of volumetric X-ray CT images of both small fragments and\nintact, rolled scrolls. The dataset also contains 2D image labels that are used\nin the supervised training of an ink detection model. Labeling is enabled by\naligning spectral photography of scroll fragments with X-ray CT images of the\nsame fragments, thus creating a machine-learnable mapping between image spaces\nand modalities. This alignment permits supervised learning for the detection of\n\"invisible\" carbon ink in X-ray CT, a task that is \"impossible\" even for human\nexpert labelers. To our knowledge, this is the first aligned dataset of its\nkind and is the largest dataset ever released in the heritage domain. Our\nmethod is capable of revealing accurate lines of text on scroll fragments with\nknown ground truth. Revealed text is verified using visual confirmation,\nquantitative image metrics, and scholarly review. EduceLab-Scrolls has also\nenabled the discovery, for the first time, of hidden texts from the Herculaneum\npapyri, which we present here. We anticipate that the EduceLab-Scrolls dataset\nwill generate more textual discovery as research continues.\n","authors":["Stephen Parsons","C. Seth Parker","Christy Chapman","Mami Hayashida","W. Brent Seales"],"pdf_url":"https://arxiv.org/pdf/2304.02084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13955v2","updated":"2023-04-04T19:15:07Z","published":"2022-11-25T08:37:17Z","title":"MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision\n  Transformer with Heterogeneous Attention","summary":"  Secure multi-party computation (MPC) enables computation directly on\nencrypted data and protects both data and model privacy in deep learning\ninference. However, existing neural network architectures, including Vision\nTransformers (ViTs), are not designed or optimized for MPC and incur\nsignificant latency overhead. We observe Softmax accounts for the major latency\nbottleneck due to a high communication complexity, but can be selectively\nreplaced or linearized without compromising the model accuracy. Hence, in this\npaper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet\nefficient ViT inference in MPC. Based on a systematic latency and accuracy\nevaluation of the Softmax attention and other attention variants, we propose a\nheterogeneous attention optimization space. We also develop a simple yet\neffective MPC-aware neural architecture search algorithm for fast Pareto\noptimization. To further boost the inference efficiency, we propose MPCViT+, to\njointly optimize the Softmax attention and other network components, including\nGeLU, matrix multiplication, etc. With extensive experiments, we demonstrate\nthat MPCViT achieves 1.9%, 1.3% and 4.6% higher accuracy with 6.2x, 2.9x and\n1.9x latency reduction compared with baseline ViT, MPCFormer and THE-X on the\nTiny-ImageNet dataset, respectively. MPCViT+ further achieves 1.2x latency\nreduction on CIFAR-100 dataset and reaches a better Pareto front compared with\nMPCViT.\n","authors":["Wenxuan Zeng","Meng Li","Wenjie Xiong","Tong Tong","Wenjie Lu","Jin Tan","Runsheng Wang","Ru Huang"],"pdf_url":"https://arxiv.org/pdf/2211.13955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00366v3","updated":"2023-04-04T18:44:03Z","published":"2023-01-01T07:42:50Z","title":"SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network\n  for Object Segmentation","summary":"  This paper proposes a novel self-supervised based Cut-and-Paste GAN to\nperform foreground object segmentation and generate realistic composite images\nwithout manual annotations. We accomplish this goal by a simple yet effective\nself-supervised approach coupled with the U-Net based discriminator. The\nproposed method extends the ability of the standard discriminators to learn not\nonly the global data representations via classification (real/fake) but also\nlearn semantic and structural information through pseudo labels created using\nthe self-supervised task. The proposed method empowers the generator to create\nmeaningful masks by forcing it to learn informative per-pixel as well as global\nimage feedback from the discriminator. Our experiments demonstrate that our\nproposed method significantly outperforms the state-of-the-art methods on the\nstandard benchmark datasets.\n","authors":["Kunal Chaturvedi","Ali Braytee","Jun Li","Mukesh Prasad"],"pdf_url":"https://arxiv.org/pdf/2301.00366v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02064v1","updated":"2023-04-04T18:32:20Z","published":"2023-04-04T18:32:20Z","title":"Algorithm-Dependent Bounds for Representation Learning of Multi-Source\n  Domain Adaptation","summary":"  We use information-theoretic tools to derive a novel analysis of Multi-source\nDomain Adaptation (MDA) from the representation learning perspective.\nConcretely, we study joint distribution alignment for supervised MDA with few\ntarget labels and unsupervised MDA with pseudo labels, where the latter is\nrelatively hard and less commonly studied. We further provide\nalgorithm-dependent generalization bounds for these two settings, where the\ngeneralization is characterized by the mutual information between the\nparameters and the data. Then we propose a novel deep MDA algorithm, implicitly\naddressing the target shift through joint alignment. Finally, the mutual\ninformation bounds are extended to this algorithm providing a non-vacuous\ngradient-norm estimation. The proposed algorithm has comparable performance to\nthe state-of-the-art on target-shifted MDA benchmark with improved memory\nefficiency.\n","authors":["Qi Chen","Mario Marchand"],"pdf_url":"https://arxiv.org/pdf/2304.02064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.17037v2","updated":"2023-04-04T18:15:29Z","published":"2022-10-31T03:20:56Z","title":"FrozenQubits: Boosting Fidelity of QAOA by Skipping Hotspot Nodes","summary":"  Quantum Approximate Optimization Algorithm (QAOA) is one of the leading\ncandidates for demonstrating the quantum advantage using near-term quantum\ncomputers. Unfortunately, high device error rates limit us from reliably\nrunning QAOA circuits for problems with more than a few qubits. In QAOA, the\nproblem graph is translated into a quantum circuit such that every edge\ncorresponds to two 2-qubit CNOT operations in each layer of the circuit. As\nCNOTs are extremely error-prone, the fidelity of QAOA circuits is dictated by\nthe number of edges in the problem graph.\n  We observe that majority of graphs corresponding to real-world applications\nfollow the ``power-law`` distribution, where some hotspot nodes have\nsignificantly higher number of connections. We leverage this insight and\npropose ``FrozenQubits`` that freezes the hotspot nodes or qubits and\nintelligently partitions the state-space of the given problem into several\nsmaller sub-spaces which are then solved independently. The corresponding QAOA\nsub-circuits are significantly less vulnerable to gate and decoherence errors\ndue to the reduced number of CNOT operations in each sub-circuit. Unlike prior\ncircuit-cutting approaches, FrozenQubits does not require any exponentially\ncomplex post-processing step. Our evaluations with 5,300 QAOA circuits on eight\ndifferent quantum computers from IBM shows that FrozenQubits can improve the\nquality of solutions by 8.73x on average (and by up to 57x), albeit utilizing\n2x more quantum resources.\n","authors":["Ramin Ayanzadeh","Narges Alavisamani","Poulami Das","Moinuddin Qureshi"],"pdf_url":"https://arxiv.org/pdf/2210.17037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06246v2","updated":"2023-04-04T18:14:54Z","published":"2022-12-12T21:04:06Z","title":"Jointly Learning Visual and Auditory Speech Representations from Raw\n  Data","summary":"  We present RAVEn, a self-supervised multi-modal approach to jointly learn\nvisual and auditory speech representations. Our pre-training objective involves\nencoding masked inputs, and then predicting contextualised targets generated by\nslowly-evolving momentum encoders. Driven by the inherent differences between\nvideo and audio, our design is asymmetric w.r.t. the two modalities' pretext\ntasks: Whereas the auditory stream predicts both the visual and auditory\ntargets, the visual one predicts only the auditory targets. We observe strong\nresults in low- and high-resource labelled data settings when fine-tuning the\nvisual and auditory encoders resulting from a single pre-training stage, in\nwhich the encoders are jointly trained. Notably, RAVEn surpasses all\nself-supervised methods on visual speech recognition (VSR) on LRS3, and\ncombining RAVEn with self-training using only 30 hours of labelled data even\noutperforms a recent semi-supervised method trained on 90,000 hours of\nnon-public data. At the same time, we achieve state-of-the-art results in the\nLRS3 low-resource setting for auditory speech recognition (as well as for VSR).\nOur findings point to the viability of learning powerful speech representations\nentirely from raw video and audio, i.e., without relying on handcrafted\nfeatures. Code and models are available at https://github.com/ahaliassos/raven.\n","authors":["Alexandros Haliassos","Pingchuan Ma","Rodrigo Mira","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2212.06246v2.pdf","comment":"ICLR 2023. Code: https://github.com/ahaliassos/raven"},{"id":"http://arxiv.org/abs/2209.02525v3","updated":"2023-04-04T18:05:50Z","published":"2022-09-06T14:30:18Z","title":"Generalisation under gradient descent via deterministic PAC-Bayes","summary":"  We establish disintegrated PAC-Bayesian generalisation bounds for models\ntrained with gradient descent methods or continuous gradient flows. Contrary to\nstandard practice in the PAC-Bayesian setting, our result applies to\noptimisation algorithms that are deterministic, without requiring any\nde-randomisation step. Our bounds are fully computable, depending on the\ndensity of the initial distribution and the Hessian of the training objective\nover the trajectory. We show that our framework can be applied to a variety of\niterative optimisation algorithms, including stochastic gradient descent (SGD),\nmomentum-based schemes, and damped Hamiltonian dynamics.\n","authors":["Eugenio Clerico","Tyler Farghly","George Deligiannidis","Benjamin Guedj","Arnaud Doucet"],"pdf_url":"https://arxiv.org/pdf/2209.02525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02049v1","updated":"2023-04-04T18:01:59Z","published":"2023-04-04T18:01:59Z","title":"Multi-Class Explainable Unlearning for Image Classification via Weight\n  Filtering","summary":"  Machine Unlearning has recently been emerging as a paradigm for selectively\nremoving the impact of training datapoints from a network. While existing\napproaches have focused on unlearning either a small subset of the training\ndata or a single class, in this paper we take a different path and devise a\nframework that can unlearn all classes of an image classification network in a\nsingle untraining round. Our proposed technique learns to modulate the inner\ncomponents of an image classification network through memory matrices so that,\nafter training, the same network can selectively exhibit an unlearning behavior\nover any of the classes. By discovering weights which are specific to each of\nthe classes, our approach also recovers a representation of the classes which\nis explainable by-design. We test the proposed framework, which we name Weight\nFiltering network (WF-Net), on small-scale and medium-scale image\nclassification datasets, with both CNN and Transformer-based backbones. Our\nwork provides interesting insights in the development of explainable solutions\nfor unlearning and could be easily extended to other vision tasks.\n","authors":["Samuele Poppi","Sara Sarto","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2304.02049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02048v1","updated":"2023-04-04T18:01:56Z","published":"2023-04-04T18:01:56Z","title":"Deep Learning for Automated Experimentation in Scanning Transmission\n  Electron Microscopy","summary":"  Machine learning (ML) has become critical for post-acquisition data analysis\nin (scanning) transmission electron microscopy, (S)TEM, imaging and\nspectroscopy. An emerging trend is the transition to real-time analysis and\nclosed-loop microscope operation. The effective use of ML in electron\nmicroscopy now requires the development of strategies for microscopy-centered\nexperiment workflow design and optimization. Here, we discuss the associated\nchallenges with the transition to active ML, including sequential data analysis\nand out-of-distribution drift effects, the requirements for the edge operation,\nlocal and cloud data storage, and theory in the loop operations. Specifically,\nwe discuss the relative contributions of human scientists and ML agents in the\nideation, orchestration, and execution of experimental workflows and the need\nto develop universal hyper languages that can apply across multiple platforms.\nThese considerations will collectively inform the operationalization of ML in\nnext-generation experimentation.\n","authors":["Sergei V. Kalinin","Debangshu Mukherjee","Kevin M. Roccapriore","Ben Blaiszik","Ayana Ghosh","Maxim A. Ziatdinov","A. Al-Najjar","Christina Doty","Sarah Akers","Nageswara S. Rao","Joshua C. Agar","Steven R. Spurgeon"],"pdf_url":"https://arxiv.org/pdf/2304.02048v1.pdf","comment":"Review Article"},{"id":"http://arxiv.org/abs/2304.02034v1","updated":"2023-04-04T18:00:01Z","published":"2023-04-04T18:00:01Z","title":"Effective Theory of Transformers at Initialization","summary":"  We perform an effective-theory analysis of forward-backward signal\npropagation in wide and deep Transformers, i.e., residual neural networks with\nmulti-head self-attention blocks and multilayer perceptron blocks. This\nanalysis suggests particular width scalings of initialization and training\nhyperparameters for these models. We then take up such suggestions, training\nVision and Language Transformers in practical setups.\n","authors":["Emily Dinan","Sho Yaida","Susan Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.02034v1.pdf","comment":"64 pages, 5 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.17489v2","updated":"2023-04-04T07:39:28Z","published":"2023-03-30T16:01:28Z","title":"Prefix tuning for automated audio captioning","summary":"  Audio captioning aims to generate text descriptions from environmental\nsounds. One challenge of audio captioning is the difficulty of the\ngeneralization due to the lack of audio-text paired training data. In this\nwork, we propose a simple yet effective method of dealing with small-scaled\ndatasets by leveraging a pre-trained language model. We keep the language model\nfrozen to maintain the expressivity for text generation, and we only learn to\nextract global and temporal features from the input audio. To bridge a modality\ngap between the audio features and the language model, we employ mapping\nnetworks that translate audio features to the continuous vectors the language\nmodel can understand, called prefixes. We evaluate our proposed method on the\nClotho and AudioCaps dataset and show our method outperforms prior arts in\ndiverse experimental settings.\n","authors":["Minkyu Kim","Kim Sung-Bin","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2303.17489v2.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2304.02051v1","updated":"2023-04-04T18:03:04Z","published":"2023-04-04T18:03:04Z","title":"Multimodal Garment Designer: Human-Centric Latent Diffusion Models for\n  Fashion Image Editing","summary":"  Fashion illustration is used by designers to communicate their vision and to\nbring the design idea from conceptualization to realization, showing how\nclothes interact with the human body. In this context, computer vision can thus\nbe used to improve the fashion design process. Differently from previous works\nthat mainly focused on the virtual try-on of garments, we propose the task of\nmultimodal-conditioned fashion image editing, guiding the generation of\nhuman-centric fashion images by following multimodal prompts, such as text,\nhuman body poses, and garment sketches. We tackle this problem by proposing a\nnew architecture based on latent diffusion models, an approach that has not\nbeen used before in the fashion domain. Given the lack of existing datasets\nsuitable for the task, we also extend two existing fashion datasets, namely\nDress Code and VITON-HD, with multimodal annotations collected in a\nsemi-automatic manner. Experimental results on these new datasets demonstrate\nthe effectiveness of our proposal, both in terms of realism and coherence with\nthe given multimodal inputs. Source code and collected multimodal annotations\nwill be publicly released at:\nhttps://github.com/aimagelab/multimodal-garment-designer.\n","authors":["Alberto Baldrati","Davide Morelli","Giuseppe Cartella","Marcella Cornia","Marco Bertini","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2304.02051v1.pdf","comment":null}]},"2023-04-05T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2304.02625v1","updated":"2023-04-05T17:51:36Z","published":"2023-04-05T17:51:36Z","title":"Towards Explainable AI Writing Assistants for Non-native English\n  Speakers","summary":"  We highlight the challenges faced by non-native speakers when using AI\nwriting assistants to paraphrase text. Through an interview study with 15\nnon-native English speakers (NNESs) with varying levels of English proficiency,\nwe observe that they face difficulties in assessing paraphrased texts generated\nby AI writing assistants, largely due to the lack of explanations accompanying\nthe suggested paraphrases. Furthermore, we examine their strategies to assess\nAI-generated texts in the absence of such explanations. Drawing on the needs of\nNNESs identified in our interview, we propose four potential user interfaces to\nenhance the writing experience of NNESs using AI writing assistants. The\nproposed designs focus on incorporating explanations to better support NNESs in\nunderstanding and evaluating the AI-generated paraphrasing suggestions.\n","authors":["Yewon Kim","Mina Lee","Donghwi Kim","Sung-Ju Lee"],"pdf_url":"https://arxiv.org/pdf/2304.02625v1.pdf","comment":"CHI In2Writing Workshop 2023 camera-ready version"},{"id":"http://arxiv.org/abs/2304.02623v1","updated":"2023-04-05T17:47:11Z","published":"2023-04-05T17:47:11Z","title":"Beyond Summarization: Designing AI Support for Real-World Expository\n  Writing Tasks","summary":"  Large language models have introduced exciting new opportunities and\nchallenges in designing and developing new AI-assisted writing support tools.\nRecent work has shown that leveraging this new technology can transform writing\nin many scenarios such as ideation during creative writing, editing support,\nand summarization. However, AI-supported expository writing--including\nreal-world tasks like scholars writing literature reviews or doctors writing\nprogress notes--is relatively understudied. In this position paper, we argue\nthat developing AI supports for expository writing has unique and exciting\nresearch challenges and can lead to high real-world impacts. We characterize\nexpository writing as evidence-based and knowledge-generating: it contains\nsummaries of external documents as well as new information or knowledge. It can\nbe seen as the product of authors' sensemaking process over a set of source\ndocuments, and the interplay between reading, reflection, and writing opens up\nnew opportunities for designing AI support. We sketch three components for AI\nsupport design and discuss considerations for future research.\n","authors":["Zejiang Shen","Tal August","Pao Siangliulue","Kyle Lo","Jonathan Bragg","Jeff Hammerbacher","Doug Downey","Joseph Chee Chang","David Sontag"],"pdf_url":"https://arxiv.org/pdf/2304.02623v1.pdf","comment":"3 pages, 1 figure, accepted by The Second Workshop on Intelligent and\n  Interactive Writing Assistants"},{"id":"http://arxiv.org/abs/2212.07983v2","updated":"2023-04-05T17:41:12Z","published":"2022-12-15T17:31:54Z","title":"Vision Transformers are Parameter-Efficient Audio-Visual Learners","summary":"  Vision transformers (ViTs) have achieved impressive results on various\ncomputer vision tasks in the last several years. In this work, we study the\ncapability of frozen ViTs, pretrained only on visual data, to generalize to\naudio-visual data without finetuning any of its original parameters. To do so,\nwe propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained\nViTs to audio-visual tasks by injecting a small number of trainable parameters\ninto every layer of a frozen ViT. To efficiently fuse visual and audio cues,\nour LAVISH adapter uses a small set of latent tokens, which form an attention\nbottleneck, thus, eliminating the quadratic cost of standard cross-attention.\nCompared to the existing modality-specific audio-visual methods, our approach\nachieves competitive or even better performance on various audio-visual tasks\nwhile using fewer tunable parameters and without relying on costly audio\npretraining or external audio encoders. Our code is available at\nhttps://genjib.github.io/project_page/LAVISH/\n","authors":["Yan-Bo Lin","Yi-Lin Sung","Jie Lei","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2212.07983v2.pdf","comment":"CVPR 2023 Project Page: https://genjib.github.io/project_page/LAVISH/"},{"id":"http://arxiv.org/abs/2301.07088v2","updated":"2023-04-05T16:22:17Z","published":"2023-01-17T18:53:24Z","title":"Vision Learners Meet Web Image-Text Pairs","summary":"  Most recent self-supervised learning methods are pre-trained on the\nwell-curated ImageNet-1K dataset. In this work, given the excellent scalability\nof web data, we consider self-supervised pre-training on noisy web sourced\nimage-text paired data. First, we conduct a benchmark study of representative\nself-supervised pre-training methods on large-scale web data in a like-for-like\nsetting. We compare a range of methods, including single-modal ones that use\nmasked training objectives and multi-modal ones that use image-text\nconstrastive training. We observe that existing multi-modal methods do not\noutperform their single-modal counterparts on vision transfer learning tasks.\nWe derive an information-theoretical view to explain these benchmark results,\nwhich provides insight into how to design a novel vision learner. Inspired by\nthis insight, we present a new visual representation pre-training method,\nMUlti-modal Generator~(MUG), that learns from scalable web sourced image-text\ndata. MUG achieves state-of-the-art transfer performance on a variety of tasks\nand demonstrates promising scaling properties. Pre-trained models and code will\nbe made public upon acceptance.\n","authors":["Bingchen Zhao","Quan Cui","Hao Wu","Osamu Yoshie","Cheng Yang","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2301.07088v2.pdf","comment":"Project page: https://bzhao.me/MUG/"},{"id":"http://arxiv.org/abs/2304.02554v1","updated":"2023-04-05T16:17:32Z","published":"2023-04-05T16:17:32Z","title":"Human-like Summarization Evaluation with ChatGPT","summary":"  Evaluating text summarization is a challenging problem, and existing\nevaluation metrics are far from satisfactory. In this study, we explored\nChatGPT's ability to perform human-like summarization evaluation using four\nhuman evaluation methods on five datasets. We found that ChatGPT was able to\ncomplete annotations relatively smoothly using Likert scale scoring, pairwise\ncomparison, Pyramid, and binary factuality evaluation. Additionally, it\noutperformed commonly used automatic evaluation metrics on some datasets.\nFurthermore, we discussed the impact of different prompts, compared its\nperformance with that of human evaluation, and analyzed the generated\nexplanations and invalid responses.\n","authors":["Mingqi Gao","Jie Ruan","Renliang Sun","Xunjian Yin","Shiping Yang","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2304.02554v1.pdf","comment":"9 pages, 5 figures, in process"},{"id":"http://arxiv.org/abs/2304.02541v1","updated":"2023-04-05T16:03:42Z","published":"2023-04-05T16:03:42Z","title":"PWESuite: Phonetic Word Embeddings and Tasks They Facilitate","summary":"  Word embeddings that map words into a fixed-dimensional vector space are the\nbackbone of modern NLP. Most word embedding methods encode semantic\ninformation. However, phonetic information, which is important for some tasks,\nis often overlooked. In this work, we develop several novel methods which\nleverage articulatory features to build phonetically informed word embeddings,\nand present a set of phonetic word embeddings to encourage their community\ndevelopment, evaluation and use. While several methods for learning phonetic\nword embeddings already exist, there is a lack of consistency in evaluating\ntheir effectiveness. Thus, we also proposes several ways to evaluate both\nintrinsic aspects of phonetic word embeddings, such as word retrieval and\ncorrelation with sound similarity, and extrinsic performances, such as rhyme\nand cognate detection and sound analogies. We hope that our suite of tasks will\npromote reproducibility and provide direction for future research on phonetic\nword embeddings.\n","authors":["Vilém Zouhar","Kalvin Chang","Chenxuan Cui","Nathaniel Carlson","Nathaniel Robinson","Mrinmaya Sachan","David Mortensen"],"pdf_url":"https://arxiv.org/pdf/2304.02541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01492v2","updated":"2023-04-05T15:18:13Z","published":"2023-04-04T03:13:03Z","title":"A Unified Contrastive Transfer Framework with Propagation Structure for\n  Boosting Low-Resource Rumor Detection","summary":"  The truth is significantly hampered by massive rumors that spread along with\nbreaking news or popular topics. Since there is sufficient corpus gathered from\nthe same domain for model training, existing rumor detection algorithms show\npromising performance on yesterday's news. However, due to a lack of training\ndata and prior expert knowledge, they are poor at spotting rumors concerning\nunforeseen events, especially those propagated in different languages (i.e.,\nlow-resource regimes). In this paper, we propose a unified contrastive transfer\nframework to detect rumors by adapting the features learned from well-resourced\nrumor data to that of the low-resourced. More specifically, we first represent\nrumor circulated on social media as an undirected topology, and then train a\nMulti-scale Graph Convolutional Network via a unified contrastive paradigm. Our\nmodel explicitly breaks the barriers of the domain and/or language issues, via\nlanguage alignment and a novel domain-adaptive contrastive learning mechanism.\nTo enhance the representation learning from a small set of target events, we\nreveal that rumor-indicative signal is closely correlated with the uniformity\nof the distribution of these events. We design a target-wise contrastive\ntraining mechanism with three data augmentation strategies, capable of unifying\nthe representations by distinguishing target events. Extensive experiments\nconducted on four low-resource datasets collected from real-world microblog\nplatforms demonstrate that our framework achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for detecting rumors\nat early stages.\n","authors":["Hongzhan Lin","Jing Ma","Ruichao Yang","Zhiwei Yang","Mingfei Cheng"],"pdf_url":"https://arxiv.org/pdf/2304.01492v2.pdf","comment":"A significant extension of the first contrastive approach for\n  low-resource rumor detection (arXiv:2204.08143)"},{"id":"http://arxiv.org/abs/2304.02496v1","updated":"2023-04-05T15:11:25Z","published":"2023-04-05T15:11:25Z","title":"Evaluation of ChatGPT Family of Models for Biomedical Reasoning and\n  Classification","summary":"  Recent advances in large language models (LLMs) have shown impressive ability\nin biomedical question-answering, but have not been adequately investigated for\nmore specific biomedical applications. This study investigates the performance\nof LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical\ntasks beyond question-answering. Because no patient data can be passed to the\nOpenAI API public interface, we evaluated model performance with over 10000\nsamples as proxies for two fundamental tasks in the clinical domain -\nclassification and reasoning. The first task is classifying whether statements\nof clinical and policy recommendations in scientific literature constitute\nhealth advice. The second task is causal relation detection from the biomedical\nliterature. We compared LLMs with simpler models, such as bag-of-words (BoW)\nwith logistic regression, and fine-tuned BioBERT models. Despite the excitement\naround viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks\nremained the best strategy. The simple BoW model performed on par with the most\ncomplex LLM prompting. Prompt engineering required significant investment.\n","authors":["Shan Chen","Yingya Li","Sheng Lu","Hoang Van","Hugo JWL Aerts","Guergana K. Savova","Danielle S. Bitterman"],"pdf_url":"https://arxiv.org/pdf/2304.02496v1.pdf","comment":"28 pages, 2 tables and 4 figures. Submitting for review"},{"id":"http://arxiv.org/abs/2304.02492v1","updated":"2023-04-05T15:08:21Z","published":"2023-04-05T15:08:21Z","title":"Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic\n  Complexity in Verb Acquisition","summary":"  Children typically learn the meanings of nouns earlier than the meanings of\nverbs. However, it is unclear whether this asymmetry is a result of complexity\nin the visual structure of categories in the world to which language refers,\nthe structure of language itself, or the interplay between the two sources of\ninformation. We quantitatively test these three hypotheses regarding early verb\nlearning by employing visual and linguistic representations of words sourced\nfrom large-scale pre-trained artificial neural networks. Examining the\nstructure of both visual and linguistic embedding spaces, we find, first, that\nthe representation of verbs is generally more variable and less discriminable\nwithin domain than the representation of nouns. Second, we find that if only\none learning instance per category is available, visual and linguistic\nrepresentations are less well aligned in the verb system than in the noun\nsystem. However, in parallel with the course of human language development, if\nmultiple learning instances per category are available, visual and linguistic\nrepresentations become almost as well aligned in the verb system as in the noun\nsystem. Third, we compare the relative contributions of factors that may\npredict learning difficulty for individual words. A regression analysis reveals\nthat visual variability is the strongest factor that internally drives verb\nlearning, followed by visual-linguistic alignment and linguistic variability.\nBased on these results, we conclude that verb acquisition is influenced by all\nthree sources of complexity, but that the variability of visual structure poses\nthe most significant challenge for verb learning.\n","authors":["Yuchen Zhou","Michael J. Tarr","Daniel Yurovsky"],"pdf_url":"https://arxiv.org/pdf/2304.02492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01240v2","updated":"2023-04-05T14:02:53Z","published":"2023-04-03T11:56:11Z","title":"Identifying Mentions of Pain in Mental Health Records Text: A Natural\n  Language Processing Approach","summary":"  Pain is a common reason for accessing healthcare resources and is a growing\narea of research, especially in its overlap with mental health. Mental health\nelectronic health records are a good data source to study this overlap.\nHowever, much information on pain is held in the free text of these records,\nwhere mentions of pain present a unique natural language processing problem due\nto its ambiguous nature. This project uses data from an anonymised mental\nhealth electronic health records database. The data are used to train a machine\nlearning based classification algorithm to classify sentences as discussing\npatient pain or not. This will facilitate the extraction of relevant pain\ninformation from large databases, and the use of such outputs for further\nstudies on pain and mental health. 1,985 documents were manually\ntriple-annotated for creation of gold standard training data, which was used to\ntrain three commonly used classification algorithms. The best performing model\nachieved an F1-score of 0.98 (95% CI 0.98-0.99).\n","authors":["Jaya Chaturvedi","Sumithra Velupillai","Robert Stewart","Angus Roberts"],"pdf_url":"https://arxiv.org/pdf/2304.01240v2.pdf","comment":"5 pages, 2 tables, submitted to MEDINFO 2023 conference"},{"id":"http://arxiv.org/abs/2304.01238v2","updated":"2023-04-05T13:38:54Z","published":"2023-04-03T10:27:53Z","title":"Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam\n  Detection","summary":"  This paper investigates the effectiveness of large language models (LLMs) in\nemail spam detection by comparing prominent models from three distinct\nfamilies: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we\nexamine well-established machine learning techniques for spam detection, such\nas Na\\\"ive Bayes and LightGBM, as baseline methods. We assess the performance\nof these models across four public datasets, utilizing different numbers of\ntraining samples (full training set and few-shot settings). Our findings reveal\nthat, in the majority of cases, LLMs surpass the performance of the popular\nbaseline techniques, particularly in few-shot scenarios. This adaptability\nrenders LLMs uniquely suited to spam detection tasks, where labeled samples are\nlimited in number and models require frequent updates. Additionally, we\nintroduce Spam-T5, a Flan-T5 model that has been specifically adapted and\nfine-tuned for the purpose of detecting email spam. Our results demonstrate\nthat Spam-T5 surpasses baseline models and other LLMs in the majority of\nscenarios, particularly when there are a limited number of training samples\navailable. Our code is publicly available at\nhttps://github.com/jpmorganchase/emailspamdetection.\n","authors":["Maxime Labonne","Sean Moran"],"pdf_url":"https://arxiv.org/pdf/2304.01238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11337v3","updated":"2023-04-05T13:38:28Z","published":"2022-11-21T10:37:56Z","title":"DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via\n  Positive-Negative Prompt-Tuning","summary":"  Large-scale text-to-image generation models have achieved remarkable progress\nin synthesizing high-quality, feature-rich images with high resolution guided\nby texts. However, these models often struggle with novel concepts, eg, new\nstyles, object entities, etc. Although recent attempts have employed\nfine-tuning or prompt-tuning strategies to teach the pre-trained diffusion\nmodel novel concepts from a reference image set,they have the drawback of\noverfitting to the given reference images, particularly in one-shot\napplications, which is harmful to generate diverse and high-quality images\nwhile maintaining generation controllability.\n  To tackle this challenge, we present a simple yet effective method called\nDreamArtist, which employs a positive-negative prompt-tuning learning strategy.\nSpecifically, DreamArtist incorporates both positive and negative embeddings\nand jointly trains them. The positive embedding aggressively captures the\nsalient characteristics of the reference image to drive diversified generation\nand the negative embedding rectifies inadequacies from the positive embedding.\nIt learns not only what is correct, but also what can be avoided or improved.\nWe have conducted extensive experiments and evaluated the proposed method from\nimage similarity and diversity, generation controllability, and style cloning.\nAnd our DreamArtist has achieved a superior generation performance over\nexisting methods. Besides, our additional evaluation on extended tasks,\nincluding concept compositions and prompt-guided image editing, demonstrates\nits effectiveness for more applications.\n","authors":["Ziyi Dong","Pengxu Wei","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2211.11337v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15413v2","updated":"2023-04-05T13:33:55Z","published":"2023-03-27T17:31:13Z","title":"Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D\n  Generation","summary":"  The view inconsistency problem in score-distilling text-to-3D generation,\nalso known as the Janus problem, arises from the intrinsic bias of 2D diffusion\nmodels, which leads to the unrealistic generation of 3D objects. In this work,\nwe explore score-distilling text-to-3D generation and identify the main causes\nof the Janus problem. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for robust text-to-3D generation. Our\nfirst approach, called score debiasing, involves gradually increasing the\ntruncation value for the score estimated by 2D diffusion models throughout the\noptimization process. Our second approach, called prompt debiasing, identifies\nconflicting words between user prompts and view prompts utilizing a language\nmodel and adjusts the discrepancy between view prompts and object-space camera\nposes. Our experimental results show that our methods improve realism by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead.\n","authors":["Susung Hong","Donghoon Ahn","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15413v2.pdf","comment":"CVPR 2023 GCV workshop"},{"id":"http://arxiv.org/abs/2304.02426v1","updated":"2023-04-05T13:12:00Z","published":"2023-04-05T13:12:00Z","title":"ParroT: Translating During Chat Using Large Language Models","summary":"  Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable\nabilities on a wide range of natural language processing (NLP) tasks, including\nvarious machine translation abilities accomplished during chat. However, these\nmodels are only accessible through restricted APIs, which creates barriers to\nnew research and advancements in the field. Therefore, we propose the\n$\\mathbf{ParroT}$ framework to enhance and regulate the translation abilities\nduring chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written\ntranslation and evaluation data. Specifically, ParroT reformulates translation\ndata into the instruction-following style, and introduces a \"Hint\" field for\nincorporating extra requirements to regulate the translation process.\nAccordingly, we propose three instruction types for finetuning ParroT models,\nincluding translation instruction, contrastive instruction, and error-guided\ninstruction. Experiments on two Flores subsets and WMT22 test sets suggest that\ntranslation instruction improves the translation performance of vanilla LLMs\nsignificantly while error-guided instruction can lead to a further improvement,\nwhich demonstrates the importance of learning from low-quality translations\nannotated by human. Meanwhile, the ParroT models can also preserve the ability\non general tasks with the Alpaca multi-task dataset involved in finetuning.\nCodes: https://github.com/wxjiao/ParroT\n","authors":["Wenxiang Jiao","Jen-tse Huang","Wenxuan Wang","Xing Wang","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2304.02426v1.pdf","comment":"9 pages; translate during chat"},{"id":"http://arxiv.org/abs/2304.00830v2","updated":"2023-04-05T12:13:48Z","published":"2023-04-03T09:15:51Z","title":"AUDIT: Audio Editing by Following Instructions with Latent Diffusion\n  Models","summary":"  Audio editing is applicable for various purposes, such as adding background\nsound effects, replacing a musical instrument, and repairing damaged audio.\nRecently, some diffusion-based methods achieved zero-shot audio editing by\nusing a diffusion and denoising process conditioned on the text description of\nthe output audio. However, these methods still have some problems: 1) they have\nnot been trained on editing tasks and cannot ensure good editing effects; 2)\nthey can erroneously modify audio segments that do not require editing; 3) they\nneed a complete description of the output audio, which is not always available\nor necessary in practical scenarios. In this work, we propose AUDIT, an\ninstruction-guided audio editing model based on latent diffusion models.\nSpecifically, AUDIT has three main design features: 1) we construct triplet\ntraining data (instruction, input audio, output audio) for different audio\nediting tasks and train a diffusion model using instruction and input (to be\nedited) audio as conditions and generating output (edited) audio; 2) it can\nautomatically learn to only modify segments that need to be edited by comparing\nthe difference between the input and output audio; 3) it only needs edit\ninstructions instead of full target audio descriptions as text input. AUDIT\nachieves state-of-the-art results in both objective and subjective metrics for\nseveral audio editing tasks (e.g., adding, dropping, replacement, inpainting,\nsuper-resolution). Demo samples are available at https://audit-demo.github.io/.\n","authors":["Yuancheng Wang","Zeqian Ju","Xu Tan","Lei He","Zhizheng Wu","Jiang Bian","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.00830v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.10384v5","updated":"2023-04-05T09:52:59Z","published":"2022-08-22T15:03:31Z","title":"The optimality of word lengths. Theoretical foundations and an empirical\n  study","summary":"  Zipf's law of abbreviation, namely the tendency of more frequent words to be\nshorter, has been viewed as a manifestation of compression, i.e. the\nminimization of the length of forms -- a universal principle of natural\ncommunication. Although the claim that languages are optimized has become\ntrendy, attempts to measure the degree of optimization of languages have been\nrather scarce. Here we present two optimality scores that are dualy normalized,\nnamely, they are normalized with respect to both the minimum and the random\nbaseline. We analyze the theoretical and statistical pros and cons of these and\nother scores. Harnessing the best score, we quantify for the first time the\ndegree of optimality of word lengths in languages. This indicates that\nlanguages are optimized to 62 or 67 percent on average (depending on the\nsource) when word lengths are measured in characters, and to 65 percent on\naverage when word lengths are measured in time. In general, spoken word\ndurations are more optimized than written word lengths in characters. Our work\npaves the way to measure the degree of optimality of the vocalizations or\ngestures of other species, and to compare them against written, spoken, or\nsigned human languages.\n","authors":["Sonia Petrini","Antoni Casas-i-Muñoz","Jordi Cluet-i-Martinell","Mengxue Wang","Christian Bentz","Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2208.10384v5.pdf","comment":"On the one hand, the article has been reduced: analyses of the law of\n  abbreviation and some of the methods have been moved to another article;\n  appendix B has been reduced. On the other hand, various parts have been\n  rewritten for clarity; new figures have been added to ease the understanding\n  of the scores; new citations added. Many typos have been corrected"},{"id":"http://arxiv.org/abs/2303.03750v2","updated":"2023-04-05T09:39:32Z","published":"2023-03-07T09:20:09Z","title":"Preparing the Vuk'uzenzele and ZA-gov-multilingual South African\n  multilingual corpora","summary":"  This paper introduces two multilingual government themed corpora in various\nSouth African languages. The corpora were collected by gathering the South\nAfrican Government newspaper (Vuk'uzenzele), as well as South African\ngovernment speeches (ZA-gov-multilingual), that are translated into all 11\nSouth African official languages. The corpora can be used for a myriad of\ndownstream NLP tasks. The corpora were created to allow researchers to study\nthe language used in South African government publications, with a focus on\nunderstanding how South African government officials communicate with their\nconstituents. In this paper we highlight the process of gathering, cleaning and\nmaking available the corpora. We create parallel sentence corpora for Neural\nMachine Translation (NMT) tasks using Language-Agnostic Sentence\nRepresentations (LASER) embeddings. With these aligned sentences we then\nprovide NMT benchmarks for 9 indigenous languages by fine-tuning a massively\nmultilingual pre-trained language model.\n","authors":["Richard Lastrucci","Isheanesu Dzingirai","Jenalea Rajab","Andani Madodonga","Matimba Shingange","Daniel Njini","Vukosi Marivate"],"pdf_url":"https://arxiv.org/pdf/2303.03750v2.pdf","comment":"Accepted and to appear at Fourth workshop on Resources for African\n  Indigenous Languages (RAIL) at EACL 2023"},{"id":"http://arxiv.org/abs/2304.02328v1","updated":"2023-04-05T09:32:25Z","published":"2023-04-05T09:32:25Z","title":"Enhancing Multimodal Entity and Relation Extraction with Variational\n  Information Bottleneck","summary":"  This paper studies the multimodal named entity recognition (MNER) and\nmultimodal relation extraction (MRE), which are important for multimedia social\nplatform analysis. The core of MNER and MRE lies in incorporating evident\nvisual information to enhance textual semantics, where two issues inherently\ndemand investigations. The first issue is modality-noise, where the\ntask-irrelevant information in each modality may be noises misleading the task\nprediction. The second issue is modality-gap, where representations from\ndifferent modalities are inconsistent, preventing from building the semantic\nalignment between the text and image. To address these issues, we propose a\nnovel method for MNER and MRE by Multi-Modal representation learning with\nInformation Bottleneck (MMIB). For the first issue, a refinement-regularizer\nprobes the information-bottleneck principle to balance the predictive evidence\nand noisy information, yielding expressive representations for prediction. For\nthe second issue, an alignment-regularizer is proposed, where a mutual\ninformation-based item works in a contrastive manner to regularize the\nconsistent text-image representations. To our best knowledge, we are the first\nto explore variational IB estimation for MNER and MRE. Experiments show that\nMMIB achieves the state-of-the-art performances on three public benchmarks.\n","authors":["Shiyao Cui","Jiangxia Cao","Xin Cong","Jiawei Sheng","Quangang Li","Tingwen Liu","Jinqiao Shi"],"pdf_url":"https://arxiv.org/pdf/2304.02328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01352v2","updated":"2023-04-05T09:23:17Z","published":"2023-04-03T20:27:10Z","title":"A Simple and Effective Method of Cross-Lingual Plagiarism Detection","summary":"  We present a simple cross-lingual plagiarism detection method applicable to a\nlarge number of languages. The presented approach leverages open multilingual\nthesauri for candidate retrieval task and pre-trained multilingual BERT-based\nlanguage models for detailed analysis. The method does not rely on machine\ntranslation and word sense disambiguation when in use, and therefore is\nsuitable for a large number of languages, including under-resourced languages.\nThe effectiveness of the proposed approach is demonstrated for several existing\nand new benchmarks, achieving state-of-the-art results for French, Russian, and\nArmenian languages.\n","authors":["Karen Avetisyan","Arthur Malajyan","Tsolak Ghukasyan","Arutyun Avetisyan"],"pdf_url":"https://arxiv.org/pdf/2304.01352v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02313v1","updated":"2023-04-05T09:09:10Z","published":"2023-04-05T09:09:10Z","title":"Personality-aware Human-centric Multimodal Reasoning: A New Task","summary":"  Multimodal reasoning, an area of artificial intelligence that aims at make\ninferences from multimodal signals such as vision, language and speech, has\ndrawn more and more attention in recent years. People with different\npersonalities may respond differently to the same situation. However, such\nindividual personalities were ignored in the previous studies. In this work, we\nintroduce a new Personality-aware Human-centric Multimodal Reasoning\n(Personality-aware HMR) task, and accordingly construct a new dataset based on\nThe Big Bang Theory television shows, to predict the behavior of a specific\nperson at a specific moment, given the multimodal information of its past and\nfuture moments. The Myers-Briggs Type Indicator (MBTI) was annotated and\nutilized in the task to represent individuals' personalities. We benchmark the\ntask by proposing three baseline methods, two were adapted from the related\ntasks and one was newly proposed for our task. The experimental results\ndemonstrate that personality can effectively improve the performance of\nhuman-centric multimodal reasoning. To further solve the lack of personality\nannotation in real-life scenes, we introduce an extended task called\nPersonality-predicted HMR, and propose the corresponding methods, to predict\nthe MBTI personality at first, and then use the predicted personality to help\nmultimodal reasoning. The experimental results show that our method can\naccurately predict personality and achieves satisfactory multimodal reasoning\nperformance without relying on personality annotations.\n","authors":["Yaochen Zhu","Xiangqing Shen","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2304.02313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11303v2","updated":"2023-04-05T09:01:21Z","published":"2022-08-24T05:18:23Z","title":"Modeling Paragraph-Level Vision-Language Semantic Alignment for\n  Multi-Modal Summarization","summary":"  Most current multi-modal summarization methods follow a cascaded manner,\nwhere an off-the-shelf object detector is first used to extract visual\nfeatures, then these features are fused with language representations to\ngenerate the summary with an encoder-decoder model. The cascaded way cannot\ncapture the semantic alignments between images and paragraphs, which are\ncrucial to a precise summary. In this paper, we propose ViL-Sum to jointly\nmodel paragraph-level \\textbf{Vi}sion-\\textbf{L}anguage Semantic Alignment and\nMulti-Modal \\textbf{Sum}marization. The core of ViL-Sum is a joint multi-modal\nencoder with two well-designed tasks, image reordering and image selection. The\njoint multi-modal encoder captures the interactions between modalities, where\nthe reordering task guides the model to learn paragraph-level semantic\nalignment and the selection task guides the model to selected summary-related\nimages in the final summary. Experimental results show that our proposed\nViL-Sum significantly outperforms current state-of-the-art methods. In further\nanalysis, we find that two well-designed tasks and joint multi-modal encoder\ncan effectively guide the model to learn reasonable paragraphs-images and\nsummary-images relations.\n","authors":["Chenhao Cui","Xinnian Liang","Shuangzhi Wu","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2208.11303v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11593v2","updated":"2023-04-05T08:58:30Z","published":"2023-03-21T04:47:45Z","title":"Difficulty in learning chirality for Transformer fed with SMILES","summary":"  Recent years have seen development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this, we\ninvestigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. The\nresults suggest that while the Transformer learns partial structures of\nmolecules quickly, it requires extended training to understand overall\nstructures. Consistently, the accuracy of molecular property predictions using\ndescriptors generated from models at different learning steps was similar from\nthe beginning to the end of training. Furthermore, we found that the\nTransformer requires particularly long training to learn chirality and\nsometimes stagnates with low translation accuracy due to misunderstanding of\nenantiomers. These findings are expected to deepen understanding of NLP models\nin chemistry.\n","authors":["Yasuhiro Yoshikai","Tadahaya Mizuno","Shumpei Nemoto","Hiroyuki Kusuhara"],"pdf_url":"https://arxiv.org/pdf/2303.11593v2.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.13072v2","updated":"2023-04-05T08:36:34Z","published":"2023-03-23T06:54:37Z","title":"Beyond Universal Transformer: block reusing with adaptor in Transformer\n  for automatic speech recognition","summary":"  Transformer-based models have recently made significant achievements in the\napplication of end-to-end (E2E) automatic speech recognition (ASR). It is\npossible to deploy the E2E ASR system on smart devices with the help of\nTransformer-based models. While these models still have the disadvantage of\nrequiring a large number of model parameters. To overcome the drawback of\nuniversal Transformer models for the application of ASR on edge devices, we\npropose a solution that can reuse the block in Transformer models for the\noccasion of the small footprint ASR system, which meets the objective of\naccommodating resource limitations without compromising recognition accuracy.\nSpecifically, we design a novel block-reusing strategy for speech Transformer\n(BRST) to enhance the effectiveness of parameters and propose an adapter module\n(ADM) that can produce a compact and adaptable model with only a few additional\ntrainable parameters accompanying each reusing block. We conducted an\nexperiment with the proposed method on the public AISHELL-1 corpus, and the\nresults show that the proposed approach achieves the character error rate (CER)\nof 9.3%/6.63% with only 7.6M/8.3M parameters without and with the ADM,\nrespectively. In addition, we also make a deeper analysis to show the effect of\nADM in the general block-reusing method.\n","authors":["Haoyu Tang","Zhaoyi Liu","Chang Zeng","Xinfeng Li"],"pdf_url":"https://arxiv.org/pdf/2303.13072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2009.13964v5","updated":"2023-04-05T07:55:56Z","published":"2020-09-29T12:29:04Z","title":"CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced\n  Pre-Trained Language Models","summary":"  Several recent efforts have been devoted to enhancing pre-trained language\nmodels (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs\n(KGs) and achieved consistent improvements on various knowledge-driven NLP\ntasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs\nof KGs (\"knowledge context\"), regardless of that the knowledge required by PLMs\nmay change dynamically according to specific text (\"textual context\"). In this\npaper, we propose a novel framework named Coke to dynamically select contextual\nknowledge and embed knowledge context according to textual context for PLMs,\nwhich can avoid the effect of redundant and ambiguous knowledge in KGs that\ncannot match the input text. Our experimental results show that Coke\noutperforms various baselines on typical knowledge-driven NLP tasks, indicating\nthe effectiveness of utilizing dynamic knowledge context for language\nunderstanding. Besides the performance improvements, the dynamically selected\nknowledge in Coke can describe the semantics of text-related knowledge in a\nmore interpretable form than the conventional PLMs. Our source code and\ndatasets will be available to provide more details for Coke.\n","authors":["Yusheng Su","Xu Han","Zhengyan Zhang","Peng Li","Zhiyuan Liu","Yankai Lin","Jie Zhou","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2009.13964v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.18027v2","updated":"2023-04-05T07:53:37Z","published":"2023-03-31T13:04:47Z","title":"Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations","summary":"  As large language models (LLMs) gain popularity among speakers of diverse\nlanguages, we believe that it is crucial to benchmark them to better understand\nmodel behaviors, failures, and limitations in languages beyond English. In this\nwork, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national\nmedical licensing examinations from the past five years, including the current\nyear. Our team comprises native Japanese-speaking NLP researchers and a\npracticing cardiologist based in Japan. Our experiments show that GPT-4\noutperforms ChatGPT and GPT-3 and passes all six years of the exams,\nhighlighting LLMs' potential in a language that is typologically distant from\nEnglish. However, our evaluation also exposes critical limitations of the\ncurrent LLM APIs. First, LLMs sometimes select prohibited choices that should\nbe strictly avoided in medical practice in Japan, such as suggesting\neuthanasia. Further, our analysis shows that the API costs are generally higher\nand the maximum context size is smaller for Japanese because of the way\nnon-Latin scripts are currently tokenized in the pipeline. We release our\nbenchmark as Igaku QA as well as all model outputs and exam metadata. We hope\nthat our results and benchmark will spur progress on more diverse applications\nof LLMs. Our benchmark is available at https://github.com/jungokasai/IgakuQA.\n","authors":["Jungo Kasai","Yuhei Kasai","Keisuke Sakaguchi","Yutaro Yamada","Dragomir Radev"],"pdf_url":"https://arxiv.org/pdf/2303.18027v2.pdf","comment":"Added results from the March 2023 exam"},{"id":"http://arxiv.org/abs/2210.08954v2","updated":"2023-04-05T07:23:29Z","published":"2022-08-27T06:54:58Z","title":"Conversion of Legal Agreements into Smart Legal Contracts using NLP","summary":"  A Smart Legal Contract (SLC) is a specialized digital agreement comprising\nnatural language and computable components. The Accord Project provides an\nopen-source SLC framework containing three main modules: Cicero, Concerto, and\nErgo. Currently, we need lawyers, programmers, and clients to work together\nwith great effort to create a usable SLC using the Accord Project. This paper\nproposes a pipeline to automate the SLC creation process with several Natural\nLanguage Processing (NLP) models to convert law contracts to the Accord\nProject's Concerto model. After evaluating the proposed pipeline, we discovered\nthat our NER pipeline accurately detects CiceroMark from Accord Project\ntemplate text with an accuracy of 0.8. Additionally, our Question Answering\nmethod can extract one-third of the Concerto variables from the template text.\nWe also delve into some limitations and possible future research for the\nproposed pipeline. Finally, we describe a web interface enabling users to build\nSLCs. This interface leverages the proposed pipeline to convert text documents\nto Smart Legal Contracts by using NLP models.\n","authors":["Eason Chen","Niall Roche","Yuen-Hsien Tseng","Walter Hernandez","Jiangbo Shangguan","Alastair Moore"],"pdf_url":"https://arxiv.org/pdf/2210.08954v2.pdf","comment":"7 pages, Companion Proceedings of the ACM Web Conference 2023 (WWW\n  '23 Companion), April 30-May 4, 2023, Austin, TX, USA"},{"id":"http://arxiv.org/abs/2304.01890v2","updated":"2023-04-05T06:37:05Z","published":"2023-04-04T15:42:08Z","title":"Sociocultural knowledge is needed for selection of shots in hate speech\n  detection tasks","summary":"  We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for\nthe countries of Brazil, Germany, India and Kenya, to aid training and\ninterpretability of models. We demonstrate how our lexicon can be used to\ninterpret model predictions, showing that models developed to classify extreme\nspeech rely heavily on target words when making predictions. Further, we\npropose a method to aid shot selection for training in low-resource settings\nvia HATELEXICON. In few-shot learning, the selection of shots is of paramount\nimportance to model performance. In our work, we simulate a few-shot setting\nfor German and Hindi, using HASOC data for training and the Multilingual\nHateCheck (MHC) as a benchmark. We show that selecting shots based on our\nlexicon leads to models performing better on MHC than models trained on shots\nsampled randomly. Thus, when given only a few training examples, using our\nlexicon to select shots containing more sociocultural information leads to\nbetter few-shot performance.\n","authors":["Antonis Maronikolakis","Abdullatif Köksal","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2304.01890v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02247v1","updated":"2023-04-05T06:35:41Z","published":"2023-04-05T06:35:41Z","title":"Disentangling Structure and Style: Political Bias Detection in News by\n  Inducing Document Hierarchy","summary":"  We address an important gap in detection of political bias in news articles.\nPrevious works that perform supervised document classification can be biased\ntowards the writing style of each news outlet, leading to overfitting and\nlimited generalizability. Our approach overcomes this limitation by considering\nboth the sentence-level semantics and the document-level rhetorical structure,\nresulting in a more robust and style-agnostic approach to detecting political\nbias in news articles. We introduce a novel multi-head hierarchical attention\nmodel that effectively encodes the structure of long documents through a\ndiverse ensemble of attention heads. While journalism follows a formalized\nrhetorical structure, the writing style may vary by news outlet. We demonstrate\nthat our method overcomes this domain dependency and outperforms previous\napproaches for robustness and accuracy. Further analysis demonstrates the\nability of our model to capture the discourse structures commonly used in the\njournalism domain.\n","authors":["Jiwoo Hong","Yejin Cho","Jaemin Jung","Jiyoung Han","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2304.02247v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2202.03086v4","updated":"2023-04-05T06:30:14Z","published":"2022-02-07T11:54:07Z","title":"Machine Translation from Signed to Spoken Languages: State of the Art\n  and Challenges","summary":"  Automatic translation from signed to spoken languages is an interdisciplinary\nresearch domain, lying on the intersection of computer vision, machine\ntranslation and linguistics. Nevertheless, research in this domain is performed\nmostly by computer scientists in isolation. As the domain is becoming\nincreasingly popular - the majority of scientific papers on the topic of sign\nlanguage translation have been published in the past three years - we provide\nan overview of the state of the art as well as some required background in the\ndifferent related disciplines. We give a high-level introduction to sign\nlanguage linguistics and machine translation to illustrate the requirements of\nautomatic sign language translation. We present a systematic literature review\nto illustrate the state of the art in the domain and then, harking back to the\nrequirements, lay out several challenges for future research. We find that\nsignificant advances have been made on the shoulders of spoken language machine\ntranslation research. However, current approaches are often not linguistically\nmotivated or are not adapted to the different input modality of sign languages.\nWe explore challenges related to the representation of sign language data, the\ncollection of datasets, the need for interdisciplinary research and\nrequirements for moving beyond research, towards applications. Based on our\nfindings, we advocate for interdisciplinary research and to base future\nresearch on linguistic analysis of sign languages. Furthermore, the inclusion\nof deaf and hearing end users of sign language translation applications in use\ncase identification, data collection and evaluation is of the utmost importance\nin the creation of useful sign language translation models. We recommend\niterative, human-in-the-loop, design and development of sign language\ntranslation models.\n","authors":["Mathieu De Coster","Dimitar Shterionov","Mieke Van Herreweghe","Joni Dambre"],"pdf_url":"https://arxiv.org/pdf/2202.03086v4.pdf","comment":"This is the version of the article submitted to peer review to\n  Universal Access in the Information Society. Please refer to \"De Coster, M.,\n  Shterionov, D., Van Herreweghe, M. et al. Machine translation from signed to\n  spoken languages: state of the art and challenges. Univ Access Inf Soc\n  (2023).\" for the published and updated version"},{"id":"http://arxiv.org/abs/2304.02233v1","updated":"2023-04-05T05:28:31Z","published":"2023-04-05T05:28:31Z","title":"Ericson: An Interactive Open-Domain Conversational Search Agent","summary":"  Open-domain conversational search (ODCS) aims to provide valuable, up-to-date\ninformation, while maintaining natural conversations to help users refine and\nultimately answer information needs. However, creating an effective and robust\nODCS agent is challenging. In this paper, we present a fully functional ODCS\nsystem, Ericson, which includes state-of-the-art question answering and\ninformation retrieval components, as well as intent inference and dialogue\nmanagement models for proactive question refinement and recommendations. Our\nsystem was stress-tested in the Amazon Alexa Prize, by engaging in live\nconversations with thousands of Alexa users, thus providing empirical basis for\nthe analysis of the ODCS system in real settings. Our interaction data analysis\nrevealed that accurate intent classification, encouraging user engagement, and\ncareful proactive recommendations contribute most to the users satisfaction.\nOur study further identifies limitations of the existing search techniques, and\ncan serve as a building block for the next generation of ODCS agents.\n","authors":["Zihao Wang","Ali Ahmadvand","Jason Choi","Payam Karisani","Eugene Agichtein"],"pdf_url":"https://arxiv.org/pdf/2304.02233v1.pdf","comment":"pre-print"},{"id":"http://arxiv.org/abs/2302.08624v3","updated":"2023-04-05T04:44:43Z","published":"2023-02-16T23:29:22Z","title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis","summary":"  In this paper, we present InstructABSA, Aspect Based Sentiment Analysis\n(ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect\nTerm Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint\nTask modeling. Our method introduces positive, negative, and neutral examples\nto each training sample, and instruction tunes the model (Tk-Instruct) for each\nABSA subtask, yielding significant performance improvements. Experimental\nresults on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA\noutperforms the previous state-of-the-art (SOTA) approaches on all three ABSA\nsubtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x\nlarger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE\nsubtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by\n8.63% points. Our results also suggest a strong generalization ability to new\ndomains across all three subtasks\n","authors":["Kevin Scaria","Himanshu Gupta","Siddharth Goyal","Saurabh Arjun Sawant","Swaroop Mishra","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2302.08624v3.pdf","comment":"4 pages, 2 figures, 5 tables, 5 appendix pages"},{"id":"http://arxiv.org/abs/2304.01487v2","updated":"2023-04-05T04:28:41Z","published":"2023-04-04T03:04:28Z","title":"To ChatGPT, or not to ChatGPT: That is the question!","summary":"  ChatGPT has become a global sensation. As ChatGPT and other Large Language\nModels (LLMs) emerge, concerns of misusing them in various ways increase, such\nas disseminating fake news, plagiarism, manipulating public opinion, cheating,\nand fraud. Hence, distinguishing AI-generated from human-generated becomes\nincreasingly essential. Researchers have proposed various detection\nmethodologies, ranging from basic binary classifiers to more complex\ndeep-learning models. Some detection techniques rely on statistical\ncharacteristics or syntactic patterns, while others incorporate semantic or\ncontextual information to improve accuracy. The primary objective of this study\nis to provide a comprehensive and contemporary assessment of the most recent\ntechniques in ChatGPT detection. Additionally, we evaluated other AI-generated\ntext detection tools that do not specifically claim to detect ChatGPT-generated\ncontent to assess their performance in detecting ChatGPT-generated content. For\nour evaluation, we have curated a benchmark dataset consisting of prompts from\nChatGPT and humans, including diverse questions from medical, open Q&A, and\nfinance domains and user-generated responses from popular social networking\nplatforms. The dataset serves as a reference to assess the performance of\nvarious techniques in detecting ChatGPT-generated content. Our evaluation\nresults demonstrate that none of the existing methods can effectively detect\nChatGPT-generated content.\n","authors":["Alessandro Pegoraro","Kavita Kumari","Hossein Fereidooni","Ahmad-Reza Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2304.01487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02213v1","updated":"2023-04-05T04:01:52Z","published":"2023-04-05T04:01:52Z","title":"Large Language Models as Master Key: Unlocking the Secrets of Materials\n  Science with GPT","summary":"  This article presents a new NLP task called structured information inference\n(SIS) to address the complexities of information extraction at the device level\nin materials science. We accomplished this task by finetuning GPT-3 on a\nexsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated\nthe dataset with all related scientific papers up to now. The produced dataset\nis formatted and normalized, enabling its direct utilization as input in\nsubsequent data analysis. This feature will enable materials scientists to\ndevelop their own models by selecting high-quality review papers within their\ndomain. Furthermore, we designed experiments to predict PCE and reverse-predict\nparameters and obtained comparable performance with DFT, which demonstrates the\npotential of large language models to judge materials and design new materials\nlike a materials scientist.\n","authors":["Tong Xie","Yuwei Wa","Wei Huang","Yufei Zhou","Yixuan Liu","Qingyuan Linghu","Shaozhou Wang","Chunyu Kit","Clara Grazian","Bram Hoex"],"pdf_url":"https://arxiv.org/pdf/2304.02213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02210v1","updated":"2023-04-05T03:49:06Z","published":"2023-04-05T03:49:06Z","title":"Document-Level Machine Translation with Large Language Models","summary":"  Large language models (LLMs) such as Chat-GPT can produce coherent, cohesive,\nrelevant, and fluent answers for various natural language processing (NLP)\ntasks. Taking document-level machine translation (MT) as a testbed, this paper\nprovides an in-depth evaluation of LLMs' ability on discourse modeling. The\nstudy fo-cuses on three aspects: 1) Effects of Discourse-Aware Prompts, where\nwe investigate the impact of different prompts on document-level translation\nquality and discourse phenomena; 2) Comparison of Translation Models, where we\ncompare the translation performance of Chat-GPT with commercial MT systems and\nadvanced document-level MT methods; 3) Analysis of Discourse Modelling\nAbilities, where we further probe discourse knowledge encoded in LLMs and\nexamine the impact of training techniques on discourse modeling. By evaluating\na number of benchmarks, we surprisingly find that 1) leveraging their powerful\nlong-text mod-eling capabilities, ChatGPT outperforms commercial MT systems in\nterms of human evaluation. 2) GPT-4 demonstrates a strong ability to explain\ndiscourse knowledge, even through it may select incorrect translation\ncandidates in contrastive testing. 3) ChatGPT and GPT-4 have demonstrated\nsuperior performance and show potential to become a new and promising paradigm\nfor document-level translation. This work highlights the challenges and\nopportunities of discourse modeling for LLMs, which we hope can inspire the\nfuture design and evaluation of LLMs.\n","authors":["Longyue Wang","Chenyang Lyu","Tianbo Ji","Zhirui Zhang","Dian Yu","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2304.02210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.05711v4","updated":"2023-04-05T02:09:02Z","published":"2022-03-11T01:45:33Z","title":"Synopses of Movie Narratives: a Video-Language Dataset for Story\n  Understanding","summary":"  Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SyMoN), containing\n5,193 video summaries of popular movies and TV series with a total length of\n869 hours. SyMoN captures naturalistic storytelling videos made by human\ncreators and intended for a human audience. As a prototypical and naturalistic\nstory dataset, SyMoN features high coverage of multimodal story events and\nabundant mental-state descriptions. Its use of storytelling techniques cause\ncross-domain semantic gaps that provide appropriate challenges to existing\nmodels. We establish benchmarks on video-text retrieval and zero-shot alignment\non movie summary videos, which showcase the importance of in-domain data and\nlong-term memory in story understanding. With SyMoN, we hope to lay the\ngroundwork for progress in multimodal story understanding.\n","authors":["Yidan Sun","Qin Chao","Yangfeng Ji","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2203.05711v4.pdf","comment":"25 pages, 17 figures"},{"id":"http://arxiv.org/abs/2304.01905v2","updated":"2023-04-05T01:22:38Z","published":"2023-04-03T01:19:39Z","title":"Dual-Attention Neural Transducers for Efficient Wake Word Spotting in\n  Speech Recognition","summary":"  We present dual-attention neural biasing, an architecture designed to boost\nWake Words (WW) recognition and improve inference time latency on speech\nrecognition tasks. This architecture enables a dynamic switch for its runtime\ncompute paths by exploiting WW spotting to select which branch of its attention\nnetworks to execute for an input audio frame. With this approach, we\neffectively improve WW spotting accuracy while saving runtime compute cost as\ndefined by floating point operations (FLOPs). Using an in-house de-identified\ndataset, we demonstrate that the proposed dual-attention network can reduce the\ncompute cost by $90\\%$ for WW audio frames, with only $1\\%$ increase in the\nnumber of parameters. This architecture improves WW F1 score by $16\\%$ relative\nand improves generic rare word error rate by $3\\%$ relative compared to the\nbaselines.\n","authors":["Saumya Y. Sahai","Jing Liu","Thejaswi Muniyappa","Kanthashree M. Sathyendra","Anastasios Alexandridis","Grant P. Strimel","Ross McGowan","Ariya Rastrow","Feng-Ju Chang","Athanasios Mouchtaris","Siegfried Kunzmann"],"pdf_url":"https://arxiv.org/pdf/2304.01905v2.pdf","comment":"Accepted to Proc. IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2304.01412v2","updated":"2023-04-05T01:20:51Z","published":"2023-04-03T23:18:30Z","title":"The StatCan Dialogue Dataset: Retrieving Data Tables through\n  Conversations with Genuine Intents","summary":"  We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation\nturns between agents working at Statistics Canada and online users looking for\npublished data tables. The conversations stem from genuine intents, are held in\nEnglish or French, and lead to agents retrieving one of over 5000 complex data\ntables. Based on this dataset, we propose two tasks: (1) automatic retrieval of\nrelevant tables based on a on-going conversation, and (2) automatic generation\nof appropriate agent responses at each turn. We investigate the difficulty of\neach task by establishing strong baselines. Our experiments on a temporal data\nsplit reveal that all models struggle to generalize to future conversations, as\nwe observe a significant drop in performance across both tasks when we move\nfrom the validation to the test set. In addition, we find that response\ngeneration models struggle to decide when to return a table. Considering that\nthe tasks pose significant challenges to existing models, we encourage the\ncommunity to develop models for our task, which can be directly used to help\nknowledge workers find relevant tables for live chat users.\n","authors":["Xing Han Lu","Siva Reddy","Harm de Vries"],"pdf_url":"https://arxiv.org/pdf/2304.01412v2.pdf","comment":"Accepted at EACL 2023"},{"id":"http://arxiv.org/abs/2304.02182v1","updated":"2023-04-05T01:17:59Z","published":"2023-04-05T01:17:59Z","title":"Unleashing the Power of ChatGPT for Translation: An Empirical Study","summary":"  The recently released ChatGPT has demonstrated surprising abilities in\nnatural language understanding and natural language generation. Machine\ntranslation is an important and extensively studied task in the field of\nnatural language processing, which heavily relies on the abilities of language\nunderstanding and generation. Thus, in this paper, we explore how to assist\nmachine translation with ChatGPT. We adopt several translation prompts on a\nwide range of translations. Our experimental results show that ChatGPT with\ndesigned translation prompts can achieve comparable or better performance over\nprofessional translation systems for high-resource language translations but\nlags behind significantly on low-resource translations. We further evaluate the\ntranslation quality using multiple references, and ChatGPT achieves superior\nperformance compared to the professional systems. We also conduct experiments\non domain-specific translations, the final results show that ChatGPT is able to\ncomprehend the provided domain keyword and adjust accordingly to output proper\ntranslations. At last, we perform few-shot prompts that show consistent\nimprovement across different base prompts. Our work provides empirical evidence\nthat ChatGPT still has great potential in translations.\n","authors":["Yuan Gao","Ruili Wang","Feng Hou"],"pdf_url":"https://arxiv.org/pdf/2304.02182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02181v1","updated":"2023-04-05T01:09:58Z","published":"2023-04-05T01:09:58Z","title":"On the Impact of Voice Anonymization on Speech-Based COVID-19 Detection","summary":"  With advances seen in deep learning, voice-based applications are burgeoning,\nranging from personal assistants, affective computing, to remote disease\ndiagnostics. As the voice contains both linguistic and paralinguistic\ninformation (e.g., vocal pitch, intonation, speech rate, loudness), there is\ngrowing interest in voice anonymization to preserve speaker privacy and\nidentity. Voice privacy challenges have emerged over the last few years and\nfocus has been placed on removing speaker identity while keeping linguistic\ncontent intact. For affective computing and disease monitoring applications,\nhowever, the paralinguistic content may be more critical. Unfortunately, the\neffects that anonymization may have on these systems are still largely unknown.\nIn this paper, we fill this gap and focus on one particular health monitoring\napplication: speech-based COVID-19 diagnosis. We test two popular anonymization\nmethods and their impact on five different state-of-the-art COVID-19 diagnostic\nsystems using three public datasets. We validate the effectiveness of the\nanonymization methods, compare their computational complexity, and quantify the\nimpact across different testing scenarios for both within- and across-dataset\nconditions. Lastly, we show the benefits of anonymization as a data\naugmentation tool to help recover some of the COVID-19 diagnostic accuracy loss\nseen with anonymized data.\n","authors":["Yi Zhu","Mohamed Imoussaïne-Aïkous","Carolyn Côté-Lussier","Tiago H. Falk"],"pdf_url":"https://arxiv.org/pdf/2304.02181v1.pdf","comment":"11 pages, 10 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2304.02643v1","updated":"2023-04-05T17:59:46Z","published":"2023-04-05T17:59:46Z","title":"Segment Anything","summary":"  We introduce the Segment Anything (SA) project: a new task, model, and\ndataset for image segmentation. Using our efficient model in a data collection\nloop, we built the largest segmentation dataset to date (by far), with over 1\nbillion masks on 11M licensed and privacy respecting images. The model is\ndesigned and trained to be promptable, so it can transfer zero-shot to new\nimage distributions and tasks. We evaluate its capabilities on numerous tasks\nand find that its zero-shot performance is impressive -- often competitive with\nor even superior to prior fully supervised results. We are releasing the\nSegment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and\n11M images at https://segment-anything.com to foster research into foundation\nmodels for computer vision.\n","authors":["Alexander Kirillov","Eric Mintun","Nikhila Ravi","Hanzi Mao","Chloe Rolland","Laura Gustafson","Tete Xiao","Spencer Whitehead","Alexander C. Berg","Wan-Yen Lo","Piotr Dollár","Ross Girshick"],"pdf_url":"https://arxiv.org/pdf/2304.02643v1.pdf","comment":"Project web-page: https://segment-anything.com"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2304.02572v1","updated":"2023-04-05T16:44:36Z","published":"2023-04-05T16:44:36Z","title":"Optimism Based Exploration in Large-Scale Recommender Systems","summary":"  Bandit learning algorithms have been an increasingly popular design choice\nfor recommender systems. Despite the strong interest in bandit learning from\nthe community, there remains multiple bottlenecks that prevent many bandit\nlearning approaches from productionalization. Two of the most important\nbottlenecks are scaling to multi-task and A/B testing. Classic bandit\nalgorithms, especially those leveraging contextual information, often requires\nreward for uncertainty estimation, which hinders their adoptions in multi-task\nrecommender systems. Moreover, different from supervised learning algorithms,\nbandit learning algorithms emphasize greatly on the data collection process\nthrough their explorative nature. Such explorative behavior induces unfair\nevaluation for bandit learning agents in a classic A/B test setting. In this\nwork, we present a novel design of production bandit learning life-cycle for\nrecommender systems, along with a novel set of metrics to measure their\nefficiency in user exploration. We show through large-scale production\nrecommender system experiments and in-depth analysis that our bandit agent\ndesign improves personalization for the production recommender system and our\nexperiment design fairly evaluates the performance of bandit learning\nalgorithms.\n","authors":["Hongbo Guo","Ruben Naeff","Alex Nikulkov","Zheqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2304.02572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02434v1","updated":"2023-04-05T13:39:58Z","published":"2023-04-05T13:39:58Z","title":"Both Efficiency and Effectiveness! A Large Scale Pre-ranking Framework\n  in Search System","summary":"  In the realm of search systems, multi-stage cascade architecture is a\nprevalent method, typically consisting of sequential modules such as matching,\npre-ranking, and ranking. It is generally acknowledged that the model used in\nthe pre-ranking stage must strike a balance between efficacy and efficiency.\nThus, the most commonly employed architecture is the representation-focused\nvector product based model. However, this architecture lacks effective\ninteraction between the query and document, resulting in a reduction in the\neffectiveness of the search system. To address this issue, we present a novel\npre-ranking framework called RankDFM. Our framework leverages DeepFM as the\nbackbone and employs a pairwise training paradigm to learn the ranking of\nvideos under a query. The capability of RankDFM to cross features provides\nsignificant improvement in offline and online A/B testing performance.\nFurthermore, we introduce a learnable feature selection scheme to optimize the\nmodel and reduce the time required for online inference, equivalent to a tree\nmodel. Currently, RankDFM has been deployed in the search system of a\nshortvideo App, providing daily services to hundreds of millions users.\n","authors":["Qihang Zhao","Rui-jie Zhu","Liu Yang","He Yongming","Bo Zhou","Luo Cheng"],"pdf_url":"https://arxiv.org/pdf/2304.02434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02350v1","updated":"2023-04-05T10:30:27Z","published":"2023-04-05T10:30:27Z","title":"Unfolded Self-Reconstruction LSH: Towards Machine Unlearning in\n  Approximate Nearest Neighbour Search","summary":"  Approximate nearest neighbour (ANN) search is an essential component of\nsearch engines, recommendation systems, etc. Many recent works focus on\nlearning-based data-distribution-dependent hashing and achieve good retrieval\nperformance. However, due to increasing demand for users' privacy and security,\nwe often need to remove users' data information from Machine Learning (ML)\nmodels to satisfy specific privacy and security requirements. This need\nrequires the ANN search algorithm to support fast online data deletion and\ninsertion. Current learning-based hashing methods need retraining the hash\nfunction, which is prohibitable due to the vast time-cost of large-scale data.\nTo address this problem, we propose a novel data-dependent hashing method named\nunfolded self-reconstruction locality-sensitive hashing (USR-LSH). Our USR-LSH\nunfolded the optimization update for instance-wise data reconstruction, which\nis better for preserving data information than data-independent LSH. Moreover,\nour USR-LSH supports fast online data deletion and insertion without\nretraining. To the best of our knowledge, we are the first to address the\nmachine unlearning of retrieval problems. Empirically, we demonstrate that\nUSR-LSH outperforms the state-of-the-art data-distribution-independent LSH in\nANN tasks in terms of precision and recall. We also show that USR-LSH has\nsignificantly faster data deletion and insertion time than learning-based\ndata-dependent hashing.\n","authors":["Kim Yong Tan","Lyu Yueming","Yew-Soon Ong","Ivor Tsang"],"pdf_url":"https://arxiv.org/pdf/2304.02350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01352v2","updated":"2023-04-05T09:23:17Z","published":"2023-04-03T20:27:10Z","title":"A Simple and Effective Method of Cross-Lingual Plagiarism Detection","summary":"  We present a simple cross-lingual plagiarism detection method applicable to a\nlarge number of languages. The presented approach leverages open multilingual\nthesauri for candidate retrieval task and pre-trained multilingual BERT-based\nlanguage models for detailed analysis. The method does not rely on machine\ntranslation and word sense disambiguation when in use, and therefore is\nsuitable for a large number of languages, including under-resourced languages.\nThe effectiveness of the proposed approach is demonstrated for several existing\nand new benchmarks, achieving state-of-the-art results for French, Russian, and\nArmenian languages.\n","authors":["Karen Avetisyan","Arthur Malajyan","Tsolak Ghukasyan","Arutyun Avetisyan"],"pdf_url":"https://arxiv.org/pdf/2304.01352v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01577v2","updated":"2023-04-05T03:55:53Z","published":"2023-04-04T07:06:54Z","title":"Form-NLU: Dataset for the Form Language Understanding","summary":"  Compared to general document analysis tasks, form document structure\nunderstanding and retrieval are challenging. Form documents are typically made\nby two types of authors; A form designer, who develops the form structure and\nkeys, and a form user, who fills out form values based on the provided keys.\nHence, the form values may not be aligned with the form designer's intention\n(structure and keys) if a form user gets confused. In this paper, we introduce\nForm-NLU, the first novel dataset for form structure understanding and its key\nand value information extraction, interpreting the form designer's intent and\nthe alignment of user-written value on it. It consists of 857 form images, 6k\nform keys and values, and 4k table keys and values. Our dataset also includes\nthree form types: digital, printed, and handwritten, which cover diverse form\nappearances and layouts. We propose a robust positional and logical\nrelation-based form key-value information extraction framework. Using this\ndataset, Form-NLU, we first examine strong object detection models for the form\nlayout understanding, then evaluate the key information extraction task on the\ndataset, providing fine-grained results for different types of forms and keys.\nFurthermore, we examine it with the off-the-shelf pdf layout extraction tool\nand prove its feasibility in real-world cases.\n","authors":["Yihao Ding","Siqu Long","Jiabin Huang","Kaixuan Ren","Xingxiang Luo","Hyunsuk Chung","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2304.01577v2.pdf","comment":"Accepted by SIGIR 2023"},{"id":"http://arxiv.org/abs/2304.02205v1","updated":"2023-04-05T03:36:40Z","published":"2023-04-05T03:36:40Z","title":"MoocRadar: A Fine-grained and Multi-aspect Knowledge Repository for\n  Improving Cognitive Student Modeling in MOOCs","summary":"  Student modeling, the task of inferring a student's learning characteristics\nthrough their interactions with coursework, is a fundamental issue in\nintelligent education. Although the recent attempts from knowledge tracing and\ncognitive diagnosis propose several promising directions for improving the\nusability and effectiveness of current models, the existing public datasets are\nstill insufficient to meet the need for these potential solutions due to their\nignorance of complete exercising contexts, fine-grained concepts, and cognitive\nlabels. In this paper, we present MoocRadar, a fine-grained, multi-aspect\nknowledge repository consisting of 2,513 exercise questions, 5,600 knowledge\nconcepts, and over 12 million behavioral records. Specifically, we propose a\nframework to guarantee a high-quality and comprehensive annotation of\nfine-grained concepts and cognitive labels. The statistical and experimental\nresults indicate that our dataset provides the basis for the future\nimprovements of existing methods. Moreover, to support the convenient usage for\nresearchers, we release a set of tools for data querying, model adaption, and\neven the extension of our repository, which are now available at\nhttps://github.com/THU-KEG/MOOC-Radar.\n","authors":["Jifan Yu","Mengying Lu","Qingyang Zhong","Zijun Yao","Shangqing Tu","Zhengshan Liao","Xiaoya Li","Manli Li","Lei Hou","Hai-Tao Zheng","Juanzi Li","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2304.02205v1.pdf","comment":"Accepted by SIGIR 2023"},{"id":"http://arxiv.org/abs/2302.06101v2","updated":"2023-04-05T00:29:05Z","published":"2023-02-13T04:58:20Z","title":"On Modeling Long-Term User Engagement from Stochastic Feedback","summary":"  An ultimate goal of recommender systems (RS) is to improve user engagement.\nReinforcement learning (RL) is a promising paradigm for this goal, as it\ndirectly optimizes overall performance of sequential recommendation. However,\nmany existing RL-based approaches induce huge computational overhead, because\nthey require not only the recommended items but also all other candidate items\nto be stored. This paper proposes an efficient alternative that does not\nrequire the candidate items. The idea is to model the correlation between user\nengagement and items directly from data. Moreover, the proposed approach\nconsider randomness in user feedback and termination behavior, which are\nubiquitous for RS but rarely discussed in RL-based prior work. With online A/B\nexperiments on real-world RS, we confirm the efficacy of the proposed approach\nand the importance of modeling the two types of randomness.\n","authors":["Guoxi Zhang","Xing Yao","Xuanji Xiao"],"pdf_url":"https://arxiv.org/pdf/2302.06101v2.pdf","comment":"Accepted by the workshop on decision making for information retrieval\n  and recommender systems (the Web Conference 2023)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2304.02643v1","updated":"2023-04-05T17:59:46Z","published":"2023-04-05T17:59:46Z","title":"Segment Anything","summary":"  We introduce the Segment Anything (SA) project: a new task, model, and\ndataset for image segmentation. Using our efficient model in a data collection\nloop, we built the largest segmentation dataset to date (by far), with over 1\nbillion masks on 11M licensed and privacy respecting images. The model is\ndesigned and trained to be promptable, so it can transfer zero-shot to new\nimage distributions and tasks. We evaluate its capabilities on numerous tasks\nand find that its zero-shot performance is impressive -- often competitive with\nor even superior to prior fully supervised results. We are releasing the\nSegment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and\n11M images at https://segment-anything.com to foster research into foundation\nmodels for computer vision.\n","authors":["Alexander Kirillov","Eric Mintun","Nikhila Ravi","Hanzi Mao","Chloe Rolland","Laura Gustafson","Tete Xiao","Spencer Whitehead","Alexander C. Berg","Wan-Yen Lo","Piotr Dollár","Ross Girshick"],"pdf_url":"https://arxiv.org/pdf/2304.02643v1.pdf","comment":"Project web-page: https://segment-anything.com"},{"id":"http://arxiv.org/abs/2304.02641v1","updated":"2023-04-05T17:59:20Z","published":"2023-04-05T17:59:20Z","title":"Self-Distillation for Gaussian Process Regression and Classification","summary":"  We propose two approaches to extend the notion of knowledge distillation to\nGaussian Process Regression (GPR) and Gaussian Process Classification (GPC);\ndata-centric and distribution-centric. The data-centric approach resembles most\ncurrent distillation techniques for machine learning, and refits a model on\ndeterministic predictions from the teacher, while the distribution-centric\napproach, re-uses the full probabilistic posterior for the next iteration. By\nanalyzing the properties of these approaches, we show that the data-centric\napproach for GPR closely relates to known results for self-distillation of\nkernel ridge regression and that the distribution-centric approach for GPR\ncorresponds to ordinary GPR with a very particular choice of hyperparameters.\nFurthermore, we demonstrate that the distribution-centric approach for GPC\napproximately corresponds to data duplication and a particular scaling of the\ncovariance and that the data-centric approach for GPC requires redefining the\nmodel from a Binomial likelihood to a continuous Bernoulli likelihood to be\nwell-specified. To the best of our knowledge, our proposed approaches are the\nfirst to formulate knowledge distillation specifically for Gaussian Process\nmodels.\n","authors":["Kenneth Borup","Lars Nørvang Andersen"],"pdf_url":"https://arxiv.org/pdf/2304.02641v1.pdf","comment":"10 pages; code at\n  https://github.com/Kennethborup/gaussian_process_self_distillation"},{"id":"http://arxiv.org/abs/2304.02637v1","updated":"2023-04-05T17:58:16Z","published":"2023-04-05T17:58:16Z","title":"GenPhys: From Physical Processes to Generative Models","summary":"  Since diffusion models (DM) and the more recent Poisson flow generative\nmodels (PFGM) are inspired by physical processes, it is reasonable to ask: Can\nphysical processes offer additional new generative models? We show that the\nanswer is yes. We introduce a general family, Generative Models from Physical\nProcesses (GenPhys), where we translate partial differential equations (PDEs)\ndescribing physical processes to generative models. We show that generative\nmodels can be constructed from s-generative PDEs (s for smooth). GenPhys\nsubsume the two existing generative models (DM and PFGM) and even give rise to\nnew families of generative models, e.g., \"Yukawa Generative Models\" inspired\nfrom weak interactions. On the other hand, some physical processes by default\ndo not belong to the GenPhys family, e.g., the wave equation and the\nSchr\\\"{o}dinger equation, but could be made into the GenPhys family with some\nmodifications. Our goal with GenPhys is to explore and expand the design space\nof generative models.\n","authors":["Ziming Liu","Di Luo","Yilun Xu","Tommi Jaakkola","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2304.02637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02632v1","updated":"2023-04-05T17:55:00Z","published":"2023-04-05T17:55:00Z","title":"Mapping historical forest biomass for stock-change assessments at parcel\n  to landscape scales","summary":"  Understanding historical forest dynamics, specifically changes in forest\nbiomass and carbon stocks, has become critical for assessing current forest\nclimate benefits and projecting future benefits under various policy,\nregulatory, and stewardship scenarios. Carbon accounting frameworks based\nexclusively on national forest inventories are limited to broad-scale\nestimates, but model-based approaches that combine these inventories with\nremotely sensed data can yield contiguous fine-resolution maps of forest\nbiomass and carbon stocks across landscapes over time. Here we describe a\nfundamental step in building a map-based stock-change framework: mapping\nhistorical forest biomass at fine temporal and spatial resolution (annual, 30m)\nacross all of New York State (USA) from 1990 to 2019, using freely available\ndata and open-source tools.\n  Using Landsat imagery, US Forest Service Forest Inventory and Analysis (FIA)\ndata, and off-the-shelf LiDAR collections we developed three modeling\napproaches for mapping historical forest aboveground biomass (AGB): training on\nFIA plot-level AGB estimates (direct), training on LiDAR-derived AGB maps\n(indirect), and an ensemble averaging predictions from the direct and indirect\nmodels. Model prediction surfaces (maps) were tested against FIA estimates at\nmultiple scales. All three approaches produced viable outputs, yet tradeoffs\nwere evident in terms of model complexity, map accuracy, saturation, and\nfine-scale pattern representation. The resulting map products can help identify\nwhere, when, and how forest carbon stocks are changing as a result of both\nanthropogenic and natural drivers alike. These products can thus serve as\ninputs to a wide range of applications including stock-change assessments,\nmonitoring reporting and verification frameworks, and prioritizing parcels for\nprotection or enrollment in improved management programs.\n","authors":["Lucas K. Johnson","Michael J. Mahoney","Madeleine L. Desrochers","Colin M. Beier"],"pdf_url":"https://arxiv.org/pdf/2304.02632v1.pdf","comment":"Manuscript: 24 pages, 7 figures; Supplements: 12 pages, 5 figures;\n  Submitted to Forest Ecology and Management"},{"id":"http://arxiv.org/abs/2201.01863v3","updated":"2023-04-05T17:45:43Z","published":"2022-01-05T23:15:58Z","title":"CFU Playground: Full-Stack Open-Source Framework for Tiny Machine\n  Learning (tinyML) Acceleration on FPGAs","summary":"  Need for the efficient processing of neural networks has given rise to the\ndevelopment of hardware accelerators. The increased adoption of specialized\nhardware has highlighted the need for more agile design flows for\nhardware-software co-design and domain-specific optimizations. In this paper,\nwe present CFU Playground: a full-stack open-source framework that enables\nrapid and iterative design and evaluation of machine learning (ML) accelerators\nfor embedded ML systems. Our tool provides a completely open-source end-to-end\nflow for hardware-software co-design on FPGAs and future systems research. This\nfull-stack framework gives the users access to explore experimental and bespoke\narchitectures that are customized and co-optimized for embedded ML. Our rapid,\ndeploy-profile-optimization feedback loop lets ML hardware and software\ndevelopers achieve significant returns out of a relatively small investment in\ncustomization. Using CFU Playground's design and evaluation loop, we show\nsubstantial speedups between 55$\\times$ and 75$\\times$. The soft CPU coupled\nwith the accelerator opens up a new, rich design space between the two\ncomponents that we explore in an automated fashion using Vizier, an open-source\nblack-box optimization service.\n","authors":["Shvetank Prakash","Tim Callahan","Joseph Bushagour","Colby Banbury","Alan V. Green","Pete Warden","Tim Ansell","Vijay Janapa Reddi"],"pdf_url":"https://arxiv.org/pdf/2201.01863v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02621v1","updated":"2023-04-05T17:43:57Z","published":"2023-04-05T17:43:57Z","title":"High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation","summary":"  The task of image-level weakly-supervised semantic segmentation (WSSS) has\ngained popularity in recent years, as it reduces the vast data annotation cost\nfor training segmentation models. The typical approach for WSSS involves\ntraining an image classification network using global average pooling (GAP) on\nconvolutional feature maps. This enables the estimation of object locations\nbased on class activation maps (CAMs), which identify the importance of image\nregions. The CAMs are then used to generate pseudo-labels, in the form of\nsegmentation masks, to supervise a segmentation model in the absence of\npixel-level ground truth. In case of the SEAM baseline, a previous work\nproposed to improve CAM learning in two ways: (1) Importance sampling, which is\na substitute for GAP, and (2) the feature similarity loss, which utilizes a\nheuristic that object contours almost exclusively align with color edges in\nimages. In this work, we propose a different probabilistic interpretation of\nCAMs for these techniques, rendering the likelihood more appropriate than the\nmultinomial posterior. As a result, we propose an add-on method that can boost\nessentially any previous WSSS method, improving both the region similarity and\ncontour quality of all implemented state-of-the-art baselines. This is\ndemonstrated on a wide variety of baselines on the PASCAL VOC dataset.\nExperiments on the MS COCO dataset show that performance gains can also be\nachieved in a large-scale setting. Our code is available at\nhttps://github.com/arvijj/hfpl.\n","authors":["Arvi Jonnarth","Yushan Zhang","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2304.02621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07983v2","updated":"2023-04-05T17:41:12Z","published":"2022-12-15T17:31:54Z","title":"Vision Transformers are Parameter-Efficient Audio-Visual Learners","summary":"  Vision transformers (ViTs) have achieved impressive results on various\ncomputer vision tasks in the last several years. In this work, we study the\ncapability of frozen ViTs, pretrained only on visual data, to generalize to\naudio-visual data without finetuning any of its original parameters. To do so,\nwe propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained\nViTs to audio-visual tasks by injecting a small number of trainable parameters\ninto every layer of a frozen ViT. To efficiently fuse visual and audio cues,\nour LAVISH adapter uses a small set of latent tokens, which form an attention\nbottleneck, thus, eliminating the quadratic cost of standard cross-attention.\nCompared to the existing modality-specific audio-visual methods, our approach\nachieves competitive or even better performance on various audio-visual tasks\nwhile using fewer tunable parameters and without relying on costly audio\npretraining or external audio encoders. Our code is available at\nhttps://genjib.github.io/project_page/LAVISH/\n","authors":["Yan-Bo Lin","Yi-Lin Sung","Jie Lei","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2212.07983v2.pdf","comment":"CVPR 2023 Project Page: https://genjib.github.io/project_page/LAVISH/"},{"id":"http://arxiv.org/abs/2304.02613v1","updated":"2023-04-05T17:33:57Z","published":"2023-04-05T17:33:57Z","title":"Efficient Quantum Algorithms for Quantum Optimal Control","summary":"  In this paper, we present efficient quantum algorithms that are exponentially\nfaster than classical algorithms for solving the quantum optimal control\nproblem. This problem involves finding the control variable that maximizes a\nphysical quantity at time $T$, where the system is governed by a time-dependent\nSchr\\\"odinger equation. This type of control problem also has an intricate\nrelation with machine learning. Our algorithms are based on a time-dependent\nHamiltonian simulation method and a fast gradient-estimation algorithm. We also\nprovide a comprehensive error analysis to quantify the total error from various\nsteps, such as the finite-dimensional representation of the control function,\nthe discretization of the Schr\\\"odinger equation, the numerical quadrature, and\noptimization. Our quantum algorithms require fault-tolerant quantum computers.\n","authors":["Xiantao Li","Chunhao Wang"],"pdf_url":"https://arxiv.org/pdf/2304.02613v1.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.17354v2","updated":"2023-04-05T17:19:17Z","published":"2023-03-30T13:11:26Z","title":"Incremental Self-Supervised Learning Based on Transformer for Anomaly\n  Detection and Localization","summary":"  In the machine learning domain, research on anomaly detection and\nlocalization within image data has garnered significant attention, particularly\nin practical applications such as industrial defect detection. While existing\napproaches predominantly rely on Convolutional Neural Networks (CNN) as their\nbackbone network, we propose an innovative method based on the Transformer\nbackbone network. Our approach employs a two-stage incremental learning\nstrategy. In the first stage, we train a Masked Autoencoder (MAE) model\nexclusively on normal images. Subsequently, in the second stage, we implement\npixel-level data augmentation techniques to generate corrupted normal images\nand their corresponding pixel labels. This process enables the model to learn\nhow to repair corrupted regions and classify the state of each pixel.\nUltimately, the model produces a pixel reconstruction error matrix and a pixel\nanomaly probability matrix, which are combined to create an anomaly scoring\nmatrix that effectively identifies abnormal regions. When compared to several\nstate-of-the-art CNN-based techniques, our method demonstrates superior\nperformance on the MVTec AD dataset, achieving an impressive 97.6% AUC.\n","authors":["Wenping Jin","Fei Guo","Li Zhu"],"pdf_url":"https://arxiv.org/pdf/2303.17354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02599v1","updated":"2023-04-05T17:10:53Z","published":"2023-04-05T17:10:53Z","title":"Query lower bounds for log-concave sampling","summary":"  Log-concave sampling has witnessed remarkable algorithmic advances in recent\nyears, but the corresponding problem of proving lower bounds for this task has\nremained elusive, with lower bounds previously known only in dimension one. In\nthis work, we establish the following query lower bounds: (1) sampling from\nstrongly log-concave and log-smooth distributions in dimension $d\\ge 2$\nrequires $\\Omega(\\log \\kappa)$ queries, which is sharp in any constant\ndimension, and (2) sampling from Gaussians in dimension $d$ (hence also from\ngeneral log-concave and log-smooth distributions in dimension $d$) requires\n$\\widetilde \\Omega(\\min(\\sqrt\\kappa \\log d, d))$ queries, which is nearly sharp\nfor the class of Gaussians. Here $\\kappa$ denotes the condition number of the\ntarget distribution. Our proofs rely upon (1) a multiscale construction\ninspired by work on the Kakeya conjecture in harmonic analysis, and (2) a novel\nreduction that demonstrates that block Krylov algorithms are optimal for this\nproblem, as well as connections to lower bound techniques based on Wishart\nmatrices developed in the matrix-vector query literature.\n","authors":["Sinho Chewi","Jaume de Dios Pont","Jerry Li","Chen Lu","Shyam Narayanan"],"pdf_url":"https://arxiv.org/pdf/2304.02599v1.pdf","comment":"46 pages, 2 figures"},{"id":"http://arxiv.org/abs/2304.02583v1","updated":"2023-04-05T16:52:35Z","published":"2023-04-05T16:52:35Z","title":"A force-sensing surgical drill for real-time force feedback in robotic\n  mastoidectomy","summary":"  Purpose: Robotic assistance in otologic surgery can reduce the task load of\noperating surgeons during the removal of bone around the critical structures in\nthe lateral skull base. However, safe deployment into the anatomical\npassageways necessitates the development of advanced sensing capabilities to\nactively limit the interaction forces between the surgical tools and critical\nanatomy.\n  Methods: We introduce a surgical drill equipped with a force sensor that is\ncapable of measuring accurate tool-tissue interaction forces to enable force\ncontrol and feedback to surgeons. The design, calibration and validation of the\nforce-sensing surgical drill mounted on a cooperatively controlled surgical\nrobot are described in this work.\n  Results: The force measurements on the tip of the surgical drill are\nvalidated with raw-egg drilling experiments, where a force sensor mounted below\nthe egg serves as ground truth. The average root mean square error (RMSE) for\npoints and path drilling experiments are 41.7 (pm 12.2) mN and 48.3 (pm 13.7)\nmN respectively.\n  Conclusions: The force-sensing prototype measures forces with sub-millinewton\nresolution and the results demonstrate that the calibrated force-sensing drill\ngenerates accurate force measurements with minimal error compared to the\nmeasured drill forces. The development of such sensing capabilities is crucial\nfor the safe use of robotic systems in a clinical context.\n","authors":["Yuxin Chen","Anna Goodridge","Manish Sahu","Aditi Kishore","Seena Vafaee","Harsha Mohan","Katherina Sapozhnikov","Francis Creighton","Russell Taylor","Deepa Galaiya"],"pdf_url":"https://arxiv.org/pdf/2304.02583v1.pdf","comment":"Accepted at IPCAI2023"},{"id":"http://arxiv.org/abs/2302.05894v2","updated":"2023-04-05T16:50:50Z","published":"2023-02-12T11:25:29Z","title":"Neural Architecture Search with Multimodal Fusion Methods for Diagnosing\n  Dementia","summary":"  Alzheimer's dementia (AD) affects memory, thinking, and language,\ndeteriorating person's life. An early diagnosis is very important as it enables\nthe person to receive medical help and ensure quality of life. Therefore,\nleveraging spontaneous speech in conjunction with machine learning methods for\nrecognizing AD patients has emerged into a hot topic. Most of the previous\nworks employ Convolutional Neural Networks (CNNs), to process the input signal.\nHowever, finding a CNN architecture is a time-consuming process and requires\ndomain expertise. Moreover, the researchers introduce early and late fusion\napproaches for fusing different modalities or concatenate the representations\nof the different modalities during training, thus the inter-modal interactions\nare not captured. To tackle these limitations, first we exploit a Neural\nArchitecture Search (NAS) method to automatically find a high performing CNN\narchitecture. Next, we exploit several fusion methods, including Multimodal\nFactorized Bilinear Pooling and Tucker Decomposition, to combine both speech\nand text modalities. To the best of our knowledge, there is no prior work\nexploiting a NAS approach and these fusion methods in the task of dementia\ndetection from spontaneous speech. We perform extensive experiments on the\nADReSS Challenge dataset and show the effectiveness of our approach over\nstate-of-the-art methods.\n","authors":["Michail Chatzianastasis","Loukas Ilias","Dimitris Askounis","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2302.05894v2.pdf","comment":"Accepted at ICASSP 2023"},{"id":"http://arxiv.org/abs/2304.02577v1","updated":"2023-04-05T16:48:24Z","published":"2023-04-05T16:48:24Z","title":"ECG Feature Importance Rankings: Cardiologists vs. Algorithms","summary":"  Feature importance methods promise to provide a ranking of features according\nto importance for a given classification task. A wide range of methods exist\nbut their rankings often disagree and they are inherently difficult to evaluate\ndue to a lack of ground truth beyond synthetic datasets. In this work, we put\nfeature importance methods to the test on real-world data in the domain of\ncardiology, where we try to distinguish three specific pathologies from healthy\nsubjects based on ECG features comparing to features used in cardiologists'\ndecision rules as ground truth. Some methods generally performed well and\nothers performed poorly, while some methods did well on some but not all of the\nproblems considered.\n","authors":["Temesgen Mehari","Ashish Sundar","Alen Bosnjakovic","Peter Harris","Steven E. Williams","Axel Loewe","Olaf Doessel","Claudia Nagel","Nils Strodthoff","Philip J. Aston"],"pdf_url":"https://arxiv.org/pdf/2304.02577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02574v1","updated":"2023-04-05T16:45:11Z","published":"2023-04-05T16:45:11Z","title":"Conformal Off-Policy Evaluation in Markov Decision Processes","summary":"  Reinforcement Learning aims at identifying and evaluating efficient control\npolicies from data. In many real-world applications, the learner is not allowed\nto experiment and cannot gather data in an online manner (this is the case when\nexperimenting is expensive, risky or unethical). For such applications, the\nreward of a given policy (the target policy) must be estimated using historical\ndata gathered under a different policy (the behavior policy). Most methods for\nthis learning task, referred to as Off-Policy Evaluation (OPE), do not come\nwith accuracy and certainty guarantees. We present a novel OPE method based on\nConformal Prediction that outputs an interval containing the true reward of the\ntarget policy with a prescribed level of certainty. The main challenge in OPE\nstems from the distribution shift due to the discrepancies between the target\nand the behavior policies. We propose and empirically evaluate different ways\nto deal with this shift. Some of these methods yield conformalized intervals\nwith reduced length compared to existing approaches, while maintaining the same\ncertainty level.\n","authors":["Daniele Foffano","Alessio Russo","Alexandre Proutiere"],"pdf_url":"https://arxiv.org/pdf/2304.02574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02572v1","updated":"2023-04-05T16:44:36Z","published":"2023-04-05T16:44:36Z","title":"Optimism Based Exploration in Large-Scale Recommender Systems","summary":"  Bandit learning algorithms have been an increasingly popular design choice\nfor recommender systems. Despite the strong interest in bandit learning from\nthe community, there remains multiple bottlenecks that prevent many bandit\nlearning approaches from productionalization. Two of the most important\nbottlenecks are scaling to multi-task and A/B testing. Classic bandit\nalgorithms, especially those leveraging contextual information, often requires\nreward for uncertainty estimation, which hinders their adoptions in multi-task\nrecommender systems. Moreover, different from supervised learning algorithms,\nbandit learning algorithms emphasize greatly on the data collection process\nthrough their explorative nature. Such explorative behavior induces unfair\nevaluation for bandit learning agents in a classic A/B test setting. In this\nwork, we present a novel design of production bandit learning life-cycle for\nrecommender systems, along with a novel set of metrics to measure their\nefficiency in user exploration. We show through large-scale production\nrecommender system experiments and in-depth analysis that our bandit agent\ndesign improves personalization for the production recommender system and our\nexperiment design fairly evaluates the performance of bandit learning\nalgorithms.\n","authors":["Hongbo Guo","Ruben Naeff","Alex Nikulkov","Zheqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2304.02572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07088v2","updated":"2023-04-05T16:22:17Z","published":"2023-01-17T18:53:24Z","title":"Vision Learners Meet Web Image-Text Pairs","summary":"  Most recent self-supervised learning methods are pre-trained on the\nwell-curated ImageNet-1K dataset. In this work, given the excellent scalability\nof web data, we consider self-supervised pre-training on noisy web sourced\nimage-text paired data. First, we conduct a benchmark study of representative\nself-supervised pre-training methods on large-scale web data in a like-for-like\nsetting. We compare a range of methods, including single-modal ones that use\nmasked training objectives and multi-modal ones that use image-text\nconstrastive training. We observe that existing multi-modal methods do not\noutperform their single-modal counterparts on vision transfer learning tasks.\nWe derive an information-theoretical view to explain these benchmark results,\nwhich provides insight into how to design a novel vision learner. Inspired by\nthis insight, we present a new visual representation pre-training method,\nMUlti-modal Generator~(MUG), that learns from scalable web sourced image-text\ndata. MUG achieves state-of-the-art transfer performance on a variety of tasks\nand demonstrates promising scaling properties. Pre-trained models and code will\nbe made public upon acceptance.\n","authors":["Bingchen Zhao","Quan Cui","Hao Wu","Osamu Yoshie","Cheng Yang","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2301.07088v2.pdf","comment":"Project page: https://bzhao.me/MUG/"},{"id":"http://arxiv.org/abs/2303.16251v2","updated":"2023-04-05T16:15:38Z","published":"2023-03-28T18:55:48Z","title":"Function Approximation with Randomly Initialized Neural Networks for\n  Approximate Model Reference Adaptive Control","summary":"  Classical results in neural network approximation theory show how arbitrary\ncontinuous functions can be approximated by networks with a single hidden\nlayer, under mild assumptions on the activation function. However, the\nclassical theory does not give a constructive means to generate the network\nparameters that achieve a desired accuracy. Recent results have demonstrated\nthat for specialized activation functions, such as ReLUs and some classes of\nanalytic functions, high accuracy can be achieved via linear combinations of\nrandomly initialized activations. These recent works utilize specialized\nintegral representations of target functions that depend on the specific\nactivation functions used. This paper defines mollified integral\nrepresentations, which provide a means to form integral representations of\ntarget functions using activations for which no direct integral representation\nis currently known. The new construction enables approximation guarantees for\nrandomly initialized networks for a variety of widely used activation\nfunctions.\n","authors":["Tyler Lekang","Andrew Lamperski"],"pdf_url":"https://arxiv.org/pdf/2303.16251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02549v1","updated":"2023-04-05T16:11:56Z","published":"2023-04-05T16:11:56Z","title":"Self-Supervised Siamese Autoencoders","summary":"  Fully supervised models often require large amounts of labeled training data,\nwhich tends to be costly and hard to acquire. In contrast, self-supervised\nrepresentation learning reduces the amount of labeled data needed for achieving\nthe same or even higher downstream performance. The goal is to pre-train deep\nneural networks on a self-supervised task such that afterwards the networks are\nable to extract meaningful features from raw input data. These features are\nthen used as inputs in downstream tasks, such as image classification.\nPreviously, autoencoders and Siamese networks such as SimSiam have been\nsuccessfully employed in those tasks. Yet, challenges remain, such as matching\ncharacteristics of the features (e.g., level of detail) to the given task and\ndata set. In this paper, we present a new self-supervised method that combines\nthe benefits of Siamese architectures and denoising autoencoders. We show that\nour model, called SidAE (Siamese denoising autoencoder), outperforms two\nself-supervised baselines across multiple data sets, settings, and scenarios.\nCrucially, this includes conditions in which only a small amount of labeled\ndata is available.\n","authors":["Friederike Baier","Sebastian Mair","Samuel G. Fadel"],"pdf_url":"https://arxiv.org/pdf/2304.02549v1.pdf","comment":"Under review. 15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2304.02539v1","updated":"2023-04-05T16:00:42Z","published":"2023-04-05T16:00:42Z","title":"Multi-annotator Deep Learning: A Probabilistic Framework for\n  Classification","summary":"  Solving complex classification tasks using deep neural networks typically\nrequires large amounts of annotated data. However, corresponding class labels\nare noisy when provided by error-prone annotators, e.g., crowd workers.\nTraining standard deep neural networks leads to subpar performances in such\nmulti-annotator supervised learning settings. We address this issue by\npresenting a probabilistic training framework named multi-annotator deep\nlearning (MaDL). A ground truth and an annotator performance model are jointly\ntrained in an end-to-end learning approach. The ground truth model learns to\npredict instances' true class labels, while the annotator performance model\ninfers probabilistic estimates of annotators' performances. A modular network\narchitecture enables us to make varying assumptions regarding annotators'\nperformances, e.g., an optional class or instance dependency. Further, we learn\nannotator embeddings to estimate annotators' densities within a latent space as\nproxies of their potentially correlated annotations. Together with a weighted\nloss function, we improve the learning from correlated annotation patterns. In\na comprehensive evaluation, we examine three research questions about\nmulti-annotator supervised learning. Our findings indicate MaDL's\nstate-of-the-art performance and robustness against many correlated, spamming\nannotators.\n","authors":["Marek Herde","Denis Huseljic","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2304.02539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02532v1","updated":"2023-04-05T15:52:34Z","published":"2023-04-05T15:52:34Z","title":"Goal-Conditioned Imitation Learning using Score-based Diffusion Policies","summary":"  We propose a new policy representation based on score-based diffusion models\n(SDMs). We apply our new policy representation in the domain of\nGoal-Conditioned Imitation Learning (GCIL) to learn general-purpose\ngoal-specified policies from large uncurated datasets without rewards. Our new\ngoal-conditioned policy architecture \"$\\textbf{BE}$havior generation with\n$\\textbf{S}$c$\\textbf{O}$re-based Diffusion Policies\" (BESO) leverages a\ngenerative, score-based diffusion model as its policy. BESO decouples the\nlearning of the score model from the inference sampling process, and, hence\nallows for fast sampling strategies to generate goal-specified behavior in just\n3 denoising steps, compared to 30+ steps of other diffusion based policies.\nFurthermore, BESO is highly expressive and can effectively capture\nmulti-modality present in the solution space of the play data. Unlike previous\nmethods such as Latent Plans or C-Bet, BESO does not rely on complex\nhierarchical policies or additional clustering for effective goal-conditioned\nbehavior learning. Finally, we show how BESO can even be used to learn a\ngoal-independent policy from play-data using classifier-free guidance. To the\nbest of our knowledge this is the first work that a) represents a behavior\npolicy based on such a decoupled SDM b) learns an SDM based policy in the\ndomain of GCIL and c) provides a way to simultaneously learn a goal-dependent\nand a goal-independent policy from play-data. We evaluate BESO through detailed\nsimulation and show that it consistently outperforms several state-of-the-art\ngoal-conditioned imitation learning methods on challenging benchmarks. We\nadditionally provide extensive ablation studies and experiments to demonstrate\nthe effectiveness of our method for effective goal-conditioned behavior\ngeneration.\n","authors":["Moritz Reuss","Maximilian Li","Xiaogang Jia","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2304.02532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02531v1","updated":"2023-04-05T15:52:07Z","published":"2023-04-05T15:52:07Z","title":"Learning to Compare Longitudinal Images","summary":"  Longitudinal studies, where a series of images from the same set of\nindividuals are acquired at different time-points, represent a popular\ntechnique for studying and characterizing temporal dynamics in biomedical\napplications. The classical approach for longitudinal comparison involves\nnormalizing for nuisance variations, such as image orientation or contrast\ndifferences, via pre-processing. Statistical analysis is, in turn, conducted to\ndetect changes of interest, either at the individual or population level. This\nclassical approach can suffer from pre-processing issues and limitations of the\nstatistical modeling. For example, normalizing for nuisance variation might be\nhard in settings where there are a lot of idiosyncratic changes. In this paper,\nwe present a simple machine learning-based approach that can alleviate these\nissues. In our approach, we train a deep learning model (called PaIRNet, for\nPairwise Image Ranking Network) to compare pairs of longitudinal images, with\nor without supervision. In the self-supervised setup, for instance, the model\nis trained to temporally order the images, which requires learning to recognize\ntime-irreversible changes. Our results from four datasets demonstrate that\nPaIRNet can be very effective in localizing and quantifying meaningful\nlongitudinal changes while discounting nuisance variation. Our code is\navailable at\n\\url{https://github.com/heejong-kim/learning-to-compare-longitudinal-images.git}\n","authors":["Heejong Kim","Mert R. Sabuncu"],"pdf_url":"https://arxiv.org/pdf/2304.02531v1.pdf","comment":"to be published in MIDL 2023"},{"id":"http://arxiv.org/abs/2304.01203v2","updated":"2023-04-05T15:39:08Z","published":"2023-04-03T17:59:58Z","title":"Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning","summary":"  In goal-reaching reinforcement learning (RL), the optimal value function has\na particular geometry, called quasimetric structure. This paper introduces\nQuasimetric Reinforcement Learning (QRL), a new RL method that utilizes\nquasimetric models to learn optimal value functions. Distinct from prior\napproaches, the QRL objective is specifically designed for quasimetrics, and\nprovides strong theoretical recovery guarantees. Empirically, we conduct\nthorough analyses on a discretized MountainCar environment, identifying\nproperties of QRL and its advantages over alternatives. On offline and online\ngoal-reaching benchmarks, QRL also demonstrates improved sample efficiency and\nperformance, across both state-based and image-based observations.\n","authors":["Tongzhou Wang","Antonio Torralba","Phillip Isola","Amy Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01203v2.pdf","comment":"Project Page: https://www.tongzhouwang.info/quasimetric_rl/"},{"id":"http://arxiv.org/abs/2212.05190v3","updated":"2023-04-05T15:25:54Z","published":"2022-12-10T03:43:23Z","title":"Neural Bandits for Data Mining: Searching for Dangerous Polypharmacy","summary":"  Polypharmacy, most often defined as the simultaneous consumption of five or\nmore drugs at once, is a prevalent phenomenon in the older population. Some of\nthese polypharmacies, deemed inappropriate, may be associated with adverse\nhealth outcomes such as death or hospitalization. Considering the combinatorial\nnature of the problem as well as the size of claims database and the cost to\ncompute an exact association measure for a given drug combination, it is\nimpossible to investigate every possible combination of drugs. Therefore, we\npropose to optimize the search for potentially inappropriate polypharmacies\n(PIPs). To this end, we propose the OptimNeuralTS strategy, based on Neural\nThompson Sampling and differential evolution, to efficiently mine claims\ndatasets and build a predictive model of the association between drug\ncombinations and health outcomes. We benchmark our method using two datasets\ngenerated by an internally developed simulator of polypharmacy data containing\n500 drugs and 100 000 distinct combinations. Empirically, our method can detect\nup to 72% of PIPs while maintaining an average precision score of 99% using 30\n000 time steps.\n","authors":["Alexandre Larouche","Audrey Durand","Richard Khoury","Caroline Sirois"],"pdf_url":"https://arxiv.org/pdf/2212.05190v3.pdf","comment":"This article is presented at the W3PHIAI-23 workshop at AAAI-2023"},{"id":"http://arxiv.org/abs/2303.16205v2","updated":"2023-04-05T15:22:32Z","published":"2023-03-27T15:12:10Z","title":"mHealth hyperspectral learning for instantaneous spatiospectral imaging\n  of hemodynamics","summary":"  Hyperspectral imaging acquires data in both the spatial and frequency domains\nto offer abundant physical or biological information. However, conventional\nhyperspectral imaging has intrinsic limitations of bulky instruments, slow data\nacquisition rate, and spatiospectral tradeoff. Here we introduce hyperspectral\nlearning for snapshot hyperspectral imaging in which sampled hyperspectral data\nin a small subarea are incorporated into a learning algorithm to recover the\nhypercube. Hyperspectral learning exploits the idea that a photograph is more\nthan merely a picture and contains detailed spectral information. A small\nsampling of hyperspectral data enables spectrally informed learning to recover\na hypercube from an RGB image. Hyperspectral learning is capable of recovering\nfull spectroscopic resolution in the hypercube, comparable to high spectral\nresolutions of scientific spectrometers. Hyperspectral learning also enables\nultrafast dynamic imaging, leveraging ultraslow video recording in an\noff-the-shelf smartphone, given that a video comprises a time series of\nmultiple RGB images. To demonstrate its versatility, an experimental model of\nvascular development is used to extract hemodynamic parameters via statistical\nand deep-learning approaches. Subsequently, the hemodynamics of peripheral\nmicrocirculation is assessed at an ultrafast temporal resolution up to a\nmillisecond, using a conventional smartphone camera. This spectrally informed\nlearning method is analogous to compressed sensing; however, it further allows\nfor reliable hypercube recovery and key feature extractions with a transparent\nlearning algorithm. This learning-powered snapshot hyperspectral imaging method\nyields high spectral and temporal resolutions and eliminates the spatiospectral\ntradeoff, offering simple hardware requirements and potential applications of\nvarious machine-learning techniques.\n","authors":["Yuhyun Ji","Sang Mok Park","Semin Kwon","Jung Woo Leem","Vidhya Vijayakrishnan Nair","Yunjie Tong","Young L. Kim"],"pdf_url":"https://arxiv.org/pdf/2303.16205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02497v1","updated":"2023-04-05T15:12:40Z","published":"2023-04-05T15:12:40Z","title":"Hyper-parameter Tuning for Adversarially Robust Models","summary":"  This work focuses on the problem of hyper-parameter tuning (HPT) for robust\n(i.e., adversarially trained) models, with the twofold goal of i) establishing\nwhich additional HPs are relevant to tune in adversarial settings, and ii)\nreducing the cost of HPT for robust models. We pursue the first goal via an\nextensive experimental study based on 3 recent models widely adopted in the\nprior literature on adversarial robustness. Our findings show that the\ncomplexity of the HPT problem, already notoriously expensive, is exacerbated in\nadversarial settings due to two main reasons: i) the need of tuning additional\nHPs which balance standard and adversarial training; ii) the need of tuning the\nHPs of the standard and adversarial training phases independently. Fortunately,\nwe also identify new opportunities to reduce the cost of HPT for robust models.\nSpecifically, we propose to leverage cheap adversarial training methods to\nobtain inexpensive, yet highly correlated, estimations of the quality\nachievable using state-of-the-art methods (PGD). We show that, by exploiting\nthis novel idea in conjunction with a recent multi-fidelity optimizer (taKG),\nthe efficiency of the HPT process can be significantly enhanced.\n","authors":["Pedro Mendes","Paolo Romano","David Garlan"],"pdf_url":"https://arxiv.org/pdf/2304.02497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02492v1","updated":"2023-04-05T15:08:21Z","published":"2023-04-05T15:08:21Z","title":"Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic\n  Complexity in Verb Acquisition","summary":"  Children typically learn the meanings of nouns earlier than the meanings of\nverbs. However, it is unclear whether this asymmetry is a result of complexity\nin the visual structure of categories in the world to which language refers,\nthe structure of language itself, or the interplay between the two sources of\ninformation. We quantitatively test these three hypotheses regarding early verb\nlearning by employing visual and linguistic representations of words sourced\nfrom large-scale pre-trained artificial neural networks. Examining the\nstructure of both visual and linguistic embedding spaces, we find, first, that\nthe representation of verbs is generally more variable and less discriminable\nwithin domain than the representation of nouns. Second, we find that if only\none learning instance per category is available, visual and linguistic\nrepresentations are less well aligned in the verb system than in the noun\nsystem. However, in parallel with the course of human language development, if\nmultiple learning instances per category are available, visual and linguistic\nrepresentations become almost as well aligned in the verb system as in the noun\nsystem. Third, we compare the relative contributions of factors that may\npredict learning difficulty for individual words. A regression analysis reveals\nthat visual variability is the strongest factor that internally drives verb\nlearning, followed by visual-linguistic alignment and linguistic variability.\nBased on these results, we conclude that verb acquisition is influenced by all\nthree sources of complexity, but that the variability of visual structure poses\nthe most significant challenge for verb learning.\n","authors":["Yuchen Zhou","Michael J. Tarr","Daniel Yurovsky"],"pdf_url":"https://arxiv.org/pdf/2304.02492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02490v1","updated":"2023-04-05T15:03:46Z","published":"2023-04-05T15:03:46Z","title":"Opening the random forest black box by the analysis of the mutual impact\n  of features","summary":"  Random forest is a popular machine learning approach for the analysis of\nhigh-dimensional data because it is flexible and provides variable importance\nmeasures for the selection of relevant features. However, the complex\nrelationships between the features are usually not considered for the selection\nand thus also neglected for the characterization of the analysed samples. Here\nwe propose two novel approaches that focus on the mutual impact of features in\nrandom forests. Mutual forest impact (MFI) is a relation parameter that\nevaluates the mutual association of the featurs to the outcome and, hence, goes\nbeyond the analysis of correlation coefficients. Mutual impurity reduction\n(MIR) is an importance measure that combines this relation parameter with the\nimportance of the individual features. MIR and MFI are implemented together\nwith testing procedures that generate p-values for the selection of related and\nimportant features. Applications to various simulated data sets and the\ncomparison to other methods for feature selection and relation analysis show\nthat MFI and MIR are very promising to shed light on the complex relationships\nbetween features and outcome. In addition, they are not affected by common\nbiases, e.g. that features with many possible splits or high minor allele\nfrequencies are prefered.\n","authors":["Lucas F. Voges","Lukas C. Jarren","Stephan Seifert"],"pdf_url":"https://arxiv.org/pdf/2304.02490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01476v2","updated":"2023-04-05T15:00:56Z","published":"2022-10-04T09:03:43Z","title":"Learning-based Design of Luenberger Observers for Autonomous Nonlinear\n  Systems","summary":"  Designing Luenberger observers for nonlinear systems involves the challenging\ntask of transforming the state to an alternate coordinate system, possibly of\nhigher dimensions, where the system is asymptotically stable and linear up to\noutput injection. The observer then estimates the system's state in the\noriginal coordinates by inverting the transformation map. However, finding a\nsuitable injective transformation whose inverse can be derived remains a\nprimary challenge for general nonlinear systems. We propose a novel approach\nthat uses supervised physics-informed neural networks to approximate both the\ntransformation and its inverse. Our method exhibits superior generalization\ncapabilities to contemporary methods and demonstrates robustness to both neural\nnetwork's approximation errors and system uncertainties.\n","authors":["Muhammad Umar B. Niazi","John Cao","Xudong Sun","Amritam Das","Karl Henrik Johansson"],"pdf_url":"https://arxiv.org/pdf/2210.01476v2.pdf","comment":"Proceedings of the 2023 American Control Conference (ACC)"},{"id":"http://arxiv.org/abs/2304.02484v1","updated":"2023-04-05T14:54:34Z","published":"2023-04-05T14:54:34Z","title":"A dynamic Bayesian optimized active recommender system for\n  curiosity-driven Human-in-the-loop automated experiments","summary":"  Optimization of experimental materials synthesis and characterization through\nactive learning methods has been growing over the last decade, with examples\nranging from measurements of diffraction on combinatorial alloys at\nsynchrotrons, to searches through chemical space with automated synthesis\nrobots for perovskites. In virtually all cases, the target property of interest\nfor optimization is defined apriori with limited human feedback during\noperation. In contrast, here we present the development of a new type of human\nin the loop experimental workflow, via a Bayesian optimized active recommender\nsystem (BOARS), to shape targets on the fly, employing human feedback. We\nshowcase examples of this framework applied to pre-acquired piezoresponse force\nspectroscopy of a ferroelectric thin film, and then implement this in real time\non an atomic force microscope, where the optimization proceeds to find\nsymmetric piezoresponse amplitude hysteresis loops. It is found that such\nfeatures appear more affected by subsurface defects than the local domain\nstructure. This work shows the utility of human-augmented machine learning\napproaches for curiosity-driven exploration of systems across experimental\ndomains. The analysis reported here is summarized in Colab Notebook for the\npurpose of tutorial and application to other data:\nhttps://github.com/arpanbiswas52/varTBO\n","authors":["Arpan Biswas","Yongtao Liu","Nicole Creange","Yu-Chen Liu","Stephen Jesse","Jan-Chi Yang","Sergei V. Kalinin","Maxim A. Ziatdinov","Rama K. Vasudevan"],"pdf_url":"https://arxiv.org/pdf/2304.02484v1.pdf","comment":"7 figures in main text, 3 figures in Supp Material"},{"id":"http://arxiv.org/abs/2304.02455v1","updated":"2023-04-05T14:26:23Z","published":"2023-04-05T14:26:23Z","title":"Selecting Features by their Resilience to the Curse of Dimensionality","summary":"  Real-world datasets are often of high dimension and effected by the curse of\ndimensionality. This hinders their comprehensibility and interpretability. To\nreduce the complexity feature selection aims to identify features that are\ncrucial to learn from said data. While measures of relevance and pairwise\nsimilarities are commonly used, the curse of dimensionality is rarely\nincorporated into the process of selecting features. Here we step in with a\nnovel method that identifies the features that allow to discriminate data\nsubsets of different sizes. By adapting recent work on computing intrinsic\ndimensionalities, our method is able to select the features that can\ndiscriminate data and thus weaken the curse of dimensionality. Our experiments\nshow that our method is competitive and commonly outperforms established\nfeature selection methods. Furthermore, we propose an approximation that allows\nour method to scale to datasets consisting of millions of data points. Our\nfindings suggest that features that discriminate data and are connected to a\nlow intrinsic dimensionality are meaningful for learning procedures.\n","authors":["Maximilian Stubbemann","Tobias Hille","Tom Hanika"],"pdf_url":"https://arxiv.org/pdf/2304.02455v1.pdf","comment":"16 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2208.03923v2","updated":"2023-04-05T14:07:06Z","published":"2022-08-08T05:53:57Z","title":"Adversarial robustness of VAEs through the lens of local geometry","summary":"  In an unsupervised attack on variational autoencoders (VAEs), an adversary\nfinds a small perturbation in an input sample that significantly changes its\nlatent space encoding, thereby compromising the reconstruction for a fixed\ndecoder. A known reason for such vulnerability is the distortions in the latent\nspace resulting from a mismatch between approximated latent posterior and a\nprior distribution. Consequently, a slight change in an input sample can move\nits encoding to a low/zero density region in the latent space resulting in an\nunconstrained generation. This paper demonstrates that an optimal way for an\nadversary to attack VAEs is to exploit a directional bias of a stochastic\npullback metric tensor induced by the encoder and decoder networks. The\npullback metric tensor of an encoder measures the change in infinitesimal\nlatent volume from an input to a latent space. Thus, it can be viewed as a lens\nto analyse the effect of input perturbations leading to latent space\ndistortions. We propose robustness evaluation scores using the eigenspectrum of\na pullback metric tensor. Moreover, we empirically show that the scores\ncorrelate with the robustness parameter $\\beta$ of the $\\beta-$VAE. Since\nincreasing $\\beta$ also degrades reconstruction quality, we demonstrate a\nsimple alternative using \\textit{mixup} training to fill the empty regions in\nthe latent space, thus improving robustness with improved reconstruction.\n","authors":["Asif Khan","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2208.03923v2.pdf","comment":"International Conference on Artificial Intelligence and Statistics\n  (AISTATS) 2023"},{"id":"http://arxiv.org/abs/2304.01240v2","updated":"2023-04-05T14:02:53Z","published":"2023-04-03T11:56:11Z","title":"Identifying Mentions of Pain in Mental Health Records Text: A Natural\n  Language Processing Approach","summary":"  Pain is a common reason for accessing healthcare resources and is a growing\narea of research, especially in its overlap with mental health. Mental health\nelectronic health records are a good data source to study this overlap.\nHowever, much information on pain is held in the free text of these records,\nwhere mentions of pain present a unique natural language processing problem due\nto its ambiguous nature. This project uses data from an anonymised mental\nhealth electronic health records database. The data are used to train a machine\nlearning based classification algorithm to classify sentences as discussing\npatient pain or not. This will facilitate the extraction of relevant pain\ninformation from large databases, and the use of such outputs for further\nstudies on pain and mental health. 1,985 documents were manually\ntriple-annotated for creation of gold standard training data, which was used to\ntrain three commonly used classification algorithms. The best performing model\nachieved an F1-score of 0.98 (95% CI 0.98-0.99).\n","authors":["Jaya Chaturvedi","Sumithra Velupillai","Robert Stewart","Angus Roberts"],"pdf_url":"https://arxiv.org/pdf/2304.01240v2.pdf","comment":"5 pages, 2 tables, submitted to MEDINFO 2023 conference"},{"id":"http://arxiv.org/abs/2304.02441v1","updated":"2023-04-05T13:54:43Z","published":"2023-04-05T13:54:43Z","title":"Decentralized gradient descent maximization method for composite\n  nonconvex strongly-concave minimax problems","summary":"  Minimax problems have recently attracted a lot of research interests. A few\nefforts have been made to solve decentralized nonconvex strongly-concave (NCSC)\nminimax-structured optimization; however, all of them focus on smooth problems\nwith at most a constraint on the maximization variable. In this paper, we make\nthe first attempt on solving composite NCSC minimax problems that can have\nconvex nonsmooth terms on both minimization and maximization variables. Our\nalgorithm is designed based on a novel reformulation of the decentralized\nminimax problem that introduces a multiplier to absorb the dual consensus\nconstraint. The removal of dual consensus constraint enables the most\naggressive (i.e., local maximization instead of a gradient ascent step) dual\nupdate that leads to the benefit of taking a larger primal stepsize and better\ncomplexity results. In addition, the decoupling of the nonsmoothness and\nconsensus on the dual variable eases the analysis of a decentralized algorithm;\nthus our reformulation creates a new way for interested researchers to design\nnew (and possibly more efficient) decentralized methods on solving NCSC minimax\nproblems. We show a global convergence result of the proposed algorithm and an\niteration complexity result to produce a (near) stationary point of the\nreformulation. Moreover, a relation is established between the (near)\nstationarities of the reformulation and the original formulation. With this\nrelation, we show that when the dual regularizer is smooth, our algorithm can\nhave lower complexity results (with reduced dependence on a condition number)\nthan existing ones to produce a near-stationary point of the original\nformulation. Numerical experiments are conducted on a distributionally robust\nlogistic regression to demonstrate the performance of the proposed algorithm.\n","authors":["Yangyang Xu"],"pdf_url":"https://arxiv.org/pdf/2304.02441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16999v2","updated":"2023-04-05T13:43:15Z","published":"2023-03-29T20:00:19Z","title":"PopSparse: Accelerated block sparse matrix multiplication on IPU","summary":"  Reducing the computational cost of running large scale neural networks using\nsparsity has attracted great attention in the deep learning community. While\nmuch success has been achieved in reducing FLOP and parameter counts while\nmaintaining acceptable task performance, achieving actual speed improvements\nhas typically been much more difficult, particularly on general purpose\naccelerators (GPAs) such as NVIDIA GPUs using low precision number formats. In\nthis work we introduce PopSparse, a library that enables fast sparse operations\non Graphcore IPUs by leveraging both the unique hardware characteristics of\nIPUs as well as any block structure defined in the data. We target two\ndifferent types of sparsity: static, where the sparsity pattern is fixed at\ncompile-time; and dynamic, where it can change each time the model is run. We\npresent benchmark results for matrix multiplication for both of these modes on\nIPU with a range of block sizes, matrix sizes and densities. Results indicate\nthat the PopSparse implementations are faster than dense matrix multiplications\non IPU at a range of sparsity levels with large matrix size and block size.\nFurthermore, static sparsity in general outperforms dynamic sparsity. While\nprevious work on GPAs has shown speedups only for very high sparsity (typically\n99\\% and above), the present work demonstrates that our static sparse\nimplementation outperforms equivalent dense calculations in FP16 at lower\nsparsity (around 90%). IPU code is available to view and run at\nipu.dev/sparsity-benchmarks, GPU code will be made available shortly.\n","authors":["Zhiyi Li","Douglas Orr","Valeriu Ohan","Godfrey Da costa","Tom Murray","Adam Sanders","Deniz Beker","Dominic Masters"],"pdf_url":"https://arxiv.org/pdf/2303.16999v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17566v2","updated":"2023-04-05T13:34:47Z","published":"2023-03-30T17:30:42Z","title":"Non-Invasive Fairness in Learning through the Lens of Data Drift","summary":"  Machine Learning (ML) models are widely employed to drive many modern data\nsystems. While they are undeniably powerful tools, ML models often demonstrate\nimbalanced performance and unfair behaviors. The root of this problem often\nlies in the fact that different subpopulations commonly display divergent\ntrends: as a learning algorithm tries to identify trends in the data, it\nnaturally favors the trends of the majority groups, leading to a model that\nperforms poorly and unfairly for minority populations. Our goal is to improve\nthe fairness and trustworthiness of ML models by applying only non-invasive\ninterventions, i.e., without altering the data or the learning algorithm. We\nuse a simple but key insight: the divergence of trends between different\npopulations, and, consecutively, between a learned model and minority\npopulations, is analogous to data drift, which indicates the poor conformance\nbetween parts of the data and the trained model. We explore two strategies\n(model-splitting and reweighing) to resolve this drift, aiming to improve the\noverall conformance of models to the underlying data. Both our methods\nintroduce novel ways to employ the recently-proposed data profiling primitive\nof Conformance Constraints. Our experimental evaluation over 7 real-world\ndatasets shows that both DifFair and ConFair improve the fairness of ML models.\nWe demonstrate scenarios where DifFair has an edge, though ConFair has the\ngreatest practical impact and outperforms other baselines. Moreover, as a\nmodel-agnostic technique, ConFair stays robust when used against different\nmodels than the ones on which the weights have been learned, which is not the\ncase for other state of the art.\n","authors":["Ke Yang","Alexandra Meliou"],"pdf_url":"https://arxiv.org/pdf/2303.17566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15413v2","updated":"2023-04-05T13:33:55Z","published":"2023-03-27T17:31:13Z","title":"Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D\n  Generation","summary":"  The view inconsistency problem in score-distilling text-to-3D generation,\nalso known as the Janus problem, arises from the intrinsic bias of 2D diffusion\nmodels, which leads to the unrealistic generation of 3D objects. In this work,\nwe explore score-distilling text-to-3D generation and identify the main causes\nof the Janus problem. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for robust text-to-3D generation. Our\nfirst approach, called score debiasing, involves gradually increasing the\ntruncation value for the score estimated by 2D diffusion models throughout the\noptimization process. Our second approach, called prompt debiasing, identifies\nconflicting words between user prompts and view prompts utilizing a language\nmodel and adjusts the discrepancy between view prompts and object-space camera\nposes. Our experimental results show that our methods improve realism by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead.\n","authors":["Susung Hong","Donghoon Ahn","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15413v2.pdf","comment":"CVPR 2023 GCV workshop"},{"id":"http://arxiv.org/abs/2210.00875v3","updated":"2023-04-05T13:32:57Z","published":"2022-09-27T12:56:56Z","title":"Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset\n  Copyright Protection","summary":"  Deep neural networks (DNNs) have demonstrated their superiority in practice.\nArguably, the rapid development of DNNs is largely benefited from high-quality\n(open-sourced) datasets, based on which researchers and developers can easily\nevaluate and improve their learning methods. Since the data collection is\nusually time-consuming or even expensive, how to protect their copyrights is of\ngreat significance and worth further exploration. In this paper, we revisit\ndataset ownership verification. We find that existing verification methods\nintroduced new security risks in DNNs trained on the protected dataset, due to\nthe targeted nature of poison-only backdoor watermarks. To alleviate this\nproblem, in this work, we explore the untargeted backdoor watermarking scheme,\nwhere the abnormal model behaviors are not deterministic. Specifically, we\nintroduce two dispersibilities and prove their correlation, based on which we\ndesign the untargeted backdoor watermark under both poisoned-label and\nclean-label settings. We also discuss how to use the proposed untargeted\nbackdoor watermark for dataset ownership verification. Experiments on benchmark\ndatasets verify the effectiveness of our methods and their resistance to\nexisting backdoor defenses. Our codes are available at\n\\url{https://github.com/THUYimingLi/Untargeted_Backdoor_Watermark}.\n","authors":["Yiming Li","Yang Bai","Yong Jiang","Yong Yang","Shu-Tao Xia","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2210.00875v3.pdf","comment":"This work is accepted by the NeurIPS 2022 (Oral, TOP 2%). The first\n  two authors contributed equally to this work. 25 pages. We have fixed some\n  typos in the previous version"},{"id":"http://arxiv.org/abs/2212.05102v2","updated":"2023-04-05T13:29:13Z","published":"2022-12-09T20:03:59Z","title":"A soft nearest-neighbor framework for continual semi-supervised learning","summary":"  Despite significant advances, the performance of state-of-the-art continual\nlearning approaches hinges on the unrealistic scenario of fully labeled data.\nIn this paper, we tackle this challenge and propose an approach for continual\nsemi-supervised learning--a setting where not all the data samples are labeled.\nA primary issue in this scenario is the model forgetting representations of\nunlabeled data and overfitting the labeled samples. We leverage the power of\nnearest-neighbor classifiers to nonlinearly partition the feature space and\nflexibly model the underlying data distribution thanks to its non-parametric\nnature. This enables the model to learn a strong representation for the current\ntask, and distill relevant information from previous tasks. We perform a\nthorough experimental evaluation and show that our method outperforms all the\nexisting approaches by large margins, setting a solid state of the art on the\ncontinual semi-supervised learning paradigm. For example, on CIFAR-100 we\nsurpass several others even when using at least 30 times less supervision (0.8%\nvs. 25% of annotations). Finally, our method works well on both low and high\nresolution images and scales seamlessly to more complex datasets such as\nImageNet-100. The code is publicly available on\nhttps://github.com/kangzhiq/NNCSL\n","authors":["Zhiqi Kang","Enrico Fini","Moin Nabi","Elisa Ricci","Karteek Alahari"],"pdf_url":"https://arxiv.org/pdf/2212.05102v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2211.15145v2","updated":"2023-04-05T13:28:44Z","published":"2022-11-28T08:56:45Z","title":"Emerging trends in machine learning for computational fluid dynamics","summary":"  The renewed interest from the scientific community in machine learning (ML)\nis opening many new areas of research. Here we focus on how novel trends in ML\nare providing opportunities to improve the field of computational fluid\ndynamics (CFD). In particular, we discuss synergies between ML and CFD that\nhave already shown benefits, and we also assess areas that are under\ndevelopment and may produce important benefits in the coming years. We believe\nthat it is also important to emphasize a balanced perspective of cautious\noptimism for these emerging approaches\n","authors":["Ricardo Vinuesa","Steve Brunton"],"pdf_url":"https://arxiv.org/pdf/2211.15145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00971v2","updated":"2023-04-05T13:27:21Z","published":"2023-04-03T13:41:35Z","title":"Joint 2D-3D Multi-Task Learning on Cityscapes-3D: 3D Detection,\n  Segmentation, and Depth Estimation","summary":"  This report serves as a supplementary document for TaskPrompter, detailing\nits implementation on a new joint 2D-3D multi-task learning benchmark based on\nCityscapes-3D. TaskPrompter presents an innovative multi-task prompting\nframework that unifies the learning of (i) task-generic representations, (ii)\ntask-specific representations, and (iii) cross-task interactions, as opposed to\nprevious approaches that separate these learning objectives into different\nnetwork modules. This unified approach not only reduces the need for meticulous\nempirical structure design but also significantly enhances the multi-task\nnetwork's representation learning capability, as the entire model capacity is\ndevoted to optimizing the three objectives simultaneously. TaskPrompter\nintroduces a new multi-task benchmark based on Cityscapes-3D dataset, which\nrequires the multi-task model to concurrently generate predictions for\nmonocular 3D vehicle detection, semantic segmentation, and monocular depth\nestimation. These tasks are essential for achieving a joint 2D-3D understanding\nof visual scenes, particularly in the development of autonomous driving\nsystems. On this challenging benchmark, our multi-task model demonstrates\nstrong performance compared to single-task state-of-the-art methods and\nestablishes new state-of-the-art results on the challenging 3D detection and\ndepth estimation tasks.\n","authors":["Hanrong Ye"],"pdf_url":"https://arxiv.org/pdf/2304.00971v2.pdf","comment":"A supplementary document for \"TaskPrompter: Spatial-Channel\n  Multi-Task Prompting for Dense Scene Understanding\" accepted by ICLR 2023.\n  Project page:\n  https://github.com/prismformore/Multi-Task-Transformer/tree/main/TaskPrompter"},{"id":"http://arxiv.org/abs/2212.04451v2","updated":"2023-04-05T13:26:52Z","published":"2022-12-06T20:33:19Z","title":"Three Variations on Variational Autoencoders","summary":"  Variational autoencoders (VAEs) are one class of generative probabilistic\nlatent-variable models designed for inference based on known data. We develop\nthree variations on VAEs by introducing a second parameterized encoder/decoder\npair and, for one variation, an additional fixed encoder. The parameters of the\nencoders/decoders are to be learned with a neural network. The fixed encoder is\nobtained by probabilistic-PCA. The variations are compared to the Evidence\nLower Bound (ELBO) approximation to the original VAE. One variation leads to an\nEvidence Upper Bound (EUBO) that can be used in conjunction with the original\nELBO to interrogate the convergence of the VAE.\n","authors":["R. I. Cukier"],"pdf_url":"https://arxiv.org/pdf/2212.04451v2.pdf","comment":"21 pages. This version, v2, has added an explicit evaluation of our\n  VAE A variational encoder. The new result is summarized in new Section 4 VAE\n  A explicitly and detailed in Appendices B and C"},{"id":"http://arxiv.org/abs/2301.13622v2","updated":"2023-04-05T13:09:54Z","published":"2023-01-31T13:29:19Z","title":"Learning Data Representations with Joint Diffusion Models","summary":"  Joint machine learning models that allow synthesizing and classifying data\noften offer uneven performance between those tasks or are unstable to train. In\nthis work, we depart from a set of empirical observations that indicate the\nusefulness of internal representations built by contemporary deep\ndiffusion-based generative models not only for generating but also predicting.\nWe then propose to extend the vanilla diffusion model with a classifier that\nallows for stable joint end-to-end training with shared parameterization\nbetween those objectives. The resulting joint diffusion model outperforms\nrecent state-of-the-art hybrid methods in terms of both classification and\ngeneration quality on all evaluated benchmarks. On top of our joint training\napproach, we present how we can directly benefit from shared generative and\ndiscriminative representations by introducing a method for visual\ncounterfactual explanations.\n","authors":["Kamil Deja","Tomasz Trzcinski","Jakub M. Tomczak"],"pdf_url":"https://arxiv.org/pdf/2301.13622v2.pdf","comment":"Code: https://github.com/KamilDeja/joint_diffusion"},{"id":"http://arxiv.org/abs/2303.17218v3","updated":"2023-04-05T12:57:48Z","published":"2023-03-30T08:25:27Z","title":"HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on\n  FPGA Devices","summary":"  For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks\nhave proven to be highly effective, achieving state-of-the-art results. This\nstudy introduces a novel streaming architecture based toolflow for mapping such\nmodels onto FPGAs considering the model's inherent characteristics and the\nfeatures of the targeted FPGA device. The HARFLOW3D toolflow takes as input a\n3D CNN in ONNX format and a description of the FPGA characteristics, generating\na design that minimizes the latency of the computation. The toolflow is\ncomprised of a number of parts, including i) a 3D CNN parser, ii) a performance\nand resource model, iii) a scheduling algorithm for executing 3D models on the\ngenerated hardware, iv) a resource-aware optimization engine tailored for 3D\nmodels, v) an automated mapping to synthesizable code for FPGAs. The ability of\nthe toolflow to support a broad range of models and devices is shown through a\nnumber of experiments on various 3D CNN and FPGA system pairs. Furthermore, the\ntoolflow has produced high-performing results for 3D CNN models that have not\nbeen mapped to FPGAs before, demonstrating the potential of FPGA-based systems\nin this space. Overall, HARFLOW3D has demonstrated its ability to deliver\ncompetitive latency compared to a range of state-of-the-art hand-tuned\napproaches being able to achieve up to 5$\\times$ better performance compared to\nsome of the existing works.\n","authors":["Petros Toupas","Alexander Montgomerie-Corcoran","Christos-Savvas Bouganis","Dimitrios Tzovaras"],"pdf_url":"https://arxiv.org/pdf/2303.17218v3.pdf","comment":"11 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2112.12985v2","updated":"2023-04-05T12:45:33Z","published":"2021-12-24T07:58:42Z","title":"DeepGANTT: A Scalable Deep Learning Scheduler for Backscatter Networks","summary":"  Novel backscatter communication techniques enable battery-free sensor tags to\ninteroperate with unmodified standard IoT devices, extending a sensor network's\ncapabilities in a scalable manner. Without requiring additional dedicated\ninfrastructure, the battery-free tags harvest energy from the environment,\nwhile the IoT devices provide them with the unmodulated carrier they need to\ncommunicate. A schedule coordinates the provision of carriers for the\ncommunications of battery-free devices with IoT nodes. Optimal carrier\nscheduling is an NP-hard problem that limits the scalability of network\ndeployments. Thus, existing solutions waste energy and other valuable resources\nby scheduling the carriers suboptimally. We present DeepGANTT, a deep learning\nscheduler that leverages graph neural networks to efficiently provide\nnear-optimal carrier scheduling. We train our scheduler with relatively small\noptimal schedules obtained from a constraint optimization solver, achieving a\nperformance within 3% of the optimal scheduler. Without the need to retrain,\nDeepGANTT generalizes to networks 6x larger in the number of nodes and 10x\nlarger in the number of tags than those used for training, breaking the\nscalability limitations of the optimal scheduler and reducing carrier\nutilization by up to 50% compared to the state-of-the-art heuristic. Our\nscheduler efficiently reduces energy and spectrum utilization in backscatter\nnetworks.\n","authors":["Daniel F. Perez-Ramirez","Carlos Pérez-Penichet","Nicolas Tsiftes","Thiemo Voigt","Dejan Kostic","Magnus Boman"],"pdf_url":"https://arxiv.org/pdf/2112.12985v2.pdf","comment":"11 pages (excluding references). Submitted version that was accepted\n  to IPSN 2023 (not the camera-ready version). Camera-ready version available\n  here: https://doi.org/10.1145/3583120.3586957"},{"id":"http://arxiv.org/abs/2109.07704v4","updated":"2023-04-05T12:38:32Z","published":"2021-09-16T03:54:54Z","title":"Federated Submodel Optimization for Hot and Cold Data Features","summary":"  We study practical data characteristics underlying federated learning, where\nnon-i.i.d. data from clients have sparse features, and a certain client's local\ndata normally involves only a small part of the full model, called a submodel.\nDue to data sparsity, the classical federated averaging (FedAvg) algorithm or\nits variants will be severely slowed down, because when updating the global\nmodel, each client's zero update of the full model excluding its submodel is\ninaccurately aggregated. Therefore, we propose federated submodel averaging\n(FedSubAvg), ensuring that the expectation of the global update of each model\nparameter is equal to the average of the local updates of the clients who\ninvolve it. We theoretically proved the convergence rate of FedSubAvg by\nderiving an upper bound under a new metric called the element-wise gradient\nnorm. In particular, this new metric can characterize the convergence of\nfederated optimization over sparse data, while the conventional metric of\nsquared gradient norm used in FedAvg and its variants cannot. We extensively\nevaluated FedSubAvg over both public and industrial datasets. The evaluation\nresults demonstrate that FedSubAvg significantly outperforms FedAvg and its\nvariants.\n","authors":["Yucheng Ding","Chaoyue Niu","Fan Wu","Shaojie Tang","Chengfei Lv","Yanghe Feng","Guihai Chen"],"pdf_url":"https://arxiv.org/pdf/2109.07704v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02407v1","updated":"2023-04-05T12:35:02Z","published":"2023-04-05T12:35:02Z","title":"Explaining Multimodal Data Fusion: Occlusion Analysis for Wilderness\n  Mapping","summary":"  Jointly harnessing complementary features of multi-modal input data in a\ncommon latent space has been found to be beneficial long ago. However, the\ninfluence of each modality on the models decision remains a puzzle. This study\nproposes a deep learning framework for the modality-level interpretation of\nmultimodal earth observation data in an end-to-end fashion. While leveraging an\nexplainable machine learning method, namely Occlusion Sensitivity, the proposed\nframework investigates the influence of modalities under an early-fusion\nscenario in which the modalities are fused before the learning process. We show\nthat the task of wilderness mapping largely benefits from auxiliary data such\nas land cover and night time light data.\n","authors":["Burak Ekim","Michael Schmitt"],"pdf_url":"https://arxiv.org/pdf/2304.02407v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2301.13731v2","updated":"2023-04-05T12:20:25Z","published":"2023-01-31T16:11:47Z","title":"A relaxed proximal gradient descent algorithm for convergent\n  plug-and-play with proximal denoiser","summary":"  This paper presents a new convergent Plug-and-Play (PnP) algorithm. PnP\nmethods are efficient iterative algorithms for solving image inverse problems\nformulated as the minimization of the sum of a data-fidelity term and a\nregularization term. PnP methods perform regularization by plugging a\npre-trained denoiser in a proximal algorithm, such as Proximal Gradient Descent\n(PGD). To ensure convergence of PnP schemes, many works study specific\nparametrizations of deep denoisers. However, existing results require either\nunverifiable or suboptimal hypotheses on the denoiser, or assume restrictive\nconditions on the parameters of the inverse problem. Observing that these\nlimitations can be due to the proximal algorithm in use, we study a relaxed\nversion of the PGD algorithm for minimizing the sum of a convex function and a\nweakly convex one. When plugged with a relaxed proximal denoiser, we show that\nthe proposed PnP-$\\alpha$PGD algorithm converges for a wider range of\nregularization parameters, thus allowing more accurate image restoration.\n","authors":["Samuel Hurault","Antonin Chambolle","Arthur Leclaire","Nicolas Papadakis"],"pdf_url":"https://arxiv.org/pdf/2301.13731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02396v1","updated":"2023-04-05T12:14:41Z","published":"2023-04-05T12:14:41Z","title":"AutoRL Hyperparameter Landscapes","summary":"  Although Reinforcement Learning (RL) has shown to be capable of producing\nimpressive results, its use is limited by the impact of its hyperparameters on\nperformance. This often makes it difficult to achieve good results in practice.\nAutomated RL (AutoRL) addresses this difficulty, yet little is known about the\ndynamics of the hyperparameter landscapes that hyperparameter optimization\n(HPO) methods traverse in search of optimal configurations. In view of existing\nAutoRL approaches dynamically adjusting hyperparameter configurations, we\npropose an approach to build and analyze these hyperparameter landscapes not\njust for one point in time but at multiple points in time throughout training.\nAddressing an important open question on the legitimacy of such dynamic AutoRL\napproaches, we provide thorough empirical evidence that the hyperparameter\nlandscapes strongly vary over time across representative algorithms from RL\nliterature (DQN and SAC) in different kinds of environments (Cartpole and\nHopper). This supports the theory that hyperparameters should be dynamically\nadjusted during training and shows the potential for more insights on AutoRL\nproblems that can be gained through landscape analyses.\n","authors":["Aditya Mohan","Carolin Benjamins","Konrad Wienecke","Alexander Dockhorn","Marius Lindauer"],"pdf_url":"https://arxiv.org/pdf/2304.02396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00830v2","updated":"2023-04-05T12:13:48Z","published":"2023-04-03T09:15:51Z","title":"AUDIT: Audio Editing by Following Instructions with Latent Diffusion\n  Models","summary":"  Audio editing is applicable for various purposes, such as adding background\nsound effects, replacing a musical instrument, and repairing damaged audio.\nRecently, some diffusion-based methods achieved zero-shot audio editing by\nusing a diffusion and denoising process conditioned on the text description of\nthe output audio. However, these methods still have some problems: 1) they have\nnot been trained on editing tasks and cannot ensure good editing effects; 2)\nthey can erroneously modify audio segments that do not require editing; 3) they\nneed a complete description of the output audio, which is not always available\nor necessary in practical scenarios. In this work, we propose AUDIT, an\ninstruction-guided audio editing model based on latent diffusion models.\nSpecifically, AUDIT has three main design features: 1) we construct triplet\ntraining data (instruction, input audio, output audio) for different audio\nediting tasks and train a diffusion model using instruction and input (to be\nedited) audio as conditions and generating output (edited) audio; 2) it can\nautomatically learn to only modify segments that need to be edited by comparing\nthe difference between the input and output audio; 3) it only needs edit\ninstructions instead of full target audio descriptions as text input. AUDIT\nachieves state-of-the-art results in both objective and subjective metrics for\nseveral audio editing tasks (e.g., adding, dropping, replacement, inpainting,\nsuper-resolution). Demo samples are available at https://audit-demo.github.io/.\n","authors":["Yuancheng Wang","Zeqian Ju","Xu Tan","Lei He","Zhizheng Wu","Jiang Bian","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2304.00830v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02389v1","updated":"2023-04-05T12:04:55Z","published":"2023-04-05T12:04:55Z","title":"DRAC: Diabetic Retinopathy Analysis Challenge with Ultra-Wide Optical\n  Coherence Tomography Angiography Images","summary":"  Computer-assisted automatic analysis of diabetic retinopathy (DR) is of great\nimportance in reducing the risks of vision loss and even blindness. Ultra-wide\noptical coherence tomography angiography (UW-OCTA) is a non-invasive and safe\nimaging modality in DR diagnosis system, but there is a lack of publicly\navailable benchmarks for model development and evaluation. To promote further\nresearch and scientific benchmarking for diabetic retinopathy analysis using\nUW-OCTA images, we organized a challenge named \"DRAC - Diabetic Retinopathy\nAnalysis Challenge\" in conjunction with the 25th International Conference on\nMedical Image Computing and Computer Assisted Intervention (MICCAI 2022). The\nchallenge consists of three tasks: segmentation of DR lesions, image quality\nassessment and DR grading. The scientific community responded positively to the\nchallenge, with 11, 12, and 13 teams from geographically diverse institutes\nsubmitting different solutions in these three tasks, respectively. This paper\npresents a summary and analysis of the top-performing solutions and results for\neach task of the challenge. The obtained results from top algorithms indicate\nthe importance of data augmentation, model architecture and ensemble of\nnetworks in improving the performance of deep learning models. These findings\nhave the potential to enable new developments in diabetic retinopathy analysis.\nThe challenge remains open for post-challenge registrations and submissions for\nbenchmarking future methodology developments.\n","authors":["Bo Qian","Hao Chen","Xiangning Wang","Haoxuan Che","Gitaek Kwon","Jaeyoung Kim","Sungjin Choi","Seoyoung Shin","Felix Krause","Markus Unterdechler","Junlin Hou","Rui Feng","Yihao Li","Mostafa El Habib Daho","Qiang Wu","Ping Zhang","Xiaokang Yang","Yiyu Cai","Weiping Jia","Huating Li","Bin Sheng"],"pdf_url":"https://arxiv.org/pdf/2304.02389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01994v2","updated":"2023-04-05T11:57:48Z","published":"2023-04-04T17:52:49Z","title":"Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image\n  Super-Resolution","summary":"  This paper presents a novel Diffusion-Wavelet (DiWa) approach for\nSingle-Image Super-Resolution (SISR). It leverages the strengths of Denoising\nDiffusion Probabilistic Models (DDPMs) and Discrete Wavelet Transformation\n(DWT). By enabling DDPMs to operate in the DWT domain, our DDPM models\neffectively hallucinate high-frequency information for super-resolved images on\nthe wavelet spectrum, resulting in high-quality and detailed reconstructions in\nimage space. Quantitatively, we outperform state-of-the-art diffusion-based\nSISR methods, namely SR3 and SRDiff, regarding PSNR, SSIM, and LPIPS on both\nface (8x scaling) and general (4x scaling) SR benchmarks. Meanwhile, using DWT\nenabled us to use fewer parameters than the compared models: 92M parameters\ninstead of 550M compared to SR3 and 9.3M instead of 12M compared to SRDiff.\nAdditionally, our method outperforms other state-of-the-art generative methods\non classical general SR datasets while saving inference time. Finally, our work\nhighlights its potential for various applications.\n","authors":["Brian Moser","Stanislav Frolov","Federico Raue","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2304.01994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02383v1","updated":"2023-04-05T11:47:27Z","published":"2023-04-05T11:47:27Z","title":"How good Neural Networks interpretation methods really are? A\n  quantitative benchmark","summary":"  Saliency Maps (SMs) have been extensively used to interpret deep learning\nmodels decision by highlighting the features deemed relevant by the model. They\nare used on highly nonlinear problems, where linear feature selection (FS)\nmethods fail at highlighting relevant explanatory variables. However, the\nreliability of gradient-based feature attribution methods such as SM has mostly\nbeen only qualitatively (visually) assessed, and quantitative benchmarks are\ncurrently missing, partially due to the lack of a definite ground truth on\nimage data. Concerned about the apophenic biases introduced by visual\nassessment of these methods, in this paper we propose a synthetic quantitative\nbenchmark for Neural Networks (NNs) interpretation methods. For this purpose,\nwe built synthetic datasets with nonlinearly separable classes and increasing\nnumber of decoy (random) features, illustrating the challenge of FS in\nhigh-dimensional settings. We also compare these methods to conventional\napproaches such as mRMR or Random Forests. Our results show that our simple\nsynthetic datasets are sufficient to challenge most of the benchmarked methods.\nTreeShap, mRMR and LassoNet are the best performing FS methods. We also show\nthat, when quantifying the relevance of a few non linearly-entangled predictive\nfeatures diluted in a large number of irrelevant noisy variables, neural\nnetwork-based FS and interpretation methods are still far from being reliable.\n","authors":["Antoine Passemiers","Pietro Folco","Daniele Raimondi","Giovanni Birolo","Yves Moreau","Piero Fariselli"],"pdf_url":"https://arxiv.org/pdf/2304.02383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.09098v2","updated":"2023-04-05T11:37:51Z","published":"2022-06-18T03:29:49Z","title":"Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary\n  Classification","summary":"  Adversarial training is one of the most popular methods for training methods\nrobust to adversarial attacks, however, it is not well-understood from a\ntheoretical perspective. We prove and existence, regularity, and minimax\ntheorems for adversarial surrogate risks. Our results explain some empirical\nobservations on adversarial robustness from prior work and suggest new\ndirections in algorithm development. Furthermore, our results extend previously\nknown existence and minimax theorems for the adversarial classification risk to\nsurrogate risks.\n","authors":["Natalie S. Frank Jonathan Niles-Weed"],"pdf_url":"https://arxiv.org/pdf/2206.09098v2.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2304.02381v1","updated":"2023-04-05T11:35:17Z","published":"2023-04-05T11:35:17Z","title":"Physics-Inspired Interpretability Of Machine Learning Models","summary":"  The ability to explain decisions made by machine learning models remains one\nof the most significant hurdles towards widespread adoption of AI in highly\nsensitive areas such as medicine, cybersecurity or autonomous driving. Great\ninterest exists in understanding which features of the input data prompt model\ndecision making. In this contribution, we propose a novel approach to identify\nrelevant features of the input data, inspired by methods from the energy\nlandscapes field, developed in the physical sciences. By identifying conserved\nweights within groups of minima of the loss landscapes, we can identify the\ndrivers of model decision making. Analogues to this idea exist in the molecular\nsciences, where coordinate invariants or order parameters are employed to\nidentify critical features of a molecule. However, no such approach exists for\nmachine learning loss landscapes. We will demonstrate the applicability of\nenergy landscape methods to machine learning models and give examples, both\nsynthetic and from the real world, for how these methods can help to make\nmodels more interpretable.\n","authors":["Maximilian P Niroomand","David J Wales"],"pdf_url":"https://arxiv.org/pdf/2304.02381v1.pdf","comment":"6 pages, 2 figures, ICLR 2023 Workshop on Physics for Machine\n  Learning"},{"id":"http://arxiv.org/abs/2304.02370v1","updated":"2023-04-05T11:21:21Z","published":"2023-04-05T11:21:21Z","title":"Effective control of two-dimensional Rayleigh--Bénard convection:\n  invariant multi-agent reinforcement learning is all you need","summary":"  Rayleigh-B\\'enard convection (RBC) is a recurrent phenomenon in several\nindustrial and geoscience flows and a well-studied system from a fundamental\nfluid-mechanics viewpoint. However, controlling RBC, for example by modulating\nthe spatial distribution of the bottom-plate heating in the canonical RBC\nconfiguration, remains a challenging topic for classical control-theory\nmethods. In the present work, we apply deep reinforcement learning (DRL) for\ncontrolling RBC. We show that effective RBC control can be obtained by\nleveraging invariant multi-agent reinforcement learning (MARL), which takes\nadvantage of the locality and translational invariance inherent to RBC flows\ninside wide channels. The MARL framework applied to RBC allows for an increase\nin the number of control segments without encountering the curse of\ndimensionality that would result from a naive increase in the DRL action-size\ndimension. This is made possible by the MARL ability for re-using the knowledge\ngenerated in different parts of the RBC domain. We show in a case study that\nMARL DRL is able to discover an advanced control strategy that destabilizes the\nspontaneous RBC double-cell pattern, changes the topology of RBC by coalescing\nadjacent convection cells, and actively controls the resulting coalesced cell\nto bring it to a new stable configuration. This modified flow configuration\nresults in reduced convective heat transfer, which is beneficial in several\nindustrial processes. Therefore, our work both shows the potential of MARL DRL\nfor controlling large RBC systems, as well as demonstrates the possibility for\nDRL to discover strategies that move the RBC configuration between different\ntopological configurations, yielding desirable heat-transfer characteristics.\nThese results are useful for both gaining further understanding of the\nintrinsic properties of RBC, as well as for developing industrial applications.\n","authors":["Colin Vignon","Jean Rabault","Joel Vasanth","Francisco Alcántara-Ávila","Mikael Mortensen","Ricardo Vinuesa"],"pdf_url":"https://arxiv.org/pdf/2304.02370v1.pdf","comment":"34 pages, 11 figures submitted to Physics of Fluids"},{"id":"http://arxiv.org/abs/2212.03787v2","updated":"2023-04-05T10:46:12Z","published":"2022-12-07T17:10:55Z","title":"A Neural Network Approach for Selecting Track-like Events in\n  Fluorescence Telescope Data","summary":"  In 2016-2017, TUS, the world's first experiment for testing the possibility\nof registering ultra-high energy cosmic rays (UHECRs) by their fluorescent\nradiation in the night atmosphere of Earth was carried out. Since 2019, the\nRussian-Italian fluorescence telescope (FT) Mini-EUSO (\"UV Atmosphere\") has\nbeen operating on the ISS. The stratospheric experiment EUSO-SPB2, which will\nemploy an FT for registering UHECRs, is planned for 2023. We show how a simple\nconvolutional neural network can be effectively used to find track-like events\nin the variety of data obtained with such instruments.\n","authors":["Mikhail Zotov","Denis Sokolinskii"],"pdf_url":"https://arxiv.org/pdf/2212.03787v2.pdf","comment":"5 pages, to be published in proceedings of the 37th Russian Cosmic\n  Ray Conference (2022)"},{"id":"http://arxiv.org/abs/2304.02353v1","updated":"2023-04-05T10:40:37Z","published":"2023-04-05T10:40:37Z","title":"Segmentation of Planning Target Volume in CT Series for Total Marrow\n  Irradiation Using U-Net","summary":"  Radiotherapy (RT) is a key component in the treatment of various cancers,\nincluding Acute Lymphocytic Leukemia (ALL) and Acute Myelogenous Leukemia\n(AML). Precise delineation of organs at risk (OARs) and target areas is\nessential for effective treatment planning. Intensity Modulated Radiotherapy\n(IMRT) techniques, such as Total Marrow Irradiation (TMI) and Total Marrow and\nLymph node Irradiation (TMLI), provide more precise radiation delivery compared\nto Total Body Irradiation (TBI). However, these techniques require\ntime-consuming manual segmentation of structures in Computerized Tomography\n(CT) scans by the Radiation Oncologist (RO). In this paper, we present a deep\nlearning-based auto-contouring method for segmenting Planning Target Volume\n(PTV) for TMLI treatment using the U-Net architecture. We trained and compared\ntwo segmentation models with two different loss functions on a dataset of 100\npatients treated with TMLI at the Humanitas Research Hospital between 2011 and\n2021. Despite challenges in lymph node areas, the best model achieved an\naverage Dice score of 0.816 for PTV segmentation. Our findings are a\npreliminary but significant step towards developing a segmentation model that\nhas the potential to save radiation oncologists a considerable amount of time.\nThis could allow for the treatment of more patients, resulting in improved\nclinical practice efficiency and more reproducible contours.\n","authors":["Ricardo Coimbra Brioso","Damiano Dei","Ciro Franzese","Nicola Lambri","Daniele Loiacono","Pietro Mancosu","Marta Scorsetti"],"pdf_url":"https://arxiv.org/pdf/2304.02353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.11494v3","updated":"2023-04-05T10:39:46Z","published":"2022-01-27T13:14:53Z","title":"GraphTune: A Learning-based Graph Generative Model with Tunable\n  Structural Features","summary":"  Generative models for graphs have been actively studied for decades, and they\nhave a wide range of applications. Recently, learning-based graph generation\nthat reproduces real-world graphs has been attracting the attention of many\nresearchers. Although several generative models that utilize modern machine\nlearning technologies have been proposed, conditional generation of general\ngraphs has been less explored in the field. In this paper, we propose a\ngenerative model that allows us to tune the value of a global-level structural\nfeature as a condition. Our model, called GraphTune, makes it possible to tune\nthe value of any structural feature of generated graphs using Long Short Term\nMemory (LSTM) and a Conditional Variational AutoEncoder (CVAE). We performed\ncomparative evaluations of GraphTune and conventional models on a real graph\ndataset. The evaluations show that GraphTune makes it possible to more clearly\ntune the value of a global-level structural feature better than conventional\nmodels.\n","authors":["Kohei Watabe","Shohei Nakazawa","Yoshiki Sato","Sho Tsugawa","Kenji Nakagawa"],"pdf_url":"https://arxiv.org/pdf/2201.11494v3.pdf","comment":"The paper was published in IEEE Transactions on Network Science and\n  Engineering. An earlier and short version of this paper was presented at the\n  41st IEEE International Conference on Distributed Computing Systems (ICDCS\n  2021) Poster Track"},{"id":"http://arxiv.org/abs/2304.02335v1","updated":"2023-04-05T09:43:58Z","published":"2023-04-05T09:43:58Z","title":"Correcting Flaws in Common Disentanglement Metrics","summary":"  Recent years have seen growing interest in learning disentangled\nrepresentations, in which distinct features, such as size or shape, are\nrepresented by distinct neurons. Quantifying the extent to which a given\nrepresentation is disentangled is not straightforward; multiple metrics have\nbeen proposed. In this paper, we identify two failings of existing metrics,\nwhich mean they can assign a high score to a model which is still entangled,\nand we propose two new metrics, which redress these problems. We then consider\nthe task of compositional generalization. Unlike prior works, we treat this as\na classification problem, which allows us to use it to measure the\ndisentanglement ability of the encoder, without depending on the decoder. We\nshow that performance on this task is (a) generally quite poor, (b) correlated\nwith most disentanglement metrics, and (c) most strongly correlated with our\nnewly proposed metrics.\n","authors":["Louis Mahon","Lei Shah","Thomas Lukasiewicz"],"pdf_url":"https://arxiv.org/pdf/2304.02335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.01357v5","updated":"2023-04-05T09:40:05Z","published":"2021-06-01T17:34:27Z","title":"Diffusion Schrödinger Bridge with Applications to Score-Based\n  Generative Modeling","summary":"  Progressively applying Gaussian noise transforms complex data distributions\nto approximately Gaussian. Reversing this dynamic defines a generative model.\nWhen the forward noising process is given by a Stochastic Differential Equation\n(SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the\nassociated reverse-time SDE may be estimated using score-matching. A limitation\nof this approach is that the forward-time SDE must be run for a sufficiently\nlong time for the final distribution to be approximately Gaussian. In contrast,\nsolving the Schr\\\"odinger Bridge problem (SB), i.e. an entropy-regularized\noptimal transport problem on path spaces, yields diffusions which generate\nsamples from the data distribution in finite time. We present Diffusion SB\n(DSB), an original approximation of the Iterative Proportional Fitting (IPF)\nprocedure to solve the SB problem, and provide theoretical analysis along with\ngenerative modeling experiments. The first DSB iteration recovers the\nmethodology proposed by Song et al. (2021), with the flexibility of using\nshorter time intervals, as subsequent DSB iterations reduce the discrepancy\nbetween the final-time marginal of the forward (resp. backward) SDE with\nrespect to the prior (resp. data) distribution. Beyond generative modeling, DSB\noffers a widely applicable computational optimal transport tool as the\ncontinuous state-space analogue of the popular Sinkhorn algorithm (Cuturi,\n2013).\n","authors":["Valentin De Bortoli","James Thornton","Jeremy Heng","Arnaud Doucet"],"pdf_url":"https://arxiv.org/pdf/2106.01357v5.pdf","comment":"NeurIPS 2021 (spotlight)"},{"id":"http://arxiv.org/abs/2303.11413v2","updated":"2023-04-05T09:36:48Z","published":"2023-03-11T00:49:45Z","title":"Structural Vibration Signal Denoising Using KLD Regularized\n  Bi-Directional LSTM","summary":"  Vibration signals have been increasingly utilized in various engineering\nfields for analysis and monitoring purposes, including structural health\nmonitoring, fault diagnosis and damage detection, where vibration signals can\nprovide valuable information about the condition and integrity of structures.\nIn recent years, there has been a growing trend towards the use of vibration\nsignals in the field of bioengineering. Activity-induced structural vibrations,\nparticularly footstep-induced signals, are useful for analyzing the movement of\nbiological systems such as the human body and animals. Footstep-induced signals\ncan provide valuable information about an individual's gait, body mass, and\nposture, making them an attractive tool for health monitoring, security, and\nhuman-computer interaction. However, the presence of various types of noise can\ncompromise the accuracy of footstep-induced signal analysis. In this paper, we\npropose a novel 'many-to-many' LSTM model with a KLD regularizer and L1\nregularization, which is effective in denoising structural vibration signals,\nparticularly for regimes with larger amplitudes. The model was trained and\ntested using synthetic data generated by a single degree of freedom oscillator.\nOur results demonstrate that the proposed approach is effective in reducing\nnoise in the signals, particularly for regimes with larger amplitudes. The\napproach is promising for a wide range of applications of footstep-induced\nstructural vibration signals, including healthcare, security, and technology.\n","authors":["Youzhi Liang","Wen Liang"],"pdf_url":"https://arxiv.org/pdf/2303.11413v2.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2110.06400v3","updated":"2023-04-05T09:36:33Z","published":"2021-10-12T23:25:03Z","title":"CyTran: A Cycle-Consistent Transformer with Multi-Level Consistency for\n  Non-Contrast to Contrast CT Translation","summary":"  We propose a novel approach to translate unpaired contrast computed\ntomography (CT) scans to non-contrast CT scans and the other way around.\nSolving this task has two important applications: (i) to automatically generate\ncontrast CT scans for patients for whom injecting contrast substance is not an\noption, and (ii) to enhance the alignment between contrast and non-contrast CT\nby reducing the differences induced by the contrast substance before\nregistration. Our approach is based on cycle-consistent generative adversarial\nconvolutional transformers, for short, CyTran. Our neural model can be trained\non unpaired images, due to the integration of a multi-level cycle-consistency\nloss. Aside from the standard cycle-consistency loss applied at the image\nlevel, we propose to apply additional cycle-consistency losses between\nintermediate feature representations, which enforces the model to be\ncycle-consistent at multiple representations levels, leading to superior\nresults. To deal with high-resolution images, we design a hybrid architecture\nbased on convolutional and multi-head attention layers. In addition, we\nintroduce a novel data set, Coltea-Lung-CT-100W, containing 100 3D triphasic\nlung CT scans (with a total of 37,290 images) collected from 100 female\npatients (there is one examination per patient). Each scan contains three\nphases (non-contrast, early portal venous, and late arterial), allowing us to\nperform experiments to compare our novel approach with state-of-the-art methods\nfor image style transfer. Our empirical results show that CyTran outperforms\nall competing methods. Moreover, we show that CyTran can be employed as a\npreliminary step to improve a state-of-the-art medical image alignment method.\nWe release our novel model and data set as open source at\nhttps://github.com/ristea/cycle-transformer.\n","authors":["Nicolae-Catalin Ristea","Andreea-Iuliana Miron","Olivian Savencu","Mariana-Iuliana Georgescu","Nicolae Verga","Fahad Shahbaz Khan","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2110.06400v3.pdf","comment":"Accepted for publication in Neurocomputing"},{"id":"http://arxiv.org/abs/2303.15739v2","updated":"2023-04-05T09:26:45Z","published":"2023-03-28T05:27:32Z","title":"Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized\n  Cases","summary":"  In many research fields in artificial intelligence, it has been shown that\ndeep neural networks are useful to estimate unknown functions on high\ndimensional input spaces. However, their generalization performance is not yet\ncompletely clarified from the theoretical point of view because they are\nnonidentifiable and singular learning machines. Moreover, a ReLU function is\nnot differentiable, to which algebraic or analytic methods in singular learning\ntheory cannot be applied. In this paper, we study a deep ReLU neural network in\noverparametrized cases and prove that the Bayesian free energy, which is equal\nto the minus log marginal likelihoodor the Bayesian stochastic complexity, is\nbounded even if the number of layers are larger than necessary to estimate an\nunknown data-generating function. Since the Bayesian generalization error is\nequal to the increase of the free energy as a function of a sample size, our\nresult also shows that the Bayesian generalization error does not increase even\nif a deep ReLU neural network is designed to be sufficiently large or in an\nopeverparametrized state.\n","authors":["Shuya Nagayasu","Sumio Watanabe"],"pdf_url":"https://arxiv.org/pdf/2303.15739v2.pdf","comment":"20pages, 2figure"},{"id":"http://arxiv.org/abs/2304.02319v1","updated":"2023-04-05T09:19:19Z","published":"2023-04-05T09:19:19Z","title":"Efficient CNNs via Passive Filter Pruning","summary":"  Convolutional neural networks (CNNs) have shown state-of-the-art performance\nin various applications. However, CNNs are resource-hungry due to their\nrequirement of high computational complexity and memory storage. Recent efforts\ntoward achieving computational efficiency in CNNs involve filter pruning\nmethods that eliminate some of the filters in CNNs based on the\n\\enquote{importance} of the filters. The majority of existing filter pruning\nmethods are either \"active\", which use a dataset and generate feature maps to\nquantify filter importance, or \"passive\", which compute filter importance using\nentry-wise norm of the filters without involving data. Under a high pruning\nratio where large number of filters are to be pruned from the network, the\nentry-wise norm methods eliminate relatively smaller norm filters without\nconsidering the significance of the filters in producing the node output,\nresulting in degradation in the performance. To address this, we present a\npassive filter pruning method where the filters are pruned based on their\ncontribution in producing output by considering the operator norm of the\nfilters. The proposed pruning method generalizes better across various CNNs\ncompared to that of the entry-wise norm-based pruning methods. In comparison to\nthe existing active filter pruning methods, the proposed pruning method is at\nleast 4.5 times faster in computing filter importance and is able to achieve\nsimilar performance compared to that of the active filter pruning methods. The\nefficacy of the proposed pruning method is evaluated on audio scene\nclassification and image classification using various CNNs architecture such as\nVGGish, DCASE21_Net, VGG-16 and ResNet-50.\n","authors":["Arshdeep Singh","Mark D. Plumbley"],"pdf_url":"https://arxiv.org/pdf/2304.02319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2005.00065v4","updated":"2023-04-05T09:11:18Z","published":"2020-04-30T19:26:46Z","title":"Generative Adversarial Networks (GANs Survey): Challenges, Solutions,\n  and Future Directions","summary":"  Generative Adversarial Networks (GANs) is a novel class of deep generative\nmodels which has recently gained significant attention. GANs learns complex and\nhigh-dimensional distributions implicitly over images, audio, and data.\nHowever, there exists major challenges in training of GANs, i.e., mode\ncollapse, non-convergence and instability, due to inappropriate design of\nnetwork architecture, use of objective function and selection of optimization\nalgorithm. Recently, to address these challenges, several solutions for better\ndesign and optimization of GANs have been investigated based on techniques of\nre-engineered network architectures, new objective functions and alternative\noptimization algorithms. To the best of our knowledge, there is no existing\nsurvey that has particularly focused on broad and systematic developments of\nthese solutions. In this study, we perform a comprehensive survey of the\nadvancements in GANs design and optimization solutions proposed to handle GANs\nchallenges. We first identify key research issues within each design and\noptimization technique and then propose a new taxonomy to structure solutions\nby key research issues. In accordance with the taxonomy, we provide a detailed\ndiscussion on different GANs variants proposed within each solution and their\nrelationships. Finally, based on the insights gained, we present the promising\nresearch directions in this rapidly growing field.\n","authors":["Divya Saxena","Jiannong Cao"],"pdf_url":"https://arxiv.org/pdf/2005.00065v4.pdf","comment":"61 pages"},{"id":"http://arxiv.org/abs/2303.11593v2","updated":"2023-04-05T08:58:30Z","published":"2023-03-21T04:47:45Z","title":"Difficulty in learning chirality for Transformer fed with SMILES","summary":"  Recent years have seen development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this, we\ninvestigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. The\nresults suggest that while the Transformer learns partial structures of\nmolecules quickly, it requires extended training to understand overall\nstructures. Consistently, the accuracy of molecular property predictions using\ndescriptors generated from models at different learning steps was similar from\nthe beginning to the end of training. Furthermore, we found that the\nTransformer requires particularly long training to learn chirality and\nsometimes stagnates with low translation accuracy due to misunderstanding of\nenantiomers. These findings are expected to deepen understanding of NLP models\nin chemistry.\n","authors":["Yasuhiro Yoshikai","Tadahaya Mizuno","Shumpei Nemoto","Hiroyuki Kusuhara"],"pdf_url":"https://arxiv.org/pdf/2303.11593v2.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2211.15259v2","updated":"2023-04-05T08:39:40Z","published":"2022-11-28T12:25:27Z","title":"A Call to Reflect on Evaluation Practices for Failure Detection in Image\n  Classification","summary":"  Reliable application of machine learning-based decision systems in the wild\nis one of the major challenges currently investigated by the field. A large\nportion of established approaches aims to detect erroneous predictions by means\nof assigning confidence scores. This confidence may be obtained by either\nquantifying the model's predictive uncertainty, learning explicit scoring\nfunctions, or assessing whether the input is in line with the training\ndistribution. Curiously, while these approaches all state to address the same\neventual goal of detecting failures of a classifier upon real-life application,\nthey currently constitute largely separated research fields with individual\nevaluation protocols, which either exclude a substantial part of relevant\nmethods or ignore large parts of relevant failure sources. In this work, we\nsystematically reveal current pitfalls caused by these inconsistencies and\nderive requirements for a holistic and realistic evaluation of failure\ndetection. To demonstrate the relevance of this unified perspective, we present\na large-scale empirical study for the first time enabling benchmarking\nconfidence scoring functions w.r.t all relevant methods and failure sources.\nThe revelation of a simple softmax response baseline as the overall best\nperforming method underlines the drastic shortcomings of current evaluation in\nthe abundance of publicized research on confidence scoring. Code and trained\nmodels are at https://github.com/IML-DKFZ/fd-shifts.\n","authors":["Paul F. Jaeger","Carsten T. Lüth","Lukas Klein","Till J. Bungert"],"pdf_url":"https://arxiv.org/pdf/2211.15259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.01401v3","updated":"2023-04-05T08:39:02Z","published":"2021-12-02T16:42:27Z","title":"Newton methods based convolution neural networks using parallel\n  processing","summary":"  Training of convolutional neural networks is a high dimensional and a\nnon-convex optimization problem. At present, it is inefficient in situations\nwhere parametric learning rates can not be confidently set. Some past works\nhave introduced Newton methods for training deep neural networks. Newton\nmethods for convolutional neural networks involve complicated operations.\nFinding the Hessian matrix in second-order methods becomes very complex as we\nmainly use the finite differences method with the image data. Newton methods\nfor convolutional neural networks deals with this by using the sub-sampled\nHessian Newton methods. In this paper, we have used the complete data instead\nof the sub-sampled methods that only handle partial data at a time. Further, we\nhave used parallel processing instead of serial processing in mini-batch\ncomputations. The results obtained using parallel processing in this study,\noutperform the time taken by the previous approach.\n","authors":["Ujjwal Thakur","Anuj Sharma"],"pdf_url":"https://arxiv.org/pdf/2112.01401v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07630v2","updated":"2023-04-05T08:32:20Z","published":"2022-06-15T16:23:03Z","title":"Rethinking Initialization of the Sinkhorn Algorithm","summary":"  While the optimal transport (OT) problem was originally formulated as a\nlinear program, the addition of entropic regularization has proven beneficial\nboth computationally and statistically, for many applications. The Sinkhorn\nfixed-point algorithm is the most popular approach to solve this regularized\nproblem, and, as a result, multiple attempts have been made to reduce its\nruntime using, e.g., annealing in the regularization parameter, momentum or\nacceleration. The premise of this work is that initialization of the Sinkhorn\nalgorithm has received comparatively little attention, possibly due to two\npreconceptions: since the regularized OT problem is convex, it may not be worth\ncrafting a good initialization, since any is guaranteed to work; secondly,\nbecause the outputs of the Sinkhorn algorithm are often unrolled in end-to-end\npipelines, a data-dependent initialization would bias Jacobian computations. We\nchallenge this conventional wisdom, and show that data-dependent initializers\nresult in dramatic speed-ups, with no effect on differentiability as long as\nimplicit differentiation is used. Our initializations rely on closed-forms for\nexact or approximate OT solutions that are known in the 1D, Gaussian or GMM\nsettings. They can be used with minimal tuning, and result in consistent\nspeed-ups for a wide variety of OT problems.\n","authors":["James Thornton","Marco Cuturi"],"pdf_url":"https://arxiv.org/pdf/2206.07630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.11453v2","updated":"2023-04-05T08:20:32Z","published":"2021-05-25T00:52:44Z","title":"Improving Semiconductor Device Modeling for Electronic Design Automation\n  by Machine Learning Techniques","summary":"  The semiconductors industry benefits greatly from the integration of Machine\nLearning (ML)-based techniques in Technology Computer-Aided Design (TCAD)\nmethods. The performance of ML models however relies heavily on the quality and\nquantity of training datasets. They can be particularly difficult to obtain in\nthe semiconductor industry due to the complexity and expense of the device\nfabrication. In this paper, we propose a self-augmentation strategy for\nimproving ML-based device modeling using variational autoencoder-based\ntechniques. These techniques require a small number of experimental data points\nand does not rely on TCAD tools. To demonstrate the effectiveness of our\napproach, we apply it to a deep neural network-based prediction task for the\nOhmic resistance value in Gallium Nitride devices. A 70% reduction in mean\nabsolute error when predicting experimental results is achieved. The inherent\nflexibility of our approach allows easy adaptation to various tasks, thus\nmaking it highly relevant to many applications of the semiconductor industry.\n","authors":["Zeheng Wang","Liang Li","Ross C. C. Leon","Jinlin Yang","Junjie Shi","Timothy van der Laan","Muhammad Usman"],"pdf_url":"https://arxiv.org/pdf/2105.11453v2.pdf","comment":"Entirely rewrote and reorganized. Updated models"},{"id":"http://arxiv.org/abs/2304.02286v1","updated":"2023-04-05T08:15:57Z","published":"2023-04-05T08:15:57Z","title":"A step towards the applicability of algorithms based on invariant causal\n  learning on observational data","summary":"  Machine learning can benefit from causal discovery for interpretation and\nfrom causal inference for generalization. In this line of research, a few\ninvariant learning algorithms for out-of-distribution (OOD) generalization have\nbeen proposed by using multiple training environments to find invariant\nrelationships. Some of them are focused on causal discovery as Invariant Causal\nPrediction (ICP), which finds causal parents of a variable of interest, and\nsome directly provide a causal optimal predictor that generalizes well in OOD\nenvironments as Invariant Risk Minimization (IRM). This group of algorithms\nworks under the assumption of multiple environments that represent different\ninterventions in the causal inference context. Those environments are not\nnormally available when working with observational data and real-world\napplications. Here we propose a method to generate them in an efficient way. We\nassess the performance of this unsupervised learning problem by implementing\nICP on simulated data. We also show how to apply ICP efficiently integrated\nwith our method for causal discovery. Finally, we proposed an improved version\nof our method in combination with ICP for datasets with multiple covariates\nwhere ICP and other causal discovery methods normally degrade in performance.\n","authors":["Borja Guerrero Santillan"],"pdf_url":"https://arxiv.org/pdf/2304.02286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.12884v3","updated":"2023-04-05T07:52:12Z","published":"2022-07-26T13:17:28Z","title":"CFLIT: Coexisting Federated Learning and Information Transfer","summary":"  Future wireless networks are expected to support diverse mobile services,\nincluding artificial intelligence (AI) services and ubiquitous data\ntransmissions. Federated learning (FL), as a revolutionary learning approach,\nenables collaborative AI model training across distributed mobile edge devices.\nBy exploiting the superposition property of multiple-access channels,\nover-the-air computation allows concurrent model uploading from massive devices\nover the same radio resources, and thus significantly reduces the communication\ncost of FL. In this paper, we study the coexistence of over-the-air FL and\ntraditional information transfer (IT) in a mobile edge network. We propose a\ncoexisting federated learning and information transfer (CFLIT) communication\nframework, where the FL and IT devices share the wireless spectrum in an OFDM\nsystem. Under this framework, we aim to maximize the IT data rate and guarantee\na given FL convergence performance by optimizing the long-term radio resource\nallocation. A key challenge that limits the spectrum efficiency of the\ncoexisting system lies in the large overhead incurred by frequent communication\nbetween the server and edge devices for FL model aggregation. To address the\nchallenge, we rigorously analyze the impact of the computation-to-communication\nratio on the convergence of over-the-air FL in wireless fading channels. The\nanalysis reveals the existence of an optimal computation-to-communication ratio\nthat minimizes the amount of radio resources needed for over-the-air FL to\nconverge to a given error tolerance. Based on the analysis, we propose a\nlow-complexity online algorithm to jointly optimize the radio resource\nallocation for both the FL devices and IT devices. Extensive numerical\nsimulations verify the superior performance of the proposed design for the\ncoexistence of FL and IT devices in wireless cellular systems.\n","authors":["Zehong Lin","Hang Liu","Ying-Jun Angela Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.12884v3.pdf","comment":"The paper has been accepted for publication by IEEE Transactions on\n  Wireless Communications (March 2023)"},{"id":"http://arxiv.org/abs/2304.02277v1","updated":"2023-04-05T07:50:05Z","published":"2023-04-05T07:50:05Z","title":"Rethinking the Trigger-injecting Position in Graph Backdoor Attack","summary":"  Backdoor attacks have been demonstrated as a security threat for machine\nlearning models. Traditional backdoor attacks intend to inject backdoor\nfunctionality into the model such that the backdoored model will perform\nabnormally on inputs with predefined backdoor triggers and still retain\nstate-of-the-art performance on the clean inputs. While there are already some\nworks on backdoor attacks on Graph Neural Networks (GNNs), the backdoor trigger\nin the graph domain is mostly injected into random positions of the sample.\nThere is no work analyzing and explaining the backdoor attack performance when\ninjecting triggers into the most important or least important area in the\nsample, which we refer to as trigger-injecting strategies MIAS and LIAS,\nrespectively. Our results show that, generally, LIAS performs better, and the\ndifferences between the LIAS and MIAS performance can be significant.\nFurthermore, we explain these two strategies' similar (better) attack\nperformance through explanation techniques, which results in a further\nunderstanding of backdoor attacks in GNNs.\n","authors":["Jing Xu","Gorka Abad","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2304.02277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02261v1","updated":"2023-04-05T07:24:19Z","published":"2023-04-05T07:24:19Z","title":"Optimal Sketching Bounds for Sparse Linear Regression","summary":"  We study oblivious sketching for $k$-sparse linear regression under various\nloss functions such as an $\\ell_p$ norm, or from a broad class of hinge-like\nloss functions, which includes the logistic and ReLU losses. We show that for\nsparse $\\ell_2$ norm regression, there is a distribution over oblivious\nsketches with $\\Theta(k\\log(d/k)/\\varepsilon^2)$ rows, which is tight up to a\nconstant factor. This extends to $\\ell_p$ loss with an additional additive\n$O(k\\log(k/\\varepsilon)/\\varepsilon^2)$ term in the upper bound. This\nestablishes a surprising separation from the related sparse recovery problem,\nwhich is an important special case of sparse regression. For this problem,\nunder the $\\ell_2$ norm, we observe an upper bound of $O(k \\log (d)/\\varepsilon\n+ k\\log(k/\\varepsilon)/\\varepsilon^2)$ rows, showing that sparse recovery is\nstrictly easier to sketch than sparse regression. For sparse regression under\nhinge-like loss functions including sparse logistic and sparse ReLU regression,\nwe give the first known sketching bounds that achieve $o(d)$ rows showing that\n$O(\\mu^2 k\\log(\\mu n d/\\varepsilon)/\\varepsilon^2)$ rows suffice, where $\\mu$\nis a natural complexity parameter needed to obtain relative error bounds for\nthese loss functions. We again show that this dimension is tight, up to lower\norder terms and the dependence on $\\mu$. Finally, we show that similar\nsketching bounds can be achieved for LASSO regression, a popular convex\nrelaxation of sparse regression, where one aims to minimize\n$\\|Ax-b\\|_2^2+\\lambda\\|x\\|_1$ over $x\\in\\mathbb{R}^d$. We show that sketching\ndimension $O(\\log(d)/(\\lambda \\varepsilon)^2)$ suffices and that the dependence\non $d$ and $\\lambda$ is tight.\n","authors":["Tung Mai","Alexander Munteanu","Cameron Musco","Anup B. Rao","Chris Schwiegelshohn","David P. Woodruff"],"pdf_url":"https://arxiv.org/pdf/2304.02261v1.pdf","comment":"AISTATS 2023"},{"id":"http://arxiv.org/abs/2303.15747v2","updated":"2023-04-05T07:08:49Z","published":"2023-03-28T06:03:41Z","title":"TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns","summary":"  We present \\emph{TabRet}, a pre-trainable Transformer-based model for tabular\ndata. TabRet is designed to work on a downstream task that contains columns not\nseen in pre-training. Unlike other methods, TabRet has an extra learning step\nbefore fine-tuning called \\emph{retokenizing}, which calibrates feature\nembeddings based on the masked autoencoding loss. In experiments, we\npre-trained TabRet with a large collection of public health surveys and\nfine-tuned it on classification tasks in healthcare, and TabRet achieved the\nbest AUC performance on four datasets. In addition, an ablation study shows\nretokenizing and random shuffle augmentation of columns during pre-training\ncontributed to performance gains.\n","authors":["Soma Onishi","Kenta Oono","Kohei Hayashi"],"pdf_url":"https://arxiv.org/pdf/2303.15747v2.pdf","comment":"Accepted at the Workshop on Understanding Foundation Models at ICLR\n  2023"},{"id":"http://arxiv.org/abs/2304.01890v2","updated":"2023-04-05T06:37:05Z","published":"2023-04-04T15:42:08Z","title":"Sociocultural knowledge is needed for selection of shots in hate speech\n  detection tasks","summary":"  We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for\nthe countries of Brazil, Germany, India and Kenya, to aid training and\ninterpretability of models. We demonstrate how our lexicon can be used to\ninterpret model predictions, showing that models developed to classify extreme\nspeech rely heavily on target words when making predictions. Further, we\npropose a method to aid shot selection for training in low-resource settings\nvia HATELEXICON. In few-shot learning, the selection of shots is of paramount\nimportance to model performance. In our work, we simulate a few-shot setting\nfor German and Hindi, using HASOC data for training and the Multilingual\nHateCheck (MHC) as a benchmark. We show that selecting shots based on our\nlexicon leads to models performing better on MHC than models trained on shots\nsampled randomly. Thus, when given only a few training examples, using our\nlexicon to select shots containing more sociocultural information leads to\nbetter few-shot performance.\n","authors":["Antonis Maronikolakis","Abdullatif Köksal","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2304.01890v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09543v2","updated":"2023-04-05T06:36:25Z","published":"2023-02-19T11:29:15Z","title":"Topological Feature Selection: A Graph-Based Filter Feature Selection\n  Approach","summary":"  In this paper, we introduce a novel unsupervised, graph-based filter feature\nselection technique which exploits the power of topologically constrained\nnetwork representations. We model dependency structures among features using a\nfamily of chordal graphs (the Triangulated Maximally Filtered Graph), and we\nmaximise the likelihood of features' relevance by studying their relative\nposition inside the network. Such an approach presents three aspects that are\nparticularly satisfactory compared to its alternatives: (i) it is highly\ntunable and easily adaptable to the nature of input data; (ii) it is fully\nexplainable, maintaining, at the same time, a remarkable level of simplicity;\n(iii) it is computationally cheaper compared to its alternatives. We test our\nalgorithm on 16 benchmark datasets from different applicative domains showing\nthat it outperforms or matches the current state-of-the-art under heterogeneous\nevaluation conditions.\n","authors":["Antonio Briola","Tomaso Aste"],"pdf_url":"https://arxiv.org/pdf/2302.09543v2.pdf","comment":"23 pages, 2 figures, 13 tables"},{"id":"http://arxiv.org/abs/2304.02247v1","updated":"2023-04-05T06:35:41Z","published":"2023-04-05T06:35:41Z","title":"Disentangling Structure and Style: Political Bias Detection in News by\n  Inducing Document Hierarchy","summary":"  We address an important gap in detection of political bias in news articles.\nPrevious works that perform supervised document classification can be biased\ntowards the writing style of each news outlet, leading to overfitting and\nlimited generalizability. Our approach overcomes this limitation by considering\nboth the sentence-level semantics and the document-level rhetorical structure,\nresulting in a more robust and style-agnostic approach to detecting political\nbias in news articles. We introduce a novel multi-head hierarchical attention\nmodel that effectively encodes the structure of long documents through a\ndiverse ensemble of attention heads. While journalism follows a formalized\nrhetorical structure, the writing style may vary by news outlet. We demonstrate\nthat our method overcomes this domain dependency and outperforms previous\napproaches for robustness and accuracy. Further analysis demonstrates the\nability of our model to capture the discourse structures commonly used in the\njournalism domain.\n","authors":["Jiwoo Hong","Yejin Cho","Jaemin Jung","Jiyoung Han","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2304.02247v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2210.17299v4","updated":"2023-04-05T06:22:35Z","published":"2022-10-28T15:24:17Z","title":"Bayesian Model Selection of Lithium-Ion Battery Models via Bayesian\n  Quadrature","summary":"  A wide variety of battery models are available, and it is not always obvious\nwhich model `best' describes a dataset. This paper presents a Bayesian model\nselection approach using Bayesian quadrature. The model evidence is adopted as\nthe selection metric, choosing the simplest model that describes the data, in\nthe spirit of Occam's razor. However, estimating this requires integral\ncomputations over parameter space, which is usually prohibitively expensive.\nBayesian quadrature offers sample-efficient integration via model-based\ninference that minimises the number of battery model evaluations. The posterior\ndistribution of model parameters can also be inferred as a byproduct without\nfurther computation. Here, the simplest lithium-ion battery models, equivalent\ncircuit models, were used to analyse the sensitivity of the selection criterion\nto given different datasets and model configurations. We show that popular\nmodel selection criteria, such as root-mean-square error and Bayesian\ninformation criterion, can fail to select a parsimonious model in the case of a\nmultimodal posterior. The model evidence can spot the optimal model in such\ncases, simultaneously providing the variance of the evidence inference itself\nas an indication of confidence. We also show that Bayesian quadrature can\ncompute the evidence faster than popular Monte Carlo based solvers.\n","authors":["Masaki Adachi","Yannick Kuhn","Birger Horstmann","Arnulf Latz","Michael A. Osborne","David A. Howey"],"pdf_url":"https://arxiv.org/pdf/2210.17299v4.pdf","comment":"11 pages, 2 figures, accepted at IFAC2023"},{"id":"http://arxiv.org/abs/2304.02240v1","updated":"2023-04-05T06:05:27Z","published":"2023-04-05T06:05:27Z","title":"List and Certificate Complexities in Replicable Learning","summary":"  We investigate replicable learning algorithms. Ideally, we would like to\ndesign algorithms that output the same canonical model over multiple runs, even\nwhen different runs observe a different set of samples from the unknown data\ndistribution. In general, such a strong notion of replicability is not\nachievable. Thus we consider two feasible notions of replicability called list\nreplicability and certificate replicability. Intuitively, these notions capture\nthe degree of (non) replicability. We design algorithms for certain learning\nproblems that are optimal in list and certificate complexity. We establish\nmatching impossibility results.\n","authors":["Peter Dixon","A. Pavan","Jason Vander Woude","N. V. Vinodchandran"],"pdf_url":"https://arxiv.org/pdf/2304.02240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02239v1","updated":"2023-04-05T06:02:58Z","published":"2023-04-05T06:02:58Z","title":"Optimal Energy Storage Scheduling for Wind Curtailment Reduction and\n  Energy Arbitrage: A Deep Reinforcement Learning Approach","summary":"  Wind energy has been rapidly gaining popularity as a means for combating\nclimate change. However, the variable nature of wind generation can undermine\nsystem reliability and lead to wind curtailment, causing substantial economic\nlosses to wind power producers. Battery energy storage systems (BESS) that\nserve as onsite backup sources are among the solutions to mitigate wind\ncurtailment. However, such an auxiliary role of the BESS might severely weaken\nits economic viability. This paper addresses the issue by proposing joint wind\ncurtailment reduction and energy arbitrage for the BESS. We decouple the market\nparticipation of the co-located wind-battery system and develop a joint-bidding\nframework for the wind farm and BESS. It is challenging to optimize the\njoint-bidding because of the stochasticity of energy prices and wind\ngeneration. Therefore, we leverage deep reinforcement learning to maximize the\noverall revenue from the spot market while unlocking the BESS's potential in\nconcurrently reducing wind curtailment and conducting energy arbitrage. We\nvalidate the proposed strategy using realistic wind farm data and demonstrate\nthat our joint-bidding strategy responds better to wind curtailment and\ngenerates higher revenues than the optimization-based benchmark. Our\nsimulations also reveal that the extra wind generation used to be curtailed can\nbe an effective power source to charge the BESS, resulting in additional\nfinancial returns.\n","authors":["Jinhao Li","Changlong Wang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2304.02239v1.pdf","comment":"2023 IEEE Power & Energy Society General Meeting (PESGM). arXiv admin\n  note: text overlap with arXiv:2212.13368"},{"id":"http://arxiv.org/abs/2304.02234v1","updated":"2023-04-05T05:30:09Z","published":"2023-04-05T05:30:09Z","title":"JPEG Compressed Images Can Bypass Protections Against AI Editing","summary":"  Recently developed text-to-image diffusion models make it easy to edit or\ncreate high-quality images. Their ease of use has raised concerns about the\npotential for malicious editing or deepfake creation. Imperceptible\nperturbations have been proposed as a means of protecting images from malicious\nediting by preventing diffusion models from generating realistic images.\nHowever, we find that the aforementioned perturbations are not robust to JPEG\ncompression, which poses a major weakness because of the common usage and\navailability of JPEG. We discuss the importance of robustness for additive\nimperceptible perturbations and encourage alternative approaches to protect\nimages against editing.\n","authors":["Pedro Sandoval-Segura","Jonas Geiping","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2304.02234v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2304.02229v1","updated":"2023-04-05T04:59:59Z","published":"2023-04-05T04:59:59Z","title":"Mixed Regression via Approximate Message Passing","summary":"  We study the problem of regression in a generalized linear model (GLM) with\nmultiple signals and latent variables. This model, which we call a matrix GLM,\ncovers many widely studied problems in statistical learning, including mixed\nlinear regression, max-affine regression, and mixture-of-experts. In mixed\nlinear regression, each observation comes from one of $L$ signal vectors\n(regressors), but we do not know which one; in max-affine regression, each\nobservation comes from the maximum of $L$ affine functions, each defined via a\ndifferent signal vector. The goal in all these problems is to estimate the\nsignals, and possibly some of the latent variables, from the observations. We\npropose a novel approximate message passing (AMP) algorithm for estimation in a\nmatrix GLM and rigorously characterize its performance in the high-dimensional\nlimit. This characterization is in terms of a state evolution recursion, which\nallows us to precisely compute performance measures such as the asymptotic\nmean-squared error. The state evolution characterization can be used to tailor\nthe AMP algorithm to take advantage of any structural information known about\nthe signals. Using state evolution, we derive an optimal choice of AMP\n`denoising' functions that minimizes the estimation error in each iteration.\n  The theoretical results are validated by numerical simulations for mixed\nlinear regression, max-affine regression, and mixture-of-experts. For\nmax-affine regression, we propose an algorithm that combines AMP with\nexpectation-maximization to estimate intercepts of the model along with the\nsignals. The numerical results show that AMP significantly outperforms other\nestimators for mixed linear regression and max-affine regression in most\nparameter regimes.\n","authors":["Nelvin Tan","Ramji Venkataramanan"],"pdf_url":"https://arxiv.org/pdf/2304.02229v1.pdf","comment":"44 pages. A shorter version of this paper will appear in the\n  proceedings of AISTATS 2023"},{"id":"http://arxiv.org/abs/2210.08323v3","updated":"2023-04-05T04:58:45Z","published":"2022-10-15T15:54:28Z","title":"A Policy-Guided Imitation Approach for Offline Reinforcement Learning","summary":"  Offline reinforcement learning (RL) methods can generally be categorized into\ntwo types: RL-based and Imitation-based. RL-based methods could in principle\nenjoy out-of-distribution generalization but suffer from erroneous off-policy\nevaluation. Imitation-based methods avoid off-policy evaluation but are too\nconservative to surpass the dataset. In this study, we propose an alternative\napproach, inheriting the training stability of imitation-style methods while\nstill allowing logical out-of-distribution generalization. We decompose the\nconventional reward-maximizing policy in offline RL into a guide-policy and an\nexecute-policy. During training, the guide-poicy and execute-policy are learned\nusing only data from the dataset, in a supervised and decoupled manner. During\nevaluation, the guide-policy guides the execute-policy by telling where it\nshould go so that the reward can be maximized, serving as the \\textit{Prophet}.\nBy doing so, our algorithm allows \\textit{state-compositionality} from the\ndataset, rather than \\textit{action-compositionality} conducted in prior\nimitation-style methods. We dumb this new approach Policy-guided Offline RL\n(\\texttt{POR}). \\texttt{POR} demonstrates the state-of-the-art performance on\nD4RL, a standard benchmark for offline RL. We also highlight the benefits of\n\\texttt{POR} in terms of improving with supplementary suboptimal data and\neasily adapting to new tasks by only changing the guide-poicy.\n","authors":["Haoran Xu","Li Jiang","Jianxiong Li","Xianyuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2210.08323v3.pdf","comment":"Oral @ NeurIPS 2022; An extended version with more experiments &\n  correct some experiment details in previous version"},{"id":"http://arxiv.org/abs/2302.08624v3","updated":"2023-04-05T04:44:43Z","published":"2023-02-16T23:29:22Z","title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis","summary":"  In this paper, we present InstructABSA, Aspect Based Sentiment Analysis\n(ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect\nTerm Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint\nTask modeling. Our method introduces positive, negative, and neutral examples\nto each training sample, and instruction tunes the model (Tk-Instruct) for each\nABSA subtask, yielding significant performance improvements. Experimental\nresults on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA\noutperforms the previous state-of-the-art (SOTA) approaches on all three ABSA\nsubtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x\nlarger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE\nsubtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by\n8.63% points. Our results also suggest a strong generalization ability to new\ndomains across all three subtasks\n","authors":["Kevin Scaria","Himanshu Gupta","Siddharth Goyal","Saurabh Arjun Sawant","Swaroop Mishra","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2302.08624v3.pdf","comment":"4 pages, 2 figures, 5 tables, 5 appendix pages"},{"id":"http://arxiv.org/abs/2304.02223v1","updated":"2023-04-05T04:36:07Z","published":"2023-04-05T04:36:07Z","title":"Local Intrinsic Dimensional Entropy","summary":"  Most entropy measures depend on the spread of the probability distribution\nover the sample space X, and the maximum entropy achievable scales\nproportionately with the sample space cardinality |X|. For a finite |X|, this\nyields robust entropy measures which satisfy many important properties, such as\ninvariance to bijections, while the same is not true for continuous spaces\n(where |X|=infinity). Furthermore, since R and R^d (d in Z+) have the same\ncardinality (from Cantor's correspondence argument), cardinality-dependent\nentropy measures cannot encode the data dimensionality. In this work, we\nquestion the role of cardinality and distribution spread in defining entropy\nmeasures for continuous spaces, which can undergo multiple rounds of\ntransformations and distortions, e.g., in neural networks. We find that the\naverage value of the local intrinsic dimension of a distribution, denoted as\nID-Entropy, can serve as a robust entropy measure for continuous spaces, while\ncapturing the data dimensionality. We find that ID-Entropy satisfies many\ndesirable properties and can be extended to conditional entropy, joint entropy\nand mutual-information variants. ID-Entropy also yields new information\nbottleneck principles and also links to causality. In the context of deep\nlearning, for feedforward architectures, we show, theoretically and\nempirically, that the ID-Entropy of a hidden layer directly controls the\ngeneralization gap for both classifiers and auto-encoders, when the target\nfunction is Lipschitz continuous. Our work primarily shows that, for continuous\nspaces, taking a structural rather than a statistical approach yields entropy\nmeasures which preserve intrinsic data dimensionality, while being relevant for\nstudying various architectures.\n","authors":["Rohan Ghosh","Mehul Motani"],"pdf_url":"https://arxiv.org/pdf/2304.02223v1.pdf","comment":"Proceedings of the AAAI Conference on Artificial Intelligence 2023"},{"id":"http://arxiv.org/abs/2304.01285v2","updated":"2023-04-05T04:33:50Z","published":"2023-04-03T18:20:31Z","title":"X-TIME: An in-memory engine for accelerating machine learning on tabular\n  data with CAMs","summary":"  Structured, or tabular, data is the most common format in data science. While\ndeep learning models have proven formidable in learning from unstructured data\nsuch as images or speech, they are less accurate than simpler approaches when\nlearning from tabular data. In contrast, modern tree-based Machine Learning\n(ML) models shine in extracting relevant information from structured data. An\nessential requirement in data science is to reduce model inference latency in\ncases where, for example, models are used in a closed loop with simulation to\naccelerate scientific discovery. However, the hardware acceleration community\nhas mostly focused on deep neural networks and largely ignored other forms of\nmachine learning. Previous work has described the use of an analog content\naddressable memory (CAM) component for efficiently mapping random forests. In\nthis work, we focus on an overall analog-digital architecture implementing a\nnovel increased precision analog CAM and a programmable network on chip\nallowing the inference of state-of-the-art tree-based ML models, such as\nXGBoost and CatBoost. Results evaluated in a single chip at 16nm technology\nshow 119x lower latency at 9740x higher throughput compared with a\nstate-of-the-art GPU, with a 19W peak power consumption.\n","authors":["Giacomo Pedretti","John Moon","Pedro Bruel","Sergey Serebryakov","Ron M. Roth","Luca Buonanno","Tobias Ziegler","Cong Xu","Martin Foltin","Paolo Faraboschi","Jim Ignowski","Catherine E. Graves"],"pdf_url":"https://arxiv.org/pdf/2304.01285v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02221v1","updated":"2023-04-05T04:29:38Z","published":"2023-04-05T04:29:38Z","title":"Zero-shot domain adaptation of anomalous samples for semi-supervised\n  anomaly detection","summary":"  Semi-supervised anomaly detection~(SSAD) is a task where normal data and a\nlimited number of anomalous data are available for training. In practical\nsituations, SSAD methods suffer adapting to domain shifts, since anomalous data\nare unlikely to be available for the target domain in the training phase. To\nsolve this problem, we propose a domain adaptation method for SSAD where no\nanomalous data are available for the target domain. First, we introduce a\ndomain-adversarial network to a variational auto-encoder-based SSAD model to\nobtain domain-invariant latent variables. Since the decoder cannot reconstruct\nthe original data solely from domain-invariant latent variables, we conditioned\nthe decoder on the domain label. To compensate for the missing anomalous data\nof the target domain, we introduce an importance sampling-based weighted loss\nfunction that approximates the ideal loss function. Experimental results\nindicate that the proposed method helps adapt SSAD models to the target domain\nwhen no anomalous data are available for the target domain.\n","authors":["Tomoya Nishida","Takashi Endo","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2304.02221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01487v2","updated":"2023-04-05T04:28:41Z","published":"2023-04-04T03:04:28Z","title":"To ChatGPT, or not to ChatGPT: That is the question!","summary":"  ChatGPT has become a global sensation. As ChatGPT and other Large Language\nModels (LLMs) emerge, concerns of misusing them in various ways increase, such\nas disseminating fake news, plagiarism, manipulating public opinion, cheating,\nand fraud. Hence, distinguishing AI-generated from human-generated becomes\nincreasingly essential. Researchers have proposed various detection\nmethodologies, ranging from basic binary classifiers to more complex\ndeep-learning models. Some detection techniques rely on statistical\ncharacteristics or syntactic patterns, while others incorporate semantic or\ncontextual information to improve accuracy. The primary objective of this study\nis to provide a comprehensive and contemporary assessment of the most recent\ntechniques in ChatGPT detection. Additionally, we evaluated other AI-generated\ntext detection tools that do not specifically claim to detect ChatGPT-generated\ncontent to assess their performance in detecting ChatGPT-generated content. For\nour evaluation, we have curated a benchmark dataset consisting of prompts from\nChatGPT and humans, including diverse questions from medical, open Q&A, and\nfinance domains and user-generated responses from popular social networking\nplatforms. The dataset serves as a reference to assess the performance of\nvarious techniques in detecting ChatGPT-generated content. Our evaluation\nresults demonstrate that none of the existing methods can effectively detect\nChatGPT-generated content.\n","authors":["Alessandro Pegoraro","Kavita Kumari","Hossein Fereidooni","Ahmad-Reza Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2304.01487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02220v1","updated":"2023-04-05T04:20:58Z","published":"2023-04-05T04:20:58Z","title":"On the universal approximation property of radial basis function neural\n  networks","summary":"  In this paper we consider a new class of RBF (Radial Basis Function) neural\nnetworks, in which smoothing factors are replaced with shifts. We prove under\ncertain conditions on the activation function that these networks are capable\nof approximating any continuous multivariate function on any compact subset of\nthe $d$-dimensional Euclidean space. For RBF networks with finitely many fixed\ncentroids we describe conditions guaranteeing approximation with arbitrary\nprecision.\n","authors":["Aysu Ismayilova","Muhammad Ismayilov"],"pdf_url":"https://arxiv.org/pdf/2304.02220v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2304.02208v1","updated":"2023-04-05T03:45:39Z","published":"2023-04-05T03:45:39Z","title":"PIKS: A Technique to Identify Actionable Trends for Policy-Makers\n  Through Open Healthcare Data","summary":"  With calls for increasing transparency, governments are releasing greater\namounts of data in multiple domains including finance, education and\nhealthcare. The efficient exploratory analysis of healthcare data constitutes a\nsignificant challenge. Key concerns in public health include the quick\nidentification and analysis of trends, and the detection of outliers. This\nallows policies to be rapidly adapted to changing circumstances. We present an\nefficient outlier detection technique, termed PIKS (Pruned iterative-k means\nsearchlight), which combines an iterative k-means algorithm with a pruned\nsearchlight based scan. We apply this technique to identify outliers in two\npublicly available healthcare datasets from the New York Statewide Planning and\nResearch Cooperative System, and California's Office of Statewide Health\nPlanning and Development. We provide a comparison of our technique with three\nother existing outlier detection techniques, consisting of auto-encoders,\nisolation forests and feature bagging. We identified outliers in conditions\nincluding suicide rates, immunity disorders, social admissions,\ncardiomyopathies, and pregnancy in the third trimester. We demonstrate that the\nPIKS technique produces results consistent with other techniques such as the\nauto-encoder. However, the auto-encoder needs to be trained, which requires\nseveral parameters to be tuned. In comparison, the PIKS technique has far fewer\nparameters to tune. This makes it advantageous for fast, \"out-of-the-box\" data\nexploration. The PIKS technique is scalable and can readily ingest new\ndatasets. Hence, it can provide valuable, up-to-date insights to citizens,\npatients and policy-makers. We have made our code open source, and with the\navailability of open data, other researchers can easily reproduce and extend\nour work. This will help promote a deeper understanding of healthcare policies\nand public health issues.\n","authors":["A. Ravishankar Rao","Subrata Garai","Soumyabrata Dey","Hang Peng"],"pdf_url":"https://arxiv.org/pdf/2304.02208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02202v1","updated":"2023-04-05T03:29:37Z","published":"2023-04-05T03:29:37Z","title":"Towards Self-Explainability of Deep Neural Networks with Heatmap\n  Captioning and Large-Language Models","summary":"  Heatmaps are widely used to interpret deep neural networks, particularly for\ncomputer vision tasks, and the heatmap-based explainable AI (XAI) techniques\nare a well-researched topic. However, most studies concentrate on enhancing the\nquality of the generated heatmap or discovering alternate heatmap generation\ntechniques, and little effort has been devoted to making heatmap-based XAI\nautomatic, interactive, scalable, and accessible. To address this gap, we\npropose a framework that includes two modules: (1) context modelling and (2)\nreasoning. We proposed a template-based image captioning approach for context\nmodelling to create text-based contextual information from the heatmap and\ninput data. The reasoning module leverages a large language model to provide\nexplanations in combination with specialised knowledge. Our qualitative\nexperiments demonstrate the effectiveness of our framework and heatmap\ncaptioning approach. The code for the proposed template-based heatmap\ncaptioning approach will be publicly available.\n","authors":["Osman Tursun","Simon Denman","Sridha Sridharan","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2304.02202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15136v3","updated":"2023-04-05T03:16:26Z","published":"2022-11-28T08:48:58Z","title":"Collective Intelligence for 2D Push Manipulations with Mobile Robots","summary":"  While natural systems often present collective intelligence that allows them\nto self-organize and adapt to changes, the equivalent is missing in most\nartificial systems. We explore the possibility of such a system in the context\nof cooperative 2D push manipulations using mobile robots. Although conventional\nworks demonstrate potential solutions for the problem in restricted settings,\nthey have computational and learning difficulties. More importantly, these\nsystems do not possess the ability to adapt when facing environmental changes.\nIn this work, we show that by distilling a planner derived from a\ndifferentiable soft-body physics simulator into an attention-based neural\nnetwork, our multi-robot push manipulation system achieves better performance\nthan baselines. In addition, our system also generalizes to configurations not\nseen during training and is able to adapt toward task completions when external\nturbulence and environmental changes are applied. Supplementary videos can be\nfound on our project website: https://sites.google.com/view/ciom/home\n","authors":["So Kuroki","Tatsuya Matsushima","Jumpei Arima","Hiroki Furuta","Yutaka Matsuo","Shixiang Shane Gu","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2211.15136v3.pdf","comment":"Robotics and Automation Letters(RA-L) 2023"},{"id":"http://arxiv.org/abs/2210.10842v2","updated":"2023-04-05T03:05:53Z","published":"2022-10-19T19:15:07Z","title":"MMRNet: Improving Reliability for Multimodal Object Detection and\n  Segmentation for Bin Picking via Multimodal Redundancy","summary":"  Recently, there has been tremendous interest in industry 4.0 infrastructure\nto address labor shortages in global supply chains. Deploying artificial\nintelligence-enabled robotic bin picking systems in real world has become\nparticularly important for reducing stress and physical demands of workers\nwhile increasing speed and efficiency of warehouses. To this end, artificial\nintelligence-enabled robotic bin picking systems may be used to automate order\npicking, but with the risk of causing expensive damage during an abnormal event\nsuch as sensor failure. As such, reliability becomes a critical factor for\ntranslating artificial intelligence research to real world applications and\nproducts. In this paper, we propose a reliable object detection and\nsegmentation system with MultiModal Redundancy (MMRNet) for tackling object\ndetection and segmentation for robotic bin picking using data from different\nmodalities. This is the first system that introduces the concept of multimodal\nredundancy to address sensor failure issues during deployment. In particular,\nwe realize the multimodal redundancy framework with a gate fusion module and\ndynamic ensemble learning. Finally, we present a new label-free multi-modal\nconsistency (MC) score that utilizes the output from all modalities to measure\nthe overall system output reliability and uncertainty. Through experiments, we\ndemonstrate that in an event of missing modality, our system provides a much\nmore reliable performance compared to baseline models. We also demonstrate that\nour MC score is a more reliability indicator for outputs during inference time\ncompared to the model generated confidence scores that are often\nover-confident.\n","authors":["Yuhao Chen","Hayden Gunraj","E. Zhixuan Zeng","Robbie Meyer","Maximilian Gilles","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2210.10842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02198v1","updated":"2023-04-05T02:46:13Z","published":"2023-04-05T02:46:13Z","title":"EigenFold: Generative Protein Structure Prediction with Diffusion Models","summary":"  Protein structure prediction has reached revolutionary levels of accuracy on\nsingle structures, yet distributional modeling paradigms are needed to capture\nthe conformational ensembles and flexibility that underlie biological function.\nTowards this goal, we develop EigenFold, a diffusion generative modeling\nframework for sampling a distribution of structures from a given protein\nsequence. We define a diffusion process that models the structure as a system\nof harmonic oscillators and which naturally induces a cascading-resolution\ngenerative process along the eigenmodes of the system. On recent CAMEO targets,\nEigenFold achieves a median TMScore of 0.84, while providing a more\ncomprehensive picture of model uncertainty via the ensemble of sampled\nstructures relative to existing methods. We then assess EigenFold's ability to\nmodel and predict conformational heterogeneity for fold-switching proteins and\nligand-induced conformational change. Code is available at\nhttps://github.com/bjing2016/EigenFold.\n","authors":["Bowen Jing","Ezra Erives","Peter Pao-Huang","Gabriele Corso","Bonnie Berger","Tommi Jaakkola"],"pdf_url":"https://arxiv.org/pdf/2304.02198v1.pdf","comment":"ICLR MLDD workshop 2023"},{"id":"http://arxiv.org/abs/2209.10095v2","updated":"2023-04-05T02:30:11Z","published":"2022-09-20T07:45:00Z","title":"A Max-relevance-min-divergence Criterion for Data Discretization with\n  Applications on Naive Bayes","summary":"  In many classification models, data is discretized to better estimate its\ndistribution. Existing discretization methods often target at maximizing the\ndiscriminant power of discretized data, while overlooking the fact that the\nprimary target of data discretization in classification is to improve the\ngeneralization performance. As a result, the data tend to be over-split into\nmany small bins since the data without discretization retain the maximal\ndiscriminant information. Thus, we propose a Max-Dependency-Min-Divergence\n(MDmD) criterion that maximizes both the discriminant information and\ngeneralization ability of the discretized data. More specifically, the\nMax-Dependency criterion maximizes the statistical dependency between the\ndiscretized data and the classification variable while the Min-Divergence\ncriterion explicitly minimizes the JS-divergence between the training data and\nthe validation data for a given discretization scheme. The proposed MDmD\ncriterion is technically appealing, but it is difficult to reliably estimate\nthe high-order joint distributions of attributes and the classification\nvariable. We hence further propose a more practical solution,\nMax-Relevance-Min-Divergence (MRmD) discretization scheme, where each attribute\nis discretized separately, by simultaneously maximizing the discriminant\ninformation and the generalization ability of the discretized data. The\nproposed MRmD is compared with the state-of-the-art discretization algorithms\nunder the naive Bayes classification framework on 45 machine-learning benchmark\ndatasets. It significantly outperforms all the compared methods on most of the\ndatasets.\n","authors":["Shihe Wang","Jianfeng Ren","Ruibin Bai","Yuan Yao","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2209.10095v2.pdf","comment":"Under major revision of Pattern Recognition"},{"id":"http://arxiv.org/abs/2111.10983v3","updated":"2023-04-05T02:26:58Z","published":"2021-11-22T04:36:40Z","title":"A Semi-Supervised Adaptive Discriminative Discretization Method\n  Improving Discrimination Power of Regularized Naive Bayes","summary":"  Recently, many improved naive Bayes methods have been developed with enhanced\ndiscrimination capabilities. Among them, regularized naive Bayes (RNB) produces\nexcellent performance by balancing the discrimination power and generalization\ncapability. Data discretization is important in naive Bayes. By grouping\nsimilar values into one interval, the data distribution could be better\nestimated. However, existing methods including RNB often discretize the data\ninto too few intervals, which may result in a significant information loss. To\naddress this problem, we propose a semi-supervised adaptive discriminative\ndiscretization framework for naive Bayes, which could better estimate the data\ndistribution by utilizing both labeled data and unlabeled data through\npseudo-labeling techniques. The proposed method also significantly reduces the\ninformation loss during discretization by utilizing an adaptive discriminative\ndiscretization scheme, and hence greatly improves the discrimination power of\nclassifiers. The proposed RNB+, i.e., regularized naive Bayes utilizing the\nproposed discretization framework, is systematically evaluated on a wide range\nof machine-learning datasets. It significantly and consistently outperforms\nstate-of-the-art NB classifiers.\n","authors":["Shihe Wang","Jianfeng Ren","Ruibin Bai"],"pdf_url":"https://arxiv.org/pdf/2111.10983v3.pdf","comment":"Accepted by Expert System with Applications"},{"id":"http://arxiv.org/abs/2303.17802v2","updated":"2023-04-05T02:13:48Z","published":"2023-03-31T05:22:56Z","title":"Time-series Anomaly Detection based on Difference Subspace between\n  Signal Subspaces","summary":"  This paper proposes a new method for anomaly detection in time-series data by\nincorporating the concept of difference subspace into the singular spectrum\nanalysis (SSA). The key idea is to monitor slight temporal variations of the\ndifference subspace between two signal subspaces corresponding to the past and\npresent time-series data, as anomaly score. It is a natural generalization of\nthe conventional SSA-based method which measures the minimum angle between the\ntwo signal subspaces as the degree of changes. By replacing the minimum angle\nwith the difference subspace, our method boosts the performance while using the\nSSA-based framework as it can capture the whole structural difference between\nthe two subspaces in its magnitude and direction. We demonstrate our method's\neffectiveness through performance evaluations on public time-series datasets.\n","authors":["Takumi Kanai","Naoya Sogi","Atsuto Maki","Kazuhiro Fukui"],"pdf_url":"https://arxiv.org/pdf/2303.17802v2.pdf","comment":"8pages, an acknowledgement was added to v1"},{"id":"http://arxiv.org/abs/2304.02192v1","updated":"2023-04-05T02:13:42Z","published":"2023-04-05T02:13:42Z","title":"A Diffusion-based Method for Multi-turn Compositional Image Generation","summary":"  Multi-turn compositional image generation (M-CIG) is a challenging task that\naims to iteratively manipulate a reference image given a modification text.\nWhile most of the existing methods for M-CIG are based on generative\nadversarial networks (GANs), recent advances in image generation have\ndemonstrated the superiority of diffusion models over GANs. In this paper, we\npropose a diffusion-based method for M-CIG named conditional denoising\ndiffusion with image compositional matching (CDD-ICM). We leverage CLIP as the\nbackbone of image and text encoders, and incorporate a gated fusion mechanism,\noriginally proposed for question answering, to compositionally fuse the\nreference image and the modification text at each turn of M-CIG. We introduce a\nconditioning scheme to generate the target image based on the fusion results.\nTo prioritize the semantic quality of the generated target image, we learn an\nauxiliary image compositional match (ICM) objective, along with the conditional\ndenoising diffusion (CDD) objective in a multi-task learning framework.\nAdditionally, we also perform ICM guidance and classifier-free guidance to\nimprove performance. Experimental results show that CDD-ICM achieves\nstate-of-the-art results on two benchmark datasets for M-CIG, i.e., CoDraw and\ni-CLEVR.\n","authors":["Chao Wang","Xiaoyu Yang","Jinmiao Huang","Kevin Ferreira"],"pdf_url":"https://arxiv.org/pdf/2304.02192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02191v1","updated":"2023-04-05T02:12:58Z","published":"2023-04-05T02:12:58Z","title":"Building predictive models of healthcare costs with open healthcare data","summary":"  Due to rapidly rising healthcare costs worldwide, there is significant\ninterest in controlling them. An important aspect concerns price transparency,\nas preliminary efforts have demonstrated that patients will shop for lower\ncosts, driving efficiency. This requires the data to be made available, and\nmodels that can predict healthcare costs for a wide range of patient\ndemographics and conditions. We present an approach to this problem by\ndeveloping a predictive model using machine-learning techniques. We analyzed\nde-identified patient data from New York State SPARCS (statewide planning and\nresearch cooperative system), consisting of 2.3 million records in 2016. We\nbuilt models to predict costs from patient diagnoses and demographics. We\ninvestigated two model classes consisting of sparse regression and decision\ntrees. We obtained the best performance by using a decision tree with depth 10.\nWe obtained an R-square value of 0.76 which is better than the values reported\nin the literature for similar problems.\n","authors":["A. Ravishankar Rao","Subrata Garai","Soumyabrata Dey","Hang Peng"],"pdf_url":"https://arxiv.org/pdf/2304.02191v1.pdf","comment":"2020 IEEE International Conference on Healthcare Informatics (ICHI)"},{"id":"http://arxiv.org/abs/2304.02190v1","updated":"2023-04-05T02:10:53Z","published":"2023-04-05T02:10:53Z","title":"Globalizing Fairness Attributes in Machine Learning: A Case Study on\n  Health in Africa","summary":"  With growing machine learning (ML) applications in healthcare, there have\nbeen calls for fairness in ML to understand and mitigate ethical concerns these\nsystems may pose. Fairness has implications for global health in Africa, which\nalready has inequitable power imbalances between the Global North and South.\nThis paper seeks to explore fairness for global health, with Africa as a case\nstudy. We propose fairness attributes for consideration in the African context\nand delineate where they may come into play in different ML-enabled medical\nmodalities. This work serves as a basis and call for action for furthering\nresearch into fairness in global health.\n","authors":["Mercy Nyamewaa Asiedu","Awa Dieng","Abigail Oppong","Maria Nagawa","Sanmi Koyejo","Katherine Heller"],"pdf_url":"https://arxiv.org/pdf/2304.02190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02189v1","updated":"2023-04-05T02:09:15Z","published":"2023-04-05T02:09:15Z","title":"A system for exploring big data: an iterative k-means searchlight for\n  outlier detection on open health data","summary":"  The interactive exploration of large and evolving datasets is challenging as\nrelationships between underlying variables may not be fully understood. There\nmay be hidden trends and patterns in the data that are worthy of further\nexploration and analysis. We present a system that methodically explores\nmultiple combinations of variables using a searchlight technique and identifies\noutliers. An iterative k-means clustering algorithm is applied to features\nderived through a split-apply-combine paradigm used in the database literature.\nOutliers are identified as singleton or small clusters. This algorithm is swept\nacross the dataset in a searchlight manner. The dimensions that contain\noutliers are combined in pairs with other dimensions using a susbset scan\ntechnique to gain further insight into the outliers. We illustrate this system\nby anaylzing open health care data released by New York State. We apply our\niterative k-means searchlight followed by subset scanning. Several anomalous\ntrends in the data are identified, including cost overruns at specific\nhospitals, and increases in diagnoses such as suicides. These constitute novel\nfindings in the literature, and are of potential use to regulatory agencies,\npolicy makers and concerned citizens.\n","authors":["A. Ravishankar Rao","Daniel Clarke","Subrata Garai","Soumyabrata Dey"],"pdf_url":"https://arxiv.org/pdf/2304.02189v1.pdf","comment":"2018 International Joint Conference on Neural Networks (IJCNN)"},{"id":"http://arxiv.org/abs/2203.05711v4","updated":"2023-04-05T02:09:02Z","published":"2022-03-11T01:45:33Z","title":"Synopses of Movie Narratives: a Video-Language Dataset for Story\n  Understanding","summary":"  Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives (SyMoN), containing\n5,193 video summaries of popular movies and TV series with a total length of\n869 hours. SyMoN captures naturalistic storytelling videos made by human\ncreators and intended for a human audience. As a prototypical and naturalistic\nstory dataset, SyMoN features high coverage of multimodal story events and\nabundant mental-state descriptions. Its use of storytelling techniques cause\ncross-domain semantic gaps that provide appropriate challenges to existing\nmodels. We establish benchmarks on video-text retrieval and zero-shot alignment\non movie summary videos, which showcase the importance of in-domain data and\nlong-term memory in story understanding. With SyMoN, we hope to lay the\ngroundwork for progress in multimodal story understanding.\n","authors":["Yidan Sun","Qin Chao","Yangfeng Ji","Boyang Li"],"pdf_url":"https://arxiv.org/pdf/2203.05711v4.pdf","comment":"25 pages, 17 figures"},{"id":"http://arxiv.org/abs/2210.05920v2","updated":"2023-04-05T02:00:19Z","published":"2022-10-12T04:48:50Z","title":"Boosting Graph Neural Networks via Adaptive Knowledge Distillation","summary":"  Graph neural networks (GNNs) have shown remarkable performance on diverse\ngraph mining tasks. Although different GNNs can be unified as the same message\npassing framework, they learn complementary knowledge from the same graph.\nKnowledge distillation (KD) is developed to combine the diverse knowledge from\nmultiple models. It transfers knowledge from high-capacity teachers to a\nlightweight student. However, to avoid oversmoothing, GNNs are often shallow,\nwhich deviates from the setting of KD. In this context, we revisit KD by\nseparating its benefits from model compression and emphasizing its power of\ntransferring knowledge. To this end, we need to tackle two challenges: how to\ntransfer knowledge from compact teachers to a student with the same capacity;\nand, how to exploit student GNN's own strength to learn knowledge. In this\npaper, we propose a novel adaptive KD framework, called BGNN, which\nsequentially transfers knowledge from multiple GNNs into a student GNN. We also\nintroduce an adaptive temperature module and a weight boosting module. These\nmodules guide the student to the appropriate knowledge for effective learning.\nExtensive experiments have demonstrated the effectiveness of BGNN. In\nparticular, we achieve up to 3.05% improvement for node classification and\n6.35% improvement for graph classification over vanilla GNNs.\n","authors":["Zhichun Guo","Chunhui Zhang","Yujie Fan","Yijun Tian","Chuxu Zhang","Nitesh Chawla"],"pdf_url":"https://arxiv.org/pdf/2210.05920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01905v2","updated":"2023-04-05T01:22:38Z","published":"2023-04-03T01:19:39Z","title":"Dual-Attention Neural Transducers for Efficient Wake Word Spotting in\n  Speech Recognition","summary":"  We present dual-attention neural biasing, an architecture designed to boost\nWake Words (WW) recognition and improve inference time latency on speech\nrecognition tasks. This architecture enables a dynamic switch for its runtime\ncompute paths by exploiting WW spotting to select which branch of its attention\nnetworks to execute for an input audio frame. With this approach, we\neffectively improve WW spotting accuracy while saving runtime compute cost as\ndefined by floating point operations (FLOPs). Using an in-house de-identified\ndataset, we demonstrate that the proposed dual-attention network can reduce the\ncompute cost by $90\\%$ for WW audio frames, with only $1\\%$ increase in the\nnumber of parameters. This architecture improves WW F1 score by $16\\%$ relative\nand improves generic rare word error rate by $3\\%$ relative compared to the\nbaselines.\n","authors":["Saumya Y. Sahai","Jing Liu","Thejaswi Muniyappa","Kanthashree M. Sathyendra","Anastasios Alexandridis","Grant P. Strimel","Ross McGowan","Ariya Rastrow","Feng-Ju Chang","Athanasios Mouchtaris","Siegfried Kunzmann"],"pdf_url":"https://arxiv.org/pdf/2304.01905v2.pdf","comment":"Accepted to Proc. IEEE ICASSP 2023"},{"id":"http://arxiv.org/abs/2209.10802v3","updated":"2023-04-05T01:22:09Z","published":"2022-09-22T06:10:35Z","title":"Robust Forecasting for Robotic Control: A Game-Theoretic Approach","summary":"  Modern robots require accurate forecasts to make optimal decisions in the\nreal world. For example, self-driving cars need an accurate forecast of other\nagents' future actions to plan safe trajectories. Current methods rely heavily\non historical time series to accurately predict the future. However, relying\nentirely on the observed history is problematic since it could be corrupted by\nnoise, have outliers, or not completely represent all possible outcomes. To\nsolve this problem, we propose a novel framework for generating robust\nforecasts for robotic control. In order to model real-world factors affecting\nfuture forecasts, we introduce the notion of an adversary, which perturbs\nobserved historical time series to increase a robot's ultimate control cost.\nSpecifically, we model this interaction as a zero-sum two-player game between a\nrobot's forecaster and this hypothetical adversary. We show that our proposed\ngame may be solved to a local Nash equilibrium using gradient-based\noptimization techniques. Furthermore, we show that a forecaster trained with\nour method performs 30.14% better on out-of-distribution real-world lane change\ndata than baselines.\n","authors":["Shubhankar Agarwal","David Fridovich-Keil","Sandeep P. Chinchali"],"pdf_url":"https://arxiv.org/pdf/2209.10802v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16386v2","updated":"2023-04-05T01:16:08Z","published":"2022-10-28T20:02:21Z","title":"Dynamic Bandits with an Auto-Regressive Temporal Structure","summary":"  Multi-armed bandit (MAB) problems are mainly studied under two extreme\nsettings known as stochastic and adversarial. These two settings, however, do\nnot capture realistic environments such as search engines and marketing and\nadvertising, in which rewards stochastically change in time. Motivated by that,\nwe introduce and study a dynamic MAB problem with stochastic temporal\nstructure, where the expected reward of each arm is governed by an\nauto-regressive (AR) model. Due to the dynamic nature of the rewards, simple\n\"explore and commit\" policies fail, as all arms have to be explored\ncontinuously over time. We formalize this by characterizing a per-round regret\nlower bound, where the regret is measured against a strong (dynamic) benchmark.\nWe then present an algorithm whose per-round regret almost matches our regret\nlower bound. Our algorithm relies on two mechanisms: (i) alternating between\nrecently pulled arms and unpulled arms with potential, and (ii) restarting.\nThese mechanisms enable the algorithm to dynamically adapt to changes and\ndiscard irrelevant past information at a suitable rate. In numerical studies,\nwe further demonstrate the strength of our algorithm under non-stationary\nsettings.\n","authors":["Qinyi Chen","Negin Golrezaei","Djallel Bouneffouf"],"pdf_url":"https://arxiv.org/pdf/2210.16386v2.pdf","comment":"41 pages, 4 figures"},{"id":"http://arxiv.org/abs/2304.00668v2","updated":"2023-04-05T00:40:41Z","published":"2023-04-03T00:45:11Z","title":"Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR","summary":"  In recent years, deep learning has been widely used in SAR ATR and achieved\nexcellent performance on the MSTAR dataset. However, due to constrained imaging\nconditions, MSTAR has data biases such as background correlation, i.e.,\nbackground clutter properties have a spurious correlation with target classes.\nDeep learning can overfit clutter to reduce training errors. Therefore, the\ndegree of overfitting for clutter reflects the non-causality of deep learning\nin SAR ATR. Existing methods only qualitatively analyze this phenomenon. In\nthis paper, we quantify the contributions of different regions to target\nrecognition based on the Shapley value. The Shapley value of clutter measures\nthe degree of overfitting. Moreover, we explain how data bias and model bias\ncontribute to non-causality. Concisely, data bias leads to comparable\nsignal-to-clutter ratios and clutter textures in training and test sets. And\nvarious model structures have different degrees of overfitting for these\nbiases. The experimental results of various models under standard operating\nconditions on the MSTAR dataset support our conclusions. Our code is available\nat https://github.com/waterdisappear/Data-Bias-in-MSTAR.\n","authors":["Weijie Li","Wei Yang","Li Liu","Wenpeng Zhang","Yongxiang Liu"],"pdf_url":"https://arxiv.org/pdf/2304.00668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.00092v3","updated":"2023-04-05T00:39:18Z","published":"2021-01-29T22:11:59Z","title":"Reinforcement Learning for Freight Booking Control Problems","summary":"  Booking control problems are sequential decision-making problems that occur\nin the domain of revenue management. More precisely, freight booking control\nfocuses on the problem of deciding to accept or reject bookings: given a\nlimited capacity, accept a booking request or reject it to reserve capacity for\nfuture bookings with potentially higher revenue. This problem can be formulated\nas a finite-horizon stochastic dynamic program, where accepting a set of\nrequests results in a profit at the end of the booking period that depends on\nthe cost of fulfilling the accepted bookings. For many freight applications,\nthe cost of fulfilling requests is obtained by solving an operational\ndecision-making problem, which often requires the solutions to mixed-integer\nlinear programs. Routinely solving such operational problems when deploying\nreinforcement learning algorithms may be too time consuming. The majority of\nbooking control policies are obtained by solving problem-specific mathematical\nprogramming relaxations that are often non-trivial to generalize to new\nproblems and, in some cases, provide quite crude approximations.\n  In this work, we propose a two-phase approach: we first train a supervised\nlearning model to predict the objective of the operational problem, and then we\ndeploy the model within reinforcement learning algorithms to compute control\npolicies. This approach is general: it can be used every time the objective\nfunction of the end-of-horizon operational problem can be predicted, and it is\nparticularly suitable to those cases where such problems are computationally\nhard. Furthermore, it allows one to leverage the recent advances in\nreinforcement learning as routinely solving the operational problem is replaced\nwith a single prediction. Our methodology is evaluated on two booking control\nproblems in the literature, namely, distributional logistics and airline cargo\nmanagement.\n","authors":["Justin Dumouchelle","Emma Frejinger","Andrea Lodi"],"pdf_url":"https://arxiv.org/pdf/2102.00092v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01218v2","updated":"2023-04-05T00:30:50Z","published":"2023-03-31T06:51:36Z","title":"POLAR-Express: Efficient and Precise Formal Reachability Analysis of\n  Neural-Network Controlled Systems","summary":"  Neural networks (NNs) playing the role of controllers have demonstrated\nimpressive empirical performances on challenging control problems. However, the\npotential adoption of NN controllers in real-life applications also gives rise\nto a growing concern over the safety of these neural-network controlled systems\n(NNCSs), especially when used in safety-critical applications. In this work, we\npresent POLAR-Express, an efficient and precise formal reachability analysis\ntool for verifying the safety of NNCSs. POLAR-Express uses Taylor model\narithmetic to propagate Taylor models (TMs) across a neural network\nlayer-by-layer to compute an overapproximation of the neural-network function.\nIt can be applied to analyze any feed-forward neural network with continuous\nactivation functions. We also present a novel approach to propagate TMs more\nefficiently and precisely across ReLU activation functions. In addition,\nPOLAR-Express provides parallel computation support for the layer-by-layer\npropagation of TMs, thus significantly improving the efficiency and scalability\nover its earlier prototype POLAR. Across the comparison with six other\nstate-of-the-art tools on a diverse set of benchmarks, POLAR-Express achieves\nthe best verification efficiency and tightness in the reachable set analysis.\n","authors":["Yixuan Wang","Weichao Zhou","Jiameng Fan","Zhilu Wang","Jiajun Li","Xin Chen","Chao Huang","Wenchao Li","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2304.01218v2.pdf","comment":"14 page, 20 figures, IEEE TCAD"},{"id":"http://arxiv.org/abs/2303.06241v2","updated":"2023-04-05T00:07:46Z","published":"2023-03-10T23:21:05Z","title":"Do we need entire training data for adversarial training?","summary":"  Deep Neural Networks (DNNs) are being used to solve a wide range of problems\nin many domains including safety-critical domains like self-driving cars and\nmedical imagery. DNNs suffer from vulnerability against adversarial attacks. In\nthe past few years, numerous approaches have been proposed to tackle this\nproblem by training networks using adversarial training. Almost all the\napproaches generate adversarial examples for the entire training dataset, thus\nincreasing the training time drastically. We show that we can decrease the\ntraining time for any adversarial training algorithm by using only a subset of\ntraining data for adversarial training. To select the subset, we filter the\nadversarially-prone samples from the training data. We perform a simple\nadversarial attack on all training examples to filter this subset. In this\nattack, we add a small perturbation to each pixel and a few grid lines to the\ninput image.\n  We perform adversarial training on the adversarially-prone subset and mix it\nwith vanilla training performed on the entire dataset. Our results show that\nwhen our method-agnostic approach is plugged into FGSM, we achieve a speedup of\n3.52x on MNIST and 1.98x on the CIFAR-10 dataset with comparable robust\naccuracy. We also test our approach on state-of-the-art Free adversarial\ntraining and achieve a speedup of 1.2x in training time with a marginal drop in\nrobust accuracy on the ImageNet dataset.\n","authors":["Vipul Gupta","Apurva Narayan"],"pdf_url":"https://arxiv.org/pdf/2303.06241v2.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2104.09343v5","updated":"2023-04-05T00:07:27Z","published":"2021-04-19T14:30:22Z","title":"Approximated Multi-Agent Fitted Q Iteration","summary":"  We formulate an efficient approximation for multi-agent batch reinforcement\nlearning, the approximated multi-agent fitted Q iteration (AMAFQI). We present\na detailed derivation of our approach. We propose an iterative policy search\nand show that it yields a greedy policy with respect to multiple approximations\nof the centralized, learned Q-function. In each iteration and policy\nevaluation, AMAFQI requires a number of computations that scales linearly with\nthe number of agents whereas the analogous number of computations increase\nexponentially for the fitted Q iteration (FQI), a commonly used approaches in\nbatch reinforcement learning. This property of AMAFQI is fundamental for the\ndesign of a tractable multi-agent approach. We evaluate the performance of\nAMAFQI and compare it to FQI in numerical simulations. The simulations\nillustrate the significant computation time reduction when using AMAFQI instead\nof FQI in multi-agent problems and corroborate the similar performance of both\napproaches.\n","authors":["Antoine Lesage-Landry","Duncan S. Callaway"],"pdf_url":"https://arxiv.org/pdf/2104.09343v5.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2211.11337v3","updated":"2023-04-05T13:38:28Z","published":"2022-11-21T10:37:56Z","title":"DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via\n  Positive-Negative Prompt-Tuning","summary":"  Large-scale text-to-image generation models have achieved remarkable progress\nin synthesizing high-quality, feature-rich images with high resolution guided\nby texts. However, these models often struggle with novel concepts, eg, new\nstyles, object entities, etc. Although recent attempts have employed\nfine-tuning or prompt-tuning strategies to teach the pre-trained diffusion\nmodel novel concepts from a reference image set,they have the drawback of\noverfitting to the given reference images, particularly in one-shot\napplications, which is harmful to generate diverse and high-quality images\nwhile maintaining generation controllability.\n  To tackle this challenge, we present a simple yet effective method called\nDreamArtist, which employs a positive-negative prompt-tuning learning strategy.\nSpecifically, DreamArtist incorporates both positive and negative embeddings\nand jointly trains them. The positive embedding aggressively captures the\nsalient characteristics of the reference image to drive diversified generation\nand the negative embedding rectifies inadequacies from the positive embedding.\nIt learns not only what is correct, but also what can be avoided or improved.\nWe have conducted extensive experiments and evaluated the proposed method from\nimage similarity and diversity, generation controllability, and style cloning.\nAnd our DreamArtist has achieved a superior generation performance over\nexisting methods. Besides, our additional evaluation on extended tasks,\nincluding concept compositions and prompt-guided image editing, demonstrates\nits effectiveness for more applications.\n","authors":["Ziyi Dong","Pengxu Wei","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2211.11337v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02328v1","updated":"2023-04-05T09:32:25Z","published":"2023-04-05T09:32:25Z","title":"Enhancing Multimodal Entity and Relation Extraction with Variational\n  Information Bottleneck","summary":"  This paper studies the multimodal named entity recognition (MNER) and\nmultimodal relation extraction (MRE), which are important for multimedia social\nplatform analysis. The core of MNER and MRE lies in incorporating evident\nvisual information to enhance textual semantics, where two issues inherently\ndemand investigations. The first issue is modality-noise, where the\ntask-irrelevant information in each modality may be noises misleading the task\nprediction. The second issue is modality-gap, where representations from\ndifferent modalities are inconsistent, preventing from building the semantic\nalignment between the text and image. To address these issues, we propose a\nnovel method for MNER and MRE by Multi-Modal representation learning with\nInformation Bottleneck (MMIB). For the first issue, a refinement-regularizer\nprobes the information-bottleneck principle to balance the predictive evidence\nand noisy information, yielding expressive representations for prediction. For\nthe second issue, an alignment-regularizer is proposed, where a mutual\ninformation-based item works in a contrastive manner to regularize the\nconsistent text-image representations. To our best knowledge, we are the first\nto explore variational IB estimation for MNER and MRE. Experiments show that\nMMIB achieves the state-of-the-art performances on three public benchmarks.\n","authors":["Shiyao Cui","Jiangxia Cao","Xin Cong","Jiawei Sheng","Quangang Li","Tingwen Liu","Jinqiao Shi"],"pdf_url":"https://arxiv.org/pdf/2304.02328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02274v1","updated":"2023-04-05T07:37:55Z","published":"2023-04-05T07:37:55Z","title":"Tangible Web: An Interactive Immersion Virtual RealityCreativity System\n  that Travels Across Reality","summary":"  With the advancement of virtual reality (VR) technology, virtual displays\nhave become integral to how museums, galleries, and other tourist destinations\npresent their collections to the public. However, the current lack of immersion\nin virtual reality displays limits the user's ability to experience and\nappreciate its aesthetics. This paper presents a case study of a creative\napproach taken by a tourist attraction venue in developing a physical network\nsystem that allows visitors to enhance VR's aesthetic aspects based on\nenvironmental parameters gathered by external sensors. Our system was\ncollaboratively developed through interviews and sessions with twelve\nstakeholder groups interested in art and exhibitions. This paper demonstrates\nhow our technological advancements in interaction, immersion, and visual\nattractiveness surpass those of earlier virtual display generations. Through\nmultimodal interaction, we aim to encourage innovation on the Web and create\nmore visually appealing and engaging virtual displays. It is hoped that the\ngreater online art community will gain fresh insight into how people interact\nwith virtual worlds as a result of this work.\n","authors":["Simin Yang","Ze Gao","Reza Hadi Mogavi","Pan Hui","Tristan Braud"],"pdf_url":"https://arxiv.org/pdf/2304.02274v1.pdf","comment":"Accepted In Proceedings of the ACM Web Conference 2023, April 30-May\n  4, 2023, Austin, TX, USA. ACM, New York, NY, USA"},{"id":"http://arxiv.org/abs/2303.17144v2","updated":"2023-04-05T00:25:58Z","published":"2023-03-30T04:34:31Z","title":"DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving","summary":"  Real-time perception, or streaming perception, is a crucial aspect of\nautonomous driving that has yet to be thoroughly explored in existing research.\nTo address this gap, we present DAMO-StreamNet, an optimized framework that\ncombines recent advances from the YOLO series with a comprehensive analysis of\nspatial and temporal perception mechanisms, delivering a cutting-edge solution.\nThe key innovations of DAMO-StreamNet are: (1) A robust neck structure\nincorporating deformable convolution, enhancing the receptive field and feature\nalignment capabilities. (2) A dual-branch structure that integrates short-path\nsemantic features and long-path temporal features, improving motion state\nprediction accuracy. (3) Logits-level distillation for efficient optimization,\naligning the logits of teacher and student networks in semantic space. (4) A\nreal-time forecasting mechanism that updates support frame features with the\ncurrent frame, ensuring seamless streaming perception during inference. Our\nexperiments demonstrate that DAMO-StreamNet surpasses existing state-of-the-art\nmethods, achieving 37.8% (normal size (600, 960)) and 43.3% (large size (1200,\n1920)) sAP without using extra data. This work not only sets a new benchmark\nfor real-time perception but also provides valuable insights for future\nresearch. Additionally, DAMO-StreamNet can be applied to various autonomous\nsystems, such as drones and robots, paving the way for real-time perception.\nThe code is available at https://github.com/zhiqic/DAMO-StreamNet.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Wangmeng Xiang","Binghui Chen","Bin Luo","Yifeng Geng","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2303.17144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02173v1","updated":"2023-04-05T00:25:27Z","published":"2023-04-05T00:25:27Z","title":"ChartReader: A Unified Framework for Chart Derendering and Comprehension\n  without Heuristic Rules","summary":"  Charts are a powerful tool for visually conveying complex data, but their\ncomprehension poses a challenge due to the diverse chart types and intricate\ncomponents. Existing chart comprehension methods suffer from either heuristic\nrules or an over-reliance on OCR systems, resulting in suboptimal performance.\nTo address these issues, we present ChartReader, a unified framework that\nseamlessly integrates chart derendering and comprehension tasks. Our approach\nincludes a transformer-based chart component detection module and an extended\npre-trained vision-language model for chart-to-X tasks. By learning the rules\nof charts automatically from annotated datasets, our approach eliminates the\nneed for manual rule-making, reducing effort and enhancing accuracy.~We also\nintroduce a data variable replacement technique and extend the input and\nposition embeddings of the pre-trained model for cross-task training. We\nevaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks,\ndemonstrating its superiority over existing methods. Our proposed framework can\nsignificantly reduce the manual effort involved in chart analysis, providing a\nstep towards a universal chart understanding model. Moreover, our approach\noffers opportunities for plug-and-play integration with mainstream LLMs such as\nT5 and TaPas, extending their capability to chart comprehension tasks. The code\nis available at https://github.com/zhiqic/ChartReader.\n","authors":["Zhi-Qi Cheng","Qi Dai","Siyao Li","Jingdong Sun","Teruko Mitamura","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2304.02173v1.pdf","comment":null}]}}